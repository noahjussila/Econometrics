<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanved Econometrics with Examples - 6&nbsp; Endogeniety I: IV and 2SLS</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./extremum.html" rel="next">
<link href="./ols.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#omitted-variables-and-measurement-error" id="toc-omitted-variables-and-measurement-error" class="nav-link active" data-scroll-target="#omitted-variables-and-measurement-error"><span class="toc-section-number">6.1</span>  Omitted Variables and Measurement Error</a></li>
  <li><a href="#an-updated-linear-model-identification-and-the-iv-estimator" id="toc-an-updated-linear-model-identification-and-the-iv-estimator" class="nav-link" data-scroll-target="#an-updated-linear-model-identification-and-the-iv-estimator"><span class="toc-section-number">6.2</span>  An Updated Linear Model, Identification, and the IV estimator</a></li>
  <li><a href="#properties-of-the-iv-estimator" id="toc-properties-of-the-iv-estimator" class="nav-link" data-scroll-target="#properties-of-the-iv-estimator"><span class="toc-section-number">6.3</span>  Properties of the IV Estimator</a></li>
  <li><a href="#many-instruments-2sls" id="toc-many-instruments-2sls" class="nav-link" data-scroll-target="#many-instruments-2sls"><span class="toc-section-number">6.4</span>  Many Instruments, 2SLS</a></li>
  <li><a href="#testing-hypotheses" id="toc-testing-hypotheses" class="nav-link" data-scroll-target="#testing-hypotheses"><span class="toc-section-number">6.5</span>  Testing Hypotheses</a>
  <ul class="collapse">
  <li><a href="#durbinwuhausman-test" id="toc-durbinwuhausman-test" class="nav-link" data-scroll-target="#durbinwuhausman-test"><span class="toc-section-number">6.5.1</span>  Durbin–Wu–Hausman Test</a></li>
  <li><a href="#sarganhansen-test" id="toc-sarganhansen-test" class="nav-link" data-scroll-target="#sarganhansen-test"><span class="toc-section-number">6.5.2</span>  Sargan–Hansen Test</a></li>
  </ul></li>
  <li><a href="#examplereplication" id="toc-examplereplication" class="nav-link" data-scroll-target="#examplereplication"><span class="toc-section-number">6.6</span>  Example/Replication</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="toc-section-number">6.7</span>  Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell" data-hash="endog_cache/html/unnamed-chunk-1_ba3a9bc63968f83ff68eed52a240759e">

</div>
<p>Our first departure from the classical linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> will come in the form of dropping the assumption that <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, i.e our regressors are endogenous. This situation is rather serious, as it prevents the linear model from being identified. Furthermore, the problem is common in applications. There are three main sources of endogeneity:</p>
<ol type="1">
<li>Omitted variables</li>
<li>Measurement error</li>
<li>Simultaneity</li>
</ol>
<p>For now we’ll consider the first two, and discuss the third on in Section @ref(endogeniety-ii-simultaneous-equation-models). Fortunately, we can address endogeneity with the instrumental variables estimator (a special case of which is the two-stage least squares estimator).</p>
<section id="omitted-variables-and-measurement-error" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="omitted-variables-and-measurement-error"><span class="header-section-number">6.1</span> Omitted Variables and Measurement Error</h2>
<div id="exm-ex1" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 </strong></span>Recall the Example @ref(exm:endogex) where we considered a model relating income to education and other determinants of salary. Suppose the true model is <span class="math display">\[ \log(income_i) = \beta_1 + \beta_2\cdot educ_i + \beta_3 \cdot experiance_i + \varepsilon_i,\]</span> where <span class="math inline">\(\varepsilon_i\)</span> are the unobserved factors impacting salary, <span class="math inline">\(educ_i\)</span> measure years of post-secondary education, <span class="math inline">\(experiance_i\)</span> measures years of work experience, and <span class="math inline">\(\text{Cov}\left(educ, experiance\right) &lt; 0\)</span> (the longer you go to school, the less work experience you tend to have). Now suppose we incorrectly specify the model <span class="math display">\[ \log(income_i) = \gamma_1 + \gamma_2\cdot educ_i + u_i,\]</span> where <span class="math inline">\(u_i\)</span> are all other factors impacting salary. We’ve omitted <span class="math inline">\(experiance_i\)</span> as a regressor, so it’s implicitly included in <span class="math inline">\(u_i\)</span>: <span class="math display">\[ u_i =  \beta_3 \cdot experiance_i + \varepsilon_i.\]</span> We no longer satisfy the assumption <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, as <span class="math inline">\(\text{Cov}\left(educ_i, u_i\right) \neq 0\)</span> because <span class="math inline">\(\text{Cov}\left(educ, experiance\right) &lt; 0\)</span>. What happens if we go ahead and attempt to estimate <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> anyway? Set <span class="math inline">\(\boldsymbol{\beta}= [1,3,2]'\)</span>, and <span class="math inline">\(\varepsilon_i \overset{iid}{\sim}N(0,1)\)</span>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-2_6bc057df4faad3d43f1ef47367323a3c">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">10</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">5</span>), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>log_y <span class="ot">=</span> beta[<span class="dv">1</span>] <span class="sc">+</span> X <span class="sc">%*%</span> beta[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">+</span> e</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>model_df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">log_income =</span> <span class="fu">as.numeric</span>(log_y),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                       <span class="at">educ =</span> X[,<span class="dv">1</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                       <span class="at">exper =</span> X[,<span class="dv">2</span>])</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_income <span class="sc">~</span> educ, <span class="at">data =</span> model_df)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Estimate Std. Error  t value      Pr(&gt;|t|)
(Intercept) 22.525979  0.4349000 51.79577 4.282241e-285
educ         2.639973  0.1034311 25.52397 5.178448e-111</code></pre>
</div>
</div>
<p>The true parameters don’t even fall within the 95% confidence intervals centered at our estimates.</p>
</div>
<p>In general, suppose <span class="math display">\[\mathbf{Y}= [\mathbb{X},\mathbb{Z}][\boldsymbol{\beta}, \boldsymbol \delta]' + \boldsymbol{\varepsilon}= \mathbb{X}\boldsymbol{\beta}+ \mathbb{Z}\boldsymbol \delta + \boldsymbol{\varepsilon},\]</span> where we attempt to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> via <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> despite omitting regressors <span class="math inline">\(\mathbf Z\)</span> from our model, and all our Gauss-Markov assumptions are met. Note that <span class="math inline">\(\boldsymbol{\beta}\)</span> is still identified, as <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>. <span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{OLS,OV} &amp; = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}\\
&amp; = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'(\mathbb{X}\boldsymbol{\beta}+ \mathbb{Z}\boldsymbol \delta + \boldsymbol{\varepsilon})\\
&amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Z}\boldsymbol\delta  + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}.
\end{align*}\]</span> Our estimator is now inconsistent: <span class="math display">\[\begin{align*}
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS,OV} &amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Z}\boldsymbol\delta\right] + \mathop{\mathrm{plim}}\left[(\mathbb{X}'\mathbb{X})^{-1}\right]\underbrace{\mathop{\mathrm{plim}}\left[\mathbb{X}'\boldsymbol{\varepsilon}\right]}_\mathbf{0}\\
&amp; =  \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Z}\boldsymbol\delta\right].
\end{align*}\]</span></p>
<p>This phenomenon is referred to as <strong><em>omitted variable bias (OVB)</em></strong>. The use of the word “bias” here is a bit misleading, but follows from an interpretation of inconsistency as a persistent bias despite <span class="math inline">\(n\to\infty\)</span></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 (OVB with a Simple Linear Model) </strong></span>Suppose our linear model is <span class="math inline">\(Y = \alpha + \beta X+ \gamma Z + \varepsilon\)</span>, and we attempt to estimate <span class="math inline">\((\beta_0,\beta_1)\)</span> with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS,OV} \)</span>. In this case, <span class="math display">\[\begin{align*}
\hat \beta_\text{OLS,OV} &amp; =\frac{\sum_{i=1}^n (X_i -\bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \frac{\sum_{i=1}^n (X_i -\bar X)[(\alpha + \beta X+ \gamma Z + \varepsilon) - \bar Y]}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \beta + \frac{\sum_{i=1}^n (X_i -\bar X)(Z_i - \bar Z)}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \boldsymbol{\beta}+ \gamma \cdot \frac{\sum_{i=1}^n (X_i -\bar X)(Z_i-\bar Z)}{\sum_{i=1}^n (X_i -\bar X)^2}.
\end{align*}\]</span> The expectation of this is <span class="math display">\[\hat\beta_\text{OLS,OV} \overset{p}{\to}\beta + \gamma \cdot \mathop{\mathrm{plim}}\frac{n^{-1}\sum_{i=1}^n (X_i -\bar X)(Z_i-\bar Z)}{n^{-1}\sum_{i=1}^n (X_i -\bar X)^2} =\boldsymbol{\beta}+ \gamma \frac{\text{Cov}\left(X,Z\right)}{\text{Var}\left(X\right)} .\]</span></p>
<p>If we let <span class="math inline">\(\alpha = 1\)</span>, <span class="math inline">\(\beta = 2\)</span>, <span class="math inline">\(\gamma = 3\)</span>, and <span class="math inline">\(\text{Var}\left(X\right) = 1\)</span>, then <span class="math display">\[\mathop{\mathrm{plim}}\hat\beta_\text{OLS,OV} = 2 + 3\text{Cov}\left(X,Y\right).\]</span> If we simulate this estimator for different values of <span class="math inline">\(\text{Cov}\left(X,Y\right)\)</span>, taking <span class="math inline">\(n\)</span> to be very large, then we should see our estimates approximately follow the line <span class="math inline">\(2 + 3\text{Cov}\left(X,Y\right)\)</span> when plotted against <span class="math inline">\(\text{Cov}\left(X,Y\right)\)</span>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-3_a752de67eb3a46f7c61a6787acaa324d">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">10</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> j<span class="sc">/</span><span class="dv">50</span>, <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span>j<span class="sc">/</span><span class="dv">50</span>, <span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  regressors <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> regressors[,<span class="dv">1</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">&lt;-</span> regressors[,<span class="dv">2</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>z <span class="sc">+</span> e</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  estimates[j] <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x))<span class="sc">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="endog_cache/html/unnamed-chunk-4_bc38754a235b2a84c0a60c73db11f288">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>What happens if instead of omitting a variable from a model, our variables happen to be prone to some degree of measurement error. This is a common scenario in the social sciences where collected data is subject to human error, rounding errors, etc. Suppose a true linear model is specified as <span class="math inline">\(\mathbf{Y}= \mathbb{X}^*\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span> for regressors <span class="math inline">\(\mathbf{X}^*\)</span> where <span class="math inline">\(\text{E}\left[\mathbf{X}^{*\prime}\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>. Much like how we do not observe realizations of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>, we do not observe realizations of <span class="math inline">\(\mathbf{X}^*\)</span>, instead observing <span class="math inline">\(\mathbf{X}= \mathbf{X}^* + \mathbf u\)</span> where <span class="math inline">\(\mathbf u\)</span> corresponds to <strong><em>measurement error</em></strong>. We’ll assume that this measurement error is independent of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>, independent of <span class="math inline">\(\mathbf{X}^*\)</span>, and is mean zero.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Our model can be rewritten as <span class="math display">\[ \mathbf{Y}= \mathbb{X}^*\boldsymbol{\beta}+ \boldsymbol{\varepsilon}= (\mathbf{X}- \mathbf u)\boldsymbol{\beta}+ \boldsymbol{\varepsilon}= \mathbf{X}\boldsymbol{\beta}+ \underbrace{(\boldsymbol{\varepsilon}- \mathbf u\boldsymbol{\beta})}_{\boldsymbol \nu}.\]</span> In this case <span class="math display">\[\text{E}\left[\mathbf{X}\boldsymbol \nu\right] = \text{E}\left[(\mathbf{X}^* + \mathbf u)(\boldsymbol{\varepsilon}- \mathbf u\boldsymbol{\beta})\right] = -\boldsymbol{\beta}\text{E}\left[\mathbf u'\mathbf u\right] = -\boldsymbol{\beta}\text{Var}\left(\mathbf u\right).\]</span> Much like in the case of OVB, <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> will be inconsistent.</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{OLS,ME} &amp;= \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}\\
     &amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'(\boldsymbol{\varepsilon}- \mathbf u\boldsymbol{\beta})\\
     &amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}- (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf u\boldsymbol{\beta}\\
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS,ME} &amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}+ \mathop{\mathrm{plim}}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf u\boldsymbol{\beta}\\
  &amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf u\boldsymbol{\beta}&amp; (\mathop{\mathrm{plim}}\mathbb{X}'\boldsymbol{\varepsilon}= \mathbf{0})
\end{align*}\]</span></p>
<p>For <span class="math inline">\(j\neq 1\)</span> (there won’t be measurement error when <span class="math inline">\(j=1\)</span>, as this is just the regressor of 1 which gives the intercept) this simplifies to</p>
<p><span class="math display">\[ \mathop{\mathrm{plim}}\hat \beta_{\text{OLS,ME},j} = \beta_j \left(\frac{\text{Var}\left(X_j^*\right)}{\text{Var}\left(X_j^*\right) + \text{Var}\left(u_j^*\right)}\right).\]</span> This phenomenon is known as <strong><em>attenuation bias</em></strong>. The term in parentheses will always fall in the interval <span class="math inline">\((0,1)\)</span>, so <span class="math inline">\(\left\lvert\mathop{\mathrm{plim}}\hat \beta_{\text{OLS,ME},j}\right\rvert &lt; \left\lvert\boldsymbol{\beta}_j\right\rvert\)</span>, hence the name attentuation bias.</p>
</section>
<section id="an-updated-linear-model-identification-and-the-iv-estimator" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="an-updated-linear-model-identification-and-the-iv-estimator"><span class="header-section-number">6.2</span> An Updated Linear Model, Identification, and the IV estimator</h2>
<p>In general, if <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span>, then <span class="math inline">\(\boldsymbol{\beta}\)</span> is not identified for the linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span>. Estimation is a non-starter in this case. Even if we had the “perfect” estimate for <span class="math inline">\(\boldsymbol{\beta},\)</span> the parameter may map to multiple elements <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2}\in \mathcal P_\text{LM}\)</span>, so it is impossible to determine which model value our data was drawn from. If we drop the assumption <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span> from the linear model, then we’ll need to replace it with some additional assumptions/structure.</p>
<p>While <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span>, <em>perhaps</em> it is the case that there exists some other random vector <span class="math inline">\(\mathbf{Z}\)</span> which does satisfy <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>. Is this helpful – no. For a given model with some structural error <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>, there are nearly infinite candidates for <span class="math inline">\(\mathbf{Z}\)</span> which satisfy this. Consider the model <span class="math display">\[\log(income_i) = \beta_0 + \beta_1\cdot educ_i + \varepsilon_i,\]</span> where <span class="math inline">\(\varepsilon\)</span> are unobserved factors impacting income. What are some random variables <span class="math inline">\(Z\)</span> which are uncorrelated with <span class="math inline">\(\varepsilon\)</span>. A ton! Weather during <span class="math inline">\(i\)</span>’s tenth birthday, <span class="math inline">\(i\)</span>’s first concert, <span class="math inline">\(i\)</span>’s favorite flavor of ice cream, etc. There is an endless list of random variables that are so completely irrelevant to someone’s income, that they are uncorrelated with <span class="math inline">\(\varepsilon\)</span>. This is why <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span> is sometimes read as “<span class="math inline">\(\mathbf{Z}\)</span> is orthogonal to <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>.” Not only does it hold in the mathematical sense of the word, but it also holds in the colloquial sense of the word meaning “has nothing to do with.” What we want is <span class="math inline">\(\mathbf{Z}\)</span> to also be correlated with <span class="math inline">\(\mathbf{X}\)</span>, such that <span class="math inline">\(\mathbf{Z}\)</span> is a sort of proxy/surrogate for <span class="math inline">\(\mathbf{X}\)</span> with no direct impact on <span class="math inline">\(\mathbf{Y}\)</span>. We’ll now generalize the linear model to introduce this set of variables <span class="math inline">\(\mathbf{Z}\)</span> in lieu of the assumption <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 </strong></span>The <span style="color:red"><strong><em>(linear) instrumental variables (IV) model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{IV} = \{P_{\boldsymbol{\beta},\sigma^2} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \sigma^2\in\mathbb R\}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\sigma^2} &amp;= \{F_{\mathbb{X},\mathbb{Z},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}, \ \text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbf{X}\right]=\sigma^2\mathbf I,\ f_{(\mathbb{X},\mathbb{Z})}=\textstyle\prod_{i=1}^n f_{(\mathbf{X}_i,\mathbf{Z}_i)},\ \text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K,\ \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] =\boldsymbol \eta,\  \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{Z}\right] = \mathbf{0}, \text{E}\left[\mathbf{Z}'\mathbf{X}\right] \neq \mathbf{0}\},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbb{Z}&amp; = [\mathbf{Z}_1, \cdots, \mathbf{Z}_j, \cdots \mathbf{Z}_K] = [\mathbf{Z}_1, \cdots, \mathbf{Z}_i, \cdots \mathbf{Z}_n]',\\
\dim(\mathbf{Z}) &amp; = \dim(\mathbf{X}) = K\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n].
\end{align*}\]</span> We refer to the random vector <span class="math inline">\(\mathbf{Z}\)</span> as <span style="color:red"><strong><em>instrumental variables (IVs)</em></strong></span>. We have assumed that <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{Z}\right] = \mathbf{0}\)</span>, which subsumes the assumption that <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>. The assumption that <span class="math inline">\(\mathbf{Z}\)</span> is (weakly) exogenous (uncorrelated with <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>) is known as <span style="color:red"><strong><em>instrumental validity</em></strong></span>, while the assumption that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{Z}\right] \neq \mathbf{0}\)</span> (<span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are correlated) is known as the <span style="color:red"><strong><em>relevance condition</em></strong></span>. The assumption that <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span> is also sometimes known as the <span style="color:red"><strong><em>exclusion restriction</em></strong></span> (<span class="math inline">\(\mathbf{Z}\)</span> is excluded from the determinants of <span class="math inline">\(Y\)</span>). Finally, we sometimes refer to <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span> as the <span style="color:red"><strong><em>rank condition</em></strong></span>.</p>
</div>
<p>Instrumental validity and the relevance condition are usually written as: <span class="math display">\[\begin{align*}
\text{Cov}\left(\mathbf{Z},\boldsymbol{\varepsilon}\right)&amp; = \mathbf{0},\\
\text{Cov}\left(\mathbf{X}, \mathbf{Z}\right) &amp;\neq \mathbf{0}.
\end{align*}\]</span> <em>Technically</em>, this is not 100% accurate. The relevance condition is actually a combination of <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span> and <span class="math inline">\(\text{Cov}\left(\mathbf{X}, \mathbf{Z}\right)\neq \mathbf{0}\)</span>. As highlighted by <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, the relevance condition is not “regressors and instruments are uncorrelated”. Instead, it is “instruments and endogenous regressors are partially correlated holding the exogenous regressors fixed.” In other words, the linear projection of the instruments onto all regressors has nontrivial coefficients for endogenous regressors. This is equivalent to <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span> and <span class="math inline">\(\text{Cov}\left(\mathbf{X}, \mathbf{Z}\right)\neq \mathbf{0}\)</span>.</p>
<p>Even if one regressor is endogenous, <span class="math inline">\(\text{E}\left[X_j\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span> for some <span class="math inline">\(j\)</span>, then <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] \neq \mathbf{0}\)</span>. If this is the case, we can simply define a portion of <span class="math inline">\(\mathbf{Z}\)</span> to be the exogenous regressors. Formally, if <span class="math inline">\([X_1,\ldots, X_J]\)</span> are weakly exogenous while <span class="math inline">\([X_{J+1}, \ldots, X_K]\)</span> are endogenous, then define <span class="math inline">\(Z_1 = X_1,\ldots ,Z_J=X_j\)</span>.</p>
<div id="exm-spec1" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 (Linear Model as Special Case of IV Model) </strong></span>A special case of the linear IV model is the classical linear model. Just let <span class="math inline">\(\boldsymbol\eta = \mathbf{0}\)</span> and <span class="math inline">\(\mathbf{X}= \mathbf{Z}\)</span>. This fact can be written as <span class="math inline">\(\mathcal P_\text{LM}\subset \mathcal P_\text{IV}\)</span>.</p>
</div>
<p>Whenever we define a new model, we need to make sure our parameters are identified.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 (Identification of Linear IV Model) </strong></span>The linear IV model is identified as a result of the following assumptions: <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{Z}\right] \neq \mathbf{0}\)</span>, and <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>First we will show <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified. Let <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} = P_{\boldsymbol{\beta}^*,\sigma^2}\)</span> for two elements of <span class="math inline">\(\mathcal P_\text{IV}\)</span>, and suppose for a contradiction that <span class="math inline">\(\boldsymbol{\beta}\neq\boldsymbol{\beta}^*\)</span>. We can begin by writing <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math display">\[\begin{align*}
&amp;\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\\
\implies &amp; \text{E}\left[\mathbf{Z}'(Y-\mathbf{X}\boldsymbol{\beta})\right] = \mathbf{0}&amp; (\boldsymbol{\varepsilon}= (Y-\mathbf{X}\boldsymbol{\beta}))\\
\implies &amp; \text{E}\left[\mathbf{Z}'Y\right]-\boldsymbol{\beta}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]= \mathbf{0}\\
\implies &amp; \text{E}\left[\mathbf{Z}'Y\right] = \boldsymbol{\beta}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\\
\end{align*}\]</span> By the definition of <span class="math inline">\(\mathcal P_\text{IV}\)</span>, the moments <span class="math inline">\(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\)</span> and <span class="math inline">\(\text{E}\left[\mathbf{Z}'Y\right]\)</span> are the same for the model values <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2}\)</span> and <span class="math inline">\(P_{\boldsymbol{\beta}^*,\sigma^2}\)</span>, so <span class="math inline">\(\boldsymbol{\beta}^*\)</span> must also satisfy <span class="math inline">\(\text{E}\left[\mathbf{Z}'Y\right] = \boldsymbol{\beta}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\)</span>. This contradicts the assumption that <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span>. If <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified, then <span class="math inline">\(\sigma^2\)</span> is as well because we can write it in terms of <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math display">\[ \text{E}\left[(\mathbf{Y}-\mathbb{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbb{X}\boldsymbol{\beta})\mid \mathbf{X}\right]=\sigma^2\mathbf I.\]</span> <span style="color:white">space</span></p>
</div>
<p>So how do we estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> for the model <span class="math inline">\(\mathcal P_\text{IV}\)</span>. If it wasn’t clear from the examples dealing with OVB and measurement error, the answer is not with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 6.1 (Inconsisteny of OLS) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{IV}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is biased and inconsistent. In particular, <span class="math display">\[\begin{align*}
\text{E}\left[\hat{\boldsymbol\beta}_\text{OLS} \mid \mathbf{X}\right] &amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol \eta \neq \boldsymbol{\beta},\\
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS} &amp; = \boldsymbol{\beta}+ \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\boldsymbol \gamma \neq \boldsymbol{\beta},\\
\end{align*}\]</span> where <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] =\boldsymbol \eta\)</span> and <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \boldsymbol \gamma\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\text{E}\left[\hat{\boldsymbol\beta}_\text{OLS} \mid \mathbb{X}\right] &amp; = \text{E}\left[\boldsymbol{\beta}+(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\\
&amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\\
&amp; = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol \eta\\
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS} &amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left[\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)\right]\\
&amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right) &amp; (\text{Slutsky's Theorem})\\
&amp; = \boldsymbol{\beta}+ \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] &amp; (\text{LLN})\\
&amp; = \boldsymbol{\beta}+ \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\boldsymbol \gamma
\end{align*}\]</span> <span style="color:white">space</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 </strong></span>Let’s verify that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is inconsistent. Suppose <span class="math inline">\(\mathbb{X}= (\mathbf 1, X)\)</span> where <span class="math inline">\((X,\varepsilon)\overset{iid}{\sim}N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span> for <span class="math inline">\(\boldsymbol \mu = [0,1]'\)</span> <span class="math display">\[\boldsymbol{\Sigma}= \begin{bmatrix}5&amp;2\\2&amp;1  \end{bmatrix}.\]</span> We have <span class="math display">\[\begin{align*}
\text{E}\left[X\varepsilon\right] &amp;= \text{Cov}\left(X,\varepsilon\right) + \underbrace{\text{E}\left[X\right]}_0\underbrace{\text{E}\left[\varepsilon\right]}_1 = 2.
\end{align*}\]</span> Therefore <span class="math display">\[\begin{align*}
\boldsymbol \gamma &amp;= \text{E}\left[\mathbb{X}'\boldsymbol{\varepsilon}\right] = \begin{bmatrix}\text{E}\left[1\boldsymbol{\varepsilon}\right]\\\text{E}\left[X\boldsymbol{\varepsilon}\right] \end{bmatrix} = \begin{bmatrix}1\\2 \end{bmatrix}
\end{align*}\]</span></p>
<p>If we draw realizations <code>x</code> and <code>e</code> of <span class="math inline">\((X,\varepsilon)\)</span>, we should find that <code>colMeans(X*e)</code> should be approximately <span class="math inline">\([1,2]'\)</span> by the LLN.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-5_01f7346b376966ed9102cedf9e984781">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>realizations <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> realizations[,<span class="dv">1</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> realizations[,<span class="dv">2</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(X<span class="sc">*</span>e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                x 
1.002076 1.984556 </code></pre>
</div>
</div>
<p>Let’s now calculate <span class="math inline">\(\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS} \)</span>. <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1} &amp;= \begin{bmatrix}\text{E}\left[1\right] &amp; \text{E}\left[X\right] \\ \text{E}\left[X\right] &amp; \text{E}\left[X^2\right] \end{bmatrix}^{-1} = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; \text{Var}\left(X\right) \end{bmatrix}^{-1} = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}^{-1}  =  \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0.2 \end{bmatrix}\\
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{OLS} &amp; = \boldsymbol{\beta}+ \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\boldsymbol \gamma= \boldsymbol{\beta}+ \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0.2 \end{bmatrix}\begin{bmatrix}1\\2 \end{bmatrix}= \boldsymbol{\beta}+ \begin{bmatrix}1\\0.4 \end{bmatrix}
\end{align*}\]</span> If we let <span class="math inline">\(\boldsymbol{\beta}= [1,2']\)</span> we should see our estimates converge to <span class="math inline">\([2, 2.4]'\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-6_17f38cd3a954352080d8e39dcea85c6f">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(N_sim <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  realizations <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> realizations[,<span class="dv">1</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> realizations[,<span class="dv">2</span>]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x))<span class="sc">$</span>coefficients[,<span class="dv">1</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  estimates[n<span class="dv">-1</span>,] <span class="ot">&lt;-</span> <span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x))<span class="sc">$</span>coefficients[,<span class="dv">1</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="endog_cache/html/unnamed-chunk-7_f15f7e1f5b779ed1dc0813191438b50a">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>An interesting feature of this problem is that our estimator for the intercept term <span class="math inline">\(\beta_1\)</span> was inconsistent even though the corresponding regressor (the trivial random variable <span class="math inline">\(X_1 = 1\)</span>) is exogenous. In general, the inconsistency of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> in the presence of endogeneity is not limited to the parameters associated with endogenous variables, and will impact each parameter <span class="math inline">\(\beta_j\)</span>.</p>
<p>Instead of using <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>, we need an estimator which makes use of the exogenous variables <span class="math inline">\(\mathbf{Z}\)</span>. There are several different ways to motivate this estimator, so we’ll go over four particularly approaches which reach the same conclusion.</p>
<ol type="1">
<li>For <span class="math inline">\(\mathcal P_\text{IV}\)</span> we can write the true population parameter as <span class="math inline">\(\boldsymbol{\beta}= \text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{Z}'Y\right]\)</span>. We can appeal to the analogy-principle here just like we did when deriving <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. It stands to reason that the estimator defined by the analogous samples will provide consistent estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> by the LLN. This estimator is <span class="math display">\[\hat {\boldsymbol{\beta}}= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_iY_i\right) = (\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}'\mathbf{Y}.\]</span></li>
<li>Another way of tackling the problem relates to marginal effects and is presented by <span class="citation" data-cites="cameron2005microeconometrics">Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span>. This will be a little informal and is only to build intuition. In an abuse of notation, we’ll write marginal effects as <span class="math inline">\(\frac{\partial Y}{\partial \mathbf{X}}\)</span>. Ideally, we want <span class="math inline">\(\beta_j = \frac{\partial Y}{\partial X_j}\)</span> for each <span class="math inline">\(j\)</span>. This way, when we estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>, we estimate the marginal effect of <span class="math inline">\(\mathbf{X}\)</span> on <span class="math inline">\(Y\)</span>. Unfortunately, we cannot estimate this directly with <span class="math inline">\(\mathbf{X}\)</span> as <span class="math display">\[ \frac{\partial Y}{\partial X_j} = \beta_j + \frac{\partial \varepsilon}{\partial X_j} \implies \beta_j \neq \frac{\partial Y}{\partial X_j}.\]</span> If we appeal to the chain rule, we can vary <span class="math inline">\(X_j\)</span> via our instruments <span class="math inline">\(\mathbf{Z}\)</span>: <span class="math display">\[\begin{align*}
&amp;\frac{\partial Y}{\partial \mathbf{Z}} = \frac{\partial Y}{\partial X_j} \frac{\partial X_j}{\partial \mathbf{Z}} \\
\implies &amp; \frac{\partial Y}{\partial X_j} = \frac{\partial Y/\partial \mathbf{Z}}{\partial X_j/\partial \mathbf{Z}}
\end{align*}\]</span> where <span class="math inline">\(\frac{\partial Y}{\partial \mathbf{Z}}\)</span> holds <span class="math inline">\(\varepsilon\)</span> constant as <span class="math inline">\(\text{Cov}\left(\mathbf{Z}, \boldsymbol{\varepsilon}\right) = \mathbf{0}\)</span>. The marginal effects <span class="math inline">\(\frac{\partial X_j}{\partial \mathbf{Z}}\)</span> and <span class="math inline">\(\frac{\partial Y}{\partial \mathbf{Z}}\)</span> correspond to the parameters in the linear projection models of <span class="math inline">\(\mathbf{Z}\)</span> on <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> These parameters are given by OLS estimates <span class="math inline">\((\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}\mathbf{X}_j\)</span> and <span class="math inline">\((\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}\mathbf{Y}\)</span>, respectively, so <span class="math display">\[ \hat\beta_j = \frac{(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}}{(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}\mathbf{X}_j}.\]</span> If we do this for all regressors <span class="math inline">\(\mathbf{X}\)</span>, then <span class="math display">\[ \hat{\boldsymbol{\beta}} = \frac{(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}}{(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}\mathbb{X}} =  (\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}'\mathbf{Y}\]</span></li>
<li>We can take a graphical approach given by <span class="citation" data-cites="pearl2009causality">Pearl (<a href="references.html#ref-pearl2009causality" role="doc-biblioref">2009</a>)</span> with the simple IV model with one endogenous regressor <span class="math inline">\(X\)</span> and one instrument <span class="math inline">\(Z\)</span>. Suppose we have <span class="math inline">\(Y = \beta_1 + \beta_2 X + \varepsilon\)</span>, where <span class="math inline">\(\text{Cov}\left(X,\varepsilon\right) \neq 0\)</span>, along with <span class="math inline">\(\text{Cov}\left(Z,X\right) \neq 0\)</span> and <span class="math inline">\(\text{Cov}\left(Z,\varepsilon\right) = 0\)</span> for some instrument <span class="math inline">\(Z\)</span>. These relationships can be illustrated in the form of a directed acyclic graph (DAG).</li>
</ol>
<div class="cell" data-layout-align="center" data-fig.asp="0.5" data-hash="endog_cache/html/unnamed-chunk-8_3c78f6d0b23392044cc0bb6510546bc9">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/iv_dag.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We can think of the paths which illustrate causation as being multiplicative in the sense that <span class="math inline">\(\text{Cov}\left(Z,X\right)\cdot \beta_2 = \text{Cov}\left(Z,Y\right)\)</span>, where <span class="math inline">\(\beta_2\)</span> is a “conversion rate” between changes in <span class="math inline">\(X\)</span> via <span class="math inline">\(Z\)</span> and changes in <span class="math inline">\(Y\)</span> via <span class="math inline">\(Z\)</span> (just like the chain rule approach). This implies <span class="math inline">\(\beta_2 = \text{Cov}\left(Z,Y\right)/\text{Cov}\left(Z,X\right)\)</span>, so <span class="math display">\[ \hat\beta_2 = \frac{\widehat{\text{Cov}}(Z,Y)}{\widehat{\text{Cov}}(Z,X)}.\]</span> This happens to be a special case of <span class="math inline">\(\hat{\boldsymbol{\beta}}= (\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}'\mathbf{Y}\)</span>.</p>
<ol start="4" type="1">
<li>Another approach in the context of the simple linear model takes advantage of a simple substitution. <span class="math display">\[ \text{Cov}\left(Y,Z\right) = \text{Cov}\left(\beta_1 + \beta_2 X + \varepsilon, Z\right) = \underbrace{\text{Cov}\left(\beta_1, Z\right)}_0 + \beta_2\text{Cov}\left(X,Z\right) + \underbrace{\text{Cov}\left(Z,\varepsilon\right)}_0 = \beta_2\text{Cov}\left(X,Z\right).\]</span> This is the same result we arrived at using the DAG.</li>
</ol>
<p>With all roads leading to Rome, we can define this estimator.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 </strong></span>The <span style="color:red"><strong><em>instrumental variables (IV) estimator</em></strong></span> is defined as <span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{IV} (\mathbb{X},\mathbb{Z},\mathbf{Y})= (\mathbb{Z}'\mathbb{X})^{-1}(\mathbb{Z}'\mathbf{Y})= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_iY_i\right)
\end{align*}\]</span> An realization of this estimator (an estimate) is <span class="math display">\[\begin{align*}
\hat{\mathbf b}_\text{IV} = \hat{\boldsymbol{\beta}}_\text{IV}(\mathbf{X},\mathbf{Z},\mathbf{y}) &amp;= (\mathbf{Z}'\mathbf{X})^{-1}(\mathbf{Z}'\mathbf{y})= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{z}_i'\mathbf{z}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{z}_iy_i\right)
\end{align*}\]</span> and will exist when the inverse <span class="math inline">\((\mathbf{Z}'\mathbf{X})^{-1}\)</span> exists.</p>
</div>
<div id="exm-refex" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 </strong></span>Suppose <span class="math inline">\((X, Z, \varepsilon) \sim N(\boldsymbol \mu,\boldsymbol{\Sigma})\)</span> where <span class="math display">\[\begin{align*}
\boldsymbol \mu &amp; = [10,10,0]',\\
\boldsymbol{\Sigma}&amp; = \begin{bmatrix}20 &amp; 5 &amp; 1\\5&amp;20&amp;0\\1&amp;0&amp;1 \end{bmatrix},
\end{align*}\]</span> and <span class="math inline">\(Y = 1 + 3X + \varepsilon\)</span>. Let’s simulate a sample of size <span class="math inline">\(n=1000\)</span> from this model <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{IV}\)</span>, and then calculate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-9_c0bd4ace900fea5234ff9c3ba5ac623a">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">1</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We should confirm that our drawn sample satisfies the assumptions of the IV model.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-10_c538b414e0e667f5da071c1c4900c70b">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Is x exogenous?</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(x,e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.24223</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Is z a valid instrument?</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(z,e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2013187</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Is z relevant?</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(z,x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.257776</code></pre>
</div>
</div>
<p>Now let’s calculate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>, keeping in mind that <span class="math inline">\(\mathbb{Z}\)</span> is comprised of the instrument <span class="math inline">\(Z\)</span> <em>and</em> the exogenous random variable <span class="math inline">\(1\)</span> which gives the intercept term.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-11_8fb255866c4d03e28395e6c48182e3f2">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>beta_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       [,1]
  0.5934421
x 3.0382897</code></pre>
</div>
</div>
</div>
<div id="exm-spec2" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 (OLS is a Special Case of IV) </strong></span>In Example @ref(exm:spec1) we saw that <span class="math inline">\(\mathcal P_\text{LM}\subset\mathcal P_\text{IV}\)</span>. In the event <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{LM}\)</span>, i.e <span class="math inline">\(\mathbb{Z}= \mathbb{X}\)</span> and , then <span class="math display">\[ \hat{\boldsymbol\beta}_\text{IV} = (\mathbb{Z}'\mathbb{X})^{-1}(\mathbb{Z}'\mathbf{Y}) = (\mathbb{X}'\mathbb{X})^{-1}(\mathbb{X}'\mathbf{Y}) = \hat{\boldsymbol\beta}_\text{OLS} .\]</span></p>
</div>
</section>
<section id="properties-of-the-iv-estimator" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="properties-of-the-iv-estimator"><span class="header-section-number">6.3</span> Properties of the IV Estimator</h2>
<p>One of the reasons OLS is so special is because we’re able to characterize its finite sample properties with the Gauss-Markov theorem. This is the exception rather than the rule when assessing estimators. In most cases, we can only arrive at tractable results in the form of an estimator’s asymptotic properties. Is this the case with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>? Let’s see if <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> satisfies our “baseline” finite sample property of unbiasedness. <span class="math display">\[\begin{align*}
\text{E}\left[\hat{\boldsymbol\beta}_\text{IV} \right] &amp;= \text{E}\left[\text{E}\left[\hat{\boldsymbol\beta}_\text{IV} \mid \mathbb{X}, \mathbb{Z}\right]\right]\\
        &amp; =  \text{E}\left[\text{E}\left[(\mathbb{Z}'\mathbb{X})^{-1}(\mathbb{Z}'\mathbf{Y}) \mid \mathbb{X}, \mathbb{Z}\right]\right]\\
        &amp;=\text{E}\left[\text{E}\left[(\mathbb{Z}'\mathbb{X})^{-1}(\mathbb{Z}'(\mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon})) \mid \mathbb{X}, \mathbb{Z}\right]\right]\\
        &amp;=\boldsymbol{\beta}+ \text{E}\left[(\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}, \mathbb{Z}\right]\right]\\
        &amp; \neq \boldsymbol{\beta}
\end{align*}\]</span> In general, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}, \mathbb{Z}\right] \neq \mathbf{0}\)</span>, so <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> has a bias.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.7 (IV Estimator is Biased) </strong></span>Return to the model from Example @ref(exm:refex), but let <span class="math inline">\(n = 10\)</span>. If we simulate the bias of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> using 10,000 simulations, we can confirm that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is biased.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-12_3d171e0ca146ce9f4e2cd219ac9b70f8">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">1</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>  estimates[k,] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(estimates)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.613375 2.951230</code></pre>
</div>
</div>
<p>Okay but is this really an issue? In most settings, we would have a sample size much larger than <span class="math inline">\(n = 10\)</span>. Let’s repeat this experiment with <span class="math inline">\(n = 1000\)</span> and see what happens to our estimators bias.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-13_ca329ce4902c7242992d6107e6ec5e87">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">1</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>  Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>  estimates[k,] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y </span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(estimates)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.011103 2.998881</code></pre>
</div>
</div>
<p>Now the bias is negligible.</p>
</div>
<p>While <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is biased, it seems as if it is asymptotically unbiased. Instead of proving this directly, we’ll actually show that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is root-N CAN, a sufficient condition for asymptotic unbiasedness. Before tackling that, let’s prove that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is consistent directly. One of the reasons we opted to not estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> via <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> was that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is not consistent for <span class="math inline">\(\mathcal P_\text{IV}\)</span>, so it shouldn’t be a surprise that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is consistent.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 6.2 (IV Estimator is Consistent) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2}\in \mathcal P_\text{IV}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \overset{p}{\to}\boldsymbol{\beta}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{IV} &amp;= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_iY_i\right)\\
    &amp; = \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i[\mathbf{X}_i\boldsymbol{\beta}+ \varepsilon_i]\right)\\
    &amp; = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)}_1\boldsymbol{\beta}+ \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i\right)\\
    &amp; = \boldsymbol{\beta}+ \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i\right)\\
\mathop{\mathrm{plim}}\hat{\boldsymbol\beta}_\text{IV} &amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\cdot \mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i\right) &amp; (\text{Slutsky's theorem})\\
&amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\cdot \text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] &amp; (\text{LLN})\\
&amp; = \boldsymbol{\beta}+ \mathop{\mathrm{plim}}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\cdot \mathbf{0}&amp; (\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0})\\
&amp; = \boldsymbol{\beta}
\end{align*}\]</span> <span style="color:white">space</span></p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2 (IV Estimator is Root-n CAN) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2} \in \mathcal P_\text{IV}\)</span>, then <span class="math display">\[\hat{\boldsymbol\beta}_\text{IV} \overset{a}{\sim}N\left(\boldsymbol{\beta}, \sigma^2\text{E}\left[\mathbb{Z}'\mathbb{X}\right]^{-1}\text{E}\left[\mathbb{Z}'\mathbb{Z}\right]\text{E}\left[\mathbb{X}'\mathbb{Z}\right]^{-1}\right) =  N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\text{E}\left[\mathbf{X}'\mathbf{Z}\right]^{-1} \right)\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is almost identical to that of Theorem @ref(thm:asymols).</p>
<p><span class="math display">\[\begin{align*}
\sqrt n(\hat{\boldsymbol\beta}_\text{IV} - \boldsymbol{\beta}) &amp; = \sqrt{n}\left[\boldsymbol{\beta}+ \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i\right) - \boldsymbol{\beta}\right]\\
&amp; = \left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i - \mathbf{0}\right)\\
&amp; =\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i - \text{E}\left[\mathbf{Z}_i\varepsilon_i\right] \right) &amp; (\text{E}\left[\mathbf{Z}'\varepsilon\right] = \mathbf{0}) \\
&amp; = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\mathbf{X}_i\right)^{-1}}_{\overset{p}{\to}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}} \underbrace{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{Z}_i'\varepsilon_i - \text{E}\left[\mathbf{Z}_i\varepsilon_i\right] \right)}_{\overset{d}{\to}N\left(\text{E}\left[\mathbf{Z}_i\varepsilon_i\right], \text{Var}\left(\textstyle \sum \mathbf{X}_i\varepsilon_i\right)/n\right)} &amp; (\text{LLN and CLT})\\
&amp;\overset{d}{\to}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\cdot N\left(\text{E}\left[\mathbf{Z}_i\varepsilon_i\right], \text{Var}\left(\textstyle \sum \mathbf{Z}_i\varepsilon_i\right)/n\right) &amp; (\text{Slutsky's Theorem})\\
&amp; = \text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\cdot N\left(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\right) \\
&amp; = \text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\cdot N\left(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\right)\\
&amp; = N\left(\mathbf{0}, \text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\sigma^2\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\left[\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\right]'\right)\\
&amp; = N\left(\mathbf{0}, \text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\sigma^2\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\text{E}\left[\mathbf{X}'\mathbf{Z}\right]^{-1}\right).
\end{align*}\]</span> This implies that <span class="math display">\[\hat{\boldsymbol\beta}_\text{IV} \overset{a}{\sim}N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\text{E}\left[\mathbf{X}'\mathbf{Z}\right]^{-1} \right).\]</span> If desired, we can write the asymptotic variance in terms of matrices <span class="math inline">\(\mathbb{X}\)</span> and <span class="math inline">\(\mathbb{Z}\)</span>: <span class="math display">\[\begin{align*}
\text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) &amp; = \frac{\sigma^2}{n}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\text{E}\left[\mathbf{X}'\mathbf{Z}\right]^{-1}\\
&amp; = \frac{\sigma^2}{n}\left[\frac{\text{E}\left[\mathbb{Z}'\mathbb{X}\right]}{n}\right]^{-1}\left[\frac{\text{E}\left[\mathbb{Z}'\mathbb{Z}\right]}{n}\right]\left[\frac{\text{E}\left[\mathbb{X}'\mathbb{Z}\right]}{n}\right]^{-1} \\
&amp; = n^2\cdot\frac{1}{n}\cdot\frac{\sigma^2}{n}\text{E}\left[\mathbb{Z}'\mathbb{X}\right]^{-1}\text{E}\left[\mathbb{Z}'\mathbb{Z}\right]\text{E}\left[\mathbb{X}'\mathbb{Z}\right]^{-1}\\
&amp; = \sigma^2\text{E}\left[\mathbb{Z}'\mathbb{X}\right]^{-1}\text{E}\left[\mathbb{Z}'\mathbb{Z}\right]\text{E}\left[\mathbb{X}'\mathbb{Z}\right]^{-1}
\end{align*}\]</span> <span style="color:white">space</span></p>
</div>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 6.1 (IV Estimator is Asymptotically Unbiased) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2} \in \mathcal P_\text{IV}\)</span>, then <span class="math display">\[ \lim_{n\to\infty}\text{Bias}(\hat{\boldsymbol\beta}_\text{IV} ) = 0.\]</span></p>
</div>
<p>In order to appeal to the asymptotic distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> to perform inference, we need a consistent estimator of the asymptotic variance. Fortunately, we can take nearly the same exact approach we took with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 6.3 (Estimation of IV Variance) </strong></span>Define the estimator <span class="math display">\[\begin{align*}
S^2 &amp;=  \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n-K}\\
\hat{\mathbf{e}} &amp;= \mathbf{Y}- \mathbb{X}\hat{\boldsymbol\beta}_\text{IV}
\end{align*}\]</span> in the context of the linear IV model. Then:</p>
<ol type="1">
<li><span class="math inline">\(S^2\)</span> is an unbiased for <span class="math inline">\(\text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right) = \sigma^2\)</span>.</li>
<li><span class="math inline">\(S^2\)</span> is a consistent estimator <span class="math inline">\(\text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right) = \sigma^2\)</span>.</li>
<li>The estimator <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV} ) = S^2(\mathbb{Z}'\mathbb{X})^{-1}(\mathbb{Z}'\mathbb{Z})(\mathbb{X}'\mathbb{Z})^{-1}\)</span> is a consistent estimator for <span class="math inline">\({\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV} )\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is nearly identical to that of Proposition @ref(prp:olsvar).</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.8 (Coding Exercise) </strong></span>R has no base function which implements <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>, so let’s write our own. For reference, we can compare our results with those given by <code>ivreg()</code> from the <code>AER</code> (applied econometrics in R) package.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-14_2c883eccd2eccd9da51577f4c1dfb0f0">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>IV <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, Z){</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#determine dimensions, perform IV</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">ncol</span>(Z) <span class="sc">!=</span> K) {<span class="fu">stop</span>(<span class="st">"K instruments required"</span>)}</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">==</span> <span class="dv">0</span>) {<span class="fu">stop</span>(<span class="st">"rank(Z'X) &lt; K"</span>)}</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  hat_beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">#use IV estimates to calculate residuals and estimate SEs</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> hat_beta)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  var_hat <span class="ot">&lt;-</span> (S2) <span class="sc">*</span>  <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> Z) </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  se_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(var_hat))</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>  <span class="co">#t-stat, confidence intervals, p values</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> hat_beta<span class="sc">/</span>se_hat</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>  lower_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>  upper_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>  p_val <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pt</span>(t, n<span class="sc">-</span>K))</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>  <span class="co">#combine everything into one table to return</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">cbind</span>(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(output) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"β"</span>, <span class="dv">1</span><span class="sc">:</span>K, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(output) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Estimate"</span>, <span class="st">"Std.Error"</span>, <span class="st">"t-Stat"</span>, <span class="st">"Lower 95% CI"</span>, <span class="st">"Upper 95% CI"</span>, <span class="st">"p-Value"</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s estimate the model from Example @ref(exm:refex) using <code>ivreg()</code>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-15_a1a1769b5998019fea23a557bb07a14a">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">1</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">ivreg</span>(y <span class="sc">~</span> x <span class="sc">|</span> z))<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Estimate Std. Error   t value    Pr(&gt;|t|)
(Intercept) 1.643405 0.34486642  4.765338 2.16504e-06
x           2.939228 0.03356573 87.566341 0.00000e+00
attr(,"df")
[1] 998
attr(,"nobs")
[1] 1000</code></pre>
</div>
</div>
<p>Now we can use our function and verify that the outputs are the same.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-16_1540145df1f5f1e54af1dbb4d66660ec">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,z)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">IV</span>(y,X,Z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Estimate  Std.Error    t-Stat Lower 95% CI Upper 95% CI     p-Value
β1 1.643405 0.34486642  4.765338    0.9674794     2.319331 2.16504e-06
β2 2.939228 0.03356573 87.566341    2.8734401     3.005015 0.00000e+00</code></pre>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.9 (Intuition behind Asymptotic Variance) </strong></span>In Example @ref(exm:csvarols) we provided some intuition as to how <span class="math inline">\(\text{Var}\left(\hat{\boldsymbol\beta}_\text{OLS} \right) = \frac{\sigma^2}{n} \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\)</span> changed in response to changes in <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(n\)</span>, and components of <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span>. The variance of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is smaller for large samples and when the error term <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> has a small variance. The more variance we have in our regressors (which is related to <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\)</span>), the more efficient our estimator. So what is the intuition behind <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) =\frac{\sigma^2}{n}\text{E}\left[\mathbf{Z}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\text{E}\left[\mathbf{X}'\mathbf{Z}\right]^{-1}\)</span>? Three things should look familiar. The variance increases with increases in <span class="math inline">\(\sigma^2\)</span> and decreases with increases in <span class="math inline">\(n\)</span>. Consider the model <span class="math inline">\(Y = X\beta + \varepsilon\)</span> when we instrument for <span class="math inline">\(X\)</span> with <span class="math inline">\(Z\)</span>, such that <span class="math display">\[\text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) =\frac{\sigma^2}{n}\frac{\text{E}\left[Z^2\right]}{\text{E}\left[ZX\right]^2} = \frac{\text{Var}\left(Z\right) + \text{E}\left[Z\right]^2}{[\text{Cov}\left(X,Z\right) + \text{E}\left[Z\right]\text{E}\left[X\right]]^2}.\]</span> Holding the average of <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> fixed, we see that the asymptotic variance of our estimator increases with linearly with <span class="math inline">\(\text{Var}\left(Z\right)\)</span>, and decreases quadratically as <span class="math inline">\(\text{Cov}\left(X,Z\right)\)</span>. The latter of these facts shouldn’t be surprising. The larger <span class="math inline">\(\text{Cov}\left(X,Z\right)\)</span>, the more relevant (“stronger”) our instrument <span class="math inline">\(Z\)</span> is, and the better our estimates.</p>
</div>
</section>
<section id="many-instruments-2sls" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="many-instruments-2sls"><span class="header-section-number">6.4</span> Many Instruments, 2SLS</h2>
<p>When defining <span class="math inline">\(\mathcal P_\text{IV}\)</span>, we assumed <span class="math inline">\(\dim(\mathbf{Z}) = \dim(\mathbf{X}) = K\)</span>. What happens if we have <span class="math inline">\(L = \dim(\mathbf{Z}) &gt; \dim(\mathbf{X}) = K\)</span>? Let’s consider a special case first.</p>
<div id="exm-ref3" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.10 (OLS vs.&nbsp;IV) </strong></span>Consider a model <span class="math inline">\(\mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span> which satisfies the Gauss-Markov assumption, <em>but</em> also specifies the existence of <span class="math inline">\(K\)</span> separate instrumental variables <span class="math inline">\(\mathbf{Z}\)</span>. In other words, we have <span class="math inline">\(2K\)</span> instruments in the form of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span>. Should we estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> via OLS (equivalent to IV using <span class="math inline">\(\mathbf{X}\)</span> as instruments for <span class="math inline">\(\mathbf{X}\)</span>), or should we estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> via IV using <span class="math inline">\(\mathbf{Z}\)</span>? It may be tempting to pick the latter. What if we are confident that <span class="math inline">\(\mathbf{Z}\)</span> are valid instruments, but aren’t positive that <span class="math inline">\(\mathbf{X}\)</span> is exogenous. If we estimated <span class="math inline">\(\boldsymbol{\beta}\)</span> with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} = (\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}'\mathbf{Y}\)</span>, then we would be playing it safe and not risk inconsistent estimates via <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} '=(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}= \hat{\boldsymbol\beta}_\text{OLS} \)</span>. Unfortunately this approach has a cost in the form of variance/standard errors. <span class="math display">\[\begin{align*}
\text{Avar}(\hat{\boldsymbol\beta}_\text{IV} ) &amp;= \sigma^2\text{E}\left[\mathbb{Z}'\mathbb{X}\right]^{-1}\text{E}\left[\mathbb{Z}'\mathbb{Z}\right]\text{E}\left[\mathbb{X}'\mathbb{Z}\right]^{-1}\\
\text{Avar}(\hat{\boldsymbol\beta}_\text{IV} ') &amp;= \text{Avar}(\hat{\boldsymbol\beta}_\text{OLS} ) = \sigma^2\text{E}\left[\mathbb{X}'\mathbb{X}\right]^{-1} &amp; (\mathbb{X}= \mathbb{Z})
\end{align*}\]</span> The difference <span class="math inline">\(\text{Avar}(\hat{\boldsymbol\beta}_\text{IV} ) - \text{Avar}(\hat{\boldsymbol\beta}_\text{OLS} )\)</span> is PSD, so <span class="math display">\[ \text{se}\left(\hat\beta_{\text{IV},j}'\right) = \text{se}\left(\hat\beta_{\text{OLS},j}\right) &lt; \text{se}\left(\hat\beta_{\text{IV},j}\right).\]</span> As the next simulation shows, this difference can be fiarly large.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-17_1e1e3d844cf50c716b8543a7425d140e">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">2</span><span class="sc">*</span>N_sim, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>estimates[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"OLS"</span>, <span class="st">"IV"</span>), N_sim)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">0</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>  Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,z)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  estimates[<span class="dv">2</span><span class="sc">*</span>k<span class="dv">-1</span> ,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>  estimates[<span class="dv">2</span><span class="sc">*</span>k,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y </span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>}  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="endog_cache/html/unnamed-chunk-18_2089debd748412128a94f2fe18775304">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>If we calculate the simulated standard errors of our estimators, we find that using <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> results in nearly a four fold decrease in standard error.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-19_164ba2ca4dd67cbc3d1a928cb46cb3b5">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">Estimator</th>
<th style="text-align: left;">Term</th>
<th style="text-align: right;">Standard Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">IV</td>
<td style="text-align: left;">β1</td>
<td style="text-align: right;">0.2929534</td>
</tr>
<tr class="even">
<td style="text-align: left;">IV</td>
<td style="text-align: left;">β2</td>
<td style="text-align: right;">0.0290895</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OLS</td>
<td style="text-align: left;">β1</td>
<td style="text-align: right;">0.0780588</td>
</tr>
<tr class="even">
<td style="text-align: left;">OLS</td>
<td style="text-align: left;">β2</td>
<td style="text-align: right;">0.0071179</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="exm-ex4" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.11 (IV vs… IV?) </strong></span>Suppose <span class="math inline">\(Y = X\beta + \varepsilon\)</span> for an endogenous <span class="math inline">\(X\)</span>. Fortunately, we have two instruments <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span>. Which instrument do we use to estimate <span class="math inline">\(\beta\)</span>. At first the answer seems simple – just estimate the model using each instrument separately, and then pick the estimates with the lower standard error. This won’t consider all the relevant cases though, as <em>any</em> linear combination of <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are also instruments. Define <span class="math inline">\(Z_3 = aZ_1 + bZ_2\)</span>. <span class="math display">\[\begin{align*}
\text{E}\left[Z_3\varepsilon\right] &amp; = \text{E}\left[(aZ_1 + bZ_2)\varepsilon\right] = a\underbrace{\text{E}\left[Z_1\varepsilon\right]}_0 + b\underbrace{\text{E}\left[Z_2\varepsilon\right]}_0 = 0\\
\text{E}\left[Z_3X\right] &amp; = \text{E}\left[(aZ_1 + bZ_2)X\right] = a\underbrace{\text{E}\left[Z_1X\right]}_{\neq 0} + b\underbrace{\text{E}\left[Z_2X\right]}_{\neq 0} \neq 0
\end{align*}\]</span> There are an infinite number of candidates for instruments, so what do we do? Let’s simulate some standard errors and see if we can notice patterns between them and the choice of instruments. We’ll restrict our attention to instruments in the set <span class="math inline">\(\{aZ_1 + bZ_2\mid a,b\in[0,1]\}\)</span>. We will calculate the <span class="math inline">\(\text{Var}\left(\hat{\boldsymbol\beta}_\text{IV} \mid \mathbb{X},\ \mathbb{Z}\right)\)</span> for a fixed sample drawn from <span class="math inline">\((X,Z_1,Z_2,\varepsilon)\overset{iid}{\sim}N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span> where</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol \mu &amp; = [10,10,10,0]',\\
\boldsymbol{\Sigma}&amp; = \begin{bmatrix}20 &amp; 5 &amp; 10 &amp; 1\\5&amp;30&amp;7&amp;0\\10&amp;7&amp;50&amp;0\\1&amp;0&amp;0&amp;1 \end{bmatrix},
\end{align*}\]</span></p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-20_f8268186c4d6cb0135bdcbadc9fd4f1c">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">30</span>,<span class="dv">7</span>,<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">7</span>,<span class="dv">50</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">4</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>] </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>z1 <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>z2 <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> sample[,<span class="dv">4</span>]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">500</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">500</span>) </span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (a <span class="cf">in</span> A) {</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (b <span class="cf">in</span> B) {</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    i <span class="ot">&lt;-</span> i <span class="sc">+</span><span class="dv">1</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(x)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    Z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(a<span class="sc">*</span>z1 <span class="sc">+</span> b<span class="sc">*</span>z2)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    store[[i]] <span class="ot">&lt;-</span> (<span class="fu">IV</span>(y, X, Z)[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s plot the calculated variances (conditional on the fixed sample) over the values <span class="math inline">\(a\times b\in [0,1]^2\)</span> which determined the instrument used to calculate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="endog_cache/html/unnamed-chunk-21_078578961ac8b7f24051857b4efa9281">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As anticipated, <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is more efficient when using certain linear combinations of elements. Furthermore, it appears that using some non-trivial linear combination of <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> is a better choice than simply using one or the other.</p>
</div>
<p>Before considering this problem in general, let’s extend the IV model to allow for more instruments than regressors.</p>
<div class="{#def=}">
<p>The <span style="color:red"><strong><em>(linear) instrumental variables (IV) model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{IV} = \{P_{\boldsymbol{\beta},\sigma^2} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \sigma^2\in\mathbb R\}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\sigma^2} &amp;= \{F_{\mathbb{X},\mathbb{Z},\boldsymbol{\varepsilon}} \mid\ \text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{Z}\right]\right) = L,\ \text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K, \ldots  \},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbb{Z}&amp; = [\mathbf{Z}_1, \cdots, \mathbf{Z}_j, \cdots \mathbf{Z}_L] = [\mathbf{Z}_1, \cdots, \mathbf{Z}_i, \cdots \mathbf{Z}_n]',\\
\dim(\mathbf{Z}) &amp; = L,\\
\dim(\mathbf{X}) &amp;= K,\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n].
\end{align*}\]</span> A necessary condition for the rank condition <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span> is the <span style="color:red"><strong><em>order condition</em></strong></span>, <span class="math inline">\(L\ge K\)</span>. In the event <span class="math inline">\(L = K\)</span> we say the model is <span style="color:red"><strong><em>exactly identified</em></strong></span>. If <span class="math inline">\(L &gt; K\)</span>, the model is <span style="color:red"><strong><em>over-identified</em></strong></span>.</p>
</div>
<p>When <span class="math inline">\(L=K\)</span> this is just the original IV model, and we know how to estimate this with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>. In the event <span class="math inline">\(L &gt; K\)</span>, the model is still identified, as we haven’t modified the identifying assumptions that <span class="math inline">\(\text{E}\left[\mathbf{Z}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{Z}\right] \neq \mathbf{0}\)</span>, and <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{Z}'\mathbf{X}\right]\right) = K\)</span>. In fact, our model would still be identified if we discarded <span class="math inline">\(L - K\)</span> instruments! This is where the term “over-identified” comes from, and as far as “problems” go, it’s not a bad problem to have. We have so many instruments, that we have an infinite number of ways to estimate the model!</p>
<p>In general, any linear combination of instruments <span class="math inline">\(Z_1,\ldots, Z_L\)</span> is also an instrument. We can only use <span class="math inline">\(K\)</span> instruments with the estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>, so we want to find the <span class="math inline">\(K\)</span> linear combinations of our instruments (some of which could reduce to a single <span class="math inline">\(Z_j\)</span>) which give us the most efficient estimator. The <span class="math inline">\(K\)</span> linear combinations of the <span class="math inline">\(L\)</span> instruments can be expressed as a <span class="math inline">\(L\times K\)</span> matrix <span class="math inline">\(\mathbf{F}\)</span> which gives instruments <span class="math inline">\(\mathbf{Z}\mathbf{F}\)</span>. If we elect to use the new instruments <span class="math inline">\(\mathbf{Z}\mathbf{F}\)</span>, the IV estimator becomes <span class="math display">\[\hat{\boldsymbol\beta}_\text{IV} = [(\mathbb{Z}\mathbf{F})'\mathbb{X}]^{-1}[(\mathbb{Z}\mathbf{F})'\mathbf{Y}].\]</span> The problem of selecting <span class="math inline">\(\mathbf{F}\)</span> such that we maximize the efficiency of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is given as:<br>
<span class="math display">\[\mathop{\mathrm{argmin}}_{\mathbf{F}} \text{Avar}(\hat{\boldsymbol\beta}_\text{IV} ) = \mathop{\mathrm{argmin}}_{\mathbf{F}} \sigma^2\text{E}\left[(\mathbb{Z}\mathbf{F})'\mathbb{X}\right]^{-1}\text{E}\left[(\mathbb{Z}\mathbf{F})'(\mathbb{Z}\mathbf{F})\right]\text{E}\left[\mathbb{X}'(\mathbb{Z}\mathbf{F})\right]^{-1}.\]</span> Solving this looks like a miserable time, so let’s stop appealing directly to math and consider what we’re <em>actually</em> doing here. We’re looking for the “best” instruments. It stands to reason that “good” instruments will be those that have more explanatory power with respect to the endogenous regressors <span class="math inline">\(\mathbf{X}\)</span> than others. So given draws of <span class="math inline">\((\mathbf{Z},\mathbf{X})\)</span>, how can we determine the best way to predict/explain <span class="math inline">\(\mathbf{X}\)</span> given <span class="math inline">\(\mathbf{Y}\)</span> — by estimating the linear projection associated with <span class="math inline">\((\mathbf{Z},\mathbf{X})\)</span> via OLS. For each <span class="math inline">\(j = 1,\ldots,K\)</span> we have <span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{OLS} ^j &amp;= (\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{X}_j &amp; (j = 1,\ldots, K)
\end{align*}\]</span> where <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} ^j\)</span> define the instrument formed from linear combinations of <span class="math inline">\(Z_1,\ldots, Z_L\)</span> which has the most predictive power for <span class="math inline">\(X_j\)</span>: <span class="math display">\[\begin{align*}
\hat X_j &amp;= \mathbf{Z}\hat{\boldsymbol\beta}_\text{OLS} ^j = \hat\beta_1^jZ_1 + \hat\beta_2^jZ_2 + \cdots + \hat\beta_L^jZ_L &amp; (j = 1,\ldots, K)
\end{align*}\]</span> Written compactly using matrices, we have <span class="math display">\[\begin{align*}
\hat{\mathbf{X}} &amp;=  \mathbf{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X},\\
\hat{\mathbb{X}} &amp;=  \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X},
\end{align*}\]</span> where <span class="math inline">\(\hat{\mathbf{X}}\)</span> is a random vector of instruments, and <span class="math inline">\(\hat{\mathbb{X}}\)</span> is <span class="math inline">\(n\)</span> random vectors <span class="math inline">\(\hat{\mathbf{X}}\)</span> “stacked” to form a random matrix. If we elect to use these instruments to estimate our model, then <span class="math display">\[ \hat{\boldsymbol\beta}_\text{IV} = [(\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X})'\mathbb{X}]^{-1}[(\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X})'\mathbf{Y}].\]</span> This estimator is a special case of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> where we have determined the <span class="math inline">\(K\)</span> instruments via linear projection, and is known as two-stage least squares.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 </strong></span>The <span style="color:red"><strong><em>two-stage least squares (2SLS) estimator</em></strong></span> is defined as <span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{2SLS} (\mathbb{X}, \mathbb{Z}, \mathbf{Y}) &amp;= [\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]^{-1}\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}= [\hat{\mathbb{X}}'\mathbb{X}]^{-1}\hat{\mathbb{X}}'\mathbf{Y},\\
                 \hat{\mathbb{X}} &amp;=  \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}.   
\end{align*}\]</span> Calculating <span class="math inline">\(\hat{\mathbb{X}}\)</span> via OLS is referred to as the <span style="color:red"><strong><em>first stage</em></strong></span>, while calculating <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> using <span class="math inline">\(\hat{\mathbb{X}}\)</span> is the <span style="color:red"><strong><em>second stage</em></strong></span>. It can be useful to define the projection matrix associated with the first stage, <span class="math display">\[\begin{align*}
\mathbb P_{\mathbb{Z}} &amp;= \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\\
\hat{\mathbb{X}} &amp; = \mathbb P_{\mathbb{Z}}\mathbb{X}.
\end{align*}\]</span></p>
</div>
<p>The name “two-stage least squares” can be a bit misleading depending on how it is presented. It’s very common to think of each stage of 2SLS as an application of OLS. In the first stage we perform OLS to calculate the instruments <span class="math inline">\(\hat{\mathbf{X}}\)</span>, and then regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(\hat{\mathbf{X}}\)</span> via OLS in the second stage. At first these seems at odds with our definition, because the second stage relies on the IV estimator, but it is equivalent in a sense. We’ll explore this in depth in Example @ref(exm:se).</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.12 (OLS as 2SLS) </strong></span>Consider the situation from Example @ref(exm:ref3) where <span class="math inline">\(\mathbf{X}\)</span> is comprised of entirely exogenous regressors. In this case our vector of instruments is actually a <span class="math inline">\(L+K\)</span> vector <span class="math inline">\([\mathbf{X},\mathbf{Z}]\)</span>. Instead of directly calculating <span class="math display">\[\begin{align*}
\hat{\mathbb{X}} &amp;= [\mathbb{X}, \mathbb{Z}]([\mathbb{X}, \mathbb{Z}]'[\mathbb{X}, \mathbb{Z}])^{-1}[\mathbb{X}, \mathbb{Z}]'\mathbb{X},\\
\end{align*}\]</span> we can intuit the result of the first stage. If we regress <span class="math inline">\(X_j\)</span> on <span class="math inline">\(X_1,\ldots, X_j,\ldots,Z_1,\ldots,Z_L\)</span>, then all coefficients should be zero except that associated with the independent variable <span class="math inline">\(X_j\)</span> which will be 1. Therefore the OLS estimates from stage one should be a <span class="math inline">\(L \times K\)</span> matrix where the first <span class="math inline">\(K\)</span> rows are a diagonal matrix of <span class="math inline">\(1\)</span>’s and the bottom <span class="math inline">\(L - K\)</span> rows are 0. If we multiply <span class="math inline">\([\mathbb{X}, \mathbb{Z}]\)</span> by this, then we have <span class="math display">\[ \hat{\mathbb{X}} = \mathbf 1\mathbb{X}+ \mathbf{0}\mathbb{Z}= \mathbb{X}.\]</span> This gives <span class="math display">\[ \hat{\boldsymbol\beta}_\text{2SLS} = [\hat{\mathbb{X}}'\mathbb{X}]^{-1}\hat{\mathbb{X}}'\mathbf{Y}= [{\mathbb{X}}'\mathbb{X}]^{-1}{\mathbb{X}}'\mathbf{Y}= \hat{\boldsymbol\beta}_\text{OLS} .\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.13 (Exactly Identified Case) </strong></span>In the event that <span class="math inline">\(L = K\)</span>, there is no</p>
</div>
<p>Because <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> is just a special case of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> using instruments <span class="math inline">\(\hat{\mathbb{X}}\)</span>, the properties of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> follow directly from those of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.3 (IV Estimator is Root-n CAN) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2} \in \mathcal P_\text{IV}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \overset{p}{\to}\boldsymbol{\beta}\)</span> and <span class="math display">\[\hat{\boldsymbol\beta}_\text{2SLS} \overset{a}{\sim}N\left(\boldsymbol{\beta}, \sigma^2\text{E}\left[\hat{\mathbb{X}}'\hat{\mathbb{X}}\right]^{-1}\right) =  N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right]^{-1}\right).\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We know that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> is Root-n CAN because <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is. All we need to show is that the asymptotic variance simplifies to the given expression when using instruments <span class="math inline">\(\hat{\mathbf{X}}\)</span>. <span class="math display">\[\begin{align*}
\text{Avar}(\hat{\boldsymbol\beta}_\text{2SLS} ) &amp;= \sigma^2\text{E}\left[\hat{\mathbb{X}}\mathbb{X}\right]^{-1}\text{E}\left[\hat{\mathbb{X}}\hat{\mathbb{X}}\right]\text{E}\left[\mathbb{X}'\hat{\mathbb{X}}\right]^{-1}\\
&amp; = \sigma^2\text{E}\left[[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]'\mathbb{X}\right]^{-1}\text{E}\left[[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]\right]\text{E}\left[\mathbb{X}'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]\right]^{-1}\\
&amp; = \sigma^2\text{E}\left[\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}\right]^{-1}\text{E}[\mathbb{X}'\mathbb{Z}\underbrace{(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{Z}}_{\mathbf I}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]\text{E}\left[\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}\right]^{-1}\\
&amp; = \sigma^2\text{E}[\mathbb{X}'\underbrace{\mathbb{Z}\mathbb{Z}^{-1}}_{\mathbf I}\underbrace{(\mathbb{Z}')^{-1}\mathbb{Z}'}_{\mathbf I}\mathbb{X}]^{-1}\text{E}[\mathbb{X}'\underbrace{\mathbb{Z}\mathbb{Z}^{-1}}_{\mathbf I}\underbrace{(\mathbb{Z}')^{-1}\mathbb{Z}'}_{\mathbf I}\mathbb{X}]\text{E}\left[\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}\right]^{-1}\\
&amp; = \sigma^2\underbrace{\text{E}[\mathbb{X}'\mathbb{X}]^{-1}\text{E}[\mathbb{X}'\mathbb{X}]}_{\mathbf I}\text{E}\left[\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}\right]^{-1}\\
&amp; = \sigma^2\text{E}\left[\mathbb{X}'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}']'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}\right]^{-1} &amp; (\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\text{ sym. and idem.}) \\
&amp; = \sigma^2\text{E}\left[[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]\right]^{-1}\\
&amp; = \sigma^2\text{E}\left[\hat{\mathbb{X}}'\hat{\mathbb{X}}\right]^{-1} &amp; (\hat{\mathbb{X}} = \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X})
\end{align*}\]</span></p>
</div>
<p>The motivation for introducing <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> was not all choices of instruments being equal with respect to efficiency, and perhaps the instruments which result from projecting <span class="math inline">\(\mathbf{X}\)</span> onto <span class="math inline">\(\mathbf{Z}\)</span> result in an IV estimator which is more efficient than most others. This IV estimator is not just more efficient that most others, it is <em>the</em> most efficient IV estimator. The proof of this is adapted from <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>.</p>
<div id="thm-eff2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.4 (2SLS is Asymptotically Efficient) </strong></span>If <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2} \in \mathcal P_\text{IV}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> is efficient in the class of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> estimators calculated using instruments linear in <span class="math inline">\(\mathbf{Z}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> is some arbitrary IV estimator which is linear in instruments <span class="math inline">\(\mathbf{Z}\)</span>. The instruments for this estimator can be written as <span class="math inline">\(\tilde{\mathbf{X}} = \mathbf{Z}\mathbf{F}\)</span> for some <span class="math inline">\(L\times K\)</span> matrix <span class="math inline">\(\mathbf{F}\)</span>. We want to show that <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{2SLS} \right)\)</span> is “less than” <span class="math inline">\(\text{Avar}\left(\tilde{\boldsymbol{\beta}}\right)\)</span>. Both of these objects are matrices, so “less” than translates to the difference being PSD. This difference is <span class="math display">\[\begin{align*}
\text{Avar}\left(\tilde{\boldsymbol{\beta}}\right) - \text{Avar}\left(\hat{\boldsymbol\beta}_\text{2SLS} \right) &amp; =\frac{\sigma^2}{n}\text{E}\left[\tilde{\mathbf{X}}'\mathbf{X}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]\text{E}\left[\mathbf{X}'\tilde{\mathbf{X}}\right]^{-1} -  \frac{\sigma^2}{n}\text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right]^{-1}\\&amp; = \frac{\sigma^2}{n}\left[\text{E}\left[\tilde{\mathbf{X}}'\mathbf{X}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]\text{E}\left[\mathbf{X}'\tilde{\mathbf{X}}\right]^{-1} - \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right]^{-1}\right]\\
&amp; = \frac{\sigma^2}{n}\left[\left[\text{E}\left[\tilde{\mathbf{X}}'\mathbf{X}\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\mathbf{X}'\tilde{\mathbf{X}}\right]\right]^{-1} - \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right]^{-1}\right] &amp; (\text{properties of inversion})
\end{align*}\]</span> Accounting for the bracketed term being the difference of two inverted matrices, this matrix will be PSD if and only if the following is PSD: <span class="math display">\[\begin{equation}
\text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - \text{E}\left[\tilde{\mathbf{X}}'\mathbf{X}\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\mathbf{X}'\tilde{\mathbf{X}}\right]  (\#eq:diff).
\end{equation}\]</span></p>
<p>We can show this by relying on a certain “trick”. Define the difference between the regressors <span class="math inline">\(\mathbf{X}\)</span> and 2SLS instruments <span class="math inline">\(\hat{\mathbf{X}}\)</span> as <span class="math inline">\(\mathbf r = \hat{\mathbf{X}} - \mathbf{X}\)</span>. This remainder term <span class="math inline">\(\mathbf r\)</span> is uncorrelated with <span class="math inline">\(\mathbf{Z}\)</span>: <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{Z}'\mathbf r\right] &amp; = \text{E}\left[\mathbf{Z}'\hat{\mathbf{X}}\right] - \text{E}\left[\mathbf{Z}'\mathbf{X}\right]\\
&amp; = \text{E}\left[\mathbf{Z}'\mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X}\right] - \text{E}\left[\mathbf{Z}'\mathbf{X}\right] &amp; (\hat{\mathbf{X}} = \mathbf{Z}(\mathbf{Z}'\mathbf{Z})^{-1}\mathbf{Z}'\mathbf{X})\\
&amp; = \text{E}\left[\mathbf{Z}'\mathbf{X}\right] - \text{E}\left[\mathbf{Z}'\mathbf{X}\right]\\
&amp; = \mathbf{0}
\end{align*}\]</span> As a result, <span class="math inline">\(\tilde{ \mathbf{X}}\)</span> and <span class="math inline">\(\mathbf r\)</span> are uncorrelated. <span class="math display">\[\text{E}\left[\tilde{ \mathbf{X}}'\mathbf r\right] = \text{E}\left[(\mathbf{Z}\mathbf{F}')\mathbf r\right]= \mathbf F'\underbrace{\text{E}\left[\mathbf{Z}'\mathbf r\right]}_\mathbf{0}= \mathbf{0}\]</span> Now we use these expectations and the definition of <span class="math inline">\(\mathbf r\)</span> to arrive at our “trick”. <span class="math display">\[\begin{align*}
&amp;\text{E}\left[\tilde{ \mathbf{X}}'\mathbf r\right] = \mathbf{0}\\
\implies &amp; \text{E}\left[\tilde{ \mathbf{X}}'(\hat{\mathbf{X}} -  \mathbf{X})\right] = \mathbf{0}&amp; (\mathbf r = \hat{\mathbf{X}} -  \mathbf{X})\\
\implies &amp; \text{E}\left[\tilde{ \mathbf{X}}'\hat{\mathbf{X}}\right] - \text{E}\left[\tilde{ \mathbf{X}}'\mathbf{X}\right] = \mathbf{0}\\
\implies &amp; \text{E}\left[\tilde{ \mathbf{X}}'\hat{\mathbf{X}}\right] = \text{E}\left[\tilde{ \mathbf{X}}'\mathbf{X}\right]
\end{align*}\]</span> Using this equality, we can rewrite the difference in Equation @ref(eq:diff) in terms of <span class="math inline">\(\tilde{\mathbf{X}}\)</span> and <span class="math inline">\(\hat{\mathbf{X}}\)</span> only. <span class="math display">\[\begin{equation}
\text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - \text{E}\left[\hat{\mathbf{X}}'\tilde{\mathbf{X}}\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]  (\#eq:diff2).
\end{equation}\]</span> This may seem like a random equation, but it’s very special if you consider the linear projection of <span class="math inline">\(\hat{\mathbf{X}}\)</span> onto <span class="math inline">\(\tilde{\mathbf{X}}\)</span>. This projection is given by the coefficient <span class="math inline">\(\boldsymbol\gamma = \text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right],\)</span> which happens to show up in @ref(eq:diff2). The errors associated with this linear projection are given as <span class="math inline">\(\boldsymbol \nu = \hat{\mathbf{X}} - \tilde{\mathbf{X}}\boldsymbol\gamma\)</span>, and have an expected value of <span class="math inline">\(\mathbf{0}\)</span> by the definition of <span class="math inline">\(\boldsymbol\gamma\)</span> and linear projection. But what is the expected value of these errors squared?</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol \nu' \boldsymbol \nu&amp; = [\hat{\mathbf{X}} - \tilde{\mathbf{X}}\boldsymbol\gamma]'[\hat{\mathbf{X}} - \tilde{\mathbf{X}}\boldsymbol\gamma]\\
&amp; = \hat{\mathbf{X}}'\hat{\mathbf{X}} - 2\boldsymbol\gamma'\tilde{\mathbf{X}}'\hat{\mathbf{X}} + \boldsymbol\gamma'\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\boldsymbol\gamma\\
\text{E}\left[ \boldsymbol \nu' \boldsymbol \nu\right] &amp; = \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - 2\boldsymbol\gamma'\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right] + \boldsymbol\gamma'\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]\boldsymbol\gamma &amp; (\boldsymbol\gamma \text{ is a constant})\\
&amp; = \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - 2\left[\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]\right]'\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right] + \left[\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]\right]'\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right] \left[\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]\right]\\
&amp; = \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - 2\text{E}\left[\tilde{\mathbf{X}}\hat{\mathbf{X}}'\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right] + \text{E}\left[\tilde{\mathbf{X}}\hat{\mathbf{X}}'\right]\underbrace{\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]}_{\mathbf I} \text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]\\
&amp; = \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - (2-1)\text{E}\left[\tilde{\mathbf{X}}\hat{\mathbf{X}}'\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right] \\
&amp; = \text{E}\left[\hat{\mathbf{X}}'\hat{\mathbf{X}}\right] - \text{E}\left[\hat{\mathbf{X}}'\tilde{\mathbf{X}}\right]\text{E}\left[\tilde{\mathbf{X}}'\tilde{\mathbf{X}}\right]^{-1}\text{E}\left[\tilde{\mathbf{X}}'\hat{\mathbf{X}}\right]
\end{align*}\]</span> Equations @ref(eq:diff) and @ref(eq:diff2) are the expectation of the sum of squared errors associated with the linear projection of <span class="math inline">\(\hat{\mathbf{X}}\)</span> onto <span class="math inline">\(\tilde{\mathbf{X}}\)</span>, meaning the matrix in question must be PSD!<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> This gives the desired result.</p>
</div>
<p>In Section @ref(generalized-method-of-moments) we’ll see another way to show this result that is a bit more intuitive.</p>
<div id="exm-se" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.14 (What’s in A Name?) </strong></span>Let’s estimate the model from @ref(exm:ex4) via 2SLS. We already have defined a function <code>IV()</code> to calculate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>, so we can just use pass the instruments <span class="math inline">\(\hat {\mathbf{X}}\)</span> to this function to perform 2SLS.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-22_251ef7a11fe1c022f48c073ee2ec8b1f">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>TSLS <span class="ot">&lt;-</span> <span class="cf">function</span>(y,X,Z){</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  X_hat <span class="ot">&lt;-</span> Z <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> X</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">IV</span>(y, X, X_hat)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For a simulated sample of size <span class="math inline">\(n=50\)</span> let’s estimate the model using <code>TSLS()</code>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-23_fc8bb2e09897e48c09f51bfc4f67ea24">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">30</span>,<span class="dv">7</span>,<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">7</span>,<span class="dv">50</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">4</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>] </span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>z1 <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>z2 <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> sample[,<span class="dv">4</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z1, z2)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="fu">TSLS</span>(y,X,Z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Estimate  Std.Error     t-Stat Lower 95% CI Upper 95% CI   p-Value
β1 0.1649153 0.72038529  0.2289266    -1.247014     1.576845 0.8198986
β2 2.1031513 0.06677504 31.4960690     1.972275     2.234028 0.0000000</code></pre>
</div>
</div>
<p>This is not the only way we could calculate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span>. Note that <span class="math inline">\(\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\)</span> is symmetric and idempotent, so <span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{2SLS} &amp; = \hat{\boldsymbol\beta}_\text{IV} (\mathbb{X}, \hat{\mathbb{X}}, \mathbf{Y})\\
&amp; = [\hat{\mathbb{X}}'{\mathbb{X}}]^{-1}\hat{\mathbb{X}}'\mathbf{Y}\\
&amp; = [\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]^{-1}\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}\\
&amp; = [\mathbb{X}'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}']\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]^{-1}\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}\\
&amp; = [[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]'[\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbb{X}]]^{-1}\mathbb{X}'\mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\mathbf{Y}\\
&amp; = [\hat{\mathbb{X}}'\hat{\mathbb{X}}]^{-1}\hat{\mathbb{X}}'\mathbf{Y}\\
&amp; = \hat{\boldsymbol\beta}_\text{OLS} (\hat{\mathbb{X}}, \mathbf{Y}).
\end{align*}\]</span> So we can interpret <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> in one of two ways:</p>
<ol type="1">
<li>Use <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} (\mathbb{Z}, \mathbb{X})\)</span> to calculate <span class="math inline">\(\hat{\mathbb{X}} = \mathbb{Z}\hat{\boldsymbol\beta}_\text{OLS} (\mathbb{Z}, \mathbb{X})\)</span> (stage 1), followed by <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} (\mathbb{X}, \hat{\mathbb{X}}, \mathbf{Y})\)</span> (stage 2). This is how our <code>TSLS()</code> function works.</li>
<li>Use <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} (\mathbb{Z}, \mathbb{X})\)</span> to calculate <span class="math inline">\(\hat{\mathbb{X}} = \mathbb{Z}\hat{\boldsymbol\beta}_\text{OLS} (\mathbb{Z}, \mathbb{X})\)</span> (stage 1), followed by <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} (\hat{\mathbb{X}}, \mathbf{Y})\)</span> (stage 2).</li>
</ol>
<p>The second interpretation is where 2SLS gets its name – we run OLS twice. But what happens when we implement <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> this way?</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-24_07224e02e50604682a7d3b8854268c43">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>X_hat <span class="ot">&lt;-</span> <span class="fu">lm</span>(x <span class="sc">~</span> z1 <span class="sc">+</span> z2)<span class="sc">$</span>fitted.values</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X_hat))<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 0.1649153  6.3378518 0.0260207 0.9793487094
X_hat       2.1031513  0.5874777 3.5799677 0.0007987861</code></pre>
</div>
</div>
<p>We get the same exact estimates (which we already showed would be the case), but our standard errors don’t coincide with those given by <code>TSLS()</code>. What is happening here?</p>
<p>When we run <code>lm(y ~ X_hat)</code>, we are estimating the model associated with linear projection model <span class="math inline">\(Y = \hat{\mathbf{X}}\boldsymbol \delta + \nu\)</span>. The function <code>lm()</code> has no way of knowing that this is the second step in an estimation process aimed at estimating the structural linear model <span class="math inline">\(Y = \mathbf{X}\boldsymbol{\beta}+ \varepsilon\)</span>. It just happens that <span class="math inline">\(\hat{\mathbf{X}}\)</span> is defined such that the population parameters satisfy <span class="math inline">\(\boldsymbol \delta = \boldsymbol{\beta}\)</span>. When <code>lm()</code> goes to calculate the standard errors, it calculates the residuals associated with the model <span class="math inline">\(Y = \hat{\mathbf{X}}\boldsymbol \delta + \nu\)</span>. These errors have no structural/economic meaning, and we don’t care about them at all (for this purpose). We really want the residuals associated with the error <span class="math inline">\(\varepsilon\)</span> in the actual IV model <span class="math inline">\(Y = \mathbf{X}\boldsymbol{\beta}+ \varepsilon\)</span>, and these residuals are not the same! <span class="math display">\[ \hat{\mathbf u} = \mathbf{Y}- \hat{\mathbb{X}}\hat{\boldsymbol\beta}_\text{OLS} (\hat{\mathbb{X}}, \mathbf{Y}) \neq \mathbf{Y}- \mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} (\hat{\mathbb{X}}, \mathbf{Y}) = \mathbf{Y}- \mathbb{X}\hat{\boldsymbol\beta}_\text{IV} (\mathbb{X}, \hat{\mathbb{X}}, \mathbf{Y}) = \hat{\mathbf{e}}\]</span> The moral of the story is that when we are calculating the residuals we need to be very deliberate and remember the actual model we are estimating. Fortunately, any statistical software with an implementation for <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> will calculate the correct standard errors, which is why it’s best to not do each step “by hand” if you favor the interpretation of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> as two OLS regressions.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-25_ed78d284faaccfec2a65389ec5d5561c">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">ivreg</span>(y <span class="sc">~</span> x <span class="sc">|</span> z1 <span class="sc">+</span> z2))<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 0.1649153 0.72038529  0.2289266 8.198986e-01
x           2.1031513 0.06677504 31.4960690 1.022975e-33
attr(,"df")
[1] 48
attr(,"nobs")
[1] 50</code></pre>
</div>
</div>
</div>
<p>The example emphasizes something that has been mentioned in passing before. The models estimated in the first and second stage, <span class="math inline">\(\mathbf{X}= \mathbf{Z}\boldsymbol + \gamma\)</span> and <span class="math inline">\(Y = \hat{\mathbf{X}}\boldsymbol \delta + \nu\)</span>, are entirely void of structural meaning. Nevertheless it is useful to us because it expresses the outcome of interest in terms of exogenous variables <span class="math inline">\(\hat{\mathbf{X}}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span>. We refer to such an equation as the <strong><em>reduced form</em></strong> of the structural model <span class="math inline">\(Y = \mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span>. We’ll talk about this term much more in Section @ref(endogeniety-ii-simultaneous-equation-models).</p>
</section>
<section id="testing-hypotheses" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="testing-hypotheses"><span class="header-section-number">6.5</span> Testing Hypotheses</h2>
<p>At this point, we have just assumed we know how to select between the classical linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> and the linear IV model <span class="math inline">\(\mathcal P_\text{IV}\)</span>, but in practice we want to be able to form hypothesis tests that inform our selection between the two models. In particular we want to be able to test:</p>
<ol type="1">
<li>Are our regressors exogenous, i.e <span class="math inline">\(H_0: \text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>?</li>
<li>Are our instruments valid, i.e…</li>
</ol>
<section id="durbinwuhausman-test" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="durbinwuhausman-test"><span class="header-section-number">6.5.1</span> Durbin–Wu–Hausman Test</h3>
<p>We’ll start with considering tests for exogenous/endogenous regressors. One sign that our model contains endogenous regressors is if there is a significant difference between <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>. This is why it’s always a good idea to estimate a model with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> even if you think regressors are endogenous. It provides a good reference to compare IV estimates with. In fact, comparing <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> serve as the basis for one of the most common tests for endogeneity as formalized by <span class="citation" data-cites="hausman1978specification">Hausman (<a href="references.html#ref-hausman1978specification" role="doc-biblioref">1978</a>)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>In the face of endogeneity, <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is an abject failure in light of its inconsistency. Fortunately, <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is consistent when <span class="math inline">\(\mathbf{X}\)</span> is endogenous. Furthermore, <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> is also consistent when <span class="math inline">\(\mathbf{X}\)</span> is exogenous, as the instruments <span class="math inline">\(\mathbf{Z}\)</span> are still valid and relevant. In other words, if <span class="math inline">\(\mathbf{X}\)</span> is exogenous, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span> <em>and</em> <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \overset{p}{\to}\boldsymbol{\beta}\)</span>, so <span class="math display">\[ \hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \overset{p}{\to}\mathbf{0}.\]</span> This suggests the following: <span class="math display">\[ H_0: \mathbf{X}\text{ exogenous} \iff H_0:\mathop{\mathrm{plim}}\left(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \right) = \mathbf{0}.\]</span> We can construct a Wald test statistic <span class="math display">\[ W = (\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} )' \left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} ) \right]^{-1}(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} ),\]</span> but we will run into a roadblock in the form of <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \right)\)</span>. If we expand this, we have</p>
<p><span class="math display">\[ \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \right) = \text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) + \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right) - 2\text{Acov}\left(\hat{\boldsymbol\beta}_\text{IV} ,\hat{\boldsymbol\beta}_\text{OLS} \right),\]</span> as <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> are presumably correlated. We don’t have a formula for the covariance of these estimators handy, and deriving one would be a pain. Fortunately, we don’t have to, because Lemma 2.1 of <span class="citation" data-cites="hausman1978specification">Hausman (<a href="references.html#ref-hausman1978specification" role="doc-biblioref">1978</a>)</span> asserts that the covariance term is such that <span class="math display">\[  \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \right) =  \text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) - \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\]</span> due to <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> being efficient relative to <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> This is a particular useful equality, and interesting result in it’s own right. <span class="citation" data-cites="hansen2022econometrics">Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span> refers to it as the “Hausman equality”, and gives details about it in Section 8.11. This gives the test statistic <span class="math display">\[\begin{align*}
W &amp;= (\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} )'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV} ) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} )\right]^{-1} (\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} ).
\end{align*}\]</span></p>
<p>Unfortunately, the term corresponding to the asymptotic variance will likely fail to be invertible. Unless <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span> contain no common variables, some column of <span class="math inline">\((\hat{\mathbb{X}}'\hat{\mathbb{X}})^{-1} - (\mathbb{X}'\mathbb{X})^{-1}\)</span> will be a linear combination of other columns. For example, if our model contains an intercept, than we cannot use this statistic, as <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span> will both contain a column of 1’s. This rules out just about every single situation we would ever want to consider. To address this shortcoming, we can take the <a href="https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html">pseudo-inverse</a> of this matrix.</p>
<p><span class="math display">\[W = (\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} )'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV} ) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} )\right]^+ (\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} )\]</span> The hypothesis <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} \overset{p}{\to}\mathbf{0}\)</span> is comprised of <span class="math inline">\(K\)</span> separate hypotheses, so it’s tempting to conclude <span class="math inline">\(W\sim \chi_K^2\)</span>. Unfortunately, this will only be the case when <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) - \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\)</span> is invertible, otherwise we will have <span class="math inline">\(W\sim \chi_q^2\)</span> where <span class="math inline">\(q = \text{rank} \left[ \text{Avar}\left(\hat{\boldsymbol\beta}_\text{IV} \right) - \text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right) \right]\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.5 (Hausman Specification Test) </strong></span>Given the hypothesis <span class="math inline">\(H_0:\hat{\boldsymbol{\theta}}_1\overset{p}{\to}\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}_2\overset{p}{\to}\boldsymbol{\theta}\)</span> for some asymptotically efficient estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}_1\)</span>, the <span style="color:red"><strong><em>Hausman statistic</em></strong></span> is defined as <span class="math display">\[H = (\hat{\boldsymbol{\theta}}_2 - \hat{\boldsymbol{\theta}}_1)'\left[\widehat{\text{Avar}}(\hat{\boldsymbol{\theta}}_2) - \widehat{\text{Avar}}(\hat{\boldsymbol{\theta}}_1)\right]^+ (\hat{\boldsymbol{\theta}}_2 - \hat{\boldsymbol{\theta}}_1).\]</span> Under <span class="math inline">\(H_0\)</span>, <span class="math display">\[ H\sim \chi_q^2\]</span> where <span class="math inline">\(q = \text{rank}\left[\text{Avar}\left( \hat{\boldsymbol{\theta}}_2\right) - \text{Avar}\left( \hat{\boldsymbol{\theta}}_1\right)\right]\)</span>. The test associated with this statistic is known as the <span style="color:red"><strong><em>Hausman specification test</em></strong></span>.</p>
</div>
<p>The Hausman test is far more general that testing for endogeneity, it just happens that testing for endogeneity is a great application of it. In this particular application, there is a much more convenient formulation that allows us to bypass the pseudo-inverse complication. We’ll begin by partitioning our regressors into the exogenous and endogenous regressors, i.e <span class="math inline">\(\mathbf{X}= [\mathbf{X}_\text{exo},\mathbf{X}_\text{end}]\)</span>. The <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> estimators can also be partitioned accordingly. <span class="math display">\[\begin{align*}
\hat\beta_\text{OLS} &amp; = \begin{bmatrix}\hat\beta_\text{OLS,exo}  \\ \hat\beta_\text{OLS,end}\end{bmatrix}\\
\hat\beta_\text{IV} &amp; = \begin{bmatrix}\hat\beta_\text{IV,exo}  \\ \hat\beta_\text{IV,end}\end{bmatrix}
\end{align*}\]</span> With some clever algebra, we can show that <span class="math inline">\((\hat\beta_\text{OLS,exo} - \hat\beta_\text{IV,exo})\)</span> is a linear function of <span class="math inline">\((\hat\beta_\text{IV,end} - \hat\beta_\text{OLS,end})\)</span>. The estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> solves the first order condition associated with minimizing the sum of squared residuals: <span class="math display">\[\mathbb{X}'\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} = \mathbb{X}'\mathbf{Y}.\]</span> If we partition this first order condition according to <span class="math inline">\(\mathbf{X}= [\mathbf{X}_\text{exo},\mathbf{X}_\text{end}]\)</span>, we have</p>
<p><span class="math display">\[\begin{align*}
&amp;\begin{bmatrix} \mathbb{X}_\text{exo} &amp; \mathbb{X}_\text{end}\end{bmatrix}'\begin{bmatrix} \mathbb{X}_\text{exo} &amp; \mathbb{X}_\text{end}\end{bmatrix}\begin{bmatrix} \hat\beta_\text{OLS,exo} \\ \hat\beta_\text{OLS,end}\end{bmatrix} = \begin{bmatrix} \mathbb{X}_\text{exo} &amp; \mathbb{X}_\text{end}\end{bmatrix}'\mathbf{Y}\\
\implies &amp; \begin{bmatrix} \mathbb{X}_\text{exo}' \\ \mathbb{X}_\text{end}' \end{bmatrix} \begin{bmatrix} \mathbb{X}_\text{exo} &amp; \mathbb{X}_\text{end} \end{bmatrix} \begin{bmatrix} \hat\beta_\text{OLS,exo} \\ \hat\beta_\text{OLS,end}\end{bmatrix} = \begin{bmatrix} \mathbb{X}_\text{exo}' \\ \mathbb{X}_\text{end}'\end{bmatrix} \mathbf{Y}\\
\implies &amp; \begin{bmatrix} \mathbb{X}_\text{exo}'\mathbb{X}_\text{exo} \hat\beta_\text{OLS,exo} + \mathbb{X}_\text{exo}' \mathbb{X}_\text{end}\hat\beta_\text{OLS,end} \\
\mathbb{X}_\text{end}'\mathbb{X}_\text{exo}\hat\beta_\text{OLS,exo} + \mathbb{X}_\text{end}'\mathbb{X}_\text{end}\hat\beta_\text{OLS,end} \end{bmatrix} = \begin{bmatrix} \mathbb{X}_\text{exo}'\mathbf{Y}\\ \mathbb{X}_\text{end}'\mathbf{Y}\end{bmatrix}\\
\implies &amp; \mathbb{X}_\text{exo}'\mathbb{X}_\text{exo} \hat\beta_\text{OLS,exo} + \mathbb{X}_\text{exo}' \mathbb{X}_\text{end}\hat\beta_\text{OLS,end} = \mathbb{X}_\text{exo}'\mathbf{Y}&amp; (\text{First Row})\\
\implies &amp; \mathbb{X}_\text{exo}'\mathbb{X}_\text{exo} \hat\beta_\text{OLS,exo}  = \mathbb{X}_\text{exo}'\left(\mathbf{Y}-  \mathbb{X}_\text{end}\hat\beta_\text{OLS,end}\right)
\end{align*}\]</span></p>
<p>Solving this for <span class="math inline">\(\hat\beta_\text{OLS,exo}\)</span> gives</p>
<p><span id="eq-63"><span class="math display">\[ \hat\beta_\text{OLS,exo} = \left(\mathbb{X}_\text{exo}'\mathbb{X}_\text{exo}\right)^{-1}\mathbb{X}_\text{exo}'\left(\mathbf{Y}-  \mathbb{X}_\text{end}\hat\beta_\text{OLS,end}\right) . \tag{6.1}\]</span></span></p>
<p>We can repeat these calculations for <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span>, and conclude</p>
<p><span id="eq-64"><span class="math display">\[ \hat\beta_\text{OLS,exo} = \left(\mathbb{X}_\text{exo}'\mathbb{X}_\text{exo}\right)^{-1}\mathbb{X}_\text{exo}'\left(\mathbf{Y}-  \mathbb{X}_\text{end}\hat\beta_\text{IV,end}\right) . \tag{6.2}\]</span></span></p>
<p>If we subtract <a href="#eq-64">Equation&nbsp;<span>6.2</span></a> from <a href="#eq-63">Equation&nbsp;<span>6.1</span></a>, we have</p>
<p><span class="math display">\[ \hat\beta_\text{OLS,exo} - \hat\beta_\text{IV,exo} = \left(\mathbb{X}_\text{exo}'\mathbb{X}_\text{exo}\right)^{-1}\mathbb{X}_\text{exo}'\mathbb{X}_\text{end}\left(\hat\beta_\text{IV,end} - \hat\beta_\text{OLS,end}\right).\]</span></p>
<p>This equality means that if <span class="math inline">\(\mathop{\mathrm{plim}}(\hat{\boldsymbol\beta}_\text{OLS} - \hat{\boldsymbol\beta}_\text{IV} )\)</span> (in which case the difference in the exogenous components approaches zero), then the difference in the endogenous portions approach zero as well. In short, we lose nothing from focusing on the difference <span class="math inline">\(\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end}\)</span>, which means the variance term which standardizes the Hausman statistic has full rank.</p>
<p><span class="math display">\[ H = (\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat\beta_\text{IV,end}) - \widehat{\text{Avar}}(\hat\beta_\text{OLS,end})\right]^{-1} (\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end})\]</span></p>
<div id="cor-hausend" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 6.2 (Hausman Test for Endogeneity) </strong></span>Given the hypothesis <span class="math inline">\(H_0:\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span> where regressors are partitioned as <span class="math inline">\(\mathbf{X}= [\mathbf{X}_\text{exo},\mathbf{X}_\text{end}]\)</span>, the <span style="color:red"><strong><em>Hausman statistic</em></strong></span> simplifies to <span class="math display">\[H = (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end}) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})\right]^{-1} (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end}).\]</span> Under <span class="math inline">\(H_0\)</span>, <span class="math display">\[ H\sim \chi_q^2\]</span> where <span class="math inline">\(q = \dim(\mathbf{X}_\text{end})\)</span>. I will call the test associated with this statistic is known as the <span style="color:red"><strong><em>Hausman test for endogeneity</em></strong></span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.15 </strong></span>Suppose <span class="math inline">\((X, Z, \varepsilon) \sim N(\boldsymbol \mu,\boldsymbol{\Sigma})\)</span> where <span class="math display">\[\begin{align*}
\boldsymbol \mu &amp; = [10,10,0]',\\
\boldsymbol{\Sigma}&amp; = \begin{bmatrix}20 &amp; 5 &amp; 0\\5&amp;20&amp;0\\0&amp;0&amp;1 \end{bmatrix},
\end{align*}\]</span> and <span class="math inline">\(Y = 1 + 3X + \varepsilon\)</span>. We have <span class="math inline">\(\text{Cov}\left(X,\varepsilon\right) = 0\)</span>, so <span class="math inline">\(\text{E}\left[X\varepsilon\right] = 0\)</span> and our model meets the assumptions of the classical linear model (<span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{LM}\subset\mathcal P_\text{IV}\)</span>). If we draw a sample of size <span class="math inline">\(n = 1,00\)</span>, we can test <span class="math inline">\(H_0:\text{E}\left[X\varepsilon\right] = 0\)</span> using the Hausman statistic calculated using only <span class="math inline">\(\beta_2\)</span> (the slope coefficient). If we repeat this simulation many times, then the distribution of our test statistic should approach <span class="math inline">\(\chi_1^2\)</span>, as <span class="math inline">\(H_0\)</span> is true for our model.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-26_68767d5f418a9227a0eb66f497ceb1b3">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>hausman_stat <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, Z, end_col){</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate OLS and variance</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  OLS <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> OLS)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>  S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  var_OLS <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calculate IV and variance</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>  IV <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> IV)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>  S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>  var_IV <span class="ot">&lt;-</span> (S2) <span class="sc">*</span>  <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> Z)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># construct the statistic using only the endogenous columns given by end_col</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>  i <span class="ot">&lt;-</span> end_col</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>  (OLS[i] <span class="sc">-</span> IV[i])<span class="sc">*</span><span class="fu">solve</span>(var_IV[i,i] <span class="sc">-</span> var_OLS[i,i])<span class="sc">*</span>(OLS[i] <span class="sc">-</span> IV[i])</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">10000</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>) {</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">0</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>  sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>  Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>  store[j] <span class="ot">&lt;-</span> <span class="fu">hausman_stat</span>(y, X, Z, <span class="dv">2</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s plot the simulated test statistics.</p>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="endog_cache/html/unnamed-chunk-27_5606ddc418e2e794d7de663454bd5384">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(store) <span class="sc">%&gt;%</span> </span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(store)) <span class="sc">+</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span>..density..), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">binwidth =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dchisq,  <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">1</span>), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="1056"></p>
<p></p><figcaption class="figure-caption">Histogram of the Hausman statistic calculated for 10,000 simulations with limiting distribution overlaid</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>Another means of testing for exogeneity is due to <span class="citation" data-cites="wu1973alternative">Wu (<a href="references.html#ref-wu1973alternative" role="doc-biblioref">1973</a>)</span> and <span class="citation" data-cites="durbin1954errors">Durbin (<a href="references.html#ref-durbin1954errors" role="doc-biblioref">1954</a>)</span>. Begin by performing the first step of 2SLS on the possible endogenous regressors. <span class="math display">\[ \mathbb{X}_\text{end} =  \mathbb{Z}\boldsymbol \delta + \mathbb V\]</span> The residuals now form a matrix, because we have multiple dependent variables <span class="math inline">\(\mathbf{X}_\text{end}\)</span>. Using the estimated projection coefficient <span class="math inline">\(\hat{\boldsymbol \delta}\)</span>, we calculated the residual values <span class="math inline">\(\hat{\mathbb V}\)</span>. Finally we augment the original linear model by including <span class="math inline">\(\hat{\mathbb V}\)</span>.</p>
<p><span class="math display">\[\mathbf{y}= \mathbb{X}\boldsymbol{\beta}+ \hat{\mathbb V}\boldsymbol\gamma + \boldsymbol{\varepsilon}^* = \begin{bmatrix} \mathbb{X}_\text{exo} &amp; \mathbb{X}_\text{end}\end{bmatrix}\begin{bmatrix} \boldsymbol{\beta}_\text{exo} \\ \boldsymbol{\beta}_\text{end}\end{bmatrix} + \underbrace{\hat{\mathbb V}\gamma + \boldsymbol {\boldsymbol{\varepsilon}}^*}_{\boldsymbol{\varepsilon}}\]</span> Consider the hypothesis <span class="math inline">\(H_0:\boldsymbol\gamma = \mathbf{0}\)</span>, and how that may related to whether <span class="math inline">\(\mathbb{X}_\text{end}\)</span> is endogenous or exogenous. When we perform the first step of this process (regressing <span class="math inline">\(\mathbb{X}_\text{end}\)</span> on <span class="math inline">\(\mathbb{Z}\)</span>) we break variation in <span class="math inline">\(\mathbb{X}_\text{end}\)</span> into two parts: the exogenous portion explained through <span class="math inline">\(\mathbb{Z}\)</span>,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, and unexplained part <span class="math inline">\(\boldsymbol \nu\)</span> which we capture through <span class="math inline">\(\hat{\mathbb V}\)</span>. If <span class="math inline">\(\boldsymbol \gamma \neq \mathbf{0}\)</span>, then the structural error <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> includes the unexplained part of the variation in <span class="math inline">\(\mathbf{X}_\text{end}\)</span>, which is another way of saying <span class="math inline">\(\text{Cov}\left(\mathbf{X}_\text{end}, \varepsilon\right)\neq 0\)</span>.</p>
<p>So do we use the Hausman test, or this new test which uses artificial regressions? Well it turns out, we don’t need to make a choice because they’re equivalent if we are attentive about which residuals we use to calculate asymptotic variance! It also turns out that we could perform the artificial regression test using the fitted values <span class="math inline">\(\hat{\mathbb{X}}_\text{end}\)</span> instead of <span class="math inline">\(\hat{\mathbb V}\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.6 (Durbin-Wu-Hausman Test) </strong></span>Suppose we want to test <span class="math inline">\(H_0:P_{\boldsymbol{\beta}, \sigma^2}\in\mathcal P_\text{LM}\)</span> (i.e <span class="math inline">\(H_0: \mathbf{X}\)</span> exogenous) for <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2}\in \mathcal P_\text{IV}\)</span>. The following test statistics are equivalent:</p>
<ol type="1">
<li>The Wald statistic associated with the null hypothesis <span class="math inline">\(H_0: \boldsymbol\gamma = \mathbf{0}\)</span> calculated using <span class="math inline">\(\hat{\boldsymbol\gamma}_\text{OLS}\)</span>, where <span class="math inline">\(\boldsymbol\gamma\)</span> is from the modified regression <span class="math inline">\(\mathbf{y}= \mathbb{X}\boldsymbol{\beta}+ \hat{\mathbf v}\boldsymbol\gamma + \boldsymbol{\varepsilon}^*\)</span>, and <span class="math inline">\(\hat{\mathbf v}\)</span> are the residuals associated with the first step projection <span class="math inline">\(\mathbb{X}_\text{end} = \mathbb{Z}\boldsymbol \delta + \boldsymbol \nu\)</span>. <span class="math display">\[W_{\boldsymbol\gamma}= \hat{\boldsymbol\gamma}_\text{OLS}' \left[\widehat{\text{Avar}}(\hat{\boldsymbol\gamma}_\text{OLS} )\right]^{-1}\hat{\boldsymbol\gamma}_\text{OLS}\]</span></li>
<li>The Wald statistic associated with the null hypothesis <span class="math inline">\(H_0: \boldsymbol\eta = \mathbf{0}\)</span> calculated using <span class="math inline">\(\hat{\boldsymbol\eta}_\text{OLS}\)</span>, where <span class="math inline">\(\boldsymbol\gamma\)</span> is from the modified regression <span class="math inline">\(\mathbf{y}= \mathbb{X}\boldsymbol{\beta}+ \hat{\mathbb{X}}_\text{end}\boldsymbol\eta + \boldsymbol{\varepsilon}^*\)</span>, and <span class="math inline">\(\hat{\mathbb{X}}_\text{end}\)</span> are the fitted values associated with the first step projection <span class="math inline">\(\mathbb{X}_\text{end} = \mathbb{Z}\boldsymbol \delta + \boldsymbol \nu\)</span>. <span class="math display">\[W_{\boldsymbol\eta}= \hat{\boldsymbol\eta}_\text{OLS}' \left[\widehat{\text{Avar}}(\hat{\boldsymbol\eta}_\text{OLS} )\right]^{-1}\hat{\boldsymbol\eta}_\text{OLS}\]</span></li>
<li>The Hausman statistics from <a href="#cor-hausend">Corollary&nbsp;<span>6.2</span></a>, <span class="math display">\[H = (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end}) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})\right]^{-1} (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end}),\]</span> when <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end})\)</span> and <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})\)</span> are calculated using <span class="math inline">\(S^2\)</span> from the artificial regression <span class="math inline">\(\mathbf{y}= \mathbb{X}\boldsymbol{\beta}+ \hat{\mathbf v}\boldsymbol\eta + \boldsymbol{\varepsilon}^*\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<p><span class="math inline">\((W_{\boldsymbol\gamma} = W_{\boldsymbol\eta})\)</span> Recall that we think of <span class="math inline">\(\hat{\mathbf v}\)</span> and <span class="math inline">\(\hat{\mathbb{X}}_\text{end}\)</span> in terms of linear projection. We have</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbb{X}}_\text{end} &amp; = \mathbb P_{\mathbb{Z}}\mathbb{X}_\text{end} &amp; (\mathbb P_{\mathbb{Z}} = \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}')\\
\hat{\mathbb V} &amp; =  \mathbb M_{\mathbb{Z}}\mathbb{X}_\text{end} &amp; (\mathbb M_{\mathbb{Z}} = \mathbb I - \mathbb P_{\mathbb{Z}})
\end{align*}\]</span></p>
<p><span class="math inline">\((W_{\boldsymbol\gamma} = H)\)</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.16 </strong></span>test</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-28_dc010f06363b4d8f468cf85981233fb4">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">0</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">5</span>,<span class="dv">1</span>, <span class="dv">5</span>,<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">3</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, mu, Sigma)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> sample[,<span class="dv">1</span>]</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> sample[,<span class="dv">2</span>]</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> sample[,<span class="dv">3</span>]</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> e</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, z)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Method 1</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>reg_1<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(x <span class="sc">~</span> z)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>v_hat <span class="ot">&lt;-</span> reg_1<span class="fl">.1</span><span class="sc">$</span>residuals</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>reg_1<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> v_hat)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_1<span class="fl">.2</span>)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate  Std. Error    t value     Pr(&gt;|t|)
(Intercept) 0.94838236 0.085203007  11.130856 1.307146e-28
x           3.00233437 0.008444734 355.527411 0.000000e+00
v_hat       0.04992151 0.008730578   5.718007 1.108732e-08</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-29_a4e2cad5127e454c9e66c3670bf74060">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_1<span class="fl">.2</span>)<span class="sc">$</span>coefficients[<span class="dv">3</span>,<span class="dv">3</span>]<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 32.6956</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-30_56ed47c9e2063337411fad5c2696b06f">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Method 2</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>reg_2<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(x <span class="sc">~</span> z)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>x_hat <span class="ot">&lt;-</span> <span class="fu">fitted.values</span>(reg_2<span class="fl">.1</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>reg_2<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> x_hat)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_2<span class="fl">.2</span>)<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               Estimate  Std. Error     t value     Pr(&gt;|t|)
(Intercept)  0.94838236 0.085203007   11.130856 1.307146e-28
x            3.05225587 0.002215732 1377.538278 0.000000e+00
x_hat       -0.04992151 0.008730578   -5.718007 1.108732e-08</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-31_12ec6a78497a3d53e86c559e501b7fcd">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg_2<span class="fl">.2</span>)<span class="sc">$</span>coefficients[<span class="dv">3</span>,<span class="dv">3</span>]<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 32.6956</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-32_504954a97d6cc51ba29af6e0eb4302e1">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">sum</span>(reg_2<span class="fl">.2</span><span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>), </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(reg_1<span class="fl">.2</span><span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 9341.802 9341.802</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-33_4eef3a2b3754a2d8a2f973c24f45d2e4">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Method 3</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>S2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(reg_2<span class="fl">.2</span><span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="sc">-</span>K)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>var_OLS <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>IV <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(Z) <span class="sc">%*%</span> y</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>var_IV <span class="ot">&lt;-</span> (S2) <span class="sc">*</span>  <span class="fu">solve</span>(<span class="fu">t</span>(Z) <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">t</span>(Z) <span class="sc">%*%</span> Z) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> Z)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>(OLS[<span class="dv">2</span>] <span class="sc">-</span> IV[<span class="dv">2</span>])<span class="sc">*</span><span class="fu">solve</span>(var_IV[<span class="dv">2</span>,<span class="dv">2</span>] <span class="sc">-</span> var_OLS[<span class="dv">2</span>,<span class="dv">2</span>])<span class="sc">*</span>(OLS[<span class="dv">2</span>] <span class="sc">-</span> IV[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         [,1]
[1,] 32.69887</code></pre>
</div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-34_82af4175e931458baaed2400da6bd145">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ivreg<span class="sc">::</span><span class="fu">ivreg</span>(y <span class="sc">~</span> x <span class="sc">|</span> z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
ivreg::ivreg(formula = y ~ x | z)

Residuals:
     Min       1Q   Median       3Q      Max 
-3.28835 -0.66660 -0.01466  0.67473  4.06233 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.948382   0.087335   10.86   &lt;2e-16 ***
x           3.002334   0.008656  346.85   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9909 on 9998 degrees of freedom
Multiple R-Squared: 0.9948, Adjusted R-squared: 0.9948 
Wald test: 1.203e+05 on 1 and 9998 DF,  p-value: &lt; 2.2e-16 </code></pre>
</div>
</div>
</div>
</section>
<section id="sarganhansen-test" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="sarganhansen-test"><span class="header-section-number">6.5.2</span> Sargan–Hansen Test</h3>
</section>
</section>
<section id="examplereplication" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="examplereplication"><span class="header-section-number">6.6</span> Example/Replication</h2>
<p>Let’s replicate <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span>, a <em>classic</em> paper which has provided a textbook example of instrumental variables. To make sure our results are correct, we can compare them to the tables in the original paper which is found <a href="https://www.nber.org/system/files/working_papers/w4483/w4483.pdf">here</a>. The data is available on David Card’s <a href="https://davidcard.berkeley.edu/data_sets.html">website</a>.</p>
<p><span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span> is concerned with estimating the returns to schooling with respect to wages. We tend to think that better educated workers earn higher wages, but to what extent does this relationship hold? Denote years of schooling as <span class="math inline">\(S_i\)</span>, log wages as <span class="math inline">\(Y_i\)</span>, and <span class="math inline">\(\mathbf{X}_i\)</span> as a collection of attributes associated with a worker. The linear model of interest is <span class="math display">\[ Y_i = \mathbf{X}_i\boldsymbol\alpha + S_i\beta + u_i,\]</span> where <span class="math inline">\(u_i\)</span> are unobserved components which affect earnings. We’re assuming that <span class="math inline">\(\text{E}\left[\mathbf{X}_iu_i\right] = 0\)</span> for all <span class="math inline">\(i\)</span> (<span class="math inline">\(\mathbf{X}_i\)</span> is exogenous), but we suspect that <span class="math inline">\(\text{E}\left[S_iu_i\right] = 0\)</span> (see Example @ref(exm:ex1)). The parameter <span class="math inline">\(\beta\)</span> is the returns of education on earnings, and is what we’re primarly interested in. Let’s just ignore this endogeneity for now and attempt to estimate <span class="math inline">\(\beta\)</span> with OLS. We can do this with data from the National Longitudinal Survey of Young Men (NLSYM) conducted in 1976. This survey contains information about agents’ wages in 1976, along with demographic information about family background, residence in 1976, and past residence (in 1966). The details of this survey, in particular how respondents were sampled, can be found in <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span>.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-35_037f898fe381e32a6164604d5fc4cd7e">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>card_95 <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">"data/nls.dat"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>.zip</code> file containing the data also contains a <code>.sas</code> file which processes and cleans the raw data. Each step is readily executed in R:<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-36_af9882696af3c6d19f98e061b54cd647">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>card_95 <span class="ot">&lt;-</span> card_95 <span class="sc">%&gt;%</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#assign the appropriate names to each variable</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">id =</span> V1,</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">nearc2 =</span> V2,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">nearc4 =</span> V3,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">nearc4a =</span> V4,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">nearc4b =</span> V5,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">ed76 =</span> V6,</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">ed66 =</span> V7,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">age76 =</span> V8,</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">daded =</span> V9,</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">nodaded =</span> V10,</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">momed =</span> V11,</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">nomomed =</span> V12,</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">weight =</span> V13,</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">momdad14  =</span> V14,</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">sinmom14  =</span> V15,</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">step14 =</span> V16,</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg661 =</span> V17,</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg662 =</span> V18,</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg663 =</span> V19,</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg664 =</span> V20,</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg665 =</span> V21,</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg666 =</span> V22,</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg667 =</span> V23,</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg668 =</span> V24,</span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg669 =</span> V25,</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">south66 =</span> V26,</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">work76 =</span> V27,</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">work78 =</span> V28,</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">lwage76 =</span> V29,</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>    <span class="at">lwage78 =</span> V30,</span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>    <span class="at">famed =</span> V31,</span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">black =</span> V32,</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">smsa76r =</span> V33,</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">smsa78r =</span> V34,</span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">south76 =</span> V35,</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg78r =</span> V36,</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>    <span class="at">reg80r =</span> V37,</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>    <span class="at">smsa66r =</span> V38,</span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>    <span class="at">wage76 =</span> V39,</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>    <span class="at">wage78 =</span> V40,</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>    <span class="at">wage80 =</span> V41,</span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">noint78 =</span> V42,</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>    <span class="at">noint80 =</span> V43,</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">enroll76 =</span> V44,</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>    <span class="at">enroll78 =</span> V45,</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>    <span class="at">enroll80 =</span> V46,</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>    <span class="at">kww =</span> V47,</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>    <span class="at">iq =</span> V48,</span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a>    <span class="at">marsta76 =</span> V49,</span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a>    <span class="at">marsta78 =</span> V50,</span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">marsta80 =</span> V51,</span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a>    <span class="at">libcrd1 =</span> V52</span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># missing values in SAS are ".", replace those with NAs</span></span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span><span class="fu">replace</span>(., . <span class="sc">==</span>  <span class="st">"."</span> , <span class="cn">NA</span>)),</span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># any column that had a "." was loaded as a string, convert them</span></span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true" tabindex="-1"></a>    <span class="fu">across</span>(<span class="fu">where</span>(is.character), as.numeric),</span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">#create variables corresponding to work experience</span></span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true" tabindex="-1"></a>    <span class="at">exp76 =</span> age76 <span class="sc">-</span> ed76 <span class="sc">-</span> <span class="dv">6</span>,</span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">exp76_sq =</span> exp76<span class="sc">*</span>exp76,</span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true" tabindex="-1"></a>    <span class="at">exp76_sq_100 =</span> (exp76<span class="sc">*</span>exp76)<span class="sc">/</span><span class="dv">100</span>,</span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true" tabindex="-1"></a>    <span class="co">#create a series of indicators for parents' level of education</span></span>
<span id="cb51-67"><a href="#cb51-67" aria-hidden="true" tabindex="-1"></a>    <span class="at">f1 =</span> (momed <span class="sc">&gt;</span> <span class="dv">12</span> <span class="sc">&amp;</span> daded <span class="sc">&gt;</span> <span class="dv">12</span>),</span>
<span id="cb51-68"><a href="#cb51-68" aria-hidden="true" tabindex="-1"></a>    <span class="at">f2 =</span> (momed <span class="sc">&gt;=</span> <span class="dv">12</span> <span class="sc">&amp;</span> daded <span class="sc">&gt;=</span> <span class="dv">12</span> <span class="sc">&amp;</span> (momed <span class="sc">!=</span> <span class="dv">12</span> <span class="sc">|</span> daded <span class="sc">!=</span> <span class="dv">12</span>)),</span>
<span id="cb51-69"><a href="#cb51-69" aria-hidden="true" tabindex="-1"></a>    <span class="at">f3 =</span> (momed <span class="sc">==</span> <span class="dv">12</span> <span class="sc">&amp;</span> daded <span class="sc">==</span> <span class="dv">12</span>),</span>
<span id="cb51-70"><a href="#cb51-70" aria-hidden="true" tabindex="-1"></a>    <span class="at">f4 =</span> (momed <span class="sc">&gt;=</span> <span class="dv">12</span> <span class="sc">&amp;</span> nodaded <span class="sc">==</span> <span class="dv">1</span>),</span>
<span id="cb51-71"><a href="#cb51-71" aria-hidden="true" tabindex="-1"></a>    <span class="at">f5 =</span> (daded <span class="sc">&gt;=</span> <span class="dv">12</span> <span class="sc">&amp;</span> momed <span class="sc">&lt;</span> <span class="dv">12</span>),</span>
<span id="cb51-72"><a href="#cb51-72" aria-hidden="true" tabindex="-1"></a>    <span class="at">f6 =</span> (momed <span class="sc">&gt;=</span> <span class="dv">12</span> <span class="sc">&amp;</span> nodaded <span class="sc">==</span> <span class="dv">0</span>),</span>
<span id="cb51-73"><a href="#cb51-73" aria-hidden="true" tabindex="-1"></a>    <span class="at">f7 =</span> (momed <span class="sc">&gt;=</span> <span class="dv">9</span> <span class="sc">&amp;</span> daded <span class="sc">&gt;=</span> <span class="dv">9</span>),</span>
<span id="cb51-74"><a href="#cb51-74" aria-hidden="true" tabindex="-1"></a>    <span class="at">f8 =</span> (nomomed <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;</span> nodaded <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb51-75"><a href="#cb51-75" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb51-76"><a href="#cb51-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-77"><a href="#cb51-77" aria-hidden="true" tabindex="-1"></a><span class="co">#Gather mutually exclusive region66 indicators into categorical </span></span>
<span id="cb51-78"><a href="#cb51-78" aria-hidden="true" tabindex="-1"></a>card_95 <span class="ot">&lt;-</span> card_95 <span class="sc">%&gt;%</span> </span>
<span id="cb51-79"><a href="#cb51-79" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(id, <span class="fu">contains</span>(<span class="st">"reg66"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb51-80"><a href="#cb51-80" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(region66, count, <span class="sc">-</span>id) <span class="sc">%&gt;%</span> </span>
<span id="cb51-81"><a href="#cb51-81" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(count <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb51-82"><a href="#cb51-82" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb51-83"><a href="#cb51-83" aria-hidden="true" tabindex="-1"></a>    <span class="at">region66 =</span> <span class="fu">str_remove</span>(region66,<span class="st">"reg66"</span>),</span>
<span id="cb51-84"><a href="#cb51-84" aria-hidden="true" tabindex="-1"></a>    <span class="at">region66 =</span> <span class="fu">as.factor</span>(region66)</span>
<span id="cb51-85"><a href="#cb51-85" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb51-86"><a href="#cb51-86" aria-hidden="true" tabindex="-1"></a>  <span class="fu">right_join</span>(card_95)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The dependent variable of interest if <span class="math inline">\(\log wage_i\)</span> (<code>lwage76</code>). Our baseline linear model is <span class="math display">\[ \log( wage_i) = \alpha_0 + \beta\cdot educ_i + \alpha_1\cdot exper_i + \frac{\alpha_2}{100}\cdot exper_i^2 + \alpha_3 \cdot black_i + \alpha_4\cdot south_i + \alpha_5\cdot SMSA_i + \varepsilon_i,\]</span> where <span class="math inline">\(educ_i\)</span> (<code>ed76</code>) is level of education, <span class="math inline">\(exper_i\)</span> (<code>exp76</code>) is amount of work experiance, <span class="math inline">\(black_i\)</span> (<code>black</code>) indicated if respondent <span class="math inline">\(i\)</span> is Black, <span class="math inline">\(south_i\)</span> (<code>south76</code>) indicated whether the respondent lives in the South (in 1976), and <span class="math inline">\(SMSA_i\)</span> (<code>smsa76r</code>) indicates whether a respondent lives in the South and in a metropolitan area.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Along with estimating this model, we will estimate similar models which include various controls for family education, family structure, and where respondents lived in 1966.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-37_0321d24ab983fa5b01e05334db50d233">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>linear_model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r, <span class="at">data =</span> card_95)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>linear_model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66, <span class="at">data =</span> card_95)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>linear_model3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66 <span class="sc">+</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>                      daded <span class="sc">+</span> momed <span class="sc">+</span> nodaded <span class="sc">+</span> nomomed, <span class="at">data =</span> card_95)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>linear_model4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>                      region66 <span class="sc">+</span> daded <span class="sc">+</span> momed <span class="sc">+</span> nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span> </span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>                      f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8, <span class="at">data =</span> card_95)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>linear_model5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>                     region66<span class="sc">+</span> daded <span class="sc">+</span> momed <span class="sc">+</span> nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span> </span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>                      f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8 <span class="sc">+</span> momdad14 <span class="sc">+</span> sinmom14, <span class="at">data =</span> card_95)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our results should match Table 2 of <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span>. The <code>stargazer</code> package gives us a flexible means of tabulating regression results.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-38_8bbebeeec42927cabfa70abc72110f71">

</div>
<p>For each model <span class="math inline">\(\hat\beta_\text{OLS}\in[0.073,0.075]\)</span>, meaning that an additional year of education corresponds to roughly a 7.4% increase in earnings. We should be suspicious of these results though, as it’s likely the case that <span class="math inline">\(educ_i\)</span> is endogenous, so we need some instrument(s) for <span class="math inline">\(educ_i\)</span>. <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span> makes the case for using a variable which indicates whether a respondent grew up in a labor market with an accredited 4-year college, <span class="math inline">\(near_\ college_i\)</span> (<code>nearc4</code>). In other words, the respondent lived near a college in the year 1966. This proximity to a college has no baring on earnings (in theory), but it is likely correlated with <span class="math inline">\(educ_i\)</span>, as the cost of higher education is lower for those who have the option to live at home while attending college. Let’s use <span class="math inline">\(near_\ college_i\)</span> to calculate <span class="math inline">\(\hat\beta_\text{IV}\)</span>. We can do this “by hand” by estimating the following reduced form equations via OLS:<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p><span class="math display">\[\begin{align*}
educ_i &amp;= \gamma_0 + \gamma_1 \cdot near_\ college_i + \gamma_2\cdot exper_i + \frac{\gamma_3}{100}\cdot exper_i^2 + \gamma_4 \cdot black_i + \gamma_5\cdot region_i + \gamma_6\cdot SMSA_i + \cdots + u_i\\
\log( wage_i) &amp;= \delta_0 + \delta_1 \cdot near_\ college_i + \delta_2\cdot exper_i + \frac{\delta_3}{100}\cdot exper_i^2 + \delta_4 \cdot black_i + \delta_5\cdot region_i + \delta_6\cdot SMSA_i + \cdots + \nu_i\\
\end{align*}\]</span></p>
<p>We could also directly estimate the structural model via <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> and not worry about messing up the standard errors. If we have done everything correctly we should see: <span class="math display">\[ \frac{\hat \delta_{\text{OLS},1}}{\hat \gamma_{\text{OLS},1}} = \hat\beta_\text{IV}.\]</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="0.5" data-hash="endog_cache/html/unnamed-chunk-39_5dec20c634be8caa02a49617171417f0">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/card_dag.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We will estimate the models with and without family controls, and restrict our attention to respondents where <code>lwage76</code> is not missing.</p>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-40_d44b38daacaab70d32eb4bbc85f42fde">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>card_95 <span class="ot">&lt;-</span> card_95 <span class="sc">%&gt;%</span> </span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(lwage76))</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Without family controls </span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>reduced_form1_stage1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ed76 <span class="sc">~</span> nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66, <span class="at">data =</span> card_95)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>reduced_form1_stage2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66, <span class="at">data =</span> card_95)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>IV_model1 <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66 <span class="sc">|</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>                     nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66, <span class="at">data =</span> card_95)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="co">#With family controls</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>reduced_form2_stage1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(ed76 <span class="sc">~</span> nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> </span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>                             region66 <span class="sc">+</span> daded <span class="sc">+</span> momed <span class="sc">+</span> nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span> </span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>                             f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8 <span class="sc">+</span> momdad14 <span class="sc">+</span> sinmom14, <span class="at">data =</span> card_95)</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>reduced_form2_stage2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lwage76 <span class="sc">~</span> nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66 <span class="sc">+</span> </span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>                             daded <span class="sc">+</span> momed <span class="sc">+</span> nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span> f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8 <span class="sc">+</span> </span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>                             momdad14 <span class="sc">+</span> sinmom14, <span class="at">data =</span> card_95)</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>IV_model2 <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(lwage76 <span class="sc">~</span> ed76 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66 <span class="sc">+</span> daded <span class="sc">+</span> momed <span class="sc">+</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>                     nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span>  f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8 <span class="sc">+</span> momdad14 <span class="sc">+</span> sinmom14 <span class="sc">|</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>                     nearc4 <span class="sc">+</span> exp76 <span class="sc">+</span> exp76_sq_100 <span class="sc">+</span> black <span class="sc">+</span> south76 <span class="sc">+</span> smsa76r <span class="sc">+</span> smsa66r <span class="sc">+</span> region66 <span class="sc">+</span> daded <span class="sc">+</span> momed <span class="sc">+</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>                     nodaded <span class="sc">+</span> nomomed <span class="sc">+</span> f1 <span class="sc">+</span> f2 <span class="sc">+</span> f3 <span class="sc">+</span> f4 <span class="sc">+</span> f5 <span class="sc">+</span> f6 <span class="sc">+</span> f7 <span class="sc">+</span> f8 <span class="sc">+</span> momdad14 <span class="sc">+</span> sinmom14, <span class="at">data =</span> card_95)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="endog_cache/html/unnamed-chunk-41_7ef6777819efc4409507bde4c3801941">

</div>
<p>Compare this to panel A of Table 3 in <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span>. Note that the ratio of our reduced form estimates do equal our IV estimates. Between our original OLS estimates (of the structural model), and our IV estimates, we have seven estimates for <span class="math inline">\(\beta\)</span>. We can plot these along with the corresponding 95% confidence intervals.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="endog_cache/html/unnamed-chunk-42_df5f7422917fc4442edb6b94c4f97923">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="endog_files/figure-html/unnamed-chunk-42-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>While our IV estimates are larger, all of our OLS estimates fall within the 95% confidence intervals for the IV estimates. As such, we’re not able to conclude that our IV estimates are significantly larger than our OLS estimates.</p>
</section>
<section id="further-reading" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">6.7</span> Further Reading</h2>
<p><strong><em>Technical treatment of IV/2SLS</em></strong>: Chapter 5 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 8 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Chapter 12 of <span class="citation" data-cites="hansen2022econometrics">Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span></p>
<p><strong><em>Testing for Endogeneity</em></strong>: Section 8.7 of <span class="citation" data-cites="davidson2004econometric">Davidson, MacKinnon, et al. (<a href="references.html#ref-davidson2004econometric" role="doc-biblioref">2004</a>)</span>, <span class="citation" data-cites="ruud1984tests">Ruud (<a href="references.html#ref-ruud1984tests" role="doc-biblioref">1984</a>)</span></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-cameron2005microeconometrics" class="csl-entry" role="doc-biblioentry">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-card1995using" class="csl-entry" role="doc-biblioentry">
Card, David. 1995. <span>“Using Geographic Variation in College Proximity to Estimate the Return to Schooling.”</span> National Bureau of Economic Research Cambridge, Mass., USA.
</div>
<div id="ref-davidson2004econometric" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, James G MacKinnon, et al. 2004. <em>Econometric Theory and Methods</em>. Vol. 5. Oxford University Press New York.
</div>
<div id="ref-durbin1954errors" class="csl-entry" role="doc-biblioentry">
Durbin, James. 1954. <span>“Errors in Variables.”</span> <em>Revue de l’institut International de Statistique</em>, 23–32.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-hansen2022econometrics" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce. 2022. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-hausman1978specification" class="csl-entry" role="doc-biblioentry">
Hausman, Jerry A. 1978. <span>“Specification Tests in Econometrics.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 1251–71.
</div>
<div id="ref-pearl2009causality" class="csl-entry" role="doc-biblioentry">
Pearl, Judea. 2009. <em>Causality</em>. Cambridge university press.
</div>
<div id="ref-ruud1984tests" class="csl-entry" role="doc-biblioentry">
Ruud, Paul A. 1984. <span>“Tests of Specification in Econometrics.”</span> <em>Econometric Reviews</em> 3 (2): 211–42.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="doc-biblioentry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
<div id="ref-wu1973alternative" class="csl-entry" role="doc-biblioentry">
Wu, De-Min. 1973. <span>“Alternative Tests of Independence Between Stochastic Regressors and Disturbances.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 733–50.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Are there situations where these assumptions may not hold?<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>It’s important to emphasize that these are linear projections, not structural linear models. There is no structural relationship between <span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(X_j\)</span> or <span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(Y\)</span>! This is really important, and we’ll introduce a definition related to this in the context of 2SLS.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Intuitively, this matrix must be PSD in the same way the sum of squared numbers cannot be negative.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>This happens to be one of the most cited papers in all of economics.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>In Example @ref(exm:ref3) we saw that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is more efficient than <span class="math inline">\(\hat{\boldsymbol\beta}_\text{IV} \)</span> in the event that our regressors are exogenous. This was formalized in Theorem @ref(thm:eff2), because in the event that all regressors <span class="math inline">\(\mathbf{X}\)</span> are exogenous and we have non-trivial instruments <span class="math inline">\(\mathbf{Z}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{2SLS} \)</span> “picks” the optimal trivial instruments <span class="math inline">\(\mathbf{X}\)</span> and our estimator reduces to <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>We know it’s exogenous because <span class="math inline">\(\mathbb{Z}\)</span> is a valid instrument and is exogenous<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>We could also use calculate <span class="math inline">\(S^2\)</span> from <span class="math inline">\(\mathbf{y}= \mathbb{X}\boldsymbol{\beta}+ \hat{\mathbb{X}}_\text{end}\boldsymbol\eta + \boldsymbol{\varepsilon}^*\)</span>, as these two estimates for <span class="math inline">\(\sigma^2\)</span> are algebraically equivalent.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>I did this to the best of my ability, as I have no experience with <code>.sas</code>, and I could have misinterpreted the accompanying documentation and commentary in the code.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The abbreviation “MSA” comes up a lot when dealing with surbey data. It stands for “metropolitan statistical area”, and are used by the Census Bureau to designate areas with a high population density.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Right now, the table is formatted in html and automatically included when this Rmarkdown file is knit, but you <code>stargazer</code> is also capable of outputting very nice tables in <span class="math inline">\(\LaTeX\)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>While we’re prone to make mistakes involving standard errors here, it’s sort of the equivelent of “showing your work” on a math test.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ols.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./extremum.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<pre class="markdown" data-shortcodes="false"><code>\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\avar}[1]{\text{Avar}\left(#1\right)}
\newcommand{\E}[1]{\text{E}\left[#1\right]}
\newcommand{\cov}[1]{\text{Cov}\left(#1\right)}
\newcommand{\mse}[1]{\text{MSE}\left(#1\right)}
\newcommand{\se}[1]{\text{se}\left(#1\right)}
\newcommand{\limfunc}{lim} 
\newcommand{\X}{\mathbf{X}}
\newcommand{\Xm}{\mathbb{X}}
\newcommand{\EER}{\bar{\thet}_\text{EE}}
\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Pe}{\mathbf{P}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xm}{\mathbb{x}}
\newcommand{\Zm}{\mathbb{Z}}
\newcommand{\Wm}{\mathbb{W}}
\newcommand{\Hm}{\mathbb{H}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}
\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}
\newcommand{\A}{\mathbf{A}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\I}{\mathbf{I}}
\renewcommand{\D}{\mathbf{D}}
\renewcommand{\C}{\mathbf{C}}
\newcommand{\zer}{\mathbf{0}}
\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }
\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }
\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }
\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }
\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }
\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }
\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }
\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }
\newcommand{\thet}{\boldsymbol{\theta}}
\newcommand{\et}{\boldsymbol{\eta}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sig}{\boldsymbol{\Sigma}}
\newcommand{\ep}{\boldsymbol{\varepsilon}}
\newcommand{\Omeg}{\boldsymbol{\Omega}}
\newcommand{\Thet}{\boldsymbol{\Theta}}
\newcommand{\bet}{\boldsymbol{\beta}}
\newcommand{\rk}{rank}
\newcommand{\tsum}{\sum}
\newcommand{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\ms}{\overset{ms}{\to}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\dto}{\overset{d}{\to}}
\newcommand{\asim}{\overset{a}{\sim}}

# Endogeniety I: IV and 2SLS

quarto-executable-code-5450563D

```r
#| echo: false
#| message: false
library(tidyverse)
library(pracma)
library(mvtnorm)
library(stargazer)
library(ivreg)
library(AER)
library(broom)
```

Our first departure from the classical linear model $\mathcal P_\text{LM}$ will come in the form of dropping the assumption that $\E{\X'\ep} = \zer$, i.e our regressors are endogenous. This situation is rather serious, as it prevents the linear model from being identified. Furthermore, the problem is common in applications. There are three main sources of endogeneity:

1. Omitted variables
2. Measurement error
3. Simultaneity 

For now we'll consider the first two, and discuss the third on in Section \@ref(endogeniety-ii-simultaneous-equation-models). Fortunately, we can address endogeneity with the instrumental variables estimator (a special case of which is the two-stage least squares estimator).

## Omitted Variables and Measurement Error

:::{#exm-ex1}
Recall the Example \@ref(exm:endogex) where we considered a model relating income to education and other determinants of salary. Suppose the true model is 
$$ \log(income_i) = \beta_1 + \beta_2\cdot educ_i + \beta_3 \cdot experiance_i + \varepsilon_i,$$ where $\varepsilon_i$ are the unobserved factors impacting salary, $educ_i$ measure years of post-secondary education, $experiance_i$ measures years of work experience, and $\cov{educ, experiance} &lt; 0$ (the longer you go to school, the less work experience you tend to have). Now suppose we incorrectly specify the model $$ \log(income_i) = \gamma_1 + \gamma_2\cdot educ_i + u_i,$$ where $u_i$ are all other factors impacting salary. We've omitted $experiance_i$ as a regressor, so it's implicitly included in $u_i$:
$$ u_i =  \beta_3 \cdot experiance_i + \varepsilon_i.$$ We no longer satisfy the assumption $\E{\X'\ep} = \zer$, as $\cov{educ_i, u_i} \neq 0$ because $\cov{educ, experiance} &lt; 0$. What happens if we go ahead and attempt to estimate $\beta_1$ and $\beta_2$ anyway? Set $\bet = [1,3,2]'$, and $\varepsilon_i \iid N(0,1)$. 

quarto-executable-code-5450563D

```r
beta &lt;- c(1,3,2)
n &lt;- 1000
mu &lt;- c(4,10)
Sigma &lt;- matrix(c(2, -0.5, -0.5, 5), nrow = 2)
X &lt;- rmvnorm(n, mu, Sigma)
e &lt;- rnorm(n)
log_y = beta[1] + X %*% beta[-1] + e

model_df &lt;- tibble(log_income = as.numeric(log_y),
                       educ = X[,1],
                       exper = X[,2])
model &lt;- lm(log_income ~ educ, data = model_df)
summary(model)$coefficients
```

The true parameters don't even fall within the 95% confidence intervals centered at our estimates.
:::

In general, suppose $$\Y = [\Xm,\Zm][\bet, \boldsymbol \delta]' + \ep = \Xm\bet + \Zm\boldsymbol \delta + \ep,$$ where we attempt to estimate $\bet$ via $\OLS$ despite omitting regressors $\mathbf Z$ from our model, and all our Gauss-Markov assumptions are met. Note that $\bet$ is still identified, as $\E{\X'\ep} = \zer$.
\begin{align*}
\OLSOV &amp; = (\Xm'\Xm)^{-1}\Xm'\Y\\
&amp; = (\Xm'\Xm)^{-1}\Xm'(\Xm\bet + \Zm\boldsymbol \delta + \ep)\\
&amp; = \bet + (\Xm'\Xm)^{-1}\Xm'\Zm\boldsymbol\delta  + (\Xm'\Xm)^{-1}\Xm'\ep.
\end{align*}
Our estimator is now inconsistent: 
\begin{align*}
\plim \OLSOV &amp; = \bet + \plim\left[(\Xm'\Xm)^{-1}\Xm'\Zm\boldsymbol\delta\right] + \plim\left[(\Xm'\Xm)^{-1}\right]\underbrace{\plim\left[\Xm'\ep\right]}_\zer\\
&amp; =  \bet + \plim\left[(\Xm'\Xm)^{-1}\Xm'\Zm\boldsymbol\delta\right].
\end{align*}

This phenomenon is referred to as **_omitted variable bias (OVB)_**. The use of the word "bias" here is a bit misleading, but follows from an interpretation of inconsistency as a persistent bias despite $n\to\infty$

:::{#exm-}

## OVB with a Simple Linear Model

Suppose our linear model is $Y = \alpha + \beta X+ \gamma Z + \varepsilon$, and we attempt to estimate $(\beta_0,\beta_1)$ with $\OLSOV$. In this case,
\begin{align*}
\hat \beta_\text{OLS,OV} &amp; =\frac{\sum_{i=1}^n (X_i -\bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \frac{\sum_{i=1}^n (X_i -\bar X)[(\alpha + \beta X+ \gamma Z + \varepsilon) - \bar Y]}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \beta + \frac{\sum_{i=1}^n (X_i -\bar X)(Z_i - \bar Z)}{\sum_{i=1}^n (X_i -\bar X)^2}\\ &amp; = \bet + \gamma \cdot \frac{\sum_{i=1}^n (X_i -\bar X)(Z_i-\bar Z)}{\sum_{i=1}^n (X_i -\bar X)^2}.
\end{align*}
The expectation of this is
$$\hat\beta_\text{OLS,OV} \pto \beta + \gamma \cdot \plim\frac{n^{-1}\sum_{i=1}^n (X_i -\bar X)(Z_i-\bar Z)}{n^{-1}\sum_{i=1}^n (X_i -\bar X)^2} =\bet + \gamma \frac{\cov{X,Z}}{\var{X}} .$$

If we let $\alpha = 1$, $\beta = 2$, $\gamma = 3$, and $\var{X} = 1$, then $$\plim \hat\beta_\text{OLS,OV} = 2 + 3\cov{X,Y}.$$ If we simulate this estimator for different values of $\cov{X,Y}$, taking $n$ to be very large, then we should see our estimates approximately follow the line $2 + 3\cov{X,Y}$ when plotted against $\cov{X,Y}$.




quarto-executable-code-5450563D

```r
n &lt;- 1e6
mu &lt;- c(4,10)
estimates &lt;- vector()
for (j in 1:100) {
  Sigma &lt;- matrix(c(1, -1 + j/50, -1 +j/50, 1), nrow = 2)
  regressors &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- regressors[,1]
  z &lt;- regressors[,2]
  e &lt;- rnorm(n)
  y &lt;- 1 + 2*x + 3*z + e
  estimates[j] &lt;- summary(lm(y ~ x))$coefficients[2,1]
}
```




```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test"}
tibble(x = -1 + (1:100)/50, 
       y = estimates
       ) %&gt;% 
  ggplot(aes(x,y)) +
  geom_point() +
  labs(x = "Cov(X,Z)",
       y = "β estimate") +
  theme_minimal()
```

:::

What happens if instead of omitting a variable from a model, our variables happen to be prone to some degree of measurement error. This is a common scenario in the social sciences where collected data is subject to human error, rounding errors, etc. Suppose a true linear model is specified as $\Y = \Xm^*\bet + \ep$ for regressors $\X^*$ where $\E{\X^{*\prime}\ep} = \zer$. Much like how we do not observe realizations of $\ep$, we do not observe realizations of $\X^*$, instead observing  $\X = \X^* + \mathbf u$ where $\mathbf u$ corresponds to **_measurement error_**. We'll assume that this measurement error is independent of $\ep$, independent of $\X^*$, and is mean zero.^[Are there situations where these assumptions may not hold?] Our model can be rewritten as 
$$ \Y = \Xm^*\bet + \ep = (\X - \mathbf u)\bet + \ep = \X\bet + \underbrace{(\ep - \mathbf u\bet)}_{\boldsymbol \nu}.$$ In this case $$\E{\X\boldsymbol \nu} = \E{(\X^* + \mathbf u)(\ep - \mathbf u\bet)} = -\bet\E{\mathbf u'\mathbf u} = -\bet \var{\mathbf u}.$$ Much like in the case of OVB, $\OLS$ will be inconsistent.

\begin{align*}
\OLSME &amp;= \bet + (\Xm'\Xm)^{-1}\Xm'\ep \\
     &amp; = \bet + (\Xm'\Xm)^{-1}\Xm'(\ep - \mathbf u\bet)\\
     &amp; = \bet + (\Xm'\Xm)^{-1}\Xm'\ep - (\Xm'\Xm)^{-1}\Xm'\mathbf u\bet\\
\plim \OLSME &amp; = \bet + \plim  (\Xm'\Xm)^{-1}\Xm'\ep + \plim (\Xm'\Xm)^{-1}\Xm'\mathbf u\bet\\
  &amp; = \bet + \plim (\Xm'\Xm)^{-1}\Xm'\mathbf u\bet &amp; (\plim \Xm'\ep = \zer)
\end{align*}

For $j\neq 1$ (there won't be measurement error when $j=1$, as this is just the regressor of 1 which gives the intercept) this simplifies to

$$ \plim \hat \beta_{\text{OLS,ME},j} = \beta_j \left(\frac{\var{X_j^*}}{\var{X_j^*} + \var{u_j^*}}\right).$$ This phenomenon is known as **_attenuation bias_**. The term in parentheses will always fall in the interval $(0,1)$, so $\abs{\plim \hat \beta_{\text{OLS,ME},j}} &lt; \abs{\bet_j}$, hence the name attentuation bias. 

## An Updated Linear Model, Identification, and the IV estimator

In general, if $\E{\X'\ep} \neq \zer$, then $\bet$ is not identified for the linear model $\mathcal P_\text{LM}$. Estimation is a non-starter in this case. Even if we had the "perfect" estimate for $\bet,$ the parameter may map to multiple elements $P_{\bet,\sigma^2}\in  \mathcal P_\text{LM}$, so it is impossible to determine which model value our data was drawn from. If we drop the assumption $\E{\X'\ep} \neq \zer$ from the linear model, then we'll need to replace it with some additional assumptions/structure. 

While $\E{\X'\ep} \neq \zer$, *perhaps* it is the case that there exists some other random vector $\Z$ which does satisfy $\E{\Z'\ep} = \zer$. Is this helpful -- no. For a given model with some structural error $\ep$, there are nearly infinite candidates for $\Z$ which satisfy this. Consider the model $$\log(income_i) = \beta_0 + \beta_1\cdot educ_i + \varepsilon_i,$$ where $\varepsilon$ are unobserved factors impacting income. What are some random variables $Z$ which are uncorrelated with $\varepsilon$. A ton! Weather during $i$'s tenth birthday, $i$'s first concert, $i$'s favorite flavor of ice cream, etc. There is an endless list of random variables that are so completely irrelevant to someone's income, that they are uncorrelated with $\varepsilon$. This is why $\E{\Z'\ep} = \zer$ is sometimes read as "$\Z$ is orthogonal to $\ep$." Not only does it hold in the mathematical sense of the word, but it also holds in the colloquial sense of the word meaning "has nothing to do with." What we want is $\Z$ to also be correlated with $\X$, such that $\Z$ is a sort of proxy/surrogate for $\X$ with no direct impact on $\Y$. We'll now generalize the linear model to introduce this set of variables $\Z$ in lieu of the assumption $\E{\X'\ep} \neq \zer$.

:::{#def-}
The &lt;span style="color:red"&gt;**_(linear) instrumental variables (IV) model_**&lt;/span&gt; is defined as $\mathcal P_\text{IV} = \{P_{\bet,\sigma^2} \mid \bet \in \mathbb R^{K}, \sigma^2\in\mathbb R\}$, where 
\begin{align*}
P_{\bet,\sigma^2} &amp;= \{F_{\Xm,\Zm,\ep} \mid \Y= \Xm\bet +\ep, \ \E{\ep'\ep\mid \X}=\sigma^2\mathbf I,\ f_{(\Xm,\Zm)}=\textstyle\prod_{i=1}^n f_{(\X_i,\Z_i)},\ \text{rank}\left(\E{\Z'\X}\right) = K,\ \E{\ep\mid \Xm} =\boldsymbol \eta,\  \E{\ep\mid \Zm} = \zer, \E{\Z'\X} \neq \zer \},\\
\Xm &amp; = [\X_1, \cdots, \X_j, \cdots \X_K] = [\X_1, \cdots, \X_i, \cdots \X_n]',\\
\Zm &amp; = [\Z_1, \cdots, \Z_j, \cdots \Z_K] = [\Z_1, \cdots, \Z_i, \cdots \Z_n]',\\
\dim(\Z) &amp; = \dim(\X) = K\\
\Y &amp; = [Y_1, \ldots, Y_n].
\end{align*}
We refer to the random vector $\Z$ as &lt;span style="color:red"&gt;**_instrumental variables (IVs)_**&lt;/span&gt;. We have assumed that $\E{\ep\mid \Zm} = \zer$, which subsumes the assumption that $\E{\Z'\ep} = \zer$. The assumption that $\Z$ is (weakly) exogenous (uncorrelated with $\ep$) is known as &lt;span style="color:red"&gt;**_instrumental validity_**&lt;/span&gt;, while the assumption that $\E{\X'\Z} \neq \zer$ ($\Z$ and $\X$ are correlated) is known as the &lt;span style="color:red"&gt;**_relevance condition_**&lt;/span&gt;. The assumption that $\E{\Z'\ep} = \zer$ is also sometimes known as the  &lt;span style="color:red"&gt;**_exclusion restriction_**&lt;/span&gt; ($\Z$ is excluded from the determinants of $Y$). Finally, we sometimes refer to $\text{rank}\left(\E{\Z'\X}\right) = K$ as the &lt;span style="color:red"&gt;**_rank condition_**&lt;/span&gt;.
:::

Instrumental validity and the relevance condition are usually written as:
\begin{align*}
\cov{\Z,\ep}&amp; = \zer,\\
\cov{\X, \Z} &amp;\neq \zer.
\end{align*}
*Technically*, this is not 100% accurate. The relevance condition is actually a combination of $\text{rank}\left(\E{\Z'\X}\right) = K$ and $\cov{\X, \Z}\neq \zer$. As highlighted by @wooldridge2010econometric, the relevance condition is not "regressors and instruments are uncorrelated". Instead, it is "instruments and endogenous regressors are partially correlated holding the exogenous regressors fixed." In other words, the linear projection of the instruments onto all regressors has nontrivial coefficients for endogenous regressors. This is equivalent to $\text{rank}\left(\E{\Z'\X}\right) = K$ and $\cov{\X, \Z}\neq \zer$.


Even if one regressor is endogenous, $\E{X_j\ep} \neq \zer$ for some $j$, then $\E{\X'\ep} \neq \zer$. If this is the case, we can simply define a portion of $\Z$ to be the exogenous regressors. Formally, if $[X_1,\ldots, X_J]$ are weakly exogenous while $[X_{J+1}, \ldots, X_K]$ are endogenous, then define $Z_1 = X_1,\ldots ,Z_J=X_j$. 

:::{#exm-spec1}

## Linear Model as Special Case of IV Model

A special case of the linear IV model is the classical linear model. Just let $\boldsymbol\eta = \zer$ and $\X = \Z$. This fact can be written as $\mathcal P_\text{LM}\subset \mathcal P_\text{IV}$.
:::

Whenever we define a new model, we need to make sure our parameters are identified. 

:::{#thm-}

## Identification of Linear IV Model

The linear IV model is identified as a result of the following assumptions: $\E{\Z'\ep} = \zer$, $\E{\X'\Z} \neq \zer$, and $\text{rank}\left(\E{\Z'\X}\right) = K$.
:::

:::{.proof}
First we will show $\bet$ is identified. Let $P_{\bet,\sigma^2} = P_{\bet^*,\sigma^2}$ for two elements of $\mathcal P_\text{IV}$, and suppose for a contradiction that $\bet\neq\bet^*$. We can begin by writing $\bet$:
\begin{align*}
&amp;\E{\Z'\ep} = \zer\\
\implies &amp; \E{\Z'(Y-\X\bet)} = \zer &amp; (\ep = (Y-\X\bet))\\
\implies &amp; \E{\Z'Y}-\bet\E{\Z'\X}= \zer\\
\implies &amp; \E{\Z'Y} = \bet\E{\Z'\X}\\
\end{align*}
By the definition of $\mathcal P_\text{IV}$, the moments $\E{\Z'\X}$ and $\E{\Z'Y}$ are the same for the model values $P_{\bet,\sigma^2}$ and  $P_{\bet^*,\sigma^2}$, so $\bet^*$ must also satisfy $\E{\Z'Y} = \bet\E{\Z'\X}$. This contradicts the assumption that $\text{rank}\left(\E{\Z'\X}\right) = K$. If $\bet$ is identified, then $\sigma^2$ is as well because we can write it in terms of $\bet$: 
$$ \E{(\Y-\Xm\bet)'(\Y-\Xm\bet)\mid \X}=\sigma^2\mathbf I.$$
&lt;span style="color:white"&gt;space&lt;/span&gt;
:::

So how do we estimate $\bet$ for the model $\mathcal P_\text{IV}$. If it wasn't clear from the examples dealing with OVB and measurement error, the answer is not with $\OLS$. 

:::{#prp-}

## Inconsisteny of OLS

If $P_{\bet,\sigma^2} \in \mathcal P_\text{IV}$, then $\OLS$ is biased and inconsistent. In particular, 
\begin{align*}
\E{\OLS\mid \X} &amp; = \bet + (\Xm'\Xm)^{-1}\Xm\boldsymbol \eta \neq \bet,\\
\plim \OLS &amp; = \bet + \E{\X'\X}^{-1}\boldsymbol \gamma \neq \bet,\\
\end{align*}
where $\E{\ep\mid \Xm} =\boldsymbol \eta$ and $\E{\X'\ep} = \boldsymbol \gamma$.
:::

:::{.proof}
\begin{align*}
\E{\OLS\mid \Xm} &amp; = \E{\bet +(\Xm'\Xm)^{-1}\Xm'\ep \mid \Xm}\\
&amp; = \bet + (\Xm'\Xm)^{-1}\Xm'\E{\ep \mid \Xm}\\
&amp; = \bet + (\Xm'\Xm)^{-1}\Xm'\boldsymbol \eta\\
\plim \OLS &amp; = \bet + \plim\left[\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)\right]\\
&amp; = \bet + \plim\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\plim \left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right) &amp; (\text{Slutsky's Theorem})\\
&amp; = \bet + \E{\X'\X}^{-1}\E{\X'\ep} &amp; (\text{LLN})\\
&amp; = \bet + \E{\X'\X}^{-1}\boldsymbol \gamma
\end{align*}
&lt;span style="color:white"&gt;space&lt;/span&gt;
:::

:::{#exm-}
Let's verify that $\OLS$ is inconsistent. Suppose $\Xm = (\mathbf 1, X)$ where $(X,\varepsilon)\iid N(\boldsymbol \mu, \Sig)$ for $\boldsymbol \mu = [0,1]'$
$$\Sig = \begin{bmatrix}5&amp;2\\2&amp;1  \end{bmatrix}.$$
We have
\begin{align*}
\E{X\varepsilon} &amp;= \cov{X,\varepsilon} + \underbrace{\E{X}}_0\underbrace{\E{\varepsilon}}_1 = 2.
\end{align*}
Therefore
\begin{align*}
\boldsymbol \gamma &amp;= \E{\Xm'\ep} = \begin{bmatrix}\E{1\ep}\\\E{X\ep} \end{bmatrix} = \begin{bmatrix}1\\2 \end{bmatrix}
\end{align*}

If we draw realizations ```x``` and ```e``` of $(X,\varepsilon)$, we should find that ```colMeans(X*e)``` should be approximately $[1,2]'$ by the LLN.

```{r, warning=FALSE}
n &lt;- 100000
Sigma &lt;- matrix(c(5,2,2,1), nrow = 2)
mu &lt;- c(0,1)
realizations &lt;- rmvnorm(n, mu, Sigma)
x &lt;- realizations[,1]
X &lt;- cbind(1,x)
e &lt;- realizations[,2]
colMeans(X*e)
```

Let's now calculate $\plim \OLS$. 
\begin{align*}
\E{\X'\X}^{-1} &amp;= \begin{bmatrix}\E{1} &amp; \E{X} \\ \E{X} &amp; \E{X^2} \end{bmatrix}^{-1} = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; \var{X} \end{bmatrix}^{-1} = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}^{-1}  =  \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0.2 \end{bmatrix}\\
\plim \OLS &amp; = \bet + \E{\X'\X}^{-1}\boldsymbol \gamma= \bet + \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0.2 \end{bmatrix}\begin{bmatrix}1\\2 \end{bmatrix}= \bet + \begin{bmatrix}1\\0.4 \end{bmatrix}
\end{align*}
If we let $\bet = [1,2']$ we should see our estimates converge to $[2, 2.4]'$ as $n\to\infty$.

quarto-executable-code-5450563D

```r
N_sim &lt;- 10000
estimates &lt;- matrix(NA, nrow = N_sim, ncol = 2)
for (n in 2:(N_sim + 1)) {
  realizations &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- realizations[,1]
  X &lt;- cbind(1,x)
  e &lt;- realizations[,2]
  y &lt;- 1 + 2*x + e
  summary(lm(y~x))$coefficients[,1]
  estimates[n-1,] &lt;- summary(lm(y~x))$coefficients[,1]
}
```

```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}
tibble(x = rep(2:(N_sim + 1), 2),
       y = c(estimates[,1], estimates[,2]),
       group = c(rep("β1", N_sim), rep("β2", N_sim))
      ) %&gt;% 
  ggplot(aes(x,y)) +
  geom_line() +
  facet_wrap(~group, ncol =1) +
  theme_minimal() +
  labs(x = "Sample Size",
       y = "Estimate") +
  ylim(1.5,2.5)
```

:::
An interesting feature of this problem is that our estimator for the intercept term $\beta_1$ was inconsistent even though the corresponding regressor (the trivial random variable $X_1 = 1$) is exogenous. In general, the inconsistency of $\OLS$ in the presence of endogeneity is not limited to the parameters associated with endogenous variables, and will impact each parameter $\beta_j$.  

Instead of using $\OLS$, we need an estimator which makes use of the exogenous variables $\Z$. There are several different ways to motivate this estimator, so we'll go over four particularly approaches which reach the same conclusion. 

1. For $\mathcal P_\text{IV}$ we can write the true population parameter as $\bet = \E{\Z'\X}^{-1}\E{\Z'Y}$. We can appeal to the analogy-principle here just like we did when deriving $\OLS$. It stands to reason that the estimator defined by the analogous samples will provide consistent estimates of $\bet$ by the LLN. This estimator is 
$$\hat {\bet}= \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_iY_i\right) = (\Zm'\Xm)^{-1}\Zm'\Y.$$
2. Another way of tackling the problem relates to marginal effects and is presented by @cameron2005microeconometrics. This will be a little informal and is only to build intuition. In an abuse of notation, we'll write marginal effects as $\frac{\partial Y}{\partial \X}$. Ideally, we want $\beta_j = \frac{\partial Y}{\partial X_j}$ for each $j$. This way, when we estimate $\bet$, we estimate the marginal effect of $\X$ on $Y$. Unfortunately, we cannot estimate this directly with $\X$ as 
$$ \frac{\partial Y}{\partial X_j} = \beta_j + \frac{\partial \varepsilon}{\partial X_j} \implies \beta_j \neq \frac{\partial Y}{\partial X_j}.$$
If we appeal to the chain rule, we can vary $X_j$ via our instruments $\Z$:
\begin{align*}
&amp;\frac{\partial Y}{\partial \Z} = \frac{\partial Y}{\partial X_j} \frac{\partial X_j}{\partial \Z} \\
\implies &amp; \frac{\partial Y}{\partial X_j} = \frac{\partial Y/\partial \Z}{\partial X_j/\partial \Z}
\end{align*}
where $\frac{\partial Y}{\partial \Z}$ holds $\varepsilon$ constant as $\cov{\Z, \ep} = \zer$. The marginal effects $\frac{\partial X_j}{\partial \Z}$ and $\frac{\partial Y}{\partial \Z}$ correspond to the parameters in the linear projection models of $\Z$ on $X_j$ and $Y$.^[It's important to emphasize that these are linear projections, not structural linear models. There is no structural relationship between $\Z$ and $X_j$ or $\Z$ and $Y$! This is really important, and we'll introduce a definition related to this in the context of 2SLS.] These parameters are given by OLS estimates $(\Zm'\Zm)^{-1}\Zm\X_j$ and $(\Zm'\Zm)^{-1}\Zm\Y$, respectively, so
$$ \hat\beta_j = \frac{(\Zm'\Zm)^{-1}\Zm'\Y}{(\Zm'\Zm)^{-1}\Zm\X_j}.$$ If we do this for all regressors $\X$, then 
$$ \hat{\bet} = \frac{(\Zm'\Zm)^{-1}\Zm'\Y}{(\Zm'\Zm)^{-1}\Zm\Xm} =  (\Zm'\Xm)^{-1}\Zm'\Y$$
3. We can take a graphical approach given by @pearl2009causality with the simple IV model with one endogenous regressor $X$ and one instrument $Z$. Suppose we have $Y = \beta_1 + \beta_2 X + \varepsilon$, where $\cov{X,\varepsilon} \neq 0$, along with $\cov{Z,X} \neq 0$ and $\cov{Z,\varepsilon} = 0$ for some instrument $Z$. These relationships can be illustrated in the form of a directed acyclic graph (DAG).
```{r, echo=FALSE, fig.align='center', fig.asp = 0.5, fig.width = 1, fig.cap ="test", out.width = "500px"}
knitr::include_graphics("figures/iv_dag.png")
```
We can think of the paths which illustrate causation as being multiplicative in the sense that $\cov{Z,X}\cdot \beta_2 = \cov{Z,Y}$, where $\beta_2$ is a "conversion rate" between changes in $X$ via $Z$ and changes in $Y$ via $Z$ (just like the chain rule approach). This implies $\beta_2 = \cov{Z,Y}/\cov{Z,X}$, so 
$$ \hat\beta_2 = \frac{\widehat{\text{Cov}}(Z,Y)}{\widehat{\text{Cov}}(Z,X)}.$$ This happens to be a special case of $\hat{\bet}=  (\Zm'\Xm)^{-1}\Zm'\Y$.

4. Another approach in the context of the simple linear model takes advantage of a simple substitution. 
$$ \cov{Y,Z} = \cov{\beta_1 + \beta_2 X + \varepsilon, Z} = \underbrace{\cov{\beta_1, Z}}_0 + \beta_2\cov{X,Z} + \underbrace{\cov{Z,\varepsilon}}_0 = \beta_2\cov{X,Z}.$$ This is the same result we arrived at using the DAG.

With all roads leading to Rome, we can define this estimator. 

:::{#def-}
The &lt;span style="color:red"&gt;**_instrumental variables (IV) estimator_**&lt;/span&gt; is defined as 
\begin{align*}
\IV(\Xm,\Zm,\Y)= (\Zm'\Xm)^{-1}(\Zm'\Y)= \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_iY_i\right)
\end{align*} An realization of this estimator (an estimate) is 
\begin{align*}
\hat{\mathbf b}_\text{IV} = \hat{\bet}_\text{IV}(\X,\Z,\y) &amp;= (\Z'\X)^{-1}(\Z'\y)= \left(\frac{1}{n}\sum_{i=1}^n\z_i'\z_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\z_iy_i\right)
\end{align*} 
and will exist when the inverse $(\Z'\X)^{-1}$ exists.
:::

:::{#exm-refex}
Suppose $(X, Z, \varepsilon) \sim N(\boldsymbol \mu,\Sig)$ where
\begin{align*}
\boldsymbol \mu &amp; = [10,10,0]',\\
\Sig &amp; = \begin{bmatrix}20 &amp; 5 &amp; 1\\5&amp;20&amp;0\\1&amp;0&amp;1 \end{bmatrix},
\end{align*}
and $Y = 1 + 3X + \varepsilon$. Let's simulate a sample of size $n=1000$ from this model $P_{\bet,\sigma^2} \in \mathcal P_\text{IV}$, and then calculate $\IV$.

quarto-executable-code-5450563D

```r
n &lt;- 1000
mu &lt;- c(10, 10, 0)
Sigma &lt;- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)
sample &lt;- rmvnorm(n, mu, Sigma)
x &lt;- sample[,1]
z &lt;- sample[,2]
e &lt;- sample[,3]
y &lt;- 1 + 3*x + e
```

We should confirm that our drawn sample satisfies the assumptions of the IV model. 

quarto-executable-code-5450563D

```r
# Is x exogenous?
cov(x,e)

# Is z a valid instrument?
cov(z,e)

# Is z relevant?
cov(z,x)
```

Now let's calculate $\IV$, keeping in mind that $\Zm$ is comprised of the instrument $Z$ *and* the exogenous random variable $1$ which gives the intercept term. 

quarto-executable-code-5450563D

```r
X &lt;- cbind(1, x)
Z &lt;- cbind(1, z)
beta_hat &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y 
beta_hat
```

:::

:::{#exm-spec2}

## OLS is a Special Case of IV

In Example \@ref(exm:spec1) we saw that $\mathcal P_\text{LM}\subset\mathcal P_\text{IV}$. In the event $P_{\bet,\sigma^2} \in \mathcal P_\text{LM}$, i.e $\Zm = \Xm$ and , then 
$$ \IV = (\Zm'\Xm)^{-1}(\Zm'\Y) = (\Xm'\Xm)^{-1}(\Xm'\Y) = \OLS.$$
:::

## Properties of the IV Estimator

One of the reasons OLS is so special is because we're able to characterize its finite sample properties with the Gauss-Markov theorem. This is the exception rather than the rule when assessing estimators. In most cases, we can only arrive at tractable results in the form of an estimator's asymptotic properties. Is this the case with $\IV$? Let's see if $\IV$ satisfies our "baseline" finite sample property of unbiasedness. 
\begin{align*}
\E{\IV} &amp;= \E{\E{\IV \mid \Xm, \Zm}}\\
        &amp; =  \E{\E{(\Zm'\Xm)^{-1}(\Zm'\Y) \mid \Xm, \Zm}}\\
        &amp;=\E{\E{(\Zm'\Xm)^{-1}(\Zm'(\Xm\bet + \ep)) \mid \Xm, \Zm}}\\
        &amp;=\bet + \E{(\Zm'\Xm)^{-1}\Zm'\E{\ep \mid \Xm, \Zm}}\\
        &amp; \neq \bet
\end{align*}
In general, $\E{\ep \mid \Xm, \Zm} \neq \zer$, so $\IV$ has a bias. 

:::{#exm-}

## IV Estimator is Biased

Return to the model from Example \@ref(exm:refex), but let $n = 10$. If we simulate the bias of $\IV$ using 10,000 simulations, we can confirm that $\IV$ is biased.  

quarto-executable-code-5450563D

```r
N_sim &lt;- 10000
estimates &lt;- matrix(NA, nrow = N_sim, ncol = 2)
for (k in 1:N_sim) {
  n &lt;- 10
  mu &lt;- c(10, 10, 0)
  Sigma &lt;- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)
  sample &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- sample[,1]
  z &lt;- sample[,2]
  e &lt;- sample[,3]
  y &lt;- 1 + 3*x + e
  X &lt;- cbind(1, x)
  Z &lt;- cbind(1, z)
  estimates[k,] &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y 
}
colMeans(estimates)
```

Okay but is this really an issue? In most settings, we would have a sample size much larger than $n = 10$. Let's repeat this experiment with $n = 1000$ and see what happens to our estimators bias.

quarto-executable-code-5450563D

```r
N_sim &lt;- 10000
estimates &lt;- matrix(NA, nrow = N_sim, ncol = 2)
for (k in 1:N_sim) {
  n &lt;- 1000
  mu &lt;- c(10, 10, 0)
  Sigma &lt;- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)
  sample &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- sample[,1]
  z &lt;- sample[,2]
  e &lt;- sample[,3]
  y &lt;- 1 + 3*x + e
  X &lt;- cbind(1, x)
  Z &lt;- cbind(1, z)
  estimates[k,] &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y 
}
colMeans(estimates)
```

Now the bias is negligible.
:::

While $\IV$ is biased, it seems as if it is asymptotically unbiased. Instead of proving this directly, we'll actually show that $\IV$ is root-N CAN, a sufficient condition for asymptotic unbiasedness. Before tackling that, let's prove that $\IV$ is consistent directly. One of the reasons we opted to not estimate $\bet$ via $\OLS$ was that $\OLS$ is not consistent for $\mathcal P_\text{IV}$, so it shouldn't be a surprise that $\IV$ is consistent. 

:::{#prp-}

## IV Estimator is Consistent

If $P_{\bet,\sigma^2}\in \mathcal P_\text{IV}$, then $\IV\pto \bet$.
:::

:::{.proof}
\begin{align*}
\IV &amp;= \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_iY_i\right)\\
    &amp; = \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_i[\X_i\bet + \varepsilon_i]\right)\\
    &amp; = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)}_1\bet + \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i\right)\\
    &amp; = \bet + \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i\right)\\
\plim \IV &amp; = \bet + \plim \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\cdot \plim \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i\right) &amp; (\text{Slutsky's theorem})\\
&amp; = \bet + \plim \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\cdot \E{\Z'\ep} &amp; (\text{LLN})\\
&amp; = \bet + \plim \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\cdot \zer &amp; (\E{\Z'\ep} = \zer)\\
&amp; = \bet
\end{align*}
&lt;span style="color:white"&gt;space&lt;/span&gt;
:::



:::{#thm-}

## IV Estimator is Root-n CAN

If $P_{\bet, \sigma^2} \in \mathcal P_\text{IV}$, then
$$\IV \asim N\left(\bet, \sigma^2\E{\Zm'\Xm}^{-1}\E{\Zm'\Zm}\E{\Xm'\Zm}^{-1}\right) =  N\left(\bet, \frac{\sigma^2}{n}\E{\Z'\X}^{-1}\E{\Z'\Z}\E{\X'\Z}^{-1} \right)$$
:::

:::{.proof}
The proof is almost identical to that of Theorem \@ref(thm:asymols).

\begin{align*}
\sqrt n(\IV - \bet) &amp; = \sqrt{n}\left[\bet + \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i\right) - \bet\right]\\
&amp; = \left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i - \zer \right)\\
&amp; =\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i - \E{\Z_i\varepsilon_i} \right) &amp; (\E{\Z'\varepsilon} = \zer ) \\
&amp; = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\X_i\right)^{-1}}_{\pto \E{\Z'\X}^{-1}} \underbrace{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n\Z_i'\varepsilon_i - \E{\Z_i\varepsilon_i} \right)}_{\dto N\left(\E{\Z_i\varepsilon_i}, \var{\textstyle \sum \X_i\varepsilon_i}/n\right)} &amp; (\text{LLN and CLT})\\
&amp;\dto \E{\Z'\X}^{-1}\cdot N\left(\E{\Z_i\varepsilon_i}, \var{\textstyle \sum \Z_i\varepsilon_i}/n\right) &amp; (\text{Slutsky's Theorem})\\
&amp; = \E{\Z'\X}^{-1}\cdot N\left(\zer, \sigma^2\E{\Z'\Z}\right) \\
&amp; = \E{\Z'\X}^{-1}\cdot N\left(\zer, \sigma^2\E{\Z'\Z}\right)\\
&amp; = N\left(\zer, \E{\Z'\X}^{-1}\sigma^2\E{\Z'\Z}\left[\E{\Z'\X}^{-1}\right]'\right)\\
&amp; = N\left(\zer, \E{\Z'\X}^{-1}\sigma^2\E{\Z'\Z}\E{\X'\Z}^{-1}\right).
\end{align*}
This implies that 
$$\IV\asim N\left(\bet, \frac{\sigma^2}{n}\E{\Z'\X}^{-1}\E{\Z'\Z}\E{\X'\Z}^{-1} \right).$$ 
If desired, we can write the asymptotic variance in terms of matrices $\Xm$ and $\Zm$:
\begin{align*}
\avar{\IV} &amp; = \frac{\sigma^2}{n}\E{\Z'\X}^{-1}\E{\Z'\Z}\E{\X'\Z}^{-1}\\
&amp; = \frac{\sigma^2}{n}\left[\frac{\E{\Zm'\Xm}}{n}\right]^{-1}\left[\frac{\E{\Zm'\Zm}}{n}\right]\left[\frac{\E{\Xm'\Zm}}{n}\right]^{-1} \\
&amp; = n^2\cdot\frac{1}{n}\cdot\frac{\sigma^2}{n}\E{\Zm'\Xm}^{-1}\E{\Zm'\Zm}\E{\Xm'\Zm}^{-1}\\
&amp; = \sigma^2\E{\Zm'\Xm}^{-1}\E{\Zm'\Zm}\E{\Xm'\Zm}^{-1}
\end{align*}
&lt;span style="color:white"&gt;space&lt;/span&gt;
:::

:::{#cor-}

## IV Estimator is Asymptotically Unbiased

If $P_{\bet, \sigma^2} \in \mathcal P_\text{IV}$, then 
$$ \lim_{n\to\infty}\text{Bias}(\IV) = 0.$$
:::

In order to appeal to the asymptotic distribution of $\IV$ to perform inference, we need a consistent estimator of the asymptotic variance. Fortunately, we can take nearly the same exact approach we took with $\OLS$. 

:::{#prp-}

## Estimation of IV Variance

Define the estimator 
\begin{align*}
S^2 &amp;=  \frac{\hat{\e}'\hat{\e}}{n-K}\\
\hat{\e} &amp;= \Y - \Xm\IV
\end{align*} in the context of the linear IV model. Then:

1. $S^2$ is an unbiased for $\var{\ep\mid\X} = \sigma^2$.
2. $S^2$ is a consistent estimator $\var{\ep\mid\X} = \sigma^2$.
3. The estimator $\widehat{\text{Avar}}(\IV) = S^2(\Zm'\Xm)^{-1}(\Zm'\Zm)(\Xm'\Zm)^{-1}$ is a consistent estimator for 
${\text{Avar}}(\IV)$.
:::

:::{.proof}
The proof is nearly identical to that of Proposition \@ref(prp:olsvar).
:::

:::{#exm-}

## Coding Exercise

R has no base function which implements $\IV$, so let's write our own. For reference, we can compare our results with those given by ```ivreg()``` from the ```AER``` (applied econometrics in R) package.

quarto-executable-code-5450563D

```r
IV &lt;- function(y, X, Z){
  #determine dimensions, perform IV
  n &lt;- length(y)
  K &lt;- ncol(X)
  if(ncol(Z) != K) {stop("K instruments required")}
  if(det(t(X) %*% X) == 0) {stop("rank(Z'X) &lt; K")}
  
  hat_beta &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y 
  
  #use IV estimates to calculate residuals and estimate SEs
  res &lt;- (y-X %*% hat_beta)
  S2 &lt;- ((t(res) %*% res)/(n - K)) %&gt;% as.numeric() 
  var_hat &lt;- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z) 
  se_hat &lt;- sqrt(diag(var_hat))
  
  #t-stat, confidence intervals, p values
  t &lt;- hat_beta/se_hat
  lower_CI &lt;- hat_beta - qnorm(0.975)*se_hat
  upper_CI &lt;- hat_beta + qnorm(0.975)*se_hat
  p_val &lt;- 2*(1 - pt(t, n-K))
  
  #combine everything into one table to return
  output &lt;- cbind(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)
  rownames(output) &lt;- paste("β", 1:K, sep = "")
  colnames(output) &lt;- c("Estimate", "Std.Error", "t-Stat", "Lower 95% CI", "Upper 95% CI", "p-Value")
  return(output)
}
```

Let's estimate the model from Example \@ref(exm:refex) using ```ivreg()```.

quarto-executable-code-5450563D

```r
n &lt;- 1000
mu &lt;- c(10, 10, 0)
Sigma &lt;- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)
sample &lt;- rmvnorm(n, mu, Sigma)
x &lt;- sample[,1]
z &lt;- sample[,2]
e &lt;- sample[,3]
y &lt;- 1 + 3*x + e
summary(ivreg(y ~ x | z))$coefficients
```

Now we can use our function and verify that the outputs are the same. 

quarto-executable-code-5450563D

```r
X &lt;- cbind(1,x)
Z &lt;- cbind(1,z)
IV(y,X,Z)
```
:::

:::{#exm-}

## Intuition behind Asymptotic Variance

In Example \@ref(exm:csvarols) we provided some intuition as to how $\var{\OLS} = \frac{\sigma^2}{n} \E{\X'\X}^{-1}$ changed in response to changes in $\sigma^2$, $n$, and components of $\E{\X'\X}$. The variance of $\OLS$ is smaller for large samples and when the error term $\ep$ has a small variance. The more variance we have in our regressors (which is related to $\E{\X'\X}^{-1}$), the more efficient our estimator. So what is the intuition behind $\avar{\IV} =\frac{\sigma^2}{n}\E{\Z'\X}^{-1}\E{\Z'\Z}\E{\X'\Z}^{-1}$? Three things should look familiar. The variance increases with increases in $\sigma^2$ and decreases with increases in $n$. Consider the model $Y = X\beta + \varepsilon$ when we instrument for $X$ with $Z$, such that 
$$\avar{\IV} =\frac{\sigma^2}{n}\frac{\E{Z^2}}{\E{ZX}^2} = \frac{\var{Z} + \E{Z}^2}{[\cov{X,Z} + \E{Z}\E{X}]^2}.$$ Holding the average of $Z$ and $X$ fixed, we see that the asymptotic variance of our estimator increases with linearly with $\var{Z}$, and decreases quadratically as $\cov{X,Z}$. The latter of these facts shouldn't be surprising. The larger $\cov{X,Z}$, the more relevant ("stronger") our instrument $Z$ is, and the better our estimates.   
:::

## Many Instruments, 2SLS

When defining $\mathcal P_\text{IV}$, we assumed $\dim(\Z) = \dim(\X) = K$. What happens if we have $L = \dim(\Z) &gt; \dim(\X) = K$? Let's consider a special case first.


:::{#exm-ref3}

## OLS vs. IV

Consider a model $\Y = \Xm \bet + \ep$ which satisfies the Gauss-Markov assumption, *but* also specifies the existence of $K$ separate instrumental variables $\Z$. In other words, we have $2K$ instruments in the form of $\X$ and $\Z$. Should we estimate $\bet$ via OLS (equivalent to IV using $\X$ as instruments for $\X$), or should we estimate $\bet$ via IV using $\Z$? It may be tempting to pick the latter. What if we are confident that $\Z$ are valid instruments, but aren't positive that $\X$ is exogenous. If we estimated $\bet$ with $\IV = (\Zm'\Xm)^{-1}\Zm'\Y$, then we would be playing it safe and not risk inconsistent estimates via $\IV'=(\Xm'\Xm)^{-1}\Xm'\Y = \OLS$. Unfortunately this approach has a cost in the form of variance/standard errors. 
\begin{align*}
\text{Avar}(\IV) &amp;= \sigma^2\E{\Zm'\Xm}^{-1}\E{\Zm'\Zm}\E{\Xm'\Zm}^{-1}\\
\text{Avar}(\IV') &amp;= \text{Avar}(\OLS) = \sigma^2\E{\Xm'\Xm}^{-1} &amp; (\Xm = \Zm)
\end{align*}
The difference $\text{Avar}(\IV) - \text{Avar}(\OLS)$ is PSD, so 
$$ \se{\hat\beta_{\text{IV},j}'} = \se{\hat\beta_{\text{OLS},j}} &lt; \se{\hat\beta_{\text{IV},j}}.$$
As the next simulation shows, this difference can be fiarly large.

quarto-executable-code-5450563D

```r
N_sim &lt;- 10000
estimates &lt;- matrix(NA, nrow = 2*N_sim, ncol = 3)
estimates[,3] &lt;- rep(c("OLS", "IV"), N_sim)

for (k in 1:N_sim) {
  n &lt;- 1000
  mu &lt;- c(10, 10, 0)
  Sigma &lt;- matrix(c(20,5,0, 5,20,0,0,0,1), nrow = 3)
  sample &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- sample[,1]
  z &lt;- sample[,2]
  e &lt;- sample[,3]
  y &lt;- 1 + 3*x + e
  X &lt;- cbind(1,x)
  Z &lt;- cbind(1,z)
  estimates[2*k-1 ,1:2] &lt;- solve(t(X) %*% X) %*% t(X) %*% y
  estimates[2*k,1:2] &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y 
}  
```

```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}
df2 &lt;- as.data.frame(estimates) %&gt;% 
  rename(`β1` = V1, `β2` = V2, estimator = V3) %&gt;% 
  gather("term", "value", -estimator) %&gt;% 
  mutate(value = as.numeric(value))

df2 %&gt;% 
  ggplot(aes(value)) +
  geom_histogram(color = "black", fill = "white", bins = 50) +
  facet_grid(estimator ~ term, scales = "free") +
  theme_minimal() +
  labs(x = "Estimate",
       y = "Frequency")
  
```

If we calculate the simulated standard errors of our estimators, we find that using $\IV$ results in nearly a four fold decrease in standard error.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
df2 %&gt;% 
  group_by(estimator, term) %&gt;% 
  summarize(value = sd(value)) %&gt;% 
  knitr::kable(col.names = c("Estimator", "Term", "Standard Error"))
```
:::

:::{#exm-ex4}

## IV vs... IV?

Suppose $Y = X\beta + \varepsilon$ for an endogenous $X$. Fortunately, we have two instruments $Z_1$ and $Z_2$. Which instrument do we use to estimate $\beta$. At first the answer seems simple -- just estimate the model using each instrument separately, and then pick the estimates with the lower standard error. This won't consider all the relevant cases though, as *any* linear combination of $Z_1$ and $Z_2$ are also instruments. Define $Z_3 = aZ_1 + bZ_2$. 
\begin{align*}
\E{Z_3\varepsilon} &amp; = \E{(aZ_1 + bZ_2)\varepsilon} = a\underbrace{\E{Z_1\varepsilon}}_0 + b\underbrace{\E{Z_2\varepsilon}}_0 = 0\\
\E{Z_3X} &amp; = \E{(aZ_1 + bZ_2)X} = a\underbrace{\E{Z_1X}}_{\neq 0} + b\underbrace{\E{Z_2X}}_{\neq 0} \neq 0
\end{align*}
There are an infinite number of candidates for instruments, so what do we do? Let's simulate some standard errors and see if we can notice patterns between them and the choice of instruments. We'll restrict our attention to instruments in the set $\{aZ_1 + bZ_2\mid a,b\in[0,1]\}$. We will calculate the $\var{\IV \mid \Xm,\ \Zm}$ for a fixed sample drawn from $(X,Z_1,Z_2,\varepsilon)\iid N(\boldsymbol \mu, \Sig)$ where

\begin{align*}
\boldsymbol \mu &amp; = [10,10,10,0]',\\
\Sig &amp; = \begin{bmatrix}20 &amp; 5 &amp; 10 &amp; 1\\5&amp;30&amp;7&amp;0\\10&amp;7&amp;50&amp;0\\1&amp;0&amp;0&amp;1 \end{bmatrix},
\end{align*}

quarto-executable-code-5450563D

```r
n &lt;- 50
mu &lt;- c(10, 10, 10, 0)
Sigma &lt;- matrix(c(20,5,10,1,5,30,7,0,10,7,50,0,1,0,0,1), nrow = 4)
sample &lt;- rmvnorm(n, mu, Sigma)
x &lt;- sample[,1] 
z1 &lt;- sample[,2]
z2 &lt;- sample[,3]
e &lt;- sample[,4]
y &lt;- 1 + 2*x + e

A &lt;- seq(0.01, 1, length = 500)
B &lt;- seq(0.01, 1, length = 500) 
store &lt;- list()
i &lt;- 0
for (a in A) {
  for (b in B) {
    i &lt;- i +1
    X &lt;- as.matrix(x)
    Z &lt;- as.matrix(a*z1 + b*z2)
    store[[i]] &lt;- (IV(y, X, Z)[2])^2
  }
}
```

Now let's plot the calculated variances (conditional on the fixed sample) over the values $a\times b\in [0,1]^2$ which determined the instrument used to calculate $\IV$. 

```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}
df &lt;- expand_grid(a = A,b = B) %&gt;% 
  mutate(var = unlist(store)) %&gt;% 
  arrange(var) %&gt;% 
  mutate(var_scale = row_number()/(500*500))


ggplot(df, aes(a, b, fill = var_scale)) +
  geom_tile() +
  theme_minimal() +
  labs(x = "a",
       y = "b",
       fill = "IV Conditional Variance, Scaled") +
  theme(legend.position = "bottom") 
```

As anticipated, $\IV$ is more efficient when using certain linear combinations of elements. Furthermore, it appears that using some non-trivial linear combination of $Z_1$ and $Z_2$ is a better choice than simply using one or the other.   
:::

Before considering this problem in general, let's extend the IV model to allow for more instruments than regressors. 

:::{#def=}
The &lt;span style="color:red"&gt;**_(linear) instrumental variables (IV) model_**&lt;/span&gt; is defined as $\mathcal P_\text{IV} = \{P_{\bet,\sigma^2} \mid \bet \in \mathbb R^{K}, \sigma^2\in\mathbb R\}$, where 
\begin{align*}
P_{\bet,\sigma^2} &amp;= \{F_{\Xm,\Zm,\ep} \mid\ \text{rank}\left(\E{\Z'\Z}\right) = L,\ \text{rank}\left(\E{\Z'\X}\right) = K, \ldots  \},\\
\Xm &amp; = [\X_1, \cdots, \X_j, \cdots \X_K] = [\X_1, \cdots, \X_i, \cdots \X_n]',\\
\Zm &amp; = [\Z_1, \cdots, \Z_j, \cdots \Z_L] = [\Z_1, \cdots, \Z_i, \cdots \Z_n]',\\
\dim(\Z) &amp; = L,\\ 
\dim(\X) &amp;= K,\\
\Y &amp; = [Y_1, \ldots, Y_n].
\end{align*}
A necessary condition for the rank condition $\text{rank}\left(\E{\Z'\X}\right) = K$ is the &lt;span style="color:red"&gt;**_order condition_**&lt;/span&gt;, $L\ge K$. In the event $L = K$ we say the model is &lt;span style="color:red"&gt;**_exactly identified_**&lt;/span&gt;. If $L &gt; K$, the model is &lt;span style="color:red"&gt;**_over-identified_**&lt;/span&gt;.
:::

When $L=K$ this is just the original IV model, and we know how to estimate this with $\IV$. In the event $L &gt; K$, the model is still identified, as we haven't modified the identifying assumptions that $\E{\Z'\ep} = \zer$, $\E{\X'\Z} \neq \zer$, and $\text{rank}\left(\E{\Z'\X}\right) = K$. In fact, our model would still be identified if we discarded $L - K$ instruments! This is where the term "over-identified" comes from, and as far as "problems" go, it's not a bad problem to have. We have so many instruments, that we have an infinite number of ways to estimate the model! 

In general, any linear combination of instruments $Z_1,\ldots, Z_L$ is also an instrument. We can only use $K$ instruments with the estimator $\IV$, so we want to find the $K$ linear combinations of our instruments (some of which could reduce to a single $Z_j$) which give us the most efficient estimator. The $K$ linear combinations of the $L$ instruments can be expressed as a $L\times K$ matrix $\F$ which gives instruments $\Z\F$. If we elect to use the new instruments $\Z\F$, the IV estimator becomes $$\IV = [(\Zm\F)'\Xm]^{-1}[(\Zm\F)'\Y].$$ The problem of selecting $\F$ such that we maximize the efficiency of $\IV$ is given as:  
$$\argmin_{\F} \text{Avar}(\IV) = \argmin_{\F} \sigma^2\E{(\Zm\F)'\Xm}^{-1}\E{(\Zm\F)'(\Zm\F)}\E{\Xm'(\Zm\F)}^{-1}.$$ Solving this looks like a miserable time, so let's stop appealing directly to math and consider what we're *actually* doing here. We're looking for the "best" instruments. It stands to reason that "good" instruments will be those that have more explanatory power with respect to the endogenous regressors $\X$ than others. So given draws of $(\Z,\X)$, how can we determine the best way to predict/explain $\X$ given $\Y$ --- by estimating the linear projection associated with $(\Z,\X)$ via OLS. For each $j = 1,\ldots,K$ we have 
\begin{align*}
\OLS^j &amp;= (\Zm'\Zm)^{-1}\Zm'\X_j &amp; (j = 1,\ldots, K)
\end{align*} where $\OLS^j$ define the instrument formed from linear combinations of $Z_1,\ldots, Z_L$ which has the most predictive power for $X_j$:
\begin{align*}
\hat X_j &amp;= \Z \OLS^j = \hat\beta_1^jZ_1 + \hat\beta_2^jZ_2 + \cdots + \hat\beta_L^jZ_L &amp; (j = 1,\ldots, K)
\end{align*}
Written compactly using matrices, we have 
\begin{align*}
\hat{\X} &amp;=  \Z(\Zm'\Zm)^{-1}\Zm'\Xm,\\
\hat{\Xm} &amp;=  \Zm(\Zm'\Zm)^{-1}\Zm'\Xm,
\end{align*}
where $\hat{\X}$ is a random vector of instruments, and $\hat{\Xm}$ is $n$ random vectors $\hat{\X}$ "stacked" to form a random matrix. If we elect to use these instruments to estimate our model, then 
$$ \IV = [(\Zm(\Zm'\Zm)^{-1}\Zm'\Xm)'\Xm]^{-1}[(\Zm(\Zm'\Zm)^{-1}\Zm'\Xm)'\Y].$$
This estimator is a special case of $\IV$ where we have determined the $K$ instruments via linear projection, and is known as two-stage least squares.

:::{#def-}
The &lt;span style="color:red"&gt;**_two-stage least squares (2SLS) estimator_**&lt;/span&gt; is defined as 
\begin{align*}
\TSLS(\Xm, \Zm, \Y) &amp;= [\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]^{-1}\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Y= [\hat{\Xm}'\Xm]^{-1}\hat{\Xm}'\Y,\\
                 \hat{\Xm} &amp;=  \Zm(\Zm'\Zm)^{-1}\Zm'\Xm.   
\end{align*}
Calculating $\hat{\Xm}$ via OLS is referred to as the &lt;span style="color:red"&gt;**_first stage_**&lt;/span&gt;, while calculating $\IV$ using $\hat{\Xm}$ is the &lt;span style="color:red"&gt;**_second stage_**&lt;/span&gt;. It can be useful to define the projection matrix associated with the first stage, 
\begin{align*}
\mathbb P_{\Zm} &amp;= \Zm(\Zm'\Zm)^{-1}\Zm'\\
 \hat{\Xm} &amp; = \mathbb P_{\Zm}\Xm.
\end{align*}
:::

The name "two-stage least squares" can be a bit misleading depending on how it is presented. It's very common to think of each stage of 2SLS as an application of OLS. In the first stage we perform OLS to calculate the instruments $\hat{\X}$, and then regress $Y$ on $\hat{\X}$ via OLS in the second stage. At first these seems at odds with our definition, because the second stage relies on the IV estimator, but it is equivalent in a sense. We'll explore this in depth in Example \@ref(exm:se).

:::{#exm-}

## OLS as 2SLS

Consider the situation from Example \@ref(exm:ref3) where $\X$ is comprised of entirely exogenous regressors. In this case our vector of instruments is actually a $L+K$ vector $[\X,\Z]$. Instead of directly calculating
\begin{align*}
\hat{\Xm} &amp;= [\Xm, \Zm]([\Xm, \Zm]'[\Xm, \Zm])^{-1}[\Xm, \Zm]'\Xm,\\
\end{align*}
we can intuit the result of the first stage. If we regress $X_j$ on $X_1,\ldots, X_j,\ldots,Z_1,\ldots,Z_L$, then all coefficients should be zero except that associated with the independent variable $X_j$ which will be 1. Therefore the OLS estimates from stage one should be a $L \times K$ matrix where the first $K$ rows are a diagonal matrix of $1$'s and the bottom $L - K$ rows are 0. If we multiply $[\Xm, \Zm]$ by this, then we have 
$$ \hat{\Xm} = \mathbf 1\Xm + \zer \Zm = \Xm.$$
This gives 
$$ \TSLS = [\hat{\Xm}'\Xm]^{-1}\hat{\Xm}'\Y = [{\Xm}'\Xm]^{-1}{\Xm}'\Y = \OLS.$$
:::

:::{#exm-}

## Exactly Identified Case

In the event that $L = K$, there is no 
:::

Because $\TSLS$ is just a special case of $\IV$ using instruments $\hat{\Xm}$, the properties of $\TSLS$ follow directly from those of $\IV$. 

:::{#thm-}

## IV Estimator is Root-n CAN

If $P_{\bet, \sigma^2} \in \mathcal P_\text{IV}$, then $\TSLS \pto \bet$ and 
$$\TSLS \asim N\left(\bet, \sigma^2\E{\hat{\Xm}'\hat{\Xm}}^{-1}\right) =  N\left(\bet, \frac{\sigma^2}{n}\E{\hat{\X}'\hat{\X}}^{-1}\right).$$ 
:::

:::{.proof}
We know that $\TSLS$ is Root-n CAN because $\IV$ is. All we need to show is that the asymptotic variance simplifies to the given expression when using instruments $\hat{\X}$.
\begin{align*}
\text{Avar}(\TSLS) &amp;= \sigma^2\E{\hat{\Xm}\Xm}^{-1}\E{\hat{\Xm}\hat{\Xm}}\E{\Xm'\hat{\Xm}}^{-1}\\
&amp; = \sigma^2\E{[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]'\Xm}^{-1}\E{[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]'[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]}\E{\Xm'[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]}^{-1}\\
&amp; = \sigma^2\E{\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm}^{-1}\text{E}[\Xm'\Zm\underbrace{(\Zm'\Zm)^{-1}\Zm'\Zm}_{\mathbf I}(\Zm'\Zm)^{-1}\Zm'\Xm]\E{\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm}^{-1}\\
 &amp; = \sigma^2\text{E}[\Xm'\underbrace{\Zm\Zm^{-1}}_{\mathbf I}\underbrace{(\Zm')^{-1}\Zm'}_{\mathbf I}\Xm]^{-1}\text{E}[\Xm'\underbrace{\Zm\Zm^{-1}}_{\mathbf I}\underbrace{(\Zm')^{-1}\Zm'}_{\mathbf I}\Xm]\E{\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm}^{-1}\\
 &amp; = \sigma^2\underbrace{\text{E}[\Xm'\Xm]^{-1}\text{E}[\Xm'\Xm]}_{\mathbf I}\E{\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm}^{-1}\\
 &amp; = \sigma^2\E{\Xm'[\Zm(\Zm'\Zm)^{-1}\Zm']'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm}^{-1} &amp; (\Zm (\Zm'\Zm)^{-1}\Zm'\text{ sym. and idem.}) \\
 &amp; = \sigma^2\E{[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]'[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]}^{-1}\\
&amp; = \sigma^2\E{\hat{\Xm}'\hat{\Xm}}^{-1} &amp; (\hat{\Xm} = \Zm(\Zm'\Zm)^{-1}\Zm'\Xm)
\end{align*}
:::

The motivation for introducing $\TSLS$ was not all choices of instruments being equal with respect to efficiency, and perhaps the instruments which result from projecting $\X$ onto $\Z$ result in an IV estimator which is more efficient than most others. This IV estimator is not just more efficient that most others, it is *the* most efficient IV estimator. The proof of this is adapted from @wooldridge2010econometric.  


:::{#thm-eff2}

## 2SLS is Asymptotically Efficient

If $P_{\bet, \sigma^2} \in \mathcal P_\text{IV}$, then $\TSLS$ is efficient in the class of $\IV$ estimators calculated using instruments linear in $\Z$. 
:::

:::{.proof}
Suppose $\tilde{\bet}$ is some arbitrary IV estimator which is linear in instruments $\Z$. The instruments for this estimator can be written as $\tilde{\X} = \Z\F$ for some $L\times K$ matrix $\F$. We want to show that $\avar{\TSLS}$ is "less than" $\avar{\tilde{\bet}}$. Both of these objects are matrices, so "less" than translates to the difference being PSD. This difference is 
\begin{align*}
\avar{\tilde{\bet}} - \avar{\TSLS} &amp; =\frac{\sigma^2}{n}\E{\tilde{\X}'\X}^{-1}\E{\tilde{\X}'\tilde{\X}}\E{\X'\tilde{\X}}^{-1} -  \frac{\sigma^2}{n}\E{\hat{\X}'\hat{\X}}^{-1}\\&amp; = \frac{\sigma^2}{n}\left[\E{\tilde{\X}'\X}^{-1}\E{\tilde{\X}'\tilde{\X}}\E{\X'\tilde{\X}}^{-1} - \E{\hat{\X}'\hat{\X}}^{-1}\right]\\
&amp; = \frac{\sigma^2}{n}\left[\left[\E{\tilde{\X}'\X}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\X'\tilde{\X}}\right]^{-1} - \E{\hat{\X}'\hat{\X}}^{-1}\right] &amp; (\text{properties of inversion})
\end{align*}
Accounting for the bracketed term being the difference of two inverted matrices, this matrix will be PSD if and only if the following is PSD: 
\begin{equation}
\E{\hat{\X}'\hat{\X}} - \E{\tilde{\X}'\X}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\X'\tilde{\X}}  (\#eq:diff).
\end{equation}

We can show this by relying on a certain "trick". Define the difference between the regressors $\X$ and 2SLS instruments $\hat{\X}$ as $\mathbf r = \hat{\X} -  \X$. This remainder term $\mathbf r$ is uncorrelated with $\Z$:
\begin{align*}
\E{\Z'\mathbf r} &amp; = \E{\Z'\hat{\X}} - \E{\Z'\X}\\
&amp; = \E{\Z'\Z(\Z'\Z)^{-1}\Z'\X} - \E{\Z'\X} &amp; (\hat{\X} = \Z(\Z'\Z)^{-1}\Z'\X)\\
&amp; = \E{\Z'\X} - \E{\Z'\X}\\
&amp; = \zer
\end{align*}
As a result, $\tilde{ \X}$ and $\mathbf r$ are uncorrelated. $$\E{\tilde{ \X}'\mathbf r} = \E{(\Z\F')\mathbf r}= \mathbf F'\underbrace{\E{\Z'\mathbf r}}_\zer = \zer$$ Now we use these expectations and the definition of $\mathbf r$ to arrive at our "trick".
\begin{align*}
&amp;\E{\tilde{ \X}'\mathbf r} = \zer \\
\implies &amp; \E{\tilde{ \X}'(\hat{\X} -  \X)} = \zer &amp; (\mathbf r = \hat{\X} -  \X)\\
\implies &amp; \E{\tilde{ \X}'\hat{\X}} - \E{\tilde{ \X}'\X} = \zer\\
\implies &amp; \E{\tilde{ \X}'\hat{\X}} = \E{\tilde{ \X}'\X}
\end{align*}
Using this equality, we can rewrite the difference in Equation \@ref(eq:diff) in terms of $\tilde{\X}$ and $\hat{\X}$ only.
\begin{equation}
\E{\hat{\X}'\hat{\X}} - \E{\hat{\X}'\tilde{\X}}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}  (\#eq:diff2).
\end{equation} This may seem like a random equation, but it's very special if you consider the linear projection of $\hat{\X}$ onto $\tilde{\X}$. This projection is given by the coefficient $\boldsymbol\gamma = \E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}},$ which happens to show up in \@ref(eq:diff2). The errors associated with this linear projection are given as $\boldsymbol \nu = \hat{\X} - \tilde{\X}\boldsymbol\gamma$, and have an expected value of $\zer$ by the definition of $\boldsymbol\gamma$ and linear projection. But what is the expected value of these errors squared? 

\begin{align*}
 \boldsymbol \nu' \boldsymbol \nu&amp; = [\hat{\X} - \tilde{\X}\boldsymbol\gamma]'[\hat{\X} - \tilde{\X}\boldsymbol\gamma]\\
&amp; = \hat{\X}'\hat{\X} - 2\boldsymbol\gamma'\tilde{\X}'\hat{\X} + \boldsymbol\gamma'\tilde{\X}'\tilde{\X}\boldsymbol\gamma\\
\E{ \boldsymbol \nu' \boldsymbol \nu} &amp; = \E{\hat{\X}'\hat{\X}} - 2\boldsymbol\gamma'\E{\tilde{\X}'\hat{\X}} + \boldsymbol\gamma'\E{\tilde{\X}'\tilde{\X}}\boldsymbol\gamma &amp; (\boldsymbol\gamma \text{ is a constant})\\
&amp; = \E{\hat{\X}'\hat{\X}} - 2\left[\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}\right]'\E{\tilde{\X}'\hat{\X}} + \left[\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}\right]'\E{\tilde{\X}'\tilde{\X}} \left[\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}\right]\\
&amp; = \E{\hat{\X}'\hat{\X}} - 2\E{\tilde{\X}\hat{\X}'}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}} + \E{\tilde{\X}\hat{\X}'}\underbrace{\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\tilde{\X}}}_{\mathbf I} \E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}\\
&amp; = \E{\hat{\X}'\hat{\X}} - (2-1)\E{\tilde{\X}\hat{\X}'}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}} \\
&amp; = \E{\hat{\X}'\hat{\X}} - \E{\hat{\X}'\tilde{\X}}\E{\tilde{\X}'\tilde{\X}}^{-1}\E{\tilde{\X}'\hat{\X}}
\end{align*}
Equations \@ref(eq:diff) and \@ref(eq:diff2) are the expectation of the sum of squared errors associated with the linear projection of $\hat{\X}$ onto $\tilde{\X}$, meaning the matrix in question must be PSD!^[Intuitively, this matrix must be PSD in the same way the sum of squared numbers cannot be negative.] This gives the desired result.
:::

In Section \@ref(generalized-method-of-moments) we'll see another way to show this result that is a bit more intuitive. 


:::{#exm-se}

## What's in A Name?

Let's estimate the model from \@ref(exm:ex4) via 2SLS. We already have defined a function ```IV()``` to calculate $\IV$, so we can just use pass the instruments $\hat {\X}$ to this function to perform 2SLS.

quarto-executable-code-5450563D

```r
TSLS &lt;- function(y,X,Z){
  X_hat &lt;- Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% X
  IV(y, X, X_hat)
}
```

For a simulated sample of size $n=50$ let's estimate the model using ```TSLS()```.  

quarto-executable-code-5450563D

```r
n &lt;- 50
mu &lt;- c(10, 10, 10, 0)
Sigma &lt;- matrix(c(20,5,10,1,5,30,7,0,10,7,50,0,1,0,0,1), nrow = 4)
sample &lt;- rmvnorm(n, mu, Sigma)
x &lt;- sample[,1] 
z1 &lt;- sample[,2]
z2 &lt;- sample[,3]
e &lt;- sample[,4]
y &lt;- 1 + 2*x + e
X &lt;- cbind(1, x)
Z &lt;- cbind(1, z1, z2)
TSLS(y,X,Z)
```

This is not the only way we could calculate $\TSLS$. Note that $\Zm(\Zm'\Zm)^{-1}\Zm'$ is symmetric and idempotent, so 
\begin{align*}
\TSLS &amp; = \IV(\Xm, \hat{\Xm}, \Y)\\
&amp; = [\hat{\Xm}'{\Xm}]^{-1}\hat{\Xm}'\Y\\
&amp; = [\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]^{-1}\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Y\\
&amp; = [\Xm'[\Zm(\Zm'\Zm)^{-1}\Zm']\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]^{-1}\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Y\\
&amp; = [[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]'[\Zm(\Zm'\Zm)^{-1}\Zm'\Xm]]^{-1}\Xm'\Zm(\Zm'\Zm)^{-1}\Zm'\Y\\
&amp; = [\hat{\Xm}'\hat{\Xm}]^{-1}\hat{\Xm}'\Y\\
&amp; = \OLS(\hat{\Xm}, \Y).
\end{align*}
So we can interpret $\TSLS$ in one of two ways:

1. Use $\OLS(\Zm, \Xm)$ to calculate $\hat{\Xm} = \Zm\OLS(\Zm, \Xm)$ (stage 1), followed by $\IV(\Xm, \hat{\Xm}, \Y)$ (stage 2). This is how our ```TSLS()``` function works.
2. Use $\OLS(\Zm, \Xm)$ to calculate $\hat{\Xm} = \Zm\OLS(\Zm, \Xm)$ (stage 1), followed by $\OLS(\hat{\Xm}, \Y)$ (stage 2).

The second interpretation is where 2SLS gets its name -- we run OLS twice. But what happens when we implement $\TSLS$ this way?
 
quarto-executable-code-5450563D

```r
X_hat &lt;- lm(x ~ z1 + z2)$fitted.values
summary(lm(y ~ X_hat))$coefficients
```

We get the same exact estimates (which we already showed would be the case), but our standard errors don't coincide with those given by ```TSLS()```. What is happening here? 

When we run ```lm(y ~ X_hat)```, we are estimating the model associated with linear projection model $Y = \hat{\X}\boldsymbol \delta + \nu$. The function ```lm()``` has no way of knowing that this is the second step in an estimation process aimed at estimating the structural linear model $Y = \X\bet + \varepsilon$. It just happens that $\hat{\X}$ is defined such that the population parameters satisfy $\boldsymbol \delta = \bet$. When ```lm()``` goes to calculate the standard errors, it calculates the residuals associated with the model $Y = \hat{\X}\boldsymbol \delta + \nu$. These errors have no structural/economic meaning, and we don't care about them at all (for this purpose). We really want the residuals associated with the error $\varepsilon$ in the actual IV model $Y = \X\bet + \varepsilon$, and these residuals are not the same!
$$ \hat{\mathbf u} = \Y - \hat{\Xm}\OLS(\hat{\Xm}, \Y) \neq \Y - \Xm\OLS(\hat{\Xm}, \Y) = \Y- \Xm\IV(\Xm, \hat{\Xm}, \Y) = \hat{\e}$$
The moral of the story is that when we are calculating the residuals we need to be very deliberate and remember the actual model we are estimating. Fortunately, any statistical software with an implementation for $\TSLS$ will calculate the correct standard errors, which is why it's best to not do each step "by hand" if you favor the interpretation of $\TSLS$ as two OLS regressions.

quarto-executable-code-5450563D

```r
summary(ivreg(y ~ x | z1 + z2))$coefficients
```

:::

The example emphasizes something that has been mentioned in passing before. The models estimated in the first and second stage, $\X = \Z\boldsymbol + \gamma$ and  $Y = \hat{\X}\boldsymbol \delta + \nu$, are entirely void of structural meaning. Nevertheless it is useful to us because it expresses the outcome of interest in terms of exogenous variables $\hat{\X}$ and $\Z$. We refer to such an equation as the **_reduced form_** of the structural model $Y = \X\bet + \ep$. We'll talk about this term much more in Section \@ref(endogeniety-ii-simultaneous-equation-models).

## Testing Hypotheses

At this point, we have just assumed we know how to select between the classical linear model $\mathcal P_\text{LM}$ and the linear IV model $\mathcal P_\text{IV}$, but in practice we want to be able to form hypothesis tests that inform our selection between the two models. In particular we want to be able to test: 

1. Are our regressors exogenous, i.e $H_0: \E{\X'\ep} = \zer$?
2. Are our instruments valid, i.e...

### Durbin–Wu–Hausman Test

We'll start with considering tests for exogenous/endogenous regressors. One sign that our model contains endogenous regressors is if there is a significant difference between $\OLS$ and $\IV$. This is why it's always a good idea to estimate a model with $\OLS$ even if you think regressors are endogenous. It provides a good reference to compare IV estimates with. In fact, comparing $\OLS$ and $\IV$ serve as the basis for one of the most common tests for endogeneity as formalized by @hausman1978specification.^[This happens to be one of the most cited papers in all of economics.] 

In the face of endogeneity, $\OLS$ is an abject failure in light of its inconsistency. Fortunately, $\IV$ is consistent when $\X$ is endogenous. Furthermore, $\IV$ is also consistent when $\X$ is exogenous, as the instruments $\Z$ are still valid and relevant. In other words, if $\X$ is exogenous, then $\OLS\pto \bet$ *and* $\IV\pto\bet$, so 
$$ \OLS - \IV \pto \zer.$$ This suggests the following: 
$$ H_0: \X \text{ exogenous} \iff H_0:\plim\left(\OLS - \IV\right) = \zer.$$ We can construct a Wald test statistic
$$ W = (\OLS - \IV)' \left[\widehat{\text{Avar}}(\OLS - \IV) \right]^{-1}(\OLS - \IV),$$ but we will run into a roadblock in the form of $\avar{\OLS - \IV}$. If we expand this, we have 

$$ \avar{\OLS - \IV} = \avar{\IV} + \avar{\OLS} - 2\text{Acov}\left(\IV,\OLS\right),$$ as $\IV$ and $\OLS$ are presumably correlated. We don't have a formula for the covariance of these estimators handy, and deriving one would be a pain. Fortunately, we don't have to, because Lemma 2.1 of @hausman1978specification asserts that the covariance term is such that 
$$  \avar{\OLS - \IV} =  \avar{\IV} - \avar{\OLS}$$
due to $\OLS$ being efficient relative to $\IV$.^[In Example \@ref(exm:ref3) we saw that $\OLS$ is more efficient than $\IV$ in the event that our regressors are exogenous. This was formalized in Theorem \@ref(thm:eff2), because in the event that all regressors $\X$ are exogenous and we have non-trivial instruments $\Z$, then $\TSLS$ "picks" the optimal trivial instruments $\X$ and our estimator reduces to $\OLS$.] This is a particular useful equality, and interesting result in it's own right. @hansen2022econometrics refers to it as the "Hausman equality", and gives details about it in Section 8.11. This gives the test statistic
\begin{align*}
W &amp;= (\OLS - \IV)'\left[\widehat{\text{Avar}}(\IV) - \widehat{\text{Avar}}(\OLS)\right]^{-1} (\OLS - \IV).
\end{align*}

Unfortunately, the term corresponding to the asymptotic variance will likely fail to be invertible. Unless $\X$ and $\Z$ contain no common variables, some column of  $(\hat{\Xm}'\hat{\Xm})^{-1} - (\Xm'\Xm)^{-1}$ will be a linear combination of other columns. For example, if our model contains an intercept, than we cannot use this statistic, as $\X$ and $\Z$ will both contain a column of 1's. This rules out just about every single situation we would ever want to consider. To address this shortcoming, we can take the [pseudo-inverse](https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html) of this matrix.

$$W = (\OLS - \IV)'\left[\widehat{\text{Avar}}(\IV) - \widehat{\text{Avar}}(\OLS)\right]^+ (\OLS - \IV)$$
The hypothesis $\OLS - \IV\pto \zer$ is comprised of $K$ separate hypotheses, so it's tempting to conclude  $W\sim \chi_K^2$. Unfortunately, this will only be the case when $\avar{\IV} - \avar{\OLS}$ is invertible, otherwise we will have $W\sim \chi_q^2$ where $q = \text{rank} \left[ \avar{\IV} - \avar{\OLS} \right]$.

:::{#thm-}

## Hausman Specification Test

Given the hypothesis $H_0:\hat{\thet}_1\pto\thet, \hat{\thet}_2\pto\thet$ for some asymptotically efficient estimator $\hat{\thet}_1$, the &lt;span style="color:red"&gt;**_Hausman statistic_**&lt;/span&gt; is defined as 
$$H = (\hat{\thet}_2 - \hat{\thet}_1)'\left[\widehat{\text{Avar}}(\hat{\thet}_2) - \widehat{\text{Avar}}(\hat{\thet}_1)\right]^+ (\hat{\thet}_2 - \hat{\thet}_1).$$ Under $H_0$,
$$ H\sim \chi_q^2$$
where $q = \text{rank}\left[\avar{ \hat{\thet}_2} - \avar{ \hat{\thet}_1}\right]$. The test associated with this statistic is known as the &lt;span style="color:red"&gt;**_Hausman specification test_**&lt;/span&gt;. 
:::
 
The Hausman test is far more general that testing for endogeneity, it just happens that testing for endogeneity is a great application of it. In this particular application, there is a much more convenient formulation that allows us to bypass the pseudo-inverse complication. We'll begin by partitioning our regressors into the exogenous and endogenous regressors, i.e $\X = [\X_\text{exo},\X_\text{end}]$. The $\OLS$ and $\IV$ estimators can also be partitioned accordingly.
\begin{align*}
\hat\beta_\text{OLS} &amp; = \begin{bmatrix}\hat\beta_\text{OLS,exo}  \\ \hat\beta_\text{OLS,end}\end{bmatrix}\\
\hat\beta_\text{IV} &amp; = \begin{bmatrix}\hat\beta_\text{IV,exo}  \\ \hat\beta_\text{IV,end}\end{bmatrix}
\end{align*}
With some clever algebra, we can show that $(\hat\beta_\text{OLS,exo} - \hat\beta_\text{IV,exo})$ is a linear function of $(\hat\beta_\text{IV,end} - \hat\beta_\text{OLS,end})$. The estimator $\OLS$ solves the first order condition associated with minimizing the sum of squared residuals: $$\Xm'\Xm\OLS = \Xm'\Y.$$ If we partition this first order condition according to $\X = [\X_\text{exo},\X_\text{end}]$, we have 

\begin{align*}
&amp;\begin{bmatrix} \Xm_\text{exo} &amp; \Xm_\text{end}\end{bmatrix}'\begin{bmatrix} \Xm_\text{exo} &amp; \Xm_\text{end}\end{bmatrix}\begin{bmatrix} \hat\beta_\text{OLS,exo} \\ \hat\beta_\text{OLS,end}\end{bmatrix} = \begin{bmatrix} \Xm_\text{exo} &amp; \Xm_\text{end}\end{bmatrix}'\Y\\
\implies &amp; \begin{bmatrix} \Xm_\text{exo}' \\ \Xm_\text{end}' \end{bmatrix} \begin{bmatrix} \Xm_\text{exo} &amp; \Xm_\text{end} \end{bmatrix} \begin{bmatrix} \hat\beta_\text{OLS,exo} \\ \hat\beta_\text{OLS,end}\end{bmatrix} = \begin{bmatrix} \Xm_\text{exo}' \\ \Xm_\text{end}'\end{bmatrix} \Y \\
\implies &amp; \begin{bmatrix} \Xm_\text{exo}'\Xm_\text{exo} \hat\beta_\text{OLS,exo} + \Xm_\text{exo}' \Xm_\text{end}\hat\beta_\text{OLS,end} \\ 
 \Xm_\text{end}'\Xm_\text{exo}\hat\beta_\text{OLS,exo} + \Xm_\text{end}'\Xm_\text{end}\hat\beta_\text{OLS,end} \end{bmatrix} = \begin{bmatrix} \Xm_\text{exo}'\Y \\ \Xm_\text{end}'\Y \end{bmatrix}\\ 
 \implies &amp; \Xm_\text{exo}'\Xm_\text{exo} \hat\beta_\text{OLS,exo} + \Xm_\text{exo}' \Xm_\text{end}\hat\beta_\text{OLS,end} = \Xm_\text{exo}'\Y &amp; (\text{First Row})\\ 
 \implies &amp; \Xm_\text{exo}'\Xm_\text{exo} \hat\beta_\text{OLS,exo}  = \Xm_\text{exo}'\left(\Y -  \Xm_\text{end}\hat\beta_\text{OLS,end}\right)
\end{align*}

Solving this for $\hat\beta_\text{OLS,exo}$ gives

$$ \hat\beta_\text{OLS,exo} = \left(\Xm_\text{exo}'\Xm_\text{exo}\right)^{-1}\Xm_\text{exo}'\left(\Y -  \Xm_\text{end}\hat\beta_\text{OLS,end}\right) .$$ {#eq-63}

We can repeat these calculations for $\IV$, and conclude 

$$ \hat\beta_\text{OLS,exo} = \left(\Xm_\text{exo}'\Xm_\text{exo}\right)^{-1}\Xm_\text{exo}'\left(\Y -  \Xm_\text{end}\hat\beta_\text{IV,end}\right) .$$ {#eq-64}

If we subtract @eq-64 from @eq-63, we have 

$$ \hat\beta_\text{OLS,exo} - \hat\beta_\text{IV,exo} = \left(\Xm_\text{exo}'\Xm_\text{exo}\right)^{-1}\Xm_\text{exo}'\Xm_\text{end}\left(\hat\beta_\text{IV,end} - \hat\beta_\text{OLS,end}\right).$$

This equality means that if $\plim(\OLS - \IV)$ (in which case the difference in the exogenous components approaches zero), then the difference in the endogenous portions approach zero as well. In short, we lose nothing from focusing on the difference $\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end}$, which means the variance term which standardizes the Hausman statistic has full rank. 

$$ H = (\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat\beta_\text{IV,end}) - \widehat{\text{Avar}}(\hat\beta_\text{OLS,end})\right]^{-1} (\hat\beta_\text{OLS,end} - \hat\beta_\text{IV,end})$$ 

:::{#cor-hausend}

## Hausman Test for Endogeneity

Given the hypothesis $H_0:\E{\X'\ep} = \zer$ where regressors are partitioned as $\X = [\X_\text{exo},\X_\text{end}]$, the &lt;span style="color:red"&gt;**_Hausman statistic_**&lt;/span&gt; simplifies to
$$H = (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end}) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})\right]^{-1} (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end}).$$ Under $H_0$,
$$ H\sim \chi_q^2$$
where $q = \dim(\X_\text{end})$. I will call the test associated with this statistic is known as the &lt;span style="color:red"&gt;**_Hausman test for endogeneity_**&lt;/span&gt;. 
:::

:::{#exm-}

Suppose $(X, Z, \varepsilon) \sim N(\boldsymbol \mu,\Sig)$ where
\begin{align*}
\boldsymbol \mu &amp; = [10,10,0]',\\
\Sig &amp; = \begin{bmatrix}20 &amp; 5 &amp; 0\\5&amp;20&amp;0\\0&amp;0&amp;1 \end{bmatrix},
\end{align*}
and $Y = 1 + 3X + \varepsilon$. We have $\cov{X,\varepsilon} = 0$, so $\E{X\varepsilon} = 0$ and our model meets the assumptions of the classical linear model ($P_{\bet,\sigma^2} \in \mathcal P_\text{LM}\subset\mathcal P_\text{IV}$). If we draw a sample of size $n = 1,00$, we can test $H_0:\E{X\varepsilon} = 0$ using the Hausman statistic calculated using only $\beta_2$ (the slope coefficient). If we repeat this simulation many times, then the distribution of our test statistic should approach $\chi_1^2$, as $H_0$ is true for our model. 

quarto-executable-code-5450563D

```r
hausman_stat &lt;- function(y, X, Z, end_col){
  K &lt;- ncol(X)
  
  # Calculate OLS and variance
  OLS &lt;- solve(t(X) %*% X) %*% t(X) %*% y
  res &lt;- (y-X %*% OLS)
  S2 &lt;- ((t(res) %*% res)/(n - K)) %&gt;% as.numeric() 
  var_OLS &lt;- (S2) * solve( t(X) %*% X )
  
  # calculate IV and variance
  IV &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y
  res &lt;- (y-X %*% IV)
  S2 &lt;- ((t(res) %*% res)/(n - K)) %&gt;% as.numeric() 
  var_IV &lt;- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z)
  
  # construct the statistic using only the endogenous columns given by end_col
  i &lt;- end_col
  (OLS[i] - IV[i])*solve(var_IV[i,i] - var_OLS[i,i])*(OLS[i] - IV[i])
}

store &lt;- vector("numeric", 10000)
for (j in 1:10000) {
  n &lt;- 1000
  mu &lt;- c(10, 10, 0)
  Sigma &lt;- matrix(c(20,5,0, 5,20,0,0,0,1), nrow = 3)
  sample &lt;- rmvnorm(n, mu, Sigma)
  x &lt;- sample[,1]
  z &lt;- sample[,2]
  e &lt;- sample[,3]
  y &lt;- 1 + 3*x + e
  X &lt;- cbind(1, x)
  Z &lt;- cbind(1, z)
  K &lt;- ncol(X)
  
  store[j] &lt;- hausman_stat(y, X, Z, 2)
}
```

Now let's plot the simulated test statistics.

quarto-executable-code-5450563D

```r
#| code-fold: true
#| fig-align: center
#| warning: false
#| message: false
#| fig-asp: 1
#| fig-width: 11
#| fig-cap: "Histogram of the Hausman statistic calculated for 10,000 simulations with limiting distribution overlaid"
#| code-summary: "Show code which generates figure"
tibble(store) %&gt;% 
  ggplot(aes(store)) +
  geom_histogram(aes(y=..density..), color = "black", fill = "white", binwidth = 0.1) +
  geom_function(fun = dchisq,  args = list(df = 1), color = "red") +
  theme_minimal() +
  ylim(0,1.5)
```

:::

Another means of testing for exogeneity is due to  @wu1973alternative and @durbin1954errors. Begin by performing the first step of 2SLS on the possible endogenous regressors.
$$ \Xm_\text{end} =  \Zm \boldsymbol \delta + \mathbb V$$ The residuals now form a matrix, because we have multiple dependent variables $\X_\text{end}$.
Using the estimated projection coefficient $\hat{\boldsymbol \delta}$, we calculated the residual values $\hat{\mathbb V}$. Finally we augment the original linear model by including $\hat{\mathbb V}$.

$$\y = \Xm\bet + \hat{\mathbb V}\boldsymbol\gamma + \ep^* = \begin{bmatrix} \Xm_\text{exo} &amp; \Xm_\text{end}\end{bmatrix}\begin{bmatrix} \bet_\text{exo} \\ \bet_\text{end}\end{bmatrix} + \underbrace{\hat{\mathbb V}\gamma + \boldsymbol {\ep}^*}_{\ep}$$ Consider the hypothesis $H_0:\boldsymbol\gamma = \zer$, and how that may related to whether $\Xm_\text{end}$ is endogenous or exogenous. When we perform the first step of this process (regressing $\Xm_\text{end}$ on $\Zm$) we break variation in $\Xm_\text{end}$ into two parts: the exogenous portion explained through $\Zm$,^[We know it's exogenous because $\Zm$ is a valid instrument and is exogenous], and unexplained part $\boldsymbol \nu$ which we capture through $\hat{\mathbb V}$. If $\boldsymbol \gamma \neq \zer$, then the structural error $\ep$ includes the unexplained part of the variation in $\X_\text{end}$, which is another way of saying $\cov{\X_\text{end}, \varepsilon}\neq 0$. 

So do we use the Hausman test, or this new test which uses artificial regressions? Well it turns out, we don't need to make a choice because they're equivalent if we are attentive about which residuals we use to calculate asymptotic variance! It also turns out that we could perform the artificial regression test using the fitted values $\hat{\Xm}_\text{end}$ instead of $\hat{\mathbb V}$. 



:::{#thm-}

## Durbin-Wu-Hausman Test

Suppose we want to test $H_0:P_{\bet, \sigma^2}\in\mathcal P_\text{LM}$ (i.e $H_0: \X$ exogenous) for $P_{\bet, \sigma^2}\in \mathcal P_\text{IV}$. The following test statistics are equivalent:

 
1. The Wald statistic associated with the null hypothesis $H_0: \boldsymbol\gamma = \zer$ calculated using $\hat{\boldsymbol\gamma}_\text{OLS}$, where $\boldsymbol\gamma$ is from the modified regression $\y = \Xm\bet + \hat{\mathbf v}\boldsymbol\gamma + \ep^*$, and $\hat{\mathbf v}$ are the residuals associated with the first step projection $\Xm_\text{end} =  \Zm \boldsymbol \delta + \boldsymbol \nu$. $$W_{\boldsymbol\gamma}= \hat{\boldsymbol\gamma}_\text{OLS}' \left[\widehat{\text{Avar}}(\hat{\boldsymbol\gamma}_\text{OLS} )\right]^{-1}\hat{\boldsymbol\gamma}_\text{OLS}$$
2. The Wald statistic associated with the null hypothesis $H_0: \boldsymbol\eta = \zer$ calculated using $\hat{\boldsymbol\eta}_\text{OLS}$, where $\boldsymbol\gamma$ is from the modified regression $\y = \Xm\bet + \hat{\Xm}_\text{end}\boldsymbol\eta + \ep^*$, and $\hat{\Xm}_\text{end}$ are the fitted values associated with the first step projection $\Xm_\text{end} =  \Zm \boldsymbol \delta + \boldsymbol \nu$. $$W_{\boldsymbol\eta}= \hat{\boldsymbol\eta}_\text{OLS}' \left[\widehat{\text{Avar}}(\hat{\boldsymbol\eta}_\text{OLS} )\right]^{-1}\hat{\boldsymbol\eta}_\text{OLS}$$
3. The Hausman statistics from @cor-hausend, $$H = (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end})'\left[\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end}) - \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})\right]^{-1} (\hat{\boldsymbol\beta}_\text{OLS,end} - \hat{\boldsymbol\beta}_\text{IV,end}),$$ when $\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{IV,end})$ and $\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS,end})$ are calculated using $S^2$ from the artificial regression $\y = \Xm\bet + \hat{\mathbf v}\boldsymbol\eta + \ep^*$.^[We could also use calculate $S^2$ from $\y = \Xm\bet + \hat{\Xm}_\text{end}\boldsymbol\eta + \ep^*$, as these two estimates for $\sigma^2$ are algebraically equivalent.] 

:::

:::{.proof}

&lt;span style="color:white"&gt;space&lt;/span&gt;

$(W_{\boldsymbol\gamma} = W_{\boldsymbol\eta})$ Recall that we think of $\hat{\mathbf v}$ and $\hat{\Xm}_\text{end}$ in terms of linear projection. We have 

\begin{align*}
\hat{\Xm}_\text{end} &amp; = \mathbb P_{\Zm}\Xm_\text{end} &amp; (\mathbb P_{\Zm} = \Zm(\Zm'\Zm)^{-1}\Zm')\\
\hat{\mathbb V} &amp; =  \mathbb M_{\Zm}\Xm_\text{end} &amp; (\mathbb M_{\Zm} = \mathbb I - \mathbb P_{\Zm})
\end{align*}

$(W_{\boldsymbol\gamma} = H)$

:::


:::{#exm-}
test

quarto-executable-code-5450563D

```r
n &lt;- 10000
mu &lt;- c(10, 10, 0)
Sigma &lt;- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)
sample &lt;- rmvnorm(n, mu, Sigma)
x &lt;- sample[,1]
z &lt;- sample[,2]
e &lt;- sample[,3]
y &lt;- 1 + 3*x + e
X &lt;- cbind(1, x)
Z &lt;- cbind(1, z)
K &lt;- ncol(X)

## Method 1
reg_1.1 &lt;- lm(x ~ z)
v_hat &lt;- reg_1.1$residuals
reg_1.2 &lt;- lm(y ~ x + v_hat)
summary(reg_1.2)$coefficients
```


quarto-executable-code-5450563D

```r
summary(reg_1.2)$coefficients[3,3]^2
```

quarto-executable-code-5450563D

```r
## Method 2
reg_2.1 &lt;- lm(x ~ z)
x_hat &lt;- fitted.values(reg_2.1)
reg_2.2 &lt;- lm(y ~ x + x_hat)
summary(reg_2.2)$coefficients
```  


quarto-executable-code-5450563D

```r
summary(reg_2.2)$coefficients[3,3]^2
```


quarto-executable-code-5450563D

```r
c(sum(reg_2.2$residuals^2), 
  sum(reg_1.2$residuals^2))
```

quarto-executable-code-5450563D

```r
#Method 3
S2 &lt;- sum(reg_2.2$residuals^2)/(n-K)
OLS &lt;- solve(t(X) %*% X) %*% t(X) %*% y
var_OLS &lt;- (S2) * solve( t(X) %*% X )
IV &lt;- solve(t(Z) %*% X) %*% t(Z) %*% y
var_IV &lt;- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z)

(OLS[2] - IV[2])*solve(var_IV[2,2] - var_OLS[2,2])*(OLS[2] - IV[2])
```

quarto-executable-code-5450563D

```r
summary(ivreg::ivreg(y ~ x | z))
```

:::

### Sargan–Hansen Test


## Example/Replication

Let's replicate @card1995using, a *classic* paper which has provided a textbook example of instrumental variables. To make sure our results are correct, we can compare them to the tables in the original paper which is found [here](https://www.nber.org/system/files/working_papers/w4483/w4483.pdf). The data is available on David Card's [website](https://davidcard.berkeley.edu/data_sets.html).

@card1995using is concerned with estimating the returns to schooling with respect to wages. We tend to think that better educated workers earn higher wages, but to what extent does this relationship hold? Denote years of schooling as $S_i$, log wages as $Y_i$, and $\X_i$ as a collection of attributes associated with a worker. The linear model of interest is 
$$ Y_i = \X_i\boldsymbol\alpha + S_i\beta + u_i,$$ where $u_i$ are unobserved components which affect earnings. We're assuming that $\E{\X_iu_i} = 0$ for all $i$ ($\X_i$ is exogenous), but we suspect that $\E{S_iu_i} = 0$ (see Example \@ref(exm:ex1)). The parameter $\beta$ is the returns of education on earnings, and is what we're primarly interested in. Let's just ignore this endogeneity for now and attempt to estimate $\beta$ with OLS. We can do this with data from the National Longitudinal Survey of Young Men (NLSYM) conducted in 1976. This survey contains information about agents' wages in 1976, along with demographic information about family background, residence in 1976, and past residence (in 1966). The details of this survey, in particular how respondents were sampled, can be found in @card1995using.

```{r, message=FALSE}
card_95 &lt;- read.table("data/nls.dat")
```

The ```.zip``` file containing the data also contains a ```.sas``` file which processes and cleans the raw data. Each step is readily executed in R:^[I did this to the best of my ability, as I have no experience with ```.sas```, and I could have misinterpreted the accompanying documentation and commentary in the code.]

```{r, message=FALSE}
card_95 &lt;- card_95 %&gt;%
  #assign the appropriate names to each variable
  rename(
    id = V1,
    nearc2 = V2,
    nearc4 = V3,
    nearc4a = V4,
    nearc4b = V5,
    ed76 = V6,
    ed66 = V7,
    age76 = V8,
    daded = V9,
    nodaded = V10,
    momed = V11,
    nomomed = V12,
    weight = V13,
    momdad14  = V14,
    sinmom14  = V15,
    step14 = V16,
    reg661 = V17,
    reg662 = V18,
    reg663 = V19,
    reg664 = V20,
    reg665 = V21,
    reg666 = V22,
    reg667 = V23,
    reg668 = V24,
    reg669 = V25,
    south66 = V26,
    work76 = V27,
    work78 = V28,
    lwage76 = V29,
    lwage78 = V30,
    famed = V31,
    black = V32,
    smsa76r = V33,
    smsa78r = V34,
    south76 = V35,
    reg78r = V36,
    reg80r = V37,
    smsa66r = V38,
    wage76 = V39,
    wage78 = V40,
    wage80 = V41,
    noint78 = V42,
    noint80 = V43,
    enroll76 = V44,
    enroll78 = V45,
    enroll80 = V46,
    kww = V47,
    iq = V48,
    marsta76 = V49,
    marsta78 = V50,
    marsta80 = V51,
    libcrd1 = V52
  ) %&gt;% 
  mutate(
    # missing values in SAS are ".", replace those with NAs
    across(everything(), ~replace(., . ==  "." , NA)),
    # any column that had a "." was loaded as a string, convert them
    across(where(is.character), as.numeric),
    #create variables corresponding to work experience
    exp76 = age76 - ed76 - 6,
    exp76_sq = exp76*exp76,
    exp76_sq_100 = (exp76*exp76)/100,
    #create a series of indicators for parents' level of education
    f1 = (momed &gt; 12 &amp; daded &gt; 12),
    f2 = (momed &gt;= 12 &amp; daded &gt;= 12 &amp; (momed != 12 | daded != 12)),
    f3 = (momed == 12 &amp; daded == 12),
    f4 = (momed &gt;= 12 &amp; nodaded == 1),
    f5 = (daded &gt;= 12 &amp; momed &lt; 12),
    f6 = (momed &gt;= 12 &amp; nodaded == 0),
    f7 = (momed &gt;= 9 &amp; daded &gt;= 9),
    f8 = (nomomed == 0 &amp; nodaded == 0)
  )

#Gather mutually exclusive region66 indicators into categorical 
card_95 &lt;- card_95 %&gt;% 
  select(id, contains("reg66")) %&gt;% 
  gather(region66, count, -id) %&gt;% 
  filter(count == 1) %&gt;% 
  mutate(
    region66 = str_remove(region66,"reg66"),
    region66 = as.factor(region66)
  ) %&gt;% 
  right_join(card_95)
```

The dependent variable of interest if $\log wage_i$ (```lwage76```). Our baseline linear model is
$$ \log( wage_i) = \alpha_0 + \beta\cdot educ_i + \alpha_1\cdot exper_i + \frac{\alpha_2}{100}\cdot exper_i^2 + \alpha_3 \cdot black_i + \alpha_4\cdot south_i + \alpha_5\cdot SMSA_i + \varepsilon_i,$$ where $educ_i$ (```ed76```) is level of education, $exper_i$ (```exp76```) is amount of work experiance, $black_i$ (```black```) indicated if respondent $i$ is Black, $south_i$ (```south76```) indicated whether the respondent lives in the South (in 1976), and $SMSA_i$ (```smsa76r```) indicates whether a respondent lives in the South and in a metropolitan area.^[The abbreviation "MSA" comes up a lot when dealing with surbey data. It stands for "metropolitan statistical area", and are used by the Census Bureau to designate areas with a high population density.] Along with estimating this model, we will estimate similar models which include various controls for family education, family structure, and where respondents lived in 1966. 

quarto-executable-code-5450563D

```r
linear_model1 &lt;- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r, data = card_95)
linear_model2 &lt;- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)
linear_model3 &lt;- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 +
                      daded + momed + nodaded + nomomed, data = card_95)
linear_model4 &lt;- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r +
                      region66 + daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + 
                      f5 + f6 + f7 + f8, data = card_95)
linear_model5 &lt;- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r +
                     region66+ daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + 
                      f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)
```

Our results should match Table 2 of @card1995using. The ```stargazer``` package gives us a flexible means of tabulating regression results.^[Right now, the table is formatted in html and automatically included when this Rmarkdown file is knit, but you ```stargazer``` is also capable of outputting very nice tables in $\LaTeX$.]

```{r, results='asis', echo=FALSE}
# stargazer(linear_model1,
#           linear_model2, 
#           linear_model3, 
#           linear_model4, 
#           linear_model5, 
#           header=FALSE, 
#           type = 'html',
#           omit = c("f", "reg", "momed", "nodaded", "daded", "momdad14", "sinmom14", "Constant", "nomomed", "smsa66"),
#           no.space = TRUE,
#           add.lines=list(c('Region in 1966', 'No','Yes', "Yes", "Yes", "Yes"),
#                          c('SMSA 1966', 'No','Yes', "Yes", "Yes", "Yes"),
#                          c('Parental Education', 'No','No', "Yes", "Yes", "Yes"),
#                          c('Interacted Parental Education Classes', 'No','No', "No", "Yes", "Yes"),
#                          c('Family Structure', 'No','No', "No", "No", "Yes")),
#           omit.stat =c("f", "adj.rsq", "ser"),
#           notes = NULL,
#           covariate.labels =c("Education", "Experiance", "Experiance-Squared/100", "Black indicator", "Live in South", "Live in SMSA"),
#           dep.var.labels = c("Log Hourly Earnings")
#           )
```

For each model $\hat\beta_\text{OLS}\in[0.073,0.075]$, meaning that an additional year of education corresponds to roughly a 7.4% increase in earnings. We should be suspicious of these results though, as it's likely the case that $educ_i$ is endogenous, so we need some instrument(s) for $educ_i$.  @card1995using makes the case for using a variable which indicates whether a respondent grew up in a labor market with an accredited 4-year college, $near_\ college_i$ (```nearc4```). In other words, the respondent lived near a college in the year 1966. This proximity to a college has no baring on earnings (in theory), but it is likely correlated with $educ_i$, as the cost of higher education is lower for those who have the option to live at home while attending college. Let's use $near_\ college_i$ to calculate $\hat\beta_\text{IV}$. We can do this "by hand" by estimating the following reduced form equations via OLS:^[While we're prone to make mistakes involving standard errors here, it's sort of the equivelent of "showing your work" on a math test.]

\begin{align*}
educ_i &amp;= \gamma_0 + \gamma_1 \cdot near_\ college_i + \gamma_2\cdot exper_i + \frac{\gamma_3}{100}\cdot exper_i^2 + \gamma_4 \cdot black_i + \gamma_5\cdot region_i + \gamma_6\cdot SMSA_i + \cdots + u_i\\
\log( wage_i) &amp;= \delta_0 + \delta_1 \cdot near_\ college_i + \delta_2\cdot exper_i + \frac{\delta_3}{100}\cdot exper_i^2 + \delta_4 \cdot black_i + \delta_5\cdot region_i + \delta_6\cdot SMSA_i + \cdots + \nu_i\\
\end{align*}

We could also directly estimate the structural model via $\IV$ and not worry about messing up the standard errors. If we have done everything correctly we should see:
$$ \frac{\hat \delta_{\text{OLS},1}}{\hat \gamma_{\text{OLS},1}} = \hat\beta_\text{IV}.$$
```{r, echo=FALSE, fig.align='center', fig.asp = 0.5, fig.width = 1, fig.cap ="test", out.width = "500px"}
knitr::include_graphics("figures/card_dag.png")
```

We will estimate the models with and without family controls, and restrict our attention to respondents where ```lwage76``` is not missing.

quarto-executable-code-5450563D

```r
card_95 &lt;- card_95 %&gt;% 
  filter(!is.na(lwage76))

#Without family controls 
reduced_form1_stage1 &lt;- lm(ed76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)
reduced_form1_stage2 &lt;- lm(lwage76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)
IV_model1 &lt;- ivreg(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 |
                     nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)

#With family controls
reduced_form2_stage1 &lt;- lm(ed76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + 
                             region66 + daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + 
                             f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)
reduced_form2_stage2 &lt;- lm(lwage76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + 
                             daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + f5 + f6 + f7 + f8 + 
                             momdad14 + sinmom14, data = card_95)
IV_model2 &lt;- ivreg(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + daded + momed +
                     nodaded + nomomed + f1 + f2 + f3 + f4 +  f5 + f6 + f7 + f8 + momdad14 + sinmom14 |
                     nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + daded + momed +
                     nodaded + nomomed + f1 + f2 + f3 + f4 + f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)
```


```{r, results='asis', echo=FALSE}
# stargazer(reduced_form1_stage1,
#           reduced_form2_stage1,
#           reduced_form1_stage2,
#           reduced_form2_stage2,
#           IV_model1,
#           IV_model2,
#           header=FALSE,
#           type = 'html',
#           omit = c("f", "reg", "exp76", "smsa", "black", "south", "momed", "nodaded", "daded", "momdad14", "sinmom14", "Constant", "nomomed", "smsa66"),
#           no.space = TRUE,
#            add.lines=list(c('Family Background Variables', 'No','Yes', "No", "Yes", "No", "Yes")),
#           omit.stat =c("f", "adj.rsq", "ser", "rsq"),
#           # notes = NULL,
#           covariate.labels =c("Live Near College in 1966 ", "Education"),
#            dep.var.labels = c("Education", "Log Hourly Earnings"),
#           column.labels   = c("Reduced Form (OLS) Estimates", "Strucutral (IV) Estimates"),
#           column.separate = c(4, 2),
#           model.names = FALSE
#           )
```

Compare this to panel A of Table 3 in @card1995using. Note that the ratio of our reduced form estimates do equal our IV estimates. Between our original OLS estimates (of the structural model), and our IV estimates, we have seven estimates for $\beta$. We can plot these along with the corresponding 95% confidence intervals.

```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}
linear_model1 &lt;- tidy(linear_model1) %&gt;% 
  mutate(model = "Linear Model 1")
linear_model2 &lt;- tidy(linear_model2) %&gt;% 
  mutate(model = "Linear Model 2")
linear_model3 &lt;- tidy(linear_model3) %&gt;% 
  mutate(model = "Linear Model 3")
linear_model4 &lt;- tidy(linear_model4) %&gt;% 
  mutate(model = "Linear Model 4")
linear_model5 &lt;- tidy(linear_model5) %&gt;% 
  mutate(model = "Linear Model 5")
IV_model1 &lt;- tidy(IV_model1) %&gt;% 
  mutate(model = "IV Model 1")
IV_model2 &lt;- tidy(IV_model2) %&gt;% 
  mutate(model = "IV Model 2")

bind_rows(linear_model1,
          linear_model2,
          linear_model3,
          linear_model4,
          linear_model5,
          IV_model1,
          IV_model2) %&gt;% 
  filter(term == "ed76") %&gt;% 
  mutate(upper = estimate + 1.965*std.error,
         lower = estimate - 1.965*std.error) %&gt;% 
  ggplot(aes(model, estimate)) +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme_minimal() +
  labs(x = "", y = "β Estimate, 95% Confidence Interval")
```
While our IV estimates are larger, all of our OLS estimates fall within the 95% confidence intervals for the IV estimates. As such, we're not able to conclude that our IV estimates are significantly larger than our OLS estimates.

## Further Reading

**_Technical treatment of IV/2SLS_**: Chapter 5 of @wooldridge2010econometric, Chapter 8 of @greene2003econometric, Chapter 12 of @hansen2022econometrics

**_Testing for Endogeneity_**: Section 8.7 of @davidson2004econometric, @ruud1984tests</code></pre>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>