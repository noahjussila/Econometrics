<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanved Econometrics with Examples - 2&nbsp; Asymptotic Properties of Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./testing.html" rel="next">
<link href="./estimators.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#convergence" id="toc-convergence" class="nav-link active" data-scroll-target="#convergence"><span class="toc-section-number">2.1</span>  Convergence</a>
  <ul class="collapse">
  <li><a href="#convergence-in-mse" id="toc-convergence-in-mse" class="nav-link" data-scroll-target="#convergence-in-mse"><span class="toc-section-number">2.1.1</span>  Convergence in MSE</a></li>
  <li><a href="#convergence-in-probability" id="toc-convergence-in-probability" class="nav-link" data-scroll-target="#convergence-in-probability"><span class="toc-section-number">2.1.2</span>  Convergence in Probability</a></li>
  <li><a href="#almost-sure-convergence" id="toc-almost-sure-convergence" class="nav-link" data-scroll-target="#almost-sure-convergence"><span class="toc-section-number">2.1.3</span>  Almost Sure Convergence</a></li>
  <li><a href="#convergence-in-distribution" id="toc-convergence-in-distribution" class="nav-link" data-scroll-target="#convergence-in-distribution"><span class="toc-section-number">2.1.4</span>  Convergence in Distribution</a></li>
  <li><a href="#putting-the-pieces-together" id="toc-putting-the-pieces-together" class="nav-link" data-scroll-target="#putting-the-pieces-together"><span class="toc-section-number">2.1.5</span>  Putting the Pieces Together</a></li>
  </ul></li>
  <li><a href="#consistency" id="toc-consistency" class="nav-link" data-scroll-target="#consistency"><span class="toc-section-number">2.2</span>  Consistency</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers"><span class="toc-section-number">2.3</span>  Law of Large Numbers</a></li>
  <li><a href="#the-continuous-mapping-theorem-and-slutskys-theorem" id="toc-the-continuous-mapping-theorem-and-slutskys-theorem" class="nav-link" data-scroll-target="#the-continuous-mapping-theorem-and-slutskys-theorem"><span class="toc-section-number">2.4</span>  The Continuous Mapping Theorem and Slutsky’s Theorem</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="toc-section-number">2.5</span>  Central Limit Theorem</a></li>
  <li><a href="#delta-method" id="toc-delta-method" class="nav-link" data-scroll-target="#delta-method"><span class="toc-section-number">2.6</span>  Delta Method</a></li>
  <li><a href="#little-o_p-big-o_p-and-taylor-expansions" id="toc-little-o_p-big-o_p-and-taylor-expansions" class="nav-link" data-scroll-target="#little-o_p-big-o_p-and-taylor-expansions"><span class="toc-section-number">2.7</span>  Little <span class="math inline">\(o_p\)</span>, Big <span class="math inline">\(O_p\)</span>, and Taylor Expansions</a></li>
  <li><a href="#asymptotically-normal-estimators" id="toc-asymptotically-normal-estimators" class="nav-link" data-scroll-target="#asymptotically-normal-estimators"><span class="toc-section-number">2.8</span>  Asymptotically Normal Estimators</a></li>
  <li><a href="#thinking-beyond-mathbbrk" id="toc-thinking-beyond-mathbbrk" class="nav-link" data-scroll-target="#thinking-beyond-mathbbrk"><span class="toc-section-number">2.9</span>  Thinking Beyond <span class="math inline">\(\mathbb{R}^k\)</span></a>
  <ul class="collapse">
  <li><a href="#normed-vector-spaces-and-metric-spaces" id="toc-normed-vector-spaces-and-metric-spaces" class="nav-link" data-scroll-target="#normed-vector-spaces-and-metric-spaces"><span class="toc-section-number">2.9.1</span>  Normed Vector Spaces and Metric Spaces</a></li>
  <li><a href="#convergence-and-continuity" id="toc-convergence-and-continuity" class="nav-link" data-scroll-target="#convergence-and-continuity"><span class="toc-section-number">2.9.2</span>  Convergence and Continuity</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="toc-section-number">2.10</span>  Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-asy" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-1_8e6f2a0bd959afa043ce86fd5868e853">

</div>
<p>When considering estimators in <a href="estimators.html"><span>Chapter&nbsp;1</span></a>, we kept the sample size <span class="math inline">\(n\)</span> fixed when assessing estimators. We now consider how estimators behave as <span class="math inline">\(n\to\infty.\)</span> In practice, we will never have infinite data, asymptotics gives us an approximate idea of how estimators perform for large data sets. A comprehensive reference in asymptotic theory is due to <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="references.html#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span>. For a treatment concerned purely with econometrics, <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span> provide a phenomenal survey, most of which we will touch on when discussing general classes of estimators.</p>
<p>With loss of some generality, we will assume that all random variables have finite expectation and variances. Dispensing with this assumption is something for a probability course.</p>
<section id="convergence" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="convergence"><span class="header-section-number">2.1</span> Convergence</h2>
<p>At some point in high school, most students encounter the concept of a numeric sequence, and how they can converge to a limit. Later on, perhaps when taking a real analysis course, sequences are generalized to spaces of functions. A sequence of functions may also converge to a limit, whether that be converging pointwise and/or converging uniformly (for details see <span class="citation" data-cites="rudin1976principles">Rudin (<a href="references.html#ref-rudin1976principles" role="doc-biblioref">1976</a>)</span>). Random variables are functions from a sample space to <span class="math inline">\(\mathbb R\)</span>, so we can consider how these functions converge. For the most part, the dimension of the random variable (i.e whether it takes on scalar values of vector values) won’t really matter, so we’ll remain agnostic about the exact case.</p>
<section id="convergence-in-mse" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="convergence-in-mse"><span class="header-section-number">2.1.1</span> Convergence in MSE</h3>
<p>The first type of convergence we’ll work with deals with MSE.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 </strong></span>A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in mean square</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n\overset{ms}{\to} X\)</span>, if <span class="math display">\[\lim_{n\to\infty} \text{E}\left[(X_n - X)^2\right] = 0.\]</span></p>
</div>
<p><span class="math inline">\(X_n \overset{ms}{\to}X\)</span> if the average distance between <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> shrinks as <span class="math inline">\(n\to\infty\)</span> where distance is measured as <span class="math inline">\((X_n - X)^2\)</span>. We can also have <span class="math inline">\(X_n \overset{ms}{\to}c\)</span> for some constant <span class="math inline">\(c\)</span>, as <span class="math inline">\(c\)</span> is a trivial random variable.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 </strong></span>Suppose we draw a sample of <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(Z_i\)</span> and define <span class="math inline">\(X_n\)</span> to be the sample mean of our observations. <span class="math display">\[X_n = \frac{1}{n}\sum_{i=1}^n Z_i\]</span> If <span class="math inline">\(\text{E}\left[Z_i\right] = \mu\)</span> and <span class="math inline">\(\text{Var}\left(Z_i\right) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>, we have <span class="math inline">\(X_n\overset{ms}{\to}\mu\)</span>: <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\text{E}\left[(X_n - \mu)^2\right] &amp; = \lim_{n\to\infty}\text{Var}\left(X_n\right) + \text{Bias}(X_n) \\
&amp;= \lim_{n\to\infty} \frac{\sigma^2}{n} + 0 &amp; (X_n \text{ unbiased}) \\
&amp; = \lim_{n\to\infty} \frac{\sigma^2}{n} \\
&amp; = 0.
\end{align*}\]</span></p>
<p>What does this convergence “look like”? If <span class="math inline">\(Z_i\overset{iid}{\sim}N(0,1)\)</span>, we know that <span class="math inline">\(X_n = \bar Z \sim N(\mu, \sigma^2/n)\)</span>. Let’s plot this distribution for increasing values of <span class="math inline">\(n\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot21_0b083d9969c19ba65a052e8cd7be5b0c">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">500</span>),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n)),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">as.factor</span>(n)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y, <span class="at">color =</span> n)) <span class="sc">+</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of X_n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>) <span class="sc">+</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot21" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot21-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.1: The distribution of X_n for various values of n</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>This example betrays a useful property related to variables which converge in mean square.</p>
<div id="prp-mse3" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.1 </strong></span>A sequence of random variables <span class="math inline">\(X_n\)</span> converges in mean square to a constant <span class="math inline">\(c\)</span> <em>if and only if</em> <span class="math inline">\(\text{E}\left[X_n\right]\to c\)</span> and <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\((\Longrightarrow)\)</span> Suppose <span class="math inline">\(X_n \overset{ms}{\to}c\)</span>. Then <span class="math display">\[\begin{align*}
&amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies &amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies&amp; \lim_{n\to\infty}\left[\text{E}\left[X_n\right]^2 -2c \text{E}\left[X_n\right] + c^2\right]= 0\\
\implies&amp; \lim_{n\to\infty}\left[(\text{E}\left[X_n\right]^2 -\text{E}\left[X_n\right]^2) + \text{E}\left[X_n\right]^2 - 2c \text{E}\left[X_n\right] + c^2\right]= 0 \\
\implies &amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left[\text{E}\left[X_n\right] -c\right]^2 = 0
\end{align*}\]</span> This final equality gives the desired result.</p>
<p><span class="math inline">\((\Longleftarrow)\)</span> Suppose <span class="math inline">\(\text{E}\left[X_n\right]\to c\)</span> and <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>. We have <span class="math display">\[\begin{align*}
&amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left[\text{E}\left[X_n\right] -c\right]^2 = 0 \\
\implies &amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies &amp; X_n\overset{ms}{\to}c
\end{align*}\]</span></p>
</div>
<div id="cor-mseconv" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.1 </strong></span>Suppose <span class="math inline">\(X_n\)</span> is a sequence of random variables such that <span class="math inline">\(\text{E}\left[X_n\right] = c\)</span> for all <span class="math inline">\(n\)</span>. Then <span class="math inline">\(X_n\overset{ms}{\to}c\)</span> <em>if and only if</em> <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>.</p>
</div>
</section>
<section id="convergence-in-probability" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="convergence-in-probability"><span class="header-section-number">2.1.2</span> Convergence in Probability</h3>
<p>Convergence in mean square captures the idea that a random variable gets “closer” to some value <span class="math inline">\(c,\)</span> but it is hardly the only way to define this behavior. A more “traditional” approach would be defining convergence using an inequality involving an arbitrarily small <span class="math inline">\(\varepsilon &gt;0\)</span> (akin the to <span class="math inline">\(\varepsilon-\delta\)</span> definition of a limit).</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 </strong></span>A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in probability</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n\overset{p}{\to}X\)</span> or <span class="math inline">\(\mathop{\mathrm{plim}}X_n = X\)</span>, if <span class="math display">\[\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon)= 0\]</span> for all <span class="math inline">\(\varepsilon &gt; 0\)</span>. Equivalently, <span class="math inline">\(X_n\overset{p}{\to}X\)</span> if for all <span class="math inline">\(\varepsilon &gt; 0\)</span> and <span class="math inline">\(\delta &gt; 0\)</span>, there exists some <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(n \ge N\)</span>, <span class="math display">\[ \Pr (|X_n - X| &gt; \varepsilon) &lt; \delta.\]</span></p>
</div>
<p>Intuitively, <span class="math inline">\(X_n \overset{p}{\to}X\)</span> if the probability that the difference <span class="math inline">\(|X_n - X|\)</span> is not small (greater than some <span class="math inline">\(\varepsilon\)</span>) goes to zero as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 </strong></span>Return to the previous example where <span class="math inline">\(X_n = \bar Z\)</span>, and assume <span class="math inline">\(Z_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We will verify that <span class="math inline">\(X_n\overset{p}{\to}\mu\)</span> using the definition of convergence in probability using the fact that <span class="math inline">\(X_n \sim N(\mu, \sigma^2/n)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot22_97eb89e25d2ba268aec06961cbdff25f">
<div class="cell-output-display">
<div id="fig-plot22" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/converge.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.2: For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For some <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[\begin{align*}
\Pr (|X_n - \mu| &gt; \varepsilon) &amp; = 1 - \Pr (\mu - \varepsilon &lt; X_n &lt; \mu + \varepsilon)\\
&amp; = 1 - (F_{X_n}(\mu + \varepsilon) + F_{X_n}(\mu - \varepsilon))\\
&amp; = 1 - 2\left[F_{X_n}(\mu + \varepsilon) - \frac{1}{2}\right] &amp; (F_{X_n} \text{symmetric about }\mu)\\
&amp; = 1 - 2\left[\Phi\left(\frac{(\mu + \varepsilon) - \mu}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right] &amp; (\Phi\text{ standard normal distribution})\\
&amp; = 1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right].
\end{align*}\]</span> Given some <span class="math inline">\(\delta &gt;0\)</span>, we can solve for the lowest value of <span class="math inline">\(n\)</span> that satisfies <span class="math inline">\(\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta\)</span>. <span class="math display">\[\begin{align*}
&amp;\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta \\
\implies &amp; 1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right] &lt; \delta \\
\implies&amp;  n &gt; \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2
\\\implies &amp; n &gt; \left\lceil \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2 \right\rceil
\end{align*}\]</span> Just to be excruciatingly pedantic, we rounded our solution up to the closest positive integer, as <span class="math inline">\(n\)</span> corresponds to a sample size. For fixed values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (say 3 and 2, respectively), we can define a function of <span class="math inline">\((\varepsilon, \delta)\)</span> which calculates the sample size required to satisfy <span class="math inline">\(\Pr(|X_n - c|&gt;\varepsilon)&lt;\delta\)</span>.</p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-4_d25eef47941a5dc9d5c802efe0062d3e">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>n_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(delta, ep){</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">ceiling</span>(((sigma<span class="sc">*</span><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>delta <span class="sc">/</span><span class="dv">2</span>))<span class="sc">/</span>ep)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s plot this function for various values of <span class="math inline">\((\varepsilon, \delta)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot23_69f74afe04cfb43fa4f3aa4920c8d79b">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9999</span><span class="sc">/</span><span class="dv">10000</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sample =</span> <span class="fu">n_fun</span>(d,e)) <span class="sc">%&gt;%</span> </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(d, sample, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_reverse</span>() <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span> </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"δ"</span>, <span class="at">y =</span> <span class="st">"Sample Size"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)<span class="sc">+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot23" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot23-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.3: The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We can also verify that <span class="math inline">\(\lim_{n\to\infty}\Pr (|X_n - \mu| &gt; \varepsilon) = 0\)</span> for various values of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>.</p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-6_0ac995ef385c050d78ff373c70674251">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>prob_ep <span class="ot">&lt;-</span> <span class="cf">function</span>(n, ep){</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">pnorm</span>(ep <span class="sc">/</span> (sigma <span class="sc">/</span> <span class="fu">sqrt</span>(n))) <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot24_2a9b41145ff7dbfcd083887e536e1f98">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> <span class="fu">prob_ep</span>(e,n)) <span class="sc">%&gt;%</span> </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, prob, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Pr(|X_n - mu| &gt; ε)"</span>, <span class="at">color =</span> <span class="st">"ε"</span>) <span class="sc">+</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot24" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot24-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.4: The probability that X_n falls outside the interval |μ-ε| for various values of (ε,n)</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>How does convergence in mean square related to convergence in probability? As it turns out the latter is a weaker condition implied by the prior. Before stating and proving this result, we will need a lemma.</p>
<div id="lem-markovineq" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1 (Markov’s inequality) </strong></span>If <span class="math inline">\(X\)</span> is a nonnegative random variable, and <span class="math inline">\(a &gt; 0\)</span>, then <span class="math display">\[\Pr(X\ge a) \le \frac{\text{E}\left[X\right]}{a}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The expectation of <span class="math inline">\(X\)</span> can be written as <span class="math display">\[\begin{align*}
\text{E}\left[X\right] &amp; = \int_{-\infty}^\infty x\ dF_X(x) \\
&amp; = \int_{0}^\infty x\ dF_X(x) &amp; (X\text{ is nonnegative}) \\
&amp; = \int_{0}^a x\ dF_X(x) + \int_{a}^\infty x\ dF_X(x) \\
&amp; \ge \int_a^\infty x\ dF_X(x)\\
&amp; \ge \int_a^\infty a\ dF_X(x) &amp; (a \ge x \text{ on }(a,\infty))\\
&amp; = a \int_a^\infty\ dF_X(x) \\
&amp; = a\Pr(X \ge a).
\end{align*}\]</span> Dividing both sides of this inequality by <span class="math inline">\(a\)</span> gives <span class="math inline">\(\Pr(X\ge a) \le \text{E}\left[X\right]/a\)</span>.</p>
</div>
<div id="prp-conv" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.2 (Convergence in MSE –&gt; Convergence in Probability) </strong></span>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables. If <span class="math inline">\(X_n\overset{ms}{\to}X\)</span>, then <span class="math inline">\(X_n\overset{p}{\to}X\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(X_n\overset{ms}{\to}X\)</span>. For all <span class="math inline">\(\varepsilon &gt; 0\)</span> <span class="math display">\[\begin{align*}
\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon) &amp; = \lim_{n\to\infty} \Pr ((X_n - X)^2 &gt; \varepsilon^2) \\
&amp; \le \lim_{n\to\infty} \frac{\text{E}\left[(X_n - X)^2\right]}{\varepsilon^2} &amp; (\text{Markov's Inequality}) \\
&amp; = \frac{0}{\varepsilon^2} &amp; (X_n\overset{ms}{\to}X)\\
&amp; = 0.
\end{align*}\]</span> Therefore <span class="math inline">\(X_n\overset{p}{\to}c\)</span>.</p>
</div>
<p>The usefulness of <a href="#prp-conv">Proposition&nbsp;<span>2.2</span></a> cannot be emphasized enough. Proving convergence in probability using the definition is cumbersome, so we will almost show convergence in mean square and then appeal to <a href="#prp-conv">Proposition&nbsp;<span>2.2</span></a> to verify convergence in probability. Nevertheless, situations can arise where <span class="math inline">\(X_n\overset{p}{\to}c\)</span>, but <span class="math inline">\(X_n \not\overset{ms}{\to} c\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Convergence in Probability but not in Mean Square) </strong></span>Suppose there is a sequence of random variables <span class="math inline">\(X_n\)</span> that take on the values <span class="math inline">\(0\)</span> and <span class="math inline">\(n^2\)</span> with probabilities <span class="math inline">\(\Pr(X_n = 0) = 1-1/n\)</span> and <span class="math inline">\(\Pr(X_n=n^2) = 1/n\)</span>. The expected value of this random variable is <span class="math display">\[\text{E}\left[X_n\right] = 0(1-1/n) + n^2(1/n) = n,\]</span> so <span class="math inline">\(\text{E}\left[X_n\right]\to\infty\)</span> as <span class="math inline">\(n\to \infty\)</span>. This rules out <span class="math inline">\(X_n\)</span> converging in mean square to any value. Nevertheless, we have <span class="math inline">\(X_n\overset{p}{\to}0\)</span>. For all <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[\Pr(|X_n - 0| &gt; \varepsilon) = \Pr(X_n \neq 0) = \Pr(X_n = n^2) = 1/n \to 0.\]</span></p>
</div>
</section>
<section id="almost-sure-convergence" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="almost-sure-convergence"><span class="header-section-number">2.1.3</span> Almost Sure Convergence</h3>
<p>Another form of convergence arises if we remember <span class="math inline">\(X_n:\mathcal X\to\mathbb{R}\)</span> is just a function from a sample space to <span class="math inline">\(\mathbb{R}\)</span> (or <span class="math inline">\(\mathbb{R}^k\)</span>). While we usually write <span class="math inline">\(X_n = x\)</span>,</p>
</section>
<section id="convergence-in-distribution" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="convergence-in-distribution"><span class="header-section-number">2.1.4</span> Convergence in Distribution</h3>
<p>The final notion of convergence we will use related to the probability distribution of random variables.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 </strong></span>A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in distribution (converges weakly)</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n \overset{d}{\to}X\)</span>, if <span class="math display">\[\lim_{n\to\infty} F_{X_n}(x)= F_X(x).\]</span> In this case, we refer to <span class="math inline">\(F_X\)</span> as the <span style="color:red"><strong><em>asymptotic distribution</em></strong></span> of <span class="math inline">\(X_n\)</span>, and sometimes write <span class="math inline">\(X_n \overset{a}{\sim}F_X\)</span>.</p>
</div>
<p>For our purposes, <span class="math inline">\(X_n\overset{d}{\to}X\)</span> means the distribution of <span class="math inline">\(X_n\)</span> can be approximated by <span class="math inline">\(F_X\)</span>, and this approximation becomes increasingly better as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="tdist">
<p>One example of convergence in distribution you may be familiar with deals with the student’s <span class="math inline">\(t-\)</span>distribution where the degrees of freedom <span class="math inline">\(n\to\infty\)</span>. If <span class="math inline">\(X_n\sim t_n\)</span>, then <span class="math inline">\(X_n \overset{d}{\to}X\)</span> where <span class="math inline">\(X\sim N(0,1)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot25_93f7c49e21dcf2b62c311a494e7c0341">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length =</span> <span class="dv">10000</span>),</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> <span class="st">"Student's t"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val =</span> <span class="fu">dt</span>(x, n)) <span class="sc">%&gt;%</span> </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, val, <span class="at">color =</span> <span class="fu">as.factor</span>(n))) <span class="sc">+</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"black"</span>, </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">0.5</span>, </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype=</span><span class="st">"dashed"</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"t-distribution degrees of freedom, n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot25" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot25-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.5: The t-distribution converges to the standard normal distribution (represented by the dashed black line) as the degrees of freedom increase.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.3 (Convergence in Probability –&gt; Convergence in Distribution) </strong></span>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables. If <span class="math inline">\(X_n\overset{p}{\to}X\)</span>, then <span class="math inline">\(X_n\overset{d}{\to}X\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(X_n\overset{p}{\to}X\)</span> and let <span class="math inline">\(\varepsilon &gt; 0\)</span>. We have, <span class="math display">\[\begin{align*}
\Pr(X_n \le x) &amp; = \Pr(X_n\le x \text{ and } X \le x + \varepsilon) + \Pr(X_n\le x \text{ and } X &gt; x + \varepsilon) \\
  &amp; = \Pr(X \le x + \varepsilon) + \Pr(X_n - X\le x - X \text{ and } x - X &lt; -\varepsilon)\\
  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon)\\
  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon) + \Pr(X - X_n &gt; \varepsilon) \\
  &amp; = \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon)
\end{align*}\]</span> Similarly, <span class="math display">\[ \Pr(X \le x-\varepsilon) \le \Pr(X_n \le x) + \Pr(|X_n - X| &gt; \varepsilon).\]</span> We can use these inequalities to find an upper and lower bound of <span class="math inline">\(\Pr(X_n \le x)\)</span>: <span class="math display">\[\begin{align*}
&amp; \Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)\le \Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) \\
\implies &amp; \lim_{n\to\infty}[\Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)]\le \lim_{n\to\infty}\Pr(X_n \le x) \le \lim_{n\to\infty}[\Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) ]\\
\implies &amp; \Pr(X \le x-\varepsilon) - \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon) }_0\le \lim_{n\to\infty}\Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon)}_0 \\
\implies &amp; \Pr(X \le x-\varepsilon)\le \lim_{n\to\infty} \Pr(X_n \le x) \le  \Pr(X \le x + \varepsilon) &amp; (X_n\overset{p}{\to}X)\\
\implies &amp; F_X(x-\varepsilon)\le \lim_{n\to\infty} F_{X_n}(x) \le  F_X(x-\varepsilon)
\end{align*}\]</span> This holds for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, so it must be the case that <span class="math inline">\(\lim_{n\to\infty} F_{X_n}(x) = F_X(x)\)</span></p>
</div>
</section>
<section id="putting-the-pieces-together" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="putting-the-pieces-together"><span class="header-section-number">2.1.5</span> Putting the Pieces Together</h3>
<p>The biggest takeaway from this should be the following relation: <span class="math display">\[ X_n \overset{ms}{\to}X \implies X_n \overset{p}{\to}X \implies X_n \overset{d}{\to}X\]</span></p>
</section>
</section>
<section id="consistency" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="consistency"><span class="header-section-number">2.2</span> Consistency</h2>
<p>Our three modes of convergence were defined for any sequence of random variables. It should come as no surprise, considering the previous examples considering whether the sample mean converged, that we are interested in the convergence of estimators <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> as sample size increases. In particular we are interested in whether <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> converges to the constant <span class="math inline">\(\theta\in\Theta\)</span> it is estimating.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 </strong></span>An estimator <span class="math inline">\(\hat\theta\)</span> is <span style="color:red"><strong><em>consistent (for estimand <span class="math inline">\(\theta\)</span>)</em></strong></span> if <span class="math inline">\(\hat\theta \overset{p}{\to}\theta\)</span>.</p>
</div>
<p>We’ve already seen that <span class="math inline">\(\bar X\)</span> is a consistent estimator for <span class="math inline">\(\mu\)</span> when we take an iid sample from a normal distribution. Let’s investigate it’s variance-counterpart <span class="math inline">\(S^2\)</span>.</p>
<div id="exm-consvarnorm" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 </strong></span>For <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, <span class="math inline">\(S^2 = \sum_{i=1}^n (X_i - \bar X)/(n-1)\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. This estimator’s MSE (which is just its variance as it is unbiased) is <span class="math inline">\(2\sigma^4/(n-1)\)</span> which converges to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\to\infty\)</span>, so <span class="math inline">\(S^2\)</span> is consistent by <a href="#prp-conv">Proposition&nbsp;<span>2.2</span></a>.</p>
</div>
<p>This example highlights the fact that proving an unbiased estimator is consistent is a matter of showing its variance converges to 0.</p>
<div id="cor-unbcon" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.2 </strong></span>Suppose <span class="math inline">\(\hat\theta\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(\hat\theta\)</span> is consistent <em>if and only if</em> <span class="math inline">\(\text{Var}\left(\hat\theta\right)\to 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Apply <a href="#cor-mseconv">Corollary&nbsp;<span>2.1</span></a> to an unbiased estimator.</p>
</div>
<p>A second type of convergence related to estimators pertains to the bias of an estimator. In <a href="estimators.html"><span>Chapter&nbsp;1</span></a> we saw a few estimators that were biased, but this bias was such that it diminished as <span class="math inline">\(n\to\infty\)</span>. In effect, they were unbiased in an asymptotic sense.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 </strong></span>An estimator <span class="math inline">\(\hat\theta\)</span> is <span style="color:red"><strong><em>asymptotically unbiased</em></strong></span> if <span class="math inline">\(\lim_{n\to\infty}\text{Bias}(\hat\theta, \theta) = 0\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 </strong></span>For <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, <span class="math inline">\(\hat\theta = \sum_{i=1}^n (X_i - \bar X)/n\)</span> is a biased estimator for <span class="math inline">\(\sigma^2\)</span>. Its bias is <span class="math display">\[\text{Bias}(\hat\theta, \sigma^2) = \frac{n-1}{n}\sigma^2 - \sigma^2.\]</span> As <span class="math inline">\(n\to\infty\)</span>, this bias vanishes. To illustrate this, we can simulate estimates for various sample sizes, taking <span class="math inline">\(X_i \sim N(0,1)\)</span>.</p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-9_a99780c32673b8f1409e7fc0793eedaa">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="cf">function</span>(X){</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((X <span class="sc">-</span> <span class="fu">mean</span>(X))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(X)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"n=10"</span>, <span class="st">"n=25"</span>, <span class="st">"n=50"</span>, <span class="st">"n=100"</span>, <span class="st">"n=500"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> <span class="fu">which</span>(sample_sizes <span class="sc">==</span> n)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim){</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">theta</span>(X) </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">#Print bias calculated over 1,000,000 simulations</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(store_estimates) <span class="sc">-</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        n=10         n=25         n=50        n=100        n=500 
-0.100384324 -0.040078003 -0.020466531 -0.010057292 -0.002121508 </code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot26_ae5bb58c47686cd819fa195b3a2be952">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">key =</span> <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V1"</span>, <span class="st">"10"</span>, </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V2"</span>, <span class="st">"25"</span>, </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V3"</span>, <span class="st">"50"</span>, </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V4"</span>, <span class="st">"100"</span>, <span class="st">"500"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>          )))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">key =</span> <span class="fu">as.numeric</span>(key)) <span class="sc">%&gt;%</span> </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">n =</span> key)  <span class="sc">%&gt;%</span> </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value, <span class="at">color =</span> <span class="fu">as.factor</span>(n))) <span class="sc">+</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot26" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot26-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.6: As the sample size increases, the bias of our estimator converges to zero.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The estimator <span class="math inline">\(\hat\theta\)</span> is also consistent, as it converges in mean square.</p>
</div>
<p>Neither asymptotic unbiasedness does not imply consistency, nor does consistency imply asymptotic unbiasedness.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Consistent, Not Asymptotically Unbiased) </strong></span>Recall that the sequence of discrete random variables <span class="math inline">\(X_n\)</span> with denisty <span class="math display">\[ f_{X_n}(x) = \begin{cases}1-1/n&amp; x=0\\ 1/n &amp; x = n^2 \end{cases}.\]</span> We established that <span class="math inline">\(X_n\overset{p}{\to}0\)</span>, so an estimator with this distribution would be a consistent estimator for <span class="math inline">\(0\)</span>. Despite this, the estimator would not be asymptotically unbiased, as <span class="math inline">\(\text{E}\left[X_n\right] = n\)</span>, which tends to infinity as <span class="math inline">\(n\)</span> grows.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Asymptotically Unbiased, Not Consistent) </strong></span>For <span class="math inline">\(X_i\overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, define the estimator <span class="math inline">\(\hat\mu(\mathbf{X}) = X_1\)</span>. We simply take the first observation to be our estimate of <span class="math inline">\(\mu\)</span>. This estimator is unbiased, <span class="math display">\[\text{E}\left[\hat\mu\right] = \text{E}\left[X_1\right] = \mu,\]</span> so it is asymptotically unbiased. Nevertheless, the estimator fails to be consistent, as <span class="math display">\[\lim_{n\to\infty} \Pr (|\hat\mu - \mu| &gt; \varepsilon)= \lim_{n\to\infty} \left\{1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma}\right) - \frac{1}{2}\right]\right\} \neq 0 .\]</span></p>
</div>
<p>The incompatibility of asymptotic unbiasedness and consistency is due to the behavoir of <span class="math inline">\(\text{Var}\left(X_n\right)\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="prp-consbias" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.4 (Relating Consistency and Asymptotic Unbiasedness) </strong></span>Suppose <span class="math inline">\(\hat\theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(\hat\theta\)</span> is consistent and there exists some <span class="math inline">\(M\)</span> such that <span class="math inline">\(\text{Var}\left(\hat\theta\right) \le M\)</span> for all <span class="math inline">\(n\)</span> (bounded variance), then <span class="math inline">\(\hat\theta\)</span> is asymptotically unbiased.</li>
<li>If <span class="math inline">\(\hat\theta\)</span> is asymptotically unbiased and <span class="math inline">\(\lim_{n\to\infty}\text{Var}\left(\hat\theta\right) = 0\)</span> (vanishing variance), then <span class="math inline">\(\hat\theta\)</span> is consistent</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>test</p>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot27_1cecac32aef6c7ccabc4eafc31999d70">
<div class="cell-output-display">
<div id="fig-plot27" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/relating_convergence.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.7: Relationship between various concepts of convergence in the context of estimators</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="law-of-large-numbers" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="law-of-large-numbers"><span class="header-section-number">2.3</span> Law of Large Numbers</h2>
<p>In most examples until now, the properties of estimators were implicitly a function of the underlying model the data is generated from. We established that <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(S^2\)</span> are consistent estimators for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, respectively, <em>when</em> <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We know the distribution of <span class="math inline">\(\bar X\)</span>, <em>when</em> <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. In an ideal world, we would be able to establish desirable properties of estimators under more robust settings where our specified model may include a wide array of distributions. Our first step in doing this will be introducing one of the most important results in all of probability – the law of large numbers. Proving this requires an inequality similar to <a href="#lem-markovineq">Lemma&nbsp;<span>2.1</span></a>.</p>
<div id="lem-" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.2 (Chebyshev’s Inequality) </strong></span>If <span class="math inline">\(X\)</span> is a random variable with an expected value <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then for all <span class="math inline">\(a &gt; 0\)</span> <span class="math display">\[\Pr(|X - \mu| \ge k) \le \frac{\sigma^2}{k^2}.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\Pr(|X - \mu| \ge k) &amp;= \Pr((X - \mu)^2 \ge k^2)\\
&amp; \le \frac{\text{E}\left[(X-\mu)^2\right]}{k^2} &amp; (\text{Markov's Inequality})\\
&amp; = \frac{\sigma^2}{k^2}
\end{align*}\]</span></p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 ((Khinchine’s Weak) Law of Large Numbers (LLN)) </strong></span>If <span class="math inline">\((X_1,\ldots, X_n)\)</span> are a set of iid random variables where <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/n\)</span>. By Chebyshev’s Inequality, <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\Pr(|X_n - \mu| \ge \varepsilon) \le \lim_{n\to\infty}\frac{(\sigma^2/n)}{\varepsilon^2} =   \lim_{n\to\infty} \frac{\sigma^2}{n\varepsilon} = 0.
\end{align*}\]</span> Therefore, <span class="math inline">\(\bar X\overset{p}{\to}\mu\)</span>.</p>
</div>
<p>Another way of thinking about the LLN is that we can approximate <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\bar X\)</span>, and this approximation gets better as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 </strong></span>To illustrate the LLN, let’s simulate realizations of iid random variables from a series of different distributions and show that regardless of the distribution, <span class="math inline">\(\bar X \to \mu\)</span>. We will use the following distributions: <span class="math display">\[\begin{align*}
X_i &amp; \overset{iid}{\sim}\text{Exp}(1/\mu)\\
X_i &amp; \overset{iid}{\sim}\chi_\mu^2\\
X_i &amp; \overset{iid}{\sim}\text{Uni}(0, 2\mu)\\
X_i &amp; \overset{iid}{\sim}\text{Gamma}(2\mu, 2)\\
X_i &amp; \overset{iid}{\sim}\text{HyperGeo}(10, 20, 15\mu)
\end{align*}\]</span> All these distributions have been selected such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>. For our simulations, we will take <span class="math inline">\(\mu = 5\)</span>. If we plot the value of the sample mean versus the sample size <span class="math inline">\(n\)</span>, we see that the values converge to the true value <span class="math inline">\(\mu = 5\)</span> regardless of the distribution of <span class="math inline">\(X_i\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot28_60417b50ba92f7c0f81bc781cc36ba68">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>max_n <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">5</span>, <span class="at">nrow =</span> max_n<span class="sc">/</span><span class="dv">10</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Exponential"</span>, <span class="st">"Chi-Squared"</span>, <span class="st">"Uniform"</span>, <span class="st">"Gamma"</span>, <span class="st">"Hypergeometric"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>exp_sample <span class="ot">&lt;-</span> <span class="fu">rexp</span>(max_n, <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>chisq_sample <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(max_n, <span class="dv">5</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>unif_sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(max_n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>gamma_sample <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(max_n, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>hyper_sample <span class="ot">&lt;-</span> <span class="fu">rhyper</span>(max_n, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">15</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(max_n<span class="sc">/</span><span class="dv">10</span>)) {</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(exp_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(chisq_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(unif_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(gamma_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(hyper_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(key) <span class="sc">%&gt;%</span> </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, value, <span class="at">color =</span> key)) <span class="sc">+</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>, <span class="at">color =</span> <span class="st">"Distribution of iid Random Sample"</span>) <span class="sc">+</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">4.5</span>, <span class="fl">5.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot28" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot28-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.8: The sample mean of all samples converges to the population mean by the LLN</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Monte Carlo Simulations) </strong></span>In <a href="estimators.html#exm-var">Example&nbsp;<span>1.9</span></a> we performed a Monte Carlo simulation to illustrate the bias of <span class="math inline">\(\hat\theta(\mathbf{X}) = \sum_{i=1}^n (X_i - \bar X)/n\)</span> and unbiasedness of <span class="math inline">\(S^2\)</span>. We did this by fixing <span class="math inline">\(n=20\)</span>, drawing a random sample, recording estimates <span class="math inline">\(\hat\theta(\mathbf{x})\)</span> and <span class="math inline">\(S^2(\mathbf{x})\)</span>, and repeating this <span class="math inline">\(k\)</span> times. This is nothing more than drawing <span class="math inline">\(j=1,\ldots,k\)</span> observations from the random variables <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> and <span class="math inline">\(S^2(\mathbf{X})\)</span>. By the LLN, <span class="math display">\[\begin{align*}
\frac{1}{k}\sum_{i=1}^k \hat\theta_j(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left[\hat\theta(\mathbf{X})\right],\\
\frac{1}{k}\sum_{i=1}^k S_j^2(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left[S^2(\mathbf{X})\right],
\end{align*}\]</span> so for a large enough <span class="math inline">\(k\)</span>, we can approximate the expected value with its sample counterpart.</p>
</div>
<p>The crucial assumption made by the LLN is that <span class="math inline">\(\bar X\)</span> is calculated with an iid random sample. If we drop this assumption, then <span class="math inline">\(\bar X\)</span> needn’t estimate <span class="math inline">\(\mu\)</span> consistently.</p>
<div id="exm-noiid" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (LLN Failing with Non-IID Data) </strong></span>Suppose <span class="math inline">\(\mathbf{X}= (X_1, \ldots, X_n)\)</span> where <span class="math inline">\(X_i \sim N(-1, i)\)</span> if <span class="math inline">\(i\)</span> is odd, and <span class="math inline">\(X_i \sim N(1,i)\)</span> is <span class="math inline">\(i\)</span> is even. The data is independent, but not identically distributed. If some LLN would hold here, we would suspect that <span class="math inline">\(\bar X\)</span> would converge <span class="math inline">\(0\)</span>, the average of the underlying population means <span class="math inline">\(1\)</span> and <span class="math inline">\(-1\)</span>.Let’s simulate <span class="math inline">\(\bar X\)</span> for <span class="math inline">\(n\)</span> ranging from 1 to 100,000.</p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-13_db9fa1a182433812ff77bf5ca7cdcc4b">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="fl">1e5</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>X_bar <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="fl">1e5</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#draw realizations for each X_i s</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(X)) {</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">#set mu modular arithmetic</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  X[i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu, i)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate sample mean </span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  X_bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[<span class="dv">1</span><span class="sc">:</span>i])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that our estimates very much do not converge to any particular value.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot29_92bdcb28a02b93780e17df5ccb8e4104">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>X_bar <span class="sc">%&gt;%</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, X_bar)) <span class="sc">+</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot29" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot29-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.9: The sample mean of non-IID data does not satisfy the LLN</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>The proof of the LLN relied on Chebyshev’s equality and the fact that <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/n \to 0\)</span> when <span class="math inline">\(\text{Var}\left(X_i\right) = \sigma^2\)</span>. Perhaps if we added an assumption regarding the variance of a non-iid sample, then we could salvage a result similar to the LLN. This is precisely what Chebyshev’s LLN does.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2 (Chebyshev’s (Weak) Law of Large Numbers) </strong></span>Suppose <span class="math inline">\((X_1,\ldots, X_n)\)</span> are a sample such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu_i\)</span>, <span class="math inline">\(\text{Cov}\left(X_i, X_j\right) = \sigma_{ij}^2\)</span>, and <span class="math inline">\(\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_{ij}^2 =0\)</span>. If <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The expected value of <span class="math inline">\(\bar X\)</span> is <span class="math display">\[\text{E}\left[\bar X\right] = \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i\right]= \frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu.\]</span> The variance is <span class="math display">\[ \text{Var}\left(\bar X\right) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) = \frac{1}{n}\sum_{i=1}^n \sigma_{ij}^2\to 0.\]</span> By Proposition <a href="#prp-mse3">Proposition&nbsp;<span>2.1</span></a>, <span class="math inline">\(\bar X\overset{ms}{\to}\mu\)</span>, so <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="corrolary">
<p>Suppose <span class="math inline">\((X_1,\ldots, X_n)\)</span> are an independent sample such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu_i\)</span>, <span class="math inline">\(\text{Var}\left(X_i\right) = \sigma_{i}^2\)</span>, and <span class="math inline">\(\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 =0\)</span>. If <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If the sample is independent, then <span class="math display">\[\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) =\frac{1}{n^2}\sum_{i=1}^n\text{Var}\left(X_i\right). \]</span></p>
</div>
<p>The reason our non-iid sample in Example <a href="#exm-noiid">Example&nbsp;<span>2.11</span></a> did not converge was because <span class="math display">\[ \frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac{n(n+1)}{2} = \frac{n^2 + n}{2n^2} \to \frac{1}{2} \neq 0.\]</span> Let’s modify it slightly so the sum of the variances does converge to zero.</p>
<div id="exm-noiid" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (LLN with Non-IID Data) </strong></span>Suppose <span class="math inline">\(\mathbf{X}= (X_1, \ldots, X_n)\)</span> where <span class="math inline">\(X_i \sim N(-1,i^{-1})\)</span> if <span class="math inline">\(i\)</span> is odd, and <span class="math inline">\(X_i \sim N(1,i^{-1})\)</span> is <span class="math inline">\(i\)</span> is even. Now we have <span class="math display">\[\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \left[\lim_{n\to\infty}\frac{1}{n^2}\right]\sum_{i=1}^\infty i^{-1} \to 0 .\]</span></p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-15_dbc07a3826a5c3bbe71f148cb91fa418">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">200</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_bar <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">200</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">#draw realizations for each X_i s</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(X)) {</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">#set mu modular arithmetic</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  X[i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu, i<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate sample mean </span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  X_bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[<span class="dv">1</span><span class="sc">:</span>i])</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we see that <span class="math inline">\(\bar X\)</span> is converging to <span class="math inline">\(0\)</span>, and doing so rather quickly.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot210_375082c05061d3d0d38ceed48bac2f52">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X_bar <span class="sc">%&gt;%</span> </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, value)) <span class="sc">+</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot210" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot210-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.10: Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-continuous-mapping-theorem-and-slutskys-theorem" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="the-continuous-mapping-theorem-and-slutskys-theorem"><span class="header-section-number">2.4</span> The Continuous Mapping Theorem and Slutsky’s Theorem</h2>
<p>At first glance, the LLN may not seem especially useful as it only applies to the sample mean. However, when paired with two key results about convergence, the LLN becomes an indispensable tool to show the convergence of many random variables and estimators. The first of these is an extension of a key result in real analysis. A useful, and defining property, of continuous functions is that they preserve limits of numeric sequences. If <span class="math inline">\(\{a_n\}\)</span> is a numeric sequence, then <span class="math display">\[\lim_{n \to\infty} f(a_n) = f\left(\lim_{n\to\infty} a_n\right) \iff f\text{ continuous}.\]</span></p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3 (Continuous Mapping Theorem I) </strong></span>Suppose <span class="math inline">\(X_n \overset{p}{\to}X\)</span>, and let <span class="math inline">\(g\)</span> be a continuous function. Then <span class="math display">\[g(X_n)\overset{p}{\to}g(X).\]</span> In other words we are able to interchange the <span class="math inline">\(\mathop{\mathrm{plim}}\)</span> operator with a continuous function: <span class="math display">\[\mathop{\mathrm{plim}}g(X_n) = g\left(\mathop{\mathrm{plim}}X_n\right).\]</span></p>
</div>
<p>The proof of this result can be found in <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="references.html#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span>. An immediate corollary follows from the fact that convergence in probability implies convergence in distribution.</p>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.3 (Continuous Mapping Theorem II) </strong></span>Suppose <span class="math inline">\(X_n \overset{p}{\to}X\)</span>, and let <span class="math inline">\(g\)</span> be a continuous function. Then <span class="math display">\[g(X_n)\overset{d}{\to}g(X)\]</span></p>
</div>
<p>An equally useful result involves the limit of a sums and products of convergent random variables.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.4 (Slusky’s Theorem) </strong></span>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> be sequences of random variables. If <span class="math inline">\(X_n\overset{d}{\to}X\)</span> for some random variable <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y_n\overset{p}{\to}c\)</span> for some constant <span class="math inline">\(c\)</span>, then <span class="math display">\[\begin{align*}
X_n + Y_n &amp;\overset{d}{\to}X + c\\
X_nY_n &amp; \overset{d}{\to}Xc.
\end{align*}\]</span> Furthermore, if <span class="math inline">\(c\neq 0\)</span>, <span class="math display">\[ X_n/Y_n \overset{d}{\to}X/c.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Define a random vector to be <span class="math inline">\(\mathbf Z_n = (X_n,Y_n)\)</span>. We have <span class="math inline">\(\mathbf Z_n \overset{d}{\to}(X,c)\)</span> as <span class="math inline">\(X_n\overset{d}{\to}X\)</span> and <span class="math inline">\(Y_n \overset{d}{\to}c\)</span> (convergence in probability implies convergence in distribution). We can apply the continuous mapping theorem to <span class="math inline">\(g(x,y) = x + y\)</span>, <span class="math inline">\(g(x,y)=xy\)</span>, and <span class="math inline">\(g(x/y)\)</span> to establish the result.</p>
</div>
<p>Slutsky’s theorem can be a bit hard to remember because it involves a sequence of random variable which converges in distribution to a random variable, and a sequence of random variables which converges in probability to a constant. These asymmetries in mode of convergence and the type of limit are essential, otherwise the result will not hold. Fortunately, the result does hold if we replace all convergences in distribution with convergence in probability (as the later implies the prior).</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 </strong></span>Suppose <span class="math inline">\(X_n \sim\text{Uni}(0,1)\)</span> and <span class="math inline">\(Y_n = - X_n\)</span>. We have <span class="math inline">\(X_n \overset{d}{\to}\text{Uni}(0,1)\)</span> and <span class="math inline">\(Y_n \overset{d}{\to}\text{Uni}(-1,0)\)</span>. Despite this <span class="math inline">\(X_n + Y_n = 0 \not\overset{d}{\to}\text{Uni}(0,1) + \text{Uni}(-1,0).\)</span></p>
</div>
<div id="exm-convar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (Consistency of Sample Variance) </strong></span><a href="#exm-consvarnorm">Example&nbsp;<span>2.4</span></a> showed that <span class="math inline">\(S^2\)</span> is a consistent estimator for <span class="math inline">\(\sigma^2\)</span> when <span class="math inline">\(X_i\overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We can use the continuous mapping theorem, Slutsky’s theorem, and the LLN to show that <span class="math inline">\(S^2\)</span> is consistent regardless of the distribution of our iid sample. Suppose <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>, <span class="math inline">\(\text{E}\left[X_i^2\right]=\mu_2\)</span>, and <span class="math inline">\(\text{E}\left[X_i^4\right]=\mu_4\)</span> for all <span class="math inline">\(i\)</span>. If we define our continuous function to be <span class="math inline">\(g(x) = x^2\)</span>, then <span class="math display">\[\begin{align*}
S^2 &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i - \frac{1}{n-1} \sum_{i=1}^n\bar X^2 \\
    &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i^2 + \frac{n}{n-1}\bar X^2  \\
    &amp; = \frac{n}{n-1}\left[\frac{1}{n}\sum_{i=1}^{n}X_i^2 - \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2\right]
\end{align*}\]</span> The first term in the brackets is an unbiased estimator of <span class="math inline">\(\mu_2\)</span> with vanishing variance, so by <a href="#prp-consbias">Proposition&nbsp;<span>2.4</span></a> it is a consistent estimator for <span class="math inline">\(\text{E}\left[X^2\right]\)</span>: <span class="math display">\[\begin{align*}
\text{E}\left[\frac{1}{n}\sum_{i=1}^{n}X_i^2\right] &amp;= \frac{1}{n}\left(n \mu_2\right) = \mu^2\\
\lim_{n\to\infty}\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_i^2\right) &amp; = \lim_{n\to\infty}\frac{1}{n^2}n\left(\text{E}\left[X_i^4\right] - \text{E}\left[X_i^2\right]^2 \right) = \lim_{n\to\infty}\frac{\mu_4 + \mu_2^2}{n} = 0
\end{align*}\]</span> The second term in the brackets can be written as <span class="math inline">\(g(\bar X)\)</span>, so by the continuous mapping theorem and the LLN, <span class="math display">\[ g(\bar X) = \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2 \overset{p}{\to}\mu^2 = g(\mu).\]</span> So <span class="math display">\[S^2= \underbrace{\frac{n}{n-1}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^{n}X_i^2}_{\overset{p}{\to}\mu_2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2}_{\overset{p}{\to}\mu^2} \Bigg].\]</span> We can apply Slutsky’s theorem to the sum of sequences of random variables which converge in probability to constants, so <span class="math display">\[ S^2 \overset{p}{\to}\mu_2 - \mu^2 = \sigma^2,\]</span> making <span class="math inline">\(S^2\)</span> consistent.</p>
</div>
</section>
<section id="central-limit-theorem" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="central-limit-theorem"><span class="header-section-number">2.5</span> Central Limit Theorem</h2>
<p>The LLN told us that our favorite estimator for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\bar X\)</span>, is consistent. We know turn to what is perhaps an even more important result regarding <span class="math inline">\(\bar X\)</span>, one that may in fact be the most important results in all of probability.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.5 (Lindeberg-Lévy Central Limit Theorem (CLT)) </strong></span>Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma^2\)</span>. Then <span class="math display">\[\sqrt{n}(\bar X - \mu) \overset{d}{\to}N(0,\sigma^2),\]</span> which is also often written as <span class="math display">\[\bar X\overset{d}{\to}N(\mu, \sigma^2/n),\]</span> or <span class="math display">\[\sum_{i=1}^n X_i \overset{d}{\to}N(n\mu, \sqrt n \sigma^2) \]</span></p>
</div>
<p>The Lindeberg-Lévy central limit theorem is often referred to as <em>the</em> central limit theorem, and is the form which most are familiar with. The proof is a bit technical and requires some measure-theoretic based probability theory. It can be found in <span class="citation" data-cites="billingsley2008probability">Billingsley (<a href="references.html#ref-billingsley2008probability" role="doc-biblioref">2008</a>)</span> or <span class="citation" data-cites="durrett2019probability">Durrett (<a href="references.html#ref-durrett2019probability" role="doc-biblioref">2019</a>)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14 </strong></span>Suppose we have an iid sample from <span class="math inline">\(\text{Exp}(1)\)</span>. If we simulate 1000 realizations of <span class="math inline">\(\sqrt n(\bar X - \mu)\)</span> for various sample sizes, we should see that the distribution of our realizations becomes approximately normal as we increase the sample size.</p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-17_f7ef5b3cb7b0b0f92c010a48ae450fb9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">100</span>, <span class="dv">1000</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"n ="</span>, sample_sizes)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>col_index <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> col_index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">rexp</span>(n, <span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Even for modest values of <span class="math inline">\(n\)</span>, we can see that <span class="math inline">\(\sqrt n(\bar X - \mu) \overset{a}{\sim}N(0, \sigma^2)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot211_04deca47f79045a8784a7140c155dc2a">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>key <span class="ot">&lt;-</span> <span class="fu">factor</span>(df<span class="sc">$</span>key, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">'n = 1'</span>,<span class="st">'n = 5'</span>,<span class="st">'n = 10'</span>,<span class="st">'n = 25'</span>, <span class="st">'n = 100'</span>, <span class="st">'n = 1000'</span>))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot211" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot211-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.11: As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>An alternate way to visually test whether our estimates are normally distributed is with a quantile-quantile plot (QQ-plot), which graphs the observed quantiles of our estimates against the theoretical quantiles of a normal distribution (or those of any distribution we suspect our data is drawn from). If our estimates are (approximately) normally distributed, then the observed quantiles should be approximately equal to the theoretical quantiles of a normal distribution, forming a 45-degree line.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot212_cd8a44e5da96d1b9694937e3fda15286">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> value)) <span class="sc">+</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key) <span class="sc">+</span> </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot212" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot212-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.12: The QQ-plot for the simulated distribution of the adjusted sample mean</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>The central limit theorem is similar to the LLN insofar that they only concern the estimator <span class="math inline">\(\bar X\)</span>, so how useful can they really be? Well with the continuous mapping theorem and Slutsky’s theorem, the answer is <em>very useful</em>!</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15 </strong></span>In Example <span class="quarto-unresolved-ref">?exm-tdist</span> we illustrated the fact that <span class="math inline">\(X_n \overset{d}{\to}N(0,1)\)</span> where <span class="math inline">\(X_n \sim t_n\)</span>, but we didn’t actually prove it. Directly proving this result is a matter of verify that <span class="math display">\[\lim_{n\to\infty} F_{X_n}(x) =\lim_{n\to\infty}\int_{-\infty}^x \frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)\sqrt{n\pi}}\left(1 + \frac{x^2}{n}\right)^{-\frac{n+1}{2}} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-x^2/2} = F_X(x)\]</span> where <span class="math inline">\(\Gamma\)</span> is the gamma function defined as <span class="math display">\[\Gamma(t) = \int_0^\infty s^{t-1}e^{-s}\ ds.\]</span> A much easier way to prove that <span class="math inline">\(X_n \overset{d}{\to}N(0,1)\)</span>, is by using Slutsky’s theorem and the continuous mapping theorem. First recall that <span class="math display">\[\frac{\bar X - \mu}{S/\sqrt n} \sim t_n,\]</span> so we can write <span class="math inline">\(X_n\)</span> as <span class="math inline">\(X_n = \frac{\bar X - \mu}{S/\sqrt n}\)</span> due to the fact that random variables are uniquely determined by their distributions. From Example <span class="quarto-unresolved-ref">?exm-consvar</span>, we know <span class="math inline">\(S^2 \overset{p}{\to}\sigma^2\)</span>. By the continuous mapping theorem <span class="math display">\[ \sqrt{S^2} = S \overset{p}{\to}\sigma = \sqrt{\sigma^2}.\]</span> So we have <span class="math display">\[ X_n = \frac{\bar X - \mu}{S/\sqrt n}= \underbrace{\sqrt{n}(\bar X - \mu)}_{\overset{d}{\to}N(0, \sigma^2)}\underbrace{\frac{1}{s}}_{\overset{p}{\to}\sigma}.\]</span> By Slutsky’s theorem, <span class="math display">\[X_n \overset{d}{\to}\frac{N(0,\sigma^2)}{\sigma} = N(0,1).\]</span></p>
</div>
<p>The CLT can be generalized to samples of random vectors <span class="math inline">\(\mathbf{X}_i\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.6 (CLT in Higher Dimmensions) </strong></span>Suppose <span class="math inline">\((\mathbf{X}_1,\ldots, \mathbf{X}_n)\)</span> is a sequence of iid random vectors with <span class="math inline">\(\text{E}\left[\mathbf{X}_i\right]=\boldsymbol\mu\)</span> and <span class="math inline">\(\text{Var}\left(\mathbf{X}_i\right)=\boldsymbol\Sigma\)</span>. Then <span class="math display">\[\sqrt{n}(\bar {\mathbf{X}}- \boldsymbol\mu) \overset{d}{\to}N(\mathbf{0},\boldsymbol\Sigma).\]</span></p>
</div>
<p>We can also generalize the CLT to the case where variables are independent but not necessarily identically distributed.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.7 (Lindeberg-Feller CLT) </strong></span>Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of independent random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu_i\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma_i^2\)</span>, and define <span class="math display">\[\begin{align*}
\bar \mu = \frac{1}{n}\sum_{i=1}^n\mu_i;\\
\bar \sigma_n^2 = \frac{1}{n}\sum_{i=1}^n\sigma_i^2.
\end{align*}\]</span> If the collection of variances <span class="math inline">\(\sigma_i^2\)</span> satisfies: <span class="math display">\[\begin{align*}
\lim_{n\to\infty} &amp;\frac{\max\{\sigma_i\}}{n\bar\sigma} = 0;\\
\lim_{n\to\infty} &amp;\bar\sigma_n^2 = \bar\sigma^2,
\end{align*}\]</span> then <span class="math inline">\(\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2).\)</span></p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>The Lindeberg-Feller conditions stipulates that <span class="math inline">\(\lim_{n\to\infty}\bar\sigma_n^2 = \bar\sigma^2\)</span> and <span class="math inline">\(\lim_{n\to\infty} \frac{\max\{\sigma_i\}}{n\bar\sigma} = 0\)</span>, a condition known as the <strong><em>Lindeberg’s condition</em></strong>. The condition is often presented in more general terms, but the intuition remains the same. For the CLT to hold for random variables that with different variances, we need to makes sure that no single term <span class="math inline">\(\sigma_i\)</span> dominates the standard deviation. We can think about the sample mean <span class="math inline">\(\bar X\)</span> as “mixing” many random variables <span class="math inline">\(X_i.\)</span> After mixing these random variables, we hope to have a normal distribution, but that only happens if tails of the various distributions of <span class="math inline">\(X_i\)</span> are negligible as <span class="math inline">\(n\to\infty\)</span>, giving us the trademark tails of a normal distribution which tapper off. Let’s consider a counterexample. Suppose <span class="math inline">\(X_i\)</span> is defined on the sample space <span class="math inline">\(\{-i,0,i\}\)</span> is distributed such that <span class="math display">\[\Pr(X_i = k) = \begin{cases} 1/2i^2 &amp; k = -i^2 \\ 1/2i^2 &amp; k = i^2 \\ 1 - 1/i^2&amp;k=0\end{cases}.\]</span> Graphing the density function for a few values of <span class="math inline">\(i\)</span> gives us a better sense of how <span class="math inline">\(X_i\)</span> behaves.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot213_4142beb2a4871caa0f4e1ef4e9244a6b">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">x =</span> <span class="sc">-</span><span class="dv">100</span><span class="sc">:</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">==</span> i<span class="sc">^</span><span class="dv">2</span> <span class="sc">|</span> x <span class="sc">==</span> <span class="sc">-</span>i<span class="sc">^</span><span class="dv">2</span><span class="sc">|</span> x<span class="sc">==</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(x <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(i<span class="sc">^</span><span class="dv">2</span>), <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>i<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>i, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"k"</span>) <span class="sc">+</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> y)) <span class="sc">+</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot213" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot213-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.13: Density of X_i for i = 1,…,5.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As <span class="math inline">\(i\to\infty\)</span>, nearly all the probability is concentrated as <span class="math inline">\(k = 0\)</span>. The remaining probability is at the extreme tails of the distribution <span class="math inline">\(\pm i^2\)</span>, and these tails become more and more extreme (quadritically so) as <span class="math inline">\(i\to\infty\)</span>. This is the exact type of behavior Lindeberg’s condition rules out. The expectation, expectation squared, and variance of <span class="math inline">\(X_i\)</span> are: <span class="math display">\[\begin{align*}
\text{E}\left[X_i\right] &amp; = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0\\
\text{E}\left[X_i^2\right] &amp; = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2\\
\text{Var}\left(X_i\right) &amp; = i^2 - 0^2 = i^2
\end{align*}\]</span></p>
<p>We can verify that Lindeberg’s condition does not hold. <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\bar \sigma_n = \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n i^2 = \lim_{n\to\infty}\frac{(n+1)(2n+1)}{6} \to\infty
\end{align*}\]</span></p>
<p>To simulate realizations of this random variable, we can define <span class="math inline">\(X_i\)</span> using <span class="math inline">\(U_i\sim\text{Uni}(0,1)\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[X_i = \begin{cases}-i^2 &amp; U_i\in[0, 1/2i^2)\\ i^2 &amp; U_i\in[1/2i^2, 1/i^2) \\ 0 &amp; U_i\in [1/i^2,1]\end{cases}\]</span></p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-21_73518d628221b40fa1f7156c8ee1a794">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  prob_pos_i <span class="ot">&lt;-</span> (U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  prob_neg_i <span class="ot">&lt;-</span> (U <span class="sc">&gt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">&amp;</span> U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>((<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_pos_i <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_neg_i</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"n ="</span>, sample_sizes)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>col_index <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> col_index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(n)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot214_a3b8f44d81a4b3c92f3c2e7a0fa21767">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>key <span class="ot">&lt;-</span> <span class="fu">factor</span>(df<span class="sc">$</span>key, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">'n = 1'</span>,<span class="st">'n = 5'</span>,<span class="st">'n = 10'</span>,<span class="st">'n = 25'</span>, <span class="st">'n = 100'</span>, <span class="st">'n = 1000'</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot214" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot214-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.14: Histograms of estimates as sample size increases.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The apparent distribution of our estimates is not converging to a normal distribution, as we always have a few outliers that are too plentiful relative to their distance from the mean to be drawn from a normal distribution. As <span class="math inline">\(n\to\infty\)</span> these outliers become even more extreme. This is also evident from QQ-plots, where the points far away from the 45-degree line are drawn even farther away as <span class="math inline">\(n\to\infty\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot215_6de0cdf2c7707fd6e079f766246c58cf">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> value)) <span class="sc">+</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot215" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot215-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.15: The QQ-plot for the simulated distribution of the adjusted sample mean</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>While theoretically important, Lindeberg’s condition can be a bit hard to verify. It is much more common to appeal to a stronger assumption which gives rise to a second CLT that holds for independent, but not necessarily identically distributed, random variables. This final CLT may not be as general as the Lindeberg-Feller CLT, but it is much easier to work with.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.8 (Lyapunov CLT) </strong></span>Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of independent random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu_i\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma_i^2\)</span>. If <span class="math inline">\(\text{E}\left[\left\lvert X_i - \mu_i\right\rvert^{2+\kappa}\right]\)</span> is finite for some <span class="math inline">\(\kappa &gt; 0,\)</span> then <span class="math inline">\(\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2)\)</span>, where <span class="math inline">\(\bar\sigma = (1/n)\sum_{i=1}^n \sigma_i.\)</span></p>
</div>
</section>
<section id="delta-method" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="delta-method"><span class="header-section-number">2.6</span> Delta Method</h2>
<p>Slutsky’s theorem and the continuous mapping theorem in tandem with the LLN give us the ability to prove that certain functions of sample means are convergent. Is it possible that we can do something similar with the CLT to find the asymptotic distribution of functions of sample means?</p>
<p>Suppose <span class="math inline">\(g\)</span> is a function of <span class="math inline">\(\bar X\)</span>, where a CLT applies to the random sample <span class="math inline">\(\mathbf{X}\)</span>. We know <span class="math inline">\(\sqrt n(\bar X-\mu)\overset{a}{\sim}N(0, \sigma^2)\)</span>. Is it possible to conclude that <span class="math inline">\(\sqrt n(g(\bar X)-g(\mu))\overset{a}{\sim}N(0, \tilde\sigma^2)\)</span> for some <span class="math inline">\(\tilde\sigma^2\)</span>? Furthermore, can we determine <span class="math inline">\(\tilde\mu\)</span> and <span class="math inline">\(\tilde\sigma^2\)</span> only knowing <span class="math inline">\(g\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma^2\)</span>?</p>
<p>We have information about the difference <span class="math inline">\(\bar X-\mu\)</span> and want information about the difference <span class="math inline">\(g(\bar X)-g(\mu)\)</span>. Situations where we know something about behavior in the domain of a function and want to relate it to the function’s behavior in the codomain are common place in math, but in particular in real analysis. This is where the mean value theorem saves the day. Assuming <span class="math inline">\(g\)</span> is continuously differentiable and fixing <span class="math inline">\(n\)</span>, there exists some <span class="math inline">\(T_n\)</span> in between <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\theta\)</span> (<span class="math inline">\(\bar X&lt; T_n &lt; \mu\)</span> or <span class="math inline">\(\mu &lt; T_n &lt; \bar X\)</span>) such that: <span class="math display">\[\begin{align*}
&amp; \frac{g(\bar X)-g(\mu)}{\bar X - \mu} = g'(T_n)\\
\implies &amp; g(\bar X) = g(\mu) + g'(T_n)(\bar X-\mu)\\
\implies &amp; \sqrt{n}[g(\bar X) - g(\mu)] = g'(T_n)\sqrt{n} (\bar X-\mu)
\end{align*}\]</span> If we let <span class="math inline">\(n\)</span> vary, we have a sequence of random variables <span class="math inline">\(\{T_n\}\)</span> such that <span class="math inline">\(\left\lvert T_n -\mu\right\rvert &lt; \left\lvert\bar X - \mu\right\rvert\)</span>. By the LLN <span class="math inline">\(\left\lvert\bar X - \mu\right\rvert \overset{p}{\to}0\)</span>, so <span class="math inline">\(\left\lvert T_n -\mu\right\rvert \overset{p}{\to}0\)</span>, which is equivalent to <span class="math inline">\(T_n \overset{p}{\to}\mu\)</span>. By the continuous mapping theorem, <span class="math inline">\(g(T_n) \overset{p}{\to}g(\mu)\)</span>. This means <span class="math display">\[\sqrt{n}[g(\bar X) - g(\mu)] = \underbrace{g'(T_n)}_{\overset{p}{\to}g(\mu)}\cdot\underbrace{\sqrt{n} (\bar X-\mu)}_{\overset{d}{\to}N(0, \sigma^2)},\]</span> so Slutsky’s theorem gives <span class="math display">\[\sqrt{n}[g(\bar X) - g(\mu)] \overset{d}{\to}g(\mu)\cdot N(0,\sigma^2) = N(0, \sigma^2[g(\mu)]^2).\]</span> This all is contingent on <span class="math inline">\(g\)</span> not being a function of <span class="math inline">\(n\)</span>, otherwise things fall apart when we apply limiting processes.</p>
<p>This result is known as the delta method, and applies to any sequence of random variables that is asymptotically normal. It is also readily generalized to higher dimensions where <span class="math inline">\(\mathbf g\)</span> is a vector valued function.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.9 (Delta Method) </strong></span>Suppose <span class="math inline">\((\mathbf{X}_1,\ldots, \mathbf{X}_n)\)</span> is a sequence of random vectors such that <span class="math inline">\(\sqrt n (\mathbf{X}_n - \mathbf t) \overset{d}{\to}N(\mathbf 0, \boldsymbol\Sigma)\)</span> for some vector <span class="math inline">\(\mathbf t\)</span> in the interior of <span class="math inline">\(\mathcal X\)</span>. If <span class="math inline">\(\mathbf g(\mathbf{X}_n)\)</span> is a vector valued function that:</p>
<ol type="1">
<li>is continuously differentiable,</li>
<li>does not involve <span class="math inline">\(n\)</span>,</li>
<li><span class="math inline">\(\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t) \neq 0\)</span>,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ol>
<p>then,</p>
<p><span class="math display">\[ \sqrt n \left[\mathbf g(\mathbf{X}_n) - \mathbf g(\mathbf t)\right] \overset{d}{\to}N\left(\mathbf 0, \left[\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right]\boldsymbol\Sigma\left[\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right]'\right)\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.16 </strong></span>Return to the example where <span class="math inline">\(X_i\overset{iid}{\sim}\text{Exp}(1)\)</span>, giving <span class="math inline">\(\text{E}\left[X_i\right] = 1\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right) = 1\)</span>. By the CLT, <span class="math inline">\(\sqrt n(\bar X - 1)\overset{d}{\to}N(0,1)\)</span>. If <span class="math inline">\(g(t) = t^2 +3\)</span>, what is the asymptotic distribution of <span class="math inline">\(\sqrt{n}[g(\bar X) - g(1)]\)</span>? According to the delta method we have <span class="math display">\[\begin{align*}
&amp;\sqrt{n}[g(\bar X) - g(1)]  \overset{a}{\sim}N(0, 1[g'(1)]^2),\\
\implies &amp; \sqrt{n}\left[\bar X^2 - 1\right] \overset{a}{\sim}N(0, 4).
\end{align*}\]</span></p>
<div class="cell" data-hash="asymptotics_cache/html/unnamed-chunk-24_22bdaba7513886c7b09e4aad9029421f">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(t){</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  t<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>g_of_mean <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>N_sim, <span class="cf">function</span>(x) <span class="fu">g</span>(<span class="fu">mean</span>(<span class="fu">rexp</span>(sample_size))))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(sample_size)<span class="sc">*</span>(g_of_mean <span class="sc">-</span> <span class="fu">g</span>(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can plot a histogram of our estimates and overlay the distribution <span class="math inline">\(N(0,4)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot216_187ab65845a57783fb074c2d7fcb57bf">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> estimates) <span class="sc">%&gt;%</span> </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x)) <span class="sc">+</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">colour =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Estimates of √n(g(X) - g(1)) "</span>) <span class="sc">+</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>), <span class="at">color =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot216" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot216-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.16: Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>The big takeaway from the delta method is that estimators which are nice functions of sample means will be asymptotically distributed according to a normal distribution. Furthermore, any nice function of such an estimator will also have a normal asymptotic distribution!</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot217_af90fead52460d9734fc6c9728f13bb6">
<div class="cell-output-display">
<div id="fig-plot217" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/meme.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.17: A mediocre meme</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="little-o_p-big-o_p-and-taylor-expansions" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="little-o_p-big-o_p-and-taylor-expansions"><span class="header-section-number">2.7</span> Little <span class="math inline">\(o_p\)</span>, Big <span class="math inline">\(O_p\)</span>, and Taylor Expansions</h2>
<p>We’ve talked a lot about whether or not random variables converge, and how they converge, but not the rate at which they converge. We can introduce some notation that allows us to quantify this rate.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 </strong></span>Given a sequence of random variables <span class="math inline">\(X_n\)</span>, we say <span class="math inline">\(X_n\)</span> is <span style="color:red"><strong><em>little “O.P” of <span class="math inline">\(n^k\)</span></em></strong></span>, denoted <span class="math inline">\(X_n = o_p(n^k)\)</span>, if <span class="math inline">\(X_n / n^k\overset{p}{\to}0\)</span>.</p>
</div>
<p>Note that <span class="math inline">\(X_n\overset{p}{\to}0\)</span> implies that <span class="math inline">\(X_n = o_p(1)\)</span>. The use of “=” is a bit misleading in this definition, as <span class="math inline">\(X_n = o_p(n^k)\)</span> does not establish any equality, instead referring to how <span class="math inline">\(X_n\)</span> behaves asymptotically. For instance, if we have two sequences of random variables <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> such that <span class="math inline">\(X_n\overset{p}{\to}X\)</span> and <span class="math inline">\(Y_n\overset{p}{\to}0\)</span>, we have $ X_n + Y_n X + 0 $, but could write <span class="math inline">\(X_n + Y_n\)</span> as <span class="math inline">\(X_n + o_p(1)\)</span>. This emphasizes the sequence <span class="math inline">\(X_n\)</span>, and frames <span class="math inline">\(Y_n\)</span> as some negligible remainder term that tends to zero.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 </strong></span>Given a sequence of random variables <span class="math inline">\(X_n\)</span>, we say <span class="math inline">\(X_n\)</span> is <span style="color:red"><strong><em>big “O.P” of <span class="math inline">\(n^k\)</span></em></strong></span>, denoted <span class="math inline">\(X_n = O_p(n^k)\)</span>, if for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some <span class="math inline">\(\delta\)</span> and <span class="math inline">\(N\)</span> such that <span class="math inline">\(\Pr(|X_n/n^k| \ge \delta) &lt;\varepsilon\)</span> for all <span class="math inline">\(n &gt; N\)</span>. In other words, <span class="math inline">\(X_n/n^k\)</span> is <span style="color:red"><strong><em>bounded in probability</em></strong></span>.</p>
</div>
<p>We are most interested in the case where <span class="math inline">\(X_n = O_p(1)\)</span>. If this is the case, then as <span class="math inline">\(n\to\infty\)</span>, we can bound the area in the tails of <span class="math inline">\(f_{X_n}\)</span> by some constant <span class="math inline">\(\delta\)</span> such that the area is negligible (less than <span class="math inline">\(\varepsilon\)</span>).</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.17 </strong></span>We know that <span class="math inline">\(\bar X \overset{d}{\to}N(\mu, \sigma^2/n)\)</span> when <span class="math inline">\(\mathbf{X}\)</span> is an iid sample. We have that <span class="math inline">\(\bar X = O_p(1)\)</span>. We have <span class="math display">\[\Pr(|\bar X/1| \ge \delta) = \Pr(-\bar X\ge -\delta \text{ and }\delta \le \bar X) = 2\cdot\Pr(\bar X\ge \delta) = 2\left[1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right].\]</span> If we take the limit of this as <span class="math inline">\(n\to \infty\)</span> we have <span class="math display">\[ \lim_{n\to \infty}\Pr(|X\bar X/1| \ge \delta) = \lim_{n\to \infty}2\left[1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right] = 0.\]</span> By the definition of a limit, there must exists some <span class="math inline">\(N\)</span> such that <span class="math inline">\(\Pr(|\bar X/1| \ge \delta) &lt; \varepsilon\)</span> for any <span class="math inline">\(n &gt; N\)</span>, so <span class="math inline">\(\bar X = O_p(1)\)</span>.</p>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.5 </strong></span>If <span class="math inline">\(X_n \overset{d}{\to}X\)</span>, then <span class="math inline">\(X_n = O_p(1)\)</span>.</p>
</div>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.4 </strong></span>If <span class="math inline">\(X_n = o_p(1)\)</span>, then <span class="math inline">\(X_n = O_p(1)\)</span>.</p>
</div>
<p>A common place to encounter <span class="math inline">\(o_p\)</span> is when performing Taylor expansions.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.18 (Taylor’s Theorem) </strong></span>Taylor’s theorem, as given in <span class="citation" data-cites="rudin1976principles">Rudin (<a href="references.html#ref-rudin1976principles" role="doc-biblioref">1976</a>)</span>, tells us that if <span class="math inline">\(f:\mathbb R\to\mathbb R\)</span> is <span class="math inline">\(k-\)</span>times differentiable at a point <span class="math inline">\(a\in \mathbb R\)</span>, then there is some element <span class="math inline">\(c\in (a,b)\)</span> such that <span class="math display">\[\begin{align*}
f(b) &amp; = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{k!}(b-a)^j + \frac{f^{(n)}(c)}{k!}(b-a)^k.
\end{align*}\]</span> For <span class="math inline">\(n = 2\)</span>, we have the mean value theorem: <span class="math display">\[ f(b)= f(a) + f'(c)(b-a).\]</span> If we let <span class="math inline">\(a\to b\)</span>, then <span class="math display">\[\begin{align*}
&amp;\lim_{a\to b}f(b)  = \sum_{j=0}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k.\\
\implies &amp; f(b)  =\lim_{a\to b} f(a) + \sum_{j=1}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j +  \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k\\
\implies &amp; f(b)  = f(b) + \underbrace{\sum_{j=1}^{k-1}\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k\\
\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!}(b-a)^k = 0\\
\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!(b-a)^k} = 0\\
\implies &amp; \frac{f^{(k)}(c)}{k!} = o(|a-b|^k)\\
\end{align*}\]</span> Here, <span class="math inline">\(o\)</span> is the deterministic counterpart of <span class="math inline">\(o_p\)</span> (we’re not working with random variables just yet). This means we can write Taylor’s theorem as <span class="math display">\[ f(b) = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).\]</span> In the event <span class="math inline">\(f\)</span> is infinitely differentiable we can make this approximation arbitrarily accurate, giving rise toa function’s Taylor series.</p>
<p>Now suppose <span class="math inline">\(f_n(X)\)</span> is a sequence of functions of random variables, where the subscript <span class="math inline">\(n\)</span> emphasizes that <span class="math inline">\(f_n\)</span> is a random variable. IF we apply Taylor’s theorem to <span class="math inline">\(f_n(X)\)</span> we have <span class="math display">\[f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)\]</span> for realizations of the random variable <span class="math inline">\(a,b,c\in\mathcal X\)</span> where <span class="math inline">\(c\in(a,b)\)</span>. Assuming <span class="math inline">\(k \ge 1\)</span>, then <span class="math inline">\(o_p(|a-b|^k)\)</span> implies <span class="math inline">\(o_p(1)\)</span>, so <span class="math display">\[f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).\]</span></p>
</div>
</section>
<section id="asymptotically-normal-estimators" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="asymptotically-normal-estimators"><span class="header-section-number">2.8</span> Asymptotically Normal Estimators</h2>
<p>When putting our asymptotic tools to work on an estimator of interest, we will almost always find that it converges to a normal distribution, is consistent, and that the rate of convergence is linked to <span class="math inline">\(\sqrt{n}\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 </strong></span>An estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is <span style="color:red"><strong><em><span class="math inline">\(\sqrt{n}-\)</span>consistent asymptotically normal (root-n CAN)</em></strong></span>, if <span class="math display">\[\sqrt{n}(\hat{\boldsymbol{\theta}}- \boldsymbol{\theta}) \overset{d}{\to}N(\mathbf 0, \mathbf V)\]</span> for a PSD matrix <span class="math inline">\(\mathbf V\)</span>. Equivalently, <span class="math display">\[ \hat{\boldsymbol{\theta}}\overset{a}{\sim}N(\boldsymbol{\theta}, \mathbf V/n).\]</span> We refer to <span class="math inline">\(\mathbf V/n\)</span> as the <span style="color:red"><strong><em>asymptotic variance</em></strong></span> of <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> and write <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol{\theta}}\right) = \mathbf V /n\)</span>.</p>
</div>
<p>“<span class="math inline">\(\sqrt n-\)</span>” emphasizes the fact that <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = O_p(1)\)</span>, which is equivalent to <span class="math inline">\(\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}+ O_p(n^{-1/2})\)</span>. In other words, as <span class="math inline">\(n\to\infty\)</span> the error term associated with our estimate decreases at a rate of <span class="math inline">\(n^{1/2}\)</span>. A fourfold increase in observations results in half the error. We also have that <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = o_p(1)\)</span>, so <span class="math inline">\(\hat{\boldsymbol{\theta}}\overset{p}{\to}\boldsymbol{\theta}\)</span>, hence the “consistent” in the previous definition. We also have that <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is asymptotically unbiased if it is root-n CAN, as <span class="math inline">\(\text{E}\left[\hat{\boldsymbol{\theta}}\right]\to \boldsymbol{\theta}\)</span>. This will be the one of, if not the, <strong><em>most important property</em></strong> an estimator can posses.</p>
<p>Our final example highlights some interesting properties of root-N CAN estimators in the context of the mean value theorem. This will be especially important in Section @ref(extremum-estimators).</p>
</section>
<section id="thinking-beyond-mathbbrk" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="thinking-beyond-mathbbrk"><span class="header-section-number">2.9</span> Thinking Beyond <span class="math inline">\(\mathbb{R}^k\)</span></h2>
<section id="normed-vector-spaces-and-metric-spaces" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="normed-vector-spaces-and-metric-spaces"><span class="header-section-number">2.9.1</span> Normed Vector Spaces and Metric Spaces</h3>
<p>A <strong><em>normed vector space</em></strong> <span class="math inline">\(V\)</span> defined over a field <span class="math inline">\(F\)</span> is, as the name implies, a vector space equipped with a <strong><em>norm</em></strong> <span class="math inline">\(\left\lVert\cdot\right\rVert:V\mapsto [0,\infty)\)</span> satisfying:</p>
<ol type="1">
<li><span class="math inline">\(\left\lVert v\right\rVert = 0 \iff v = 0\)</span>, where <span class="math inline">\(0\)</span> is the additive identity;</li>
<li><span class="math inline">\(\left\lVert av\right\rVert = \left\lvert a\right\rvert\left\lVert v\right\rVert\)</span> for all <span class="math inline">\(a\in F\)</span> and <span class="math inline">\(v\in V\)</span>;</li>
<li><span class="math inline">\(\left\lVert w + v\right\rVert \le \left\lVert w\right\rVert + \left\lVert v\right\rVert\)</span> for all <span class="math inline">\(w,v\in V\)</span>.</li>
</ol>
<p>A norm measures the the “distance” an element of <span class="math inline">\(V\)</span> is from the origin <span class="math inline">\(0\)</span>. A <strong><em>metric space</em></strong> <span class="math inline">\((X,d)\)</span> is some set <span class="math inline">\(X\)</span> with a <strong><em>metric</em></strong> <span class="math inline">\(d:X\times X\to [0,\infty)\)</span> such that:</p>
<ol type="1">
<li><span class="math inline">\(d(x,y) = 0 \iff x=y\)</span> for <span class="math inline">\(x,y\in X\)</span>;</li>
<li><span class="math inline">\(d(y,x)=d(x,y\)</span> for all <span class="math inline">\(x,y\in X\)</span>;</li>
<li><span class="math inline">\(d(x,z) \le d(x,y) + d(y,z)\)</span>.</li>
</ol>
<p>A metric measures the distance between any two points in a metric space. Any normed vector space is a metric space if we define</p>
<p><span class="math display">\[d(x,y)= \left\lVert x-y\right\rVert\]</span> for <span class="math inline">\(x,y\in V\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.19 (Euclidean Space) </strong></span><span class="math inline">\(\mathbb{R}^k\)</span> is a vector space, where the norm is given as <span class="math display">\[ \left\lVert\mathbf{x}\right\rVert = \left(\sum_{i=1}^k x_i^2\right)^{1/2},\]</span> and is called the <strong><em>Euclidean norm</em></strong>. To measure the distance between two vectors in <span class="math inline">\(\mathbb{R}^k\)</span> we use a familiar formula. <span class="math display">\[ \left\lVert\mathbf{x}-\mathbf{y}\right\rVert = \left(\sum_{i=1}^k (x_i-y_i)^2\right)^{1/2}.\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.20 (Euclidean Space with the <span class="math inline">\(p\)</span>-norm) </strong></span>We could also define the vector space <span class="math inline">\(\mathbb{R}^k\)</span> with the norm <span class="math display">\[\left\lVert\mathbf{x}\right\rVert_p = \left(\sum_{i=1}^k \left\lvert x_i\right\rvert^p\right)^{1/p}.\]</span> If <span class="math inline">\(p = 1\)</span>, then <span class="math display">\[\left\lVert\mathbf{x}- \mathbf{y}\right\rVert_1 = \sum_{i=1}^n \left\lvert x_i - y_i\right\rvert,\]</span> whose name follows from how you would define distance if you were driving between two points in a city with a grid layout. If <span class="math inline">\(p=2\)</span> we have the Euclidean norm. Why bother considering values of <span class="math inline">\(p\)</span> other than <span class="math inline">\(2\)</span>? We can think of <span class="math inline">\(p\)</span> as the way we weight the component-wise “distances” from the origin <span class="math inline">\(\left\lvert x_1\right\rvert,\left\lvert x_2\right\rvert,\ldots \left\lvert x_k\right\rvert\)</span>. For <span class="math inline">\(p = 2\)</span>, we square each of these distances (meaning the result is always positive rendering the absolute value moot), assigning more weight to components where <span class="math inline">\(x_i\)</span> is relatively large. What happens as <span class="math inline">\(p\to\infty?\)</span>? Let’s look at some plots.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="asymptotics_cache/html/fig-plot2213_f266abf5929bdf2e4c09ff4f395046b0">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>p_norm <span class="ot">&lt;-</span> <span class="cf">function</span>(p, x){</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>(<span class="fu">abs</span>(x)<span class="sc">^</span>p))<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>p)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">x2 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">x4 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">4</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>       ) <span class="sc">%&gt;%</span> </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vector =</span> <span class="fu">ifelse</span>(x4 <span class="sc">==</span> <span class="sc">-</span><span class="dv">4</span>, <span class="st">"x = (1,2,3,-4)"</span>, <span class="fu">ifelse</span>(x3 <span class="sc">==</span> <span class="dv">3</span>, <span class="st">"x = (1,2,3)"</span>, <span class="fu">ifelse</span>(x2 <span class="sc">==</span> <span class="dv">2</span>, <span class="st">"x = (1,2)"</span>, <span class="st">"x = 1"</span>))))  <span class="sc">%&gt;%</span> </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand_grid</span>(<span class="at">p =</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>, <span class="at">length =</span> <span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">norm =</span> <span class="fu">p_norm</span>(p, <span class="fu">c</span>(x1, x2, x3, x4))) <span class="sc">%&gt;%</span> </span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, norm)) <span class="sc">+</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>vector, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p"</span>, <span class="at">y =</span> <span class="st">"p-norm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot2213" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="asymptotics_files/figure-html/fig-plot2213-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.18: p norm</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It appears that <span class="math display">\[ \lim_{p\to\infty}\left\lVert\mathbf{x}\right\rVert_p= \max_{x_i}\left\lvert x_i\right\rvert.\]</span> For this reason we define the limiting case of the <span class="math inline">\(p-\)</span>norm as <span class="math display">\[ \left\lVert\mathbf{x}\right\rVert_\infty= \max_{x_i}\left\lvert x_i\right\rvert.\]</span> A useful illustration is of the unit circle in <span class="math inline">\((\mathbb{R}^k,\left\lVert\cdot\right\rVert_p)\)</span> for various values of <span class="math inline">\(p\)</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="0.5" data-hash="asymptotics_cache/html/fig-plot1241123_f8f1da50d145c23f8fa63403261341f7">
<div class="cell-output-display">
<div id="fig-plot1241123" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/lp_norm.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2.19: test</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>We want to think about a vector space formed by random variables where each vector is its own random variable. Before jumping straight to this case, let’s consider a vector space formed by real functions, and then worry about randomness later on.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.21 (The Space of Real Continuous Bounded Functions) </strong></span>Define the space <span class="math inline">\(C(X)\)</span> as</p>
<p><span class="math display">\[C(X) = \{f:X\to\mathbb{R}\mid X\subset\mathbb{R}, f\text{ continuous and bounded}\}.\]</span> This is a vector space over the field <span class="math inline">\(\mathbb{R}\)</span> where addition and scalar multiplication are defined as: <span class="math display">\[\begin{align*}
(f +g)(x)&amp;=f(x)+g(x),\\
(cf)(x) &amp; = c\times f(x).
\end{align*}\]</span></p>
<p>How would we define a norm on a space of functions though? Firstly, by the definition of addition of vectors, the <span class="math inline">\(0\)</span> elements is <span class="math inline">\(f(x) = 0\)</span>. So how do we measure the distance between some <span class="math inline">\(f\in C(X)\)</span> and <span class="math inline">\(f(x) = 0\)</span>? In the event <span class="math inline">\(X\)</span> is an interval of <span class="math inline">\(\mathbb{R}\)</span>, it contains uncountably many points, so we cannot use the <span class="math inline">\(p\)</span>-norm and sum <span class="math inline">\(|f(x)|\)</span> over all the values of <span class="math inline">\(x\)</span>. We can however integrate over all such values, as one interpretation of the integral is as an uncountably infinite counterpart to a countable sum. We define the <span class="math inline">\(p-\)</span>norm on <span class="math inline">\(C(X)\)</span> as <span class="math display">\[ \left\lVert f\right\rVert_p = \left[\int_X \left\lvert f(x)\right\rvert^p\ dx\right]^{1/p}.\]</span> In the event we want to take <span class="math inline">\(p\to\infty\)</span>, we have <span class="math display">\[ \left\lVert f\right\rVert_\infty = \sup_{x\in X}\left\lvert f(x)\right\rvert.\]</span> We know can use this norm to define the distance between two functions in <span class="math inline">\(C(X)\)</span>, <span class="math display">\[ \left\lVert f\right\rVert_\infty = \sup_{x\in X}\left\lvert f(x) - g(x)\right\rvert.\]</span> This gives the distance between two functions as the distance between their evaluated values where this distance is its largest. We refer to <span class="math inline">\(\left\lVert\cdot\right\rVert_\infty\)</span> on <span class="math inline">\(C(X)\)</span> as the <strong><em>uniform norm</em></strong> or <strong><em>supremum norm</em></strong>.</p>
</div>
</section>
<section id="convergence-and-continuity" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="convergence-and-continuity"><span class="header-section-number">2.9.2</span> Convergence and Continuity</h3>
<p>Given a metric space <span class="math inline">\((X,d)\)</span>, a sequence of elements <span class="math inline">\(\{x_n\}\subset X\)</span> <strong><em>converges</em></strong> to an element <span class="math inline">\(x\)</span> if <em>in</em> <span class="math inline">\(X\)</span> for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some integer <span class="math inline">\(N\in \mathbf{Z}^+\)</span> such that <span class="math inline">\(d(x_n,x)&lt;\varepsilon\)</span> for all <span class="math inline">\(n&gt;N\)</span>. If <span class="math inline">\(\{x_n\}\)</span> converges to <span class="math inline">\(x\)</span>, we write <span class="math inline">\(x_n\to x\)</span> or <span class="math inline">\(\lim_{n\to\infty}x_n=x\)</span>. It’s important to remember that convergence is defined in relation to the metric space <span class="math inline">\(X\)</span>. A sequence <span class="math inline">\(\{x_n\}\)</span> may converge in one metric space but not another, so it’s important to remember what space we’re working in, and the metric the space is equipped with.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.22 (Euclidean Space) </strong></span>A sequence of vectors <span class="math inline">\(\{\mathbf{x}_n\}\subset \mathbb{R}^k\)</span> converges to <span class="math inline">\(\mathbf{x}\)</span> if for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some integer <span class="math inline">\(N\in \mathbb{Z}^+\)</span> such that <span class="math display">\[\left\lVert x_n -x\right\rVert =\left(\sum_{i=1}^k (x_{n,i} - x_i)^2\right)^{1/2} &lt;\varepsilon\]</span> for all <span class="math inline">\(n&gt;N\)</span>. It can be shown that a sequence of vectors only converges to a limit if and only if each of its components converges to their respective limits.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.23 (Pointwise Convergence) </strong></span>Suppose we have a sequence of real functions <span class="math inline">\(\{f_n\}\)</span> where <span class="math inline">\(f_n:X\to \mathbb{R}\)</span> for some <span class="math inline">\(X\subset \mathbb{R}\)</span>. One way to think about <span class="math inline">\(f_n\)</span> converging to some limit is by considering <span class="math inline">\(\{f_n(x)\}\subset \mathbb{R}\)</span> as a sequence in Euclidean space for each <span class="math inline">\(x\in X\)</span>. If <span class="math inline">\(f_n(x)\to f(x)\)</span> for all <span class="math inline">\(x\in X\)</span>, then we say <span class="math inline">\(f\)</span> <strong><em>converges pointwise</em></strong> to <span class="math inline">\(f\)</span>, and write <span class="math inline">\(f_n\to f\)</span>. In other words, <span class="math inline">\(f_n\to f\)</span> if for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some <span class="math inline">\(N_x\)</span> such that <span class="math inline">\(\left\lvert f_n(x) - f(x)\right\rvert&lt; \varepsilon\)</span> for all <span class="math inline">\(n&gt; N\)</span>, for each <span class="math inline">\(x\in X\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.24 (Uniform Convergence) </strong></span>In the definition of pointwise convergence, note that <span class="math inline">\(N_x\)</span> depends on <span class="math inline">\(x\)</span>. This means that while <span class="math inline">\(\left\lvert f_n(x_1) - f(x_1)\right\rvert &lt; \varepsilon\)</span>, it may not be the case that <span class="math inline">\(\left\lvert f_n(x_2) - f(x_2)\right\rvert &lt; \varepsilon\)</span>. We can strengthen pointwise convergence by insuring that <span class="math inline">\(N\)</span> does not depend on <span class="math inline">\(x\)</span>, and <span class="math inline">\(\{f_n\}\)</span> gets arbitrarily close to <span class="math inline">\(f\)</span> uniformly on <span class="math inline">\(X\)</span>. We say <span class="math inline">\(f\)</span> <strong><em>converges uniformly</em></strong> to <span class="math inline">\(f\)</span>, and write <span class="math inline">\(f_n\overset{uni}\to f\)</span> if for all <span class="math inline">\(\varepsilon &gt;0\)</span>, there exists some <span class="math inline">\(N\)</span> (idenpendent of <span class="math inline">\(x\)</span>) such that <span class="math inline">\(\left\lvert f_n(x) - f(x)\right\rvert&lt; \varepsilon\)</span> for all <span class="math inline">\(n&gt; N\)</span> and any <span class="math inline">\(x\in X\)</span>. Uniform convergence has some great properties that pointwise convergence lacks,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> making it much more attractive, albeit a much stricter condition. A more natural formulation of uniform convergence arises by considering the vector space of bounded continuous functions <span class="math inline">\(f:X\to \mathbb{R}\)</span>, <span class="math inline">\(C(X)\)</span>, equipped with the supremum norm <span class="math inline">\(\left\lVert\cdot\right\rVert_\infty\)</span>. In this case, <span class="math inline">\(f_n\to f\)</span> (in <span class="math inline">\((C(X),\left\lVert\cdot\right\rVert_\infty)\)</span>) if for all <span class="math inline">\(\varepsilon &gt; 0\)</span> there exists some <span class="math inline">\(N\in \mathbb Z^+\)</span> such that <span class="math display">\[d(f_n,f) = \left\lVert f_n-f\right\rVert = \sup_{x\in X}\left\lvert f_n-f\right\rvert &lt; \varepsilon\]</span> for all <span class="math inline">\(n &gt; N\)</span>. By the definition of <span class="math inline">\(\left\lVert\cdot\right\rVert_\infty\)</span>, <span class="math display">\[\left\lvert f_n(x)-f(x)\right\rvert \le \sup_{x\in X}\left\lvert f_n-f\right\rvert \le \varepsilon\]</span> for all <span class="math inline">\(x\in X\)</span> when <span class="math inline">\(n&gt;N\)</span>.</p>
</div>
<p>Recall from real analysis that a function <span class="math inline">\(f:X\to Y\)</span> defined on metric spaces <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each equipped with metrics <span class="math inline">\(d_X(\cdot,\cdot)\)</span> and <span class="math inline">\(d_Y(\cdot,\cdot)\)</span>, is continuous at <span class="math inline">\(x\in X\)</span> if for all <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists some <span class="math inline">\(\delta_x\)</span> such that <span class="math display">\[ d_Y(f(x),f(p)) &lt; \varepsilon\]</span> when <span class="math inline">\(d_X(x,p) &lt; \delta_x\)</span>. That is, for any arbitrarily small distance <span class="math inline">\(\varepsilon\)</span>, we can find some point <span class="math inline">\(p\)</span> which is arbitrarily close (within a distance of <span class="math inline">\(\delta_x\)</span> to be exact) to <span class="math inline">\(x\)</span> such that the distance between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(p)\)</span> is less than <span class="math inline">\(\varepsilon\)</span>. If <span class="math inline">\(f\)</span> is continuous for all <span class="math inline">\(p\)</span> in some subset <span class="math inline">\(E\subset X\)</span>, then we say <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\(E\)</span>.</p>
<p>One of the more subtle parts about the definition of continuity is that <span class="math inline">\(\delta_x\)</span> depends on <span class="math inline">\(x\)</span>. If we want to ensure that a single value of <span class="math inline">\(\delta\)</span> can be used for all <span class="math inline">\(x\in E\)</span>, then we need to strengthen the definition of continuity. We say <span class="math inline">\(f\)</span> is uniformly continuous on <span class="math inline">\(E\)</span> if for every <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some <span class="math inline">\(\delta\)</span> independent of <span class="math inline">\(x\)</span> such that <span class="math display">\[d_Y(f(x),f(p)) &lt; \varepsilon\]</span> whenever <span class="math inline">\(d(x,p) &lt;\delta\)</span> for all <span class="math inline">\(x,p\in E\)</span>. Now we can use one uniform <span class="math inline">\(\delta\)</span> on the entire set <span class="math inline">\(x\in E\)</span>. Uniform continuity is <em>much</em> stronger then continuity, but it naturally arises in many common settings.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Now suppose we have a collection of a functions <span class="math inline">\(\mathcal F\)</span>, each of which is uniformly continuous on <span class="math inline">\(E\subset X\)</span>. This means for all <span class="math inline">\(f\in\mathcal F\)</span> and <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists a <span class="math inline">\(\delta_{f}\)</span> independent of <span class="math inline">\(x\)</span> such that <span class="math display">\[d_Y(f(x),f(p)) &lt; \varepsilon\]</span> when <span class="math inline">\(d_X(x,p) &lt; \delta_f\)</span>. Of course <span class="math inline">\(\delta_f\)</span> depends on the function <span class="math inline">\(f\in\mathcal F\)</span> here. Once we start looking at multiple functions, we need to adjust <span class="math inline">\(\delta_f\)</span> accordingly. Even if <span class="math inline">\(\delta_f\)</span> is uniform across <span class="math inline">\(x\in E\)</span> for each separate <span class="math inline">\(f\)</span>, it isn’t guaranteed it will be uniform across <span class="math inline">\(x\in E\)</span> uniformly across <span class="math inline">\(f\in \mathcal F\)</span>. A special case we’re interested in is when <span class="math inline">\(\mathcal F\)</span> is some sequence sequence of functions indexed by <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathcal F=\{f_n\mid n\in\mathbf{Z}^+\}\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.25 </strong></span>Define the sequence of functions <span class="math inline">\(f_n:[0,1]\to \mathbb{R}\)</span> where <span class="math inline">\(f_n(x)=x^n\)</span> and let our collection of functions be this sequence. We will focus our attention on <span class="math inline">\(x = 1\)</span>. The function <span class="math inline">\(f_n\)</span> is continuous on <span class="math inline">\([0,1]\)</span> for all <span class="math inline">\(n\)</span>. In fact, one can show that for all <span class="math inline">\(\varepsilon &gt; 0\)</span>,</p>
<p><span class="math display">\[\left\lvert f_n(x) - f_n(p)\right\rvert &lt; \varepsilon\]</span> whenever <span class="math display">\[ \delta_{n,x} = \frac{\varepsilon}{n(x + 1)^{n-1}}.\]</span> We can go a step further and conclude that <span class="math inline">\(f_n\)</span> is uniformly continuous on <span class="math inline">\([0,1]\)</span>, by setting <span class="math display">\[ \delta_{n} = \inf_{x\in[0,1]}\delta_{n,x} = \frac{\varepsilon}{n(1 + 1)^{n-1}} = \frac{\varepsilon}{n2^{n-1}}.\]</span> By construction <span class="math inline">\(\delta_{n}\)</span> will now work on the entire interval <span class="math inline">\([0,1]\)</span> and doesn’t depend on <span class="math inline">\(x\)</span>. Unfortunately, <span class="math inline">\(\delta_n\)</span> still depends on the function <span class="math inline">\(f_n\)</span> (hence the subscript <span class="math inline">\(n\)</span>), and we cannot perform the same “trick” to get a <span class="math inline">\(\delta\)</span> common to all <span class="math inline">\(n\)</span>.</p>
<section id="convergence-of-stochastic-process" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="convergence-of-stochastic-process">Convergence of Stochastic Process</h3>
</section>
<section id="functional-central-limit-theorem" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="functional-central-limit-theorem">Functional Central Limit Theorem</h3>
</section>
</div>
</section>
</section>
<section id="further-reading" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">2.10</span> Further Reading</h2>
<ul>
<li>Eric Zivot’s <a href="http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf">primer on asymptotics</a></li>
<li><span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 3</li>
<li><span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Appendix D</li>
<li><span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="references.html#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span></li>
<li><span class="citation" data-cites="white2014asymptotic">White (<a href="references.html#ref-white2014asymptotic" role="doc-biblioref">1984</a>)</span></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-billingsley2008probability" class="csl-entry" role="doc-biblioentry">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-durrett2019probability" class="csl-entry" role="doc-biblioentry">
Durrett, Rick. 2019. <em>Probability: Theory and Examples</em>. Vol. 49. Cambridge university press.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-newey1994large" class="csl-entry" role="doc-biblioentry">
Newey, Whitney K, and Daniel McFadden. 1994. <span>“Large Sample Estimation and Hypothesis Testing.”</span> <em>Handbook of Econometrics</em> 4: 2111–2245.
</div>
<div id="ref-rudin1976principles" class="csl-entry" role="doc-biblioentry">
Rudin, Walter. 1976. <em>Principles of Mathematical Analysis</em>. Vol. 3. McGraw-hill New York.
</div>
<div id="ref-van2000asymptotic" class="csl-entry" role="doc-biblioentry">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-white2014asymptotic" class="csl-entry" role="doc-biblioentry">
White, Halbert. 1984. <em>Asymptotic Theory for Econometricians</em>. Academic press.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="doc-biblioentry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We can actually draw observations of <em>any</em> random variable using the uniform distribution on <span class="math inline">\([0,1]\)</span> via <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions"><strong><em>inverse transform sampling</em></strong></a>. This is one of the primary ways computers generate random observations from a given distribution.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="math inline">\(\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\)</span> is the <span class="math inline">\(\dim(\mathbf g(\mathbf{X}_n)) \times \dim(\mathbf X_n)\)</span> Jacobian matrix comprised of the partial derivatives of the components of <span class="math inline">\(\mathbf g\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>It preserves continuity, integration, and under an additional weak condition it also preserves differentiation.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Any continuous function on a compact set is uniformly continuous, because compactness allows you to “chop up” <span class="math inline">\(E\)</span> into discrete points and then take <span class="math inline">\(\delta\)</span> to be the smallest <span class="math inline">\(\delta_x\)</span> associated with the discrete points. Intuitively, there must be some <span class="math inline">\(\delta_x\)</span> that is the “strictest” among all <span class="math inline">\(x\in E\)</span>, and that one will work for all the other points in <span class="math inline">\(E\)</span>. This intuition is what makes the proof of this result so beautiful.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./estimators.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./testing.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb25" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\asto}{\overset{as}{\to}}</span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a><span class="fu"># Asymptotic Properties of Estimators {#sec-asy}</span></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>When considering estimators in @sec-est, we </span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>kept the sample size $n$ fixed when assessing estimators. We now consider how estimators behave </span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a>as $n\to\infty.$ In practice, we will never have infinite data, asymptotics gives us an approximate idea of how estimators perform for large data sets. A comprehensive reference in asymptotic theory is due to @van2000asymptotic. For a treatment concerned purely with econometrics, @newey1994large provide a phenomenal survey, most of which we will touch on when discussing general classes of estimators. </span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>With loss of some generality, we will assume that all random variables have finite expectation and variances. Dispensing with this assumption is something for a probability course. </span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence</span></span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>At some point in high school, most students encounter the concept of a numeric sequence, and how they can converge to a limit. Later on, perhaps when taking a real analysis course, sequences are generalized to spaces of functions. A sequence of functions may also converge to a limit, whether that be converging pointwise and/or converging uniformly (for details see @rudin1976principles). Random variables are functions from a sample space to $\mathbb R$, so we can consider how these functions converge. For the most part, the dimension of the random variable (i.e whether it takes on scalar values of vector values) won't really matter, so we'll remain agnostic about the exact case.</span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in MSE</span></span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a>The first type of convergence we'll work with deals with MSE.</span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_converges in mean square_**<span class="kw">&lt;/span&gt;</span> to a random variable $X$, written as $X_n\overset{ms}{\to} X$, if $$\lim_{n\to\infty} \E{(X_n - X)^2} = 0.$$</span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a>$X_n \ms X$ if the average distance between $X_n$ and $X$ shrinks as $n\to\infty$ where distance is measured as $(X_n - X)^2$. We can also have $X_n \ms c$ for some constant $c$, as $c$ is a trivial random variable.</span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a>Suppose we draw a sample of $n$ iid random variables $Z_i$ and define $X_n$ to be the sample mean of our observations.</span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a>$$X_n = \frac{1}{n}\sum_{i=1}^n Z_i$$</span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a>If $\E{Z_i} = \mu$ and $\var{Z_i} = \sigma^2$ for all $i$, we have $X_n\ms \mu$:</span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\E{(X_n - \mu)^2} &amp; = \lim_{n\to\infty}\var{X_n} + \text{Bias}(X_n) <span class="sc">\\</span></span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a>&amp;= \lim_{n\to\infty} \frac{\sigma^2}{n} + 0 &amp; (X_n \text{ unbiased}) <span class="sc">\\</span></span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a>&amp; = \lim_{n\to\infty} \frac{\sigma^2}{n} <span class="sc">\\</span></span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>What does this convergence "look like"? If $Z_i\iid N(0,1)$, we know that $X_n = \bar Z \sim N(\mu, \sigma^2/n)$. Let's plot this distribution for increasing values of $n$.</span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot21</span></span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The distribution of X_n for various values of n"</span></span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">500</span>),</span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>)</span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n)),</span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">as.factor</span>(n)</span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb25-132"><a href="#cb25-132" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y, <span class="at">color =</span> n)) <span class="sc">+</span></span>
<span id="cb25-133"><a href="#cb25-133" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-134"><a href="#cb25-134" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-135"><a href="#cb25-135" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of X_n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>) <span class="sc">+</span></span>
<span id="cb25-136"><a href="#cb25-136" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb25-137"><a href="#cb25-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-138"><a href="#cb25-138" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb25-139"><a href="#cb25-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-140"><a href="#cb25-140" aria-hidden="true" tabindex="-1"></a>This example betrays a useful property related to variables which converge in mean square. </span>
<span id="cb25-141"><a href="#cb25-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-142"><a href="#cb25-142" aria-hidden="true" tabindex="-1"></a>::: {#prp-mse3}</span>
<span id="cb25-143"><a href="#cb25-143" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ converges in mean square to a constant $c$ *if and only if* $\E{X_n}\to c$ and $\var{X_n}\to 0$.</span>
<span id="cb25-144"><a href="#cb25-144" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-145"><a href="#cb25-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-146"><a href="#cb25-146" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb25-147"><a href="#cb25-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-148"><a href="#cb25-148" aria-hidden="true" tabindex="-1"></a>$(\Longrightarrow)$ Suppose $X_n \ms c$. Then </span>
<span id="cb25-149"><a href="#cb25-149" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-150"><a href="#cb25-150" aria-hidden="true" tabindex="-1"></a>&amp; \lim_{n\to\infty}\E{(X_n - c)^2} = 0<span class="sc">\\</span></span>
<span id="cb25-151"><a href="#cb25-151" aria-hidden="true" tabindex="-1"></a> \implies &amp; \lim_{n\to\infty}\E{(X_n - c)^2} = 0<span class="sc">\\</span></span>
<span id="cb25-152"><a href="#cb25-152" aria-hidden="true" tabindex="-1"></a> \implies&amp; \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\E{X_n}^2 -2c \E{X_n} + c^2\right</span><span class="co">]</span>= 0<span class="sc">\\</span></span>
<span id="cb25-153"><a href="#cb25-153" aria-hidden="true" tabindex="-1"></a> \implies&amp; \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">(\E{X_n}^2 -\E{X_n}^2) + \E{X_n}^2 - 2c \E{X_n} + c^2\right</span><span class="co">]</span>= 0 <span class="sc">\\</span> </span>
<span id="cb25-154"><a href="#cb25-154" aria-hidden="true" tabindex="-1"></a> \implies &amp;\lim_{n\to\infty} \var{X_n} + \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\E{X_n} -c\right</span><span class="co">]</span>^2 = 0 </span>
<span id="cb25-155"><a href="#cb25-155" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-156"><a href="#cb25-156" aria-hidden="true" tabindex="-1"></a>This final equality gives the desired result. </span>
<span id="cb25-157"><a href="#cb25-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-158"><a href="#cb25-158" aria-hidden="true" tabindex="-1"></a>$(\Longleftarrow)$ Suppose $\E{X_n}\to c$ and $\var{X_n}\to 0$. We have </span>
<span id="cb25-159"><a href="#cb25-159" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-160"><a href="#cb25-160" aria-hidden="true" tabindex="-1"></a>&amp;\lim_{n\to\infty} \var{X_n} + \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\E{X_n} -c\right</span><span class="co">]</span>^2 = 0 <span class="sc">\\</span></span>
<span id="cb25-161"><a href="#cb25-161" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to\infty}\E{(X_n - c)^2} = 0<span class="sc">\\</span></span>
<span id="cb25-162"><a href="#cb25-162" aria-hidden="true" tabindex="-1"></a>\implies &amp; X_n\ms c</span>
<span id="cb25-163"><a href="#cb25-163" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-164"><a href="#cb25-164" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-165"><a href="#cb25-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-166"><a href="#cb25-166" aria-hidden="true" tabindex="-1"></a>::: {#cor-mseconv}</span>
<span id="cb25-167"><a href="#cb25-167" aria-hidden="true" tabindex="-1"></a>Suppose $X_n$ is a sequence of random variables such that $\E{X_n} = c$ for all $n$. Then $X_n\ms c$ *if and only if* $\var{X_n}\to 0$.</span>
<span id="cb25-168"><a href="#cb25-168" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-169"><a href="#cb25-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-170"><a href="#cb25-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Probability</span></span>
<span id="cb25-171"><a href="#cb25-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-172"><a href="#cb25-172" aria-hidden="true" tabindex="-1"></a>Convergence in mean square captures the idea that a random variable gets "closer" to some value $c,$ but it is hardly the only way to define this behavior. A more "traditional" approach would be defining convergence using an inequality involving an arbitrarily small $\varepsilon &gt;0$ (akin the to $\varepsilon-\delta$ definition of a limit).</span>
<span id="cb25-173"><a href="#cb25-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-174"><a href="#cb25-174" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb25-175"><a href="#cb25-175" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_converges in probability_**<span class="kw">&lt;/span&gt;</span> to a random variable $X$, written as $X_n\pto X$ or $\plim X_n = X$, if $$\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon)= 0$$ for all $\varepsilon &gt; 0$. Equivalently, $X_n\pto X$ if for all $\varepsilon &gt; 0$ and $\delta &gt; 0$, there exists some $N$ such that for all $n \ge N$, </span>
<span id="cb25-176"><a href="#cb25-176" aria-hidden="true" tabindex="-1"></a>$$ \Pr (|X_n - X| &gt; \varepsilon) &lt; \delta.$$</span>
<span id="cb25-177"><a href="#cb25-177" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-178"><a href="#cb25-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-179"><a href="#cb25-179" aria-hidden="true" tabindex="-1"></a>Intuitively, $X_n \pto X$ if the probability that the difference $|X_n - X|$ is not small (greater than some $\varepsilon$) goes to zero as $n\to\infty$.</span>
<span id="cb25-180"><a href="#cb25-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-181"><a href="#cb25-181" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-182"><a href="#cb25-182" aria-hidden="true" tabindex="-1"></a>Return to the previous example where $X_n = \bar Z$, and assume $Z_i \iid N(\mu,\sigma^2)$. We will verify that $X_n\pto \mu$ using the definition of convergence in probability using the fact that $X_n \sim N(\mu, \sigma^2/n)$.</span>
<span id="cb25-183"><a href="#cb25-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-184"><a href="#cb25-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-187"><a href="#cb25-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-188"><a href="#cb25-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-189"><a href="#cb25-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot22</span></span>
<span id="cb25-190"><a href="#cb25-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-191"><a href="#cb25-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-192"><a href="#cb25-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small"</span></span>
<span id="cb25-193"><a href="#cb25-193" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb25-194"><a href="#cb25-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-195"><a href="#cb25-195" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/converge.png"</span>)</span>
<span id="cb25-196"><a href="#cb25-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-197"><a href="#cb25-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-198"><a href="#cb25-198" aria-hidden="true" tabindex="-1"></a>For some $\varepsilon &gt; 0$, </span>
<span id="cb25-199"><a href="#cb25-199" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-200"><a href="#cb25-200" aria-hidden="true" tabindex="-1"></a>\Pr (|X_n - \mu| &gt; \varepsilon) &amp; = 1 - \Pr (\mu - \varepsilon &lt; X_n &lt; \mu + \varepsilon)<span class="sc">\\</span></span>
<span id="cb25-201"><a href="#cb25-201" aria-hidden="true" tabindex="-1"></a>&amp; = 1 - (F_{X_n}(\mu + \varepsilon) + F_{X_n}(\mu - \varepsilon))<span class="sc">\\</span></span>
<span id="cb25-202"><a href="#cb25-202" aria-hidden="true" tabindex="-1"></a>&amp; = 1 - 2\left<span class="co">[</span><span class="ot">F_{X_n}(\mu + \varepsilon) - \frac{1}{2}\right</span><span class="co">]</span> &amp; (F_{X_n} \text{symmetric about }\mu)<span class="sc">\\</span></span>
<span id="cb25-203"><a href="#cb25-203" aria-hidden="true" tabindex="-1"></a> &amp; = 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{(\mu + \varepsilon) - \mu}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span> &amp; (\Phi\text{ standard normal distribution})<span class="sc">\\</span></span>
<span id="cb25-204"><a href="#cb25-204" aria-hidden="true" tabindex="-1"></a> &amp; = 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span>.</span>
<span id="cb25-205"><a href="#cb25-205" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-206"><a href="#cb25-206" aria-hidden="true" tabindex="-1"></a>Given some $\delta &gt;0$, we can solve for the lowest value of $n$ that satisfies $\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta$.</span>
<span id="cb25-207"><a href="#cb25-207" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-208"><a href="#cb25-208" aria-hidden="true" tabindex="-1"></a>&amp;\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta <span class="sc">\\</span> </span>
<span id="cb25-209"><a href="#cb25-209" aria-hidden="true" tabindex="-1"></a>\implies &amp; 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span> &lt; \delta <span class="sc">\\</span></span>
<span id="cb25-210"><a href="#cb25-210" aria-hidden="true" tabindex="-1"></a>\implies&amp;  n &gt; \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2</span>
<span id="cb25-211"><a href="#cb25-211" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>\implies &amp; n &gt; \left\lceil \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2 \right\rceil</span>
<span id="cb25-212"><a href="#cb25-212" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-213"><a href="#cb25-213" aria-hidden="true" tabindex="-1"></a>Just to be excruciatingly pedantic, we rounded our solution up to the closest positive integer, as $n$ corresponds to a sample size. For fixed values of $\mu$ and $\sigma^2$ (say 3 and 2, respectively), we can define a function of $(\varepsilon, \delta)$ which calculates the sample size required to satisfy $\Pr(|X_n - c|&gt;\varepsilon)&lt;\delta$.</span>
<span id="cb25-214"><a href="#cb25-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-217"><a href="#cb25-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-218"><a href="#cb25-218" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb25-219"><a href="#cb25-219" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb25-220"><a href="#cb25-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-221"><a href="#cb25-221" aria-hidden="true" tabindex="-1"></a>n_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(delta, ep){</span>
<span id="cb25-222"><a href="#cb25-222" aria-hidden="true" tabindex="-1"></a> <span class="fu">ceiling</span>(((sigma<span class="sc">*</span><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>delta <span class="sc">/</span><span class="dv">2</span>))<span class="sc">/</span>ep)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb25-223"><a href="#cb25-223" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-224"><a href="#cb25-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-225"><a href="#cb25-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-226"><a href="#cb25-226" aria-hidden="true" tabindex="-1"></a>Let's plot this function for various values of $(\varepsilon, \delta)$.</span>
<span id="cb25-227"><a href="#cb25-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-230"><a href="#cb25-230" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-231"><a href="#cb25-231" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-232"><a href="#cb25-232" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot23</span></span>
<span id="cb25-233"><a href="#cb25-233" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-234"><a href="#cb25-234" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-235"><a href="#cb25-235" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-236"><a href="#cb25-236" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)"</span></span>
<span id="cb25-237"><a href="#cb25-237" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-238"><a href="#cb25-238" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb25-239"><a href="#cb25-239" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb25-240"><a href="#cb25-240" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9999</span><span class="sc">/</span><span class="dv">10000</span></span>
<span id="cb25-241"><a href="#cb25-241" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-242"><a href="#cb25-242" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sample =</span> <span class="fu">n_fun</span>(d,e)) <span class="sc">%&gt;%</span> </span>
<span id="cb25-243"><a href="#cb25-243" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(d, sample, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb25-244"><a href="#cb25-244" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_reverse</span>() <span class="sc">+</span></span>
<span id="cb25-245"><a href="#cb25-245" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span> </span>
<span id="cb25-246"><a href="#cb25-246" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-247"><a href="#cb25-247" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-248"><a href="#cb25-248" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"δ"</span>, <span class="at">y =</span> <span class="st">"Sample Size"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)<span class="sc">+</span></span>
<span id="cb25-249"><a href="#cb25-249" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb25-250"><a href="#cb25-250" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-251"><a href="#cb25-251" aria-hidden="true" tabindex="-1"></a>We can also verify that $\lim_{n\to\infty}\Pr (|X_n - \mu| &gt; \varepsilon) = 0$ for various values of $\ep$.</span>
<span id="cb25-252"><a href="#cb25-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-255"><a href="#cb25-255" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-256"><a href="#cb25-256" aria-hidden="true" tabindex="-1"></a>prob_ep <span class="ot">&lt;-</span> <span class="cf">function</span>(n, ep){</span>
<span id="cb25-257"><a href="#cb25-257" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">pnorm</span>(ep <span class="sc">/</span> (sigma <span class="sc">/</span> <span class="fu">sqrt</span>(n))) <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb25-258"><a href="#cb25-258" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-259"><a href="#cb25-259" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-260"><a href="#cb25-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-263"><a href="#cb25-263" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-264"><a href="#cb25-264" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-265"><a href="#cb25-265" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot24</span></span>
<span id="cb25-266"><a href="#cb25-266" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-267"><a href="#cb25-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-268"><a href="#cb25-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-269"><a href="#cb25-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The probability that X_n falls outside the interval  |μ-ε| for various values of (ε,n) "</span></span>
<span id="cb25-270"><a href="#cb25-270" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-271"><a href="#cb25-271" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb25-272"><a href="#cb25-272" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb25-273"><a href="#cb25-273" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb25-274"><a href="#cb25-274" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-275"><a href="#cb25-275" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> <span class="fu">prob_ep</span>(e,n)) <span class="sc">%&gt;%</span> </span>
<span id="cb25-276"><a href="#cb25-276" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, prob, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb25-277"><a href="#cb25-277" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-278"><a href="#cb25-278" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb25-279"><a href="#cb25-279" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-280"><a href="#cb25-280" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Pr(|X_n - mu| &gt; ε)"</span>, <span class="at">color =</span> <span class="st">"ε"</span>) <span class="sc">+</span></span>
<span id="cb25-281"><a href="#cb25-281" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span>
<span id="cb25-282"><a href="#cb25-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-283"><a href="#cb25-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-284"><a href="#cb25-284" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-285"><a href="#cb25-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-286"><a href="#cb25-286" aria-hidden="true" tabindex="-1"></a>How does convergence in mean square related to convergence in probability? As it turns out the latter is a weaker condition implied by the prior. Before stating and proving this result, we will need a lemma. </span>
<span id="cb25-287"><a href="#cb25-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-288"><a href="#cb25-288" aria-hidden="true" tabindex="-1"></a>::: {#lem-markovineq}</span>
<span id="cb25-289"><a href="#cb25-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-290"><a href="#cb25-290" aria-hidden="true" tabindex="-1"></a><span class="fu">## Markov's inequality</span></span>
<span id="cb25-291"><a href="#cb25-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-292"><a href="#cb25-292" aria-hidden="true" tabindex="-1"></a>If $X$ is a nonnegative random variable, and $a &gt; 0$, then $$\Pr(X\ge a) \le \frac{\E{X}}{a}$$   </span>
<span id="cb25-293"><a href="#cb25-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-294"><a href="#cb25-294" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-295"><a href="#cb25-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-296"><a href="#cb25-296" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb25-297"><a href="#cb25-297" aria-hidden="true" tabindex="-1"></a>The expectation of $X$ can be written as </span>
<span id="cb25-298"><a href="#cb25-298" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-299"><a href="#cb25-299" aria-hidden="true" tabindex="-1"></a>\E{X} &amp; = \int_{-\infty}^\infty x\ dF_X(x) <span class="sc">\\</span> </span>
<span id="cb25-300"><a href="#cb25-300" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{0}^\infty x\ dF_X(x) &amp; (X\text{ is nonnegative}) <span class="sc">\\</span> </span>
<span id="cb25-301"><a href="#cb25-301" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{0}^a x\ dF_X(x) + \int_{a}^\infty x\ dF_X(x) <span class="sc">\\</span> </span>
<span id="cb25-302"><a href="#cb25-302" aria-hidden="true" tabindex="-1"></a>&amp; \ge \int_a^\infty x\ dF_X(x)<span class="sc">\\</span></span>
<span id="cb25-303"><a href="#cb25-303" aria-hidden="true" tabindex="-1"></a>&amp; \ge \int_a^\infty a\ dF_X(x) &amp; (a \ge x \text{ on }(a,\infty))<span class="sc">\\</span></span>
<span id="cb25-304"><a href="#cb25-304" aria-hidden="true" tabindex="-1"></a>&amp; = a \int_a^\infty\ dF_X(x) <span class="sc">\\</span></span>
<span id="cb25-305"><a href="#cb25-305" aria-hidden="true" tabindex="-1"></a>&amp; = a\Pr(X \ge a).</span>
<span id="cb25-306"><a href="#cb25-306" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-307"><a href="#cb25-307" aria-hidden="true" tabindex="-1"></a>Dividing both sides of this inequality by $a$ gives $\Pr(X\ge a) \le \E{X}/a$.</span>
<span id="cb25-308"><a href="#cb25-308" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-309"><a href="#cb25-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-310"><a href="#cb25-310" aria-hidden="true" tabindex="-1"></a>::: {#prp-conv}</span>
<span id="cb25-311"><a href="#cb25-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-312"><a href="#cb25-312" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in MSE --&gt; Convergence in Probability</span></span>
<span id="cb25-313"><a href="#cb25-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-314"><a href="#cb25-314" aria-hidden="true" tabindex="-1"></a>Let $X_n$ be a sequence of random variables. If $X_n\ms X$, then $X_n\pto X$. </span>
<span id="cb25-315"><a href="#cb25-315" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-316"><a href="#cb25-316" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb25-317"><a href="#cb25-317" aria-hidden="true" tabindex="-1"></a>Suppose $X_n\ms X$. For all $\varepsilon &gt; 0$</span>
<span id="cb25-318"><a href="#cb25-318" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-319"><a href="#cb25-319" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon) &amp; = \lim_{n\to\infty} \Pr ((X_n - X)^2 &gt; \varepsilon^2) <span class="sc">\\</span></span>
<span id="cb25-320"><a href="#cb25-320" aria-hidden="true" tabindex="-1"></a>&amp; \le \lim_{n\to\infty} \frac{\E{(X_n - X)^2}}{\varepsilon^2} &amp; (\text{Markov's Inequality}) <span class="sc">\\</span></span>
<span id="cb25-321"><a href="#cb25-321" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{0}{\varepsilon^2} &amp; (X_n\ms X)<span class="sc">\\</span></span>
<span id="cb25-322"><a href="#cb25-322" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb25-323"><a href="#cb25-323" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-324"><a href="#cb25-324" aria-hidden="true" tabindex="-1"></a>Therefore $X_n\pto c$.</span>
<span id="cb25-325"><a href="#cb25-325" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-326"><a href="#cb25-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-327"><a href="#cb25-327" aria-hidden="true" tabindex="-1"></a>The usefulness of @prp-conv cannot be emphasized enough. Proving convergence in probability using the definition is cumbersome, so we will almost show convergence in mean square and then appeal to  @prp-conv to verify convergence in probability. Nevertheless, situations can arise where $X_n\pto c$, but $X_n \not\overset{ms}{\to} c$.</span>
<span id="cb25-328"><a href="#cb25-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-329"><a href="#cb25-329" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-330"><a href="#cb25-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-331"><a href="#cb25-331" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in Probability but not in Mean Square</span></span>
<span id="cb25-332"><a href="#cb25-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-333"><a href="#cb25-333" aria-hidden="true" tabindex="-1"></a>Suppose there is a sequence of random variables $X_n$ that take on the values $0$ and $n^2$ with probabilities $\Pr(X_n = 0) = 1-1/n$ and $\Pr(X_n=n^2) = 1/n$. The expected value of this random variable is </span>
<span id="cb25-334"><a href="#cb25-334" aria-hidden="true" tabindex="-1"></a>$$\E{X_n} = 0(1-1/n) + n^2(1/n) = n,$$ so $\E{X_n}\to\infty$ as $n\to \infty$. This rules out $X_n$ converging in mean square to any value. Nevertheless, we have $X_n\pto 0$. For all $\varepsilon &gt; 0$, </span>
<span id="cb25-335"><a href="#cb25-335" aria-hidden="true" tabindex="-1"></a>$$\Pr(|X_n - 0| &gt; \varepsilon) = \Pr(X_n \neq 0) = \Pr(X_n = n^2) = 1/n \to 0.$$</span>
<span id="cb25-336"><a href="#cb25-336" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-337"><a href="#cb25-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-338"><a href="#cb25-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### Almost Sure Convergence</span></span>
<span id="cb25-339"><a href="#cb25-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-340"><a href="#cb25-340" aria-hidden="true" tabindex="-1"></a>Another form of convergence arises if we remember $X_n:\mathcal X\to\R$ is just a function from a sample space to $\R$ (or $\R^k$). While we usually write $X_n = x$, </span>
<span id="cb25-341"><a href="#cb25-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-342"><a href="#cb25-342" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Distribution</span></span>
<span id="cb25-343"><a href="#cb25-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-344"><a href="#cb25-344" aria-hidden="true" tabindex="-1"></a>The final notion of convergence we will use related to the probability distribution of random variables. </span>
<span id="cb25-345"><a href="#cb25-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-346"><a href="#cb25-346" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb25-347"><a href="#cb25-347" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_converges in distribution (converges weakly)_**&lt;/span&gt; to a random variable $X$, written as $X_n \dto X$, if $$\lim_{n\to\infty} F_{X_n}(x)= F_X(x).$$ In this case, we refer to $F_X$ as the &lt;span style="color:red"&gt;**_asymptotic distribution_**<span class="kw">&lt;/span&gt;</span> of $X_n$, and sometimes write $X_n \asim F_X$.  </span>
<span id="cb25-348"><a href="#cb25-348" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-349"><a href="#cb25-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-350"><a href="#cb25-350" aria-hidden="true" tabindex="-1"></a>For our purposes, $X_n\dto X$ means the distribution of $X_n$ can be approximated by $F_X$, and this approximation becomes increasingly better as $n\to\infty$. </span>
<span id="cb25-351"><a href="#cb25-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-352"><a href="#cb25-352" aria-hidden="true" tabindex="-1"></a>::: {#exm- #tdist}</span>
<span id="cb25-353"><a href="#cb25-353" aria-hidden="true" tabindex="-1"></a>One example of convergence in distribution you may be familiar with deals with the student's $t-$distribution where the degrees of freedom $n\to\infty$. If $X_n\sim t_n$, then $X_n \dto X$ where $X\sim N(0,1)$.</span>
<span id="cb25-354"><a href="#cb25-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-357"><a href="#cb25-357" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-358"><a href="#cb25-358" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-359"><a href="#cb25-359" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot25</span></span>
<span id="cb25-360"><a href="#cb25-360" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-361"><a href="#cb25-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-362"><a href="#cb25-362" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-363"><a href="#cb25-363" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The t-distribution converges to the standard normal distribution (represented by the dashed black line) as the degrees of freedom increase."</span></span>
<span id="cb25-364"><a href="#cb25-364" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-365"><a href="#cb25-365" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(</span>
<span id="cb25-366"><a href="#cb25-366" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length =</span> <span class="dv">10000</span>),</span>
<span id="cb25-367"><a href="#cb25-367" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>),</span>
<span id="cb25-368"><a href="#cb25-368" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> <span class="st">"Student's t"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-369"><a href="#cb25-369" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val =</span> <span class="fu">dt</span>(x, n)) <span class="sc">%&gt;%</span> </span>
<span id="cb25-370"><a href="#cb25-370" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, val, <span class="at">color =</span> <span class="fu">as.factor</span>(n))) <span class="sc">+</span></span>
<span id="cb25-371"><a href="#cb25-371" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-372"><a href="#cb25-372" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb25-373"><a href="#cb25-373" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, </span>
<span id="cb25-374"><a href="#cb25-374" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb25-375"><a href="#cb25-375" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"black"</span>, </span>
<span id="cb25-376"><a href="#cb25-376" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">0.5</span>, </span>
<span id="cb25-377"><a href="#cb25-377" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype=</span><span class="st">"dashed"</span></span>
<span id="cb25-378"><a href="#cb25-378" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb25-379"><a href="#cb25-379" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-380"><a href="#cb25-380" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"t-distribution degrees of freedom, n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb25-381"><a href="#cb25-381" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span>
<span id="cb25-382"><a href="#cb25-382" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-383"><a href="#cb25-383" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-384"><a href="#cb25-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-385"><a href="#cb25-385" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb25-386"><a href="#cb25-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-387"><a href="#cb25-387" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in Probability --&gt; Convergence in Distribution</span></span>
<span id="cb25-388"><a href="#cb25-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-389"><a href="#cb25-389" aria-hidden="true" tabindex="-1"></a>Let $X_n$ be a sequence of random variables. If $X_n\pto X$, then $X_n\dto X$. </span>
<span id="cb25-390"><a href="#cb25-390" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-391"><a href="#cb25-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-392"><a href="#cb25-392" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-393"><a href="#cb25-393" aria-hidden="true" tabindex="-1"></a>Suppose $X_n\pto X$ and let $\varepsilon &gt; 0$. We have,</span>
<span id="cb25-394"><a href="#cb25-394" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-395"><a href="#cb25-395" aria-hidden="true" tabindex="-1"></a>\Pr(X_n \le x) &amp; = \Pr(X_n\le x \text{ and } X \le x + \varepsilon) + \Pr(X_n\le x \text{ and } X &gt; x + \varepsilon) <span class="sc">\\</span></span>
<span id="cb25-396"><a href="#cb25-396" aria-hidden="true" tabindex="-1"></a>  &amp; = \Pr(X \le x + \varepsilon) + \Pr(X_n - X\le x - X \text{ and } x - X &lt; -\varepsilon)<span class="sc">\\</span> </span>
<span id="cb25-397"><a href="#cb25-397" aria-hidden="true" tabindex="-1"></a>  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon)<span class="sc">\\</span></span>
<span id="cb25-398"><a href="#cb25-398" aria-hidden="true" tabindex="-1"></a>  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon) + \Pr(X - X_n &gt; \varepsilon) <span class="sc">\\</span></span>
<span id="cb25-399"><a href="#cb25-399" aria-hidden="true" tabindex="-1"></a>  &amp; = \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon)</span>
<span id="cb25-400"><a href="#cb25-400" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-401"><a href="#cb25-401" aria-hidden="true" tabindex="-1"></a>Similarly, </span>
<span id="cb25-402"><a href="#cb25-402" aria-hidden="true" tabindex="-1"></a>$$ \Pr(X \le x-\varepsilon) \le \Pr(X_n \le x) + \Pr(|X_n - X| &gt; \varepsilon).$$</span>
<span id="cb25-403"><a href="#cb25-403" aria-hidden="true" tabindex="-1"></a>We can use these inequalities to find an upper and lower bound of $\Pr(X_n \le x)$:</span>
<span id="cb25-404"><a href="#cb25-404" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-405"><a href="#cb25-405" aria-hidden="true" tabindex="-1"></a>&amp; \Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)\le \Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) <span class="sc">\\</span></span>
<span id="cb25-406"><a href="#cb25-406" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to\infty}<span class="co">[</span><span class="ot">\Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)</span><span class="co">]</span>\le \lim_{n\to\infty}\Pr(X_n \le x) \le \lim_{n\to\infty}<span class="co">[</span><span class="ot">\Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) </span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb25-407"><a href="#cb25-407" aria-hidden="true" tabindex="-1"></a>\implies &amp; \Pr(X \le x-\varepsilon) - \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon) }_0\le \lim_{n\to\infty}\Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon)}_0 <span class="sc">\\</span></span>
<span id="cb25-408"><a href="#cb25-408" aria-hidden="true" tabindex="-1"></a>\implies &amp; \Pr(X \le x-\varepsilon)\le \lim_{n\to\infty} \Pr(X_n \le x) \le  \Pr(X \le x + \varepsilon) &amp; (X_n\pto X)<span class="sc">\\</span> </span>
<span id="cb25-409"><a href="#cb25-409" aria-hidden="true" tabindex="-1"></a>\implies &amp; F_X(x-\varepsilon)\le \lim_{n\to\infty} F_{X_n}(x) \le  F_X(x-\varepsilon) </span>
<span id="cb25-410"><a href="#cb25-410" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-411"><a href="#cb25-411" aria-hidden="true" tabindex="-1"></a>This holds for all $\varepsilon &gt; 0$, so it must be the case that $\lim_{n\to\infty} F_{X_n}(x) = F_X(x)$</span>
<span id="cb25-412"><a href="#cb25-412" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-413"><a href="#cb25-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-414"><a href="#cb25-414" aria-hidden="true" tabindex="-1"></a><span class="fu">### Putting the Pieces Together</span></span>
<span id="cb25-415"><a href="#cb25-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-416"><a href="#cb25-416" aria-hidden="true" tabindex="-1"></a>The biggest takeaway from this should be the following relation:</span>
<span id="cb25-417"><a href="#cb25-417" aria-hidden="true" tabindex="-1"></a>$$ X_n \ms X \implies X_n \pto X \implies X_n \dto X$$</span>
<span id="cb25-418"><a href="#cb25-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-419"><a href="#cb25-419" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency</span></span>
<span id="cb25-420"><a href="#cb25-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-421"><a href="#cb25-421" aria-hidden="true" tabindex="-1"></a>Our three modes of convergence were defined for any sequence of random variables. It should come as no surprise, considering the previous examples considering whether the sample mean converged, that we are interested in the convergence of estimators $\hat\theta(\X)$ as sample size increases. In particular we are interested in whether $\hat\theta(\X)$ converges to the constant $\theta\in\Theta$ it is estimating.</span>
<span id="cb25-422"><a href="#cb25-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-423"><a href="#cb25-423" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb25-424"><a href="#cb25-424" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_consistent (for estimand $\theta$)_**<span class="kw">&lt;/span&gt;</span> if $\hat\theta \pto \theta$.  </span>
<span id="cb25-425"><a href="#cb25-425" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-426"><a href="#cb25-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-427"><a href="#cb25-427" aria-hidden="true" tabindex="-1"></a>We've already seen that $\bar X$ is a consistent estimator for $\mu$ when we take an iid sample from a normal distribution. Let's investigate it's variance-counterpart $S^2$.</span>
<span id="cb25-428"><a href="#cb25-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-429"><a href="#cb25-429" aria-hidden="true" tabindex="-1"></a>::: {#exm-consvarnorm}</span>
<span id="cb25-430"><a href="#cb25-430" aria-hidden="true" tabindex="-1"></a>For $X_i \iid N(\mu,\sigma^2)$, $S^2 = \sum_{i=1}^n (X_i - \bar X)/(n-1)$ is an unbiased estimator for $\sigma^2$. This estimator's MSE (which is just its variance as it is unbiased) is $2\sigma^4/(n-1)$ which converges to $0$ as $n\to\infty$, so $S^2$ is consistent by @prp-conv. </span>
<span id="cb25-431"><a href="#cb25-431" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-432"><a href="#cb25-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-433"><a href="#cb25-433" aria-hidden="true" tabindex="-1"></a>This example highlights the fact that proving an unbiased estimator is consistent is a matter of showing its variance converges to 0.  </span>
<span id="cb25-434"><a href="#cb25-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-435"><a href="#cb25-435" aria-hidden="true" tabindex="-1"></a>::: {#cor-unbcon}</span>
<span id="cb25-436"><a href="#cb25-436" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta$ is an unbiased estimator for $\theta$. Then $\hat\theta$ is consistent *if and only if* $\var{\hat\theta}\to 0$.</span>
<span id="cb25-437"><a href="#cb25-437" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-438"><a href="#cb25-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-439"><a href="#cb25-439" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb25-440"><a href="#cb25-440" aria-hidden="true" tabindex="-1"></a>Apply @cor-mseconv to an unbiased estimator. </span>
<span id="cb25-441"><a href="#cb25-441" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-442"><a href="#cb25-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-443"><a href="#cb25-443" aria-hidden="true" tabindex="-1"></a>A second type of convergence related to estimators pertains to the bias of an estimator. In @sec-est we saw a few estimators that were biased, but this bias was such that it diminished as $n\to\infty$. In effect, they were unbiased in an asymptotic sense.  </span>
<span id="cb25-444"><a href="#cb25-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-445"><a href="#cb25-445" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb25-446"><a href="#cb25-446" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_asymptotically unbiased_**<span class="kw">&lt;/span&gt;</span> if $\lim_{n\to\infty}\text{Bias}(\hat\theta, \theta) = 0$.  </span>
<span id="cb25-447"><a href="#cb25-447" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-448"><a href="#cb25-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-449"><a href="#cb25-449" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-450"><a href="#cb25-450" aria-hidden="true" tabindex="-1"></a>For $X_i \iid N(\mu,\sigma^2)$, $\hat\theta = \sum_{i=1}^n (X_i - \bar X)/n$ is a biased estimator for $\sigma^2$. Its bias is </span>
<span id="cb25-451"><a href="#cb25-451" aria-hidden="true" tabindex="-1"></a>$$\text{Bias}(\hat\theta, \sigma^2) = \frac{n-1}{n}\sigma^2 - \sigma^2.$$ As $n\to\infty$, this bias vanishes. To illustrate this, we can simulate estimates for various sample sizes, taking $X_i \sim N(0,1)$. </span>
<span id="cb25-452"><a href="#cb25-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-455"><a href="#cb25-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-456"><a href="#cb25-456" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="cf">function</span>(X){</span>
<span id="cb25-457"><a href="#cb25-457" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((X <span class="sc">-</span> <span class="fu">mean</span>(X))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(X)</span>
<span id="cb25-458"><a href="#cb25-458" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-459"><a href="#cb25-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-460"><a href="#cb25-460" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb25-461"><a href="#cb25-461" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>)</span>
<span id="cb25-462"><a href="#cb25-462" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb25-463"><a href="#cb25-463" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"n=10"</span>, <span class="st">"n=25"</span>, <span class="st">"n=50"</span>, <span class="st">"n=100"</span>, <span class="st">"n=500"</span>)</span>
<span id="cb25-464"><a href="#cb25-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-465"><a href="#cb25-465" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb25-466"><a href="#cb25-466" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> <span class="fu">which</span>(sample_sizes <span class="sc">==</span> n)</span>
<span id="cb25-467"><a href="#cb25-467" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim){</span>
<span id="cb25-468"><a href="#cb25-468" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb25-469"><a href="#cb25-469" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">theta</span>(X) </span>
<span id="cb25-470"><a href="#cb25-470" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-471"><a href="#cb25-471" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-472"><a href="#cb25-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-473"><a href="#cb25-473" aria-hidden="true" tabindex="-1"></a><span class="co">#Print bias calculated over 1,000,000 simulations</span></span>
<span id="cb25-474"><a href="#cb25-474" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(store_estimates) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb25-475"><a href="#cb25-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-476"><a href="#cb25-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-479"><a href="#cb25-479" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-480"><a href="#cb25-480" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-481"><a href="#cb25-481" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot26</span></span>
<span id="cb25-482"><a href="#cb25-482" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-483"><a href="#cb25-483" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-484"><a href="#cb25-484" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-485"><a href="#cb25-485" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As the sample size increases, the bias of our estimator converges to zero."</span></span>
<span id="cb25-486"><a href="#cb25-486" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-487"><a href="#cb25-487" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb25-488"><a href="#cb25-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-489"><a href="#cb25-489" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb25-490"><a href="#cb25-490" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb25-491"><a href="#cb25-491" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-492"><a href="#cb25-492" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-493"><a href="#cb25-493" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb25-494"><a href="#cb25-494" aria-hidden="true" tabindex="-1"></a>    <span class="at">key =</span> <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V1"</span>, <span class="st">"10"</span>, </span>
<span id="cb25-495"><a href="#cb25-495" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V2"</span>, <span class="st">"25"</span>, </span>
<span id="cb25-496"><a href="#cb25-496" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V3"</span>, <span class="st">"50"</span>, </span>
<span id="cb25-497"><a href="#cb25-497" aria-hidden="true" tabindex="-1"></a>          <span class="fu">ifelse</span>(key <span class="sc">==</span> <span class="st">"V4"</span>, <span class="st">"100"</span>, <span class="st">"500"</span>)</span>
<span id="cb25-498"><a href="#cb25-498" aria-hidden="true" tabindex="-1"></a>          )))</span>
<span id="cb25-499"><a href="#cb25-499" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb25-500"><a href="#cb25-500" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">key =</span> <span class="fu">as.numeric</span>(key)) <span class="sc">%&gt;%</span> </span>
<span id="cb25-501"><a href="#cb25-501" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">n =</span> key)  <span class="sc">%&gt;%</span> </span>
<span id="cb25-502"><a href="#cb25-502" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value, <span class="at">color =</span> <span class="fu">as.factor</span>(n))) <span class="sc">+</span></span>
<span id="cb25-503"><a href="#cb25-503" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb25-504"><a href="#cb25-504" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb25-505"><a href="#cb25-505" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-506"><a href="#cb25-506" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb25-507"><a href="#cb25-507" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>)</span>
<span id="cb25-508"><a href="#cb25-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-509"><a href="#cb25-509" aria-hidden="true" tabindex="-1"></a>The estimator $\hat\theta$ is also consistent, as it converges in mean square. </span>
<span id="cb25-510"><a href="#cb25-510" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-511"><a href="#cb25-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-512"><a href="#cb25-512" aria-hidden="true" tabindex="-1"></a>Neither asymptotic unbiasedness does not imply consistency, nor does consistency imply asymptotic unbiasedness.</span>
<span id="cb25-513"><a href="#cb25-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-514"><a href="#cb25-514" aria-hidden="true" tabindex="-1"></a>::: {#exm-} </span>
<span id="cb25-515"><a href="#cb25-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-516"><a href="#cb25-516" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistent, Not Asymptotically Unbiased</span></span>
<span id="cb25-517"><a href="#cb25-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-518"><a href="#cb25-518" aria-hidden="true" tabindex="-1"></a>Recall that the sequence of discrete random variables $X_n$ with denisty </span>
<span id="cb25-519"><a href="#cb25-519" aria-hidden="true" tabindex="-1"></a>$$ f_{X_n}(x) = \begin{cases}1-1/n&amp; x=0<span class="sc">\\</span> 1/n &amp; x = n^2 \end{cases}.$$</span>
<span id="cb25-520"><a href="#cb25-520" aria-hidden="true" tabindex="-1"></a>We established that $X_n\pto 0$, so an estimator with this distribution would be a consistent estimator for $0$. Despite this, the estimator would not be asymptotically unbiased, as $\E{X_n} = n$, which tends to infinity as $n$ grows.</span>
<span id="cb25-521"><a href="#cb25-521" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-522"><a href="#cb25-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-523"><a href="#cb25-523" aria-hidden="true" tabindex="-1"></a>::: {#exm-} </span>
<span id="cb25-524"><a href="#cb25-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-525"><a href="#cb25-525" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotically Unbiased, Not Consistent</span></span>
<span id="cb25-526"><a href="#cb25-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-527"><a href="#cb25-527" aria-hidden="true" tabindex="-1"></a>For $X_i\iid N(\mu,\sigma^2)$, define the estimator $\hat\mu(\X) =  X_1$. We simply take the first observation to be our estimate of $\mu$. This estimator is unbiased,</span>
<span id="cb25-528"><a href="#cb25-528" aria-hidden="true" tabindex="-1"></a>$$\E{\hat\mu} = \E{X_1} = \mu,$$ so it is asymptotically unbiased. Nevertheless, the estimator fails to be consistent, as </span>
<span id="cb25-529"><a href="#cb25-529" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} \Pr (|\hat\mu - \mu| &gt; \varepsilon)= \lim_{n\to\infty} \left<span class="sc">\{</span>1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma}\right) - \frac{1}{2}\right</span><span class="co">]</span>\right<span class="sc">\}</span> \neq 0 .$$</span>
<span id="cb25-530"><a href="#cb25-530" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-531"><a href="#cb25-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-532"><a href="#cb25-532" aria-hidden="true" tabindex="-1"></a>The incompatibility of asymptotic unbiasedness and consistency is due to the behavoir of $\var{X_n}$ as $n\to\infty$.</span>
<span id="cb25-533"><a href="#cb25-533" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb25-534"><a href="#cb25-534" aria-hidden="true" tabindex="-1"></a>::: {#prp-consbias}</span>
<span id="cb25-535"><a href="#cb25-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-536"><a href="#cb25-536" aria-hidden="true" tabindex="-1"></a><span class="fu">## Relating Consistency and Asymptotic Unbiasedness</span></span>
<span id="cb25-537"><a href="#cb25-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-538"><a href="#cb25-538" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta$ is an estimator for $\theta$.</span>
<span id="cb25-539"><a href="#cb25-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-540"><a href="#cb25-540" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If $\hat\theta$ is consistent and there exists some $M$ such that $\var{\hat\theta} \le M$ for all $n$ (bounded variance), then $\hat\theta$ is asymptotically unbiased.</span>
<span id="cb25-541"><a href="#cb25-541" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If $\hat\theta$ is asymptotically unbiased and $\lim_{n\to\infty}\var{\hat\theta} = 0$ (vanishing variance), then $\hat\theta$ is consistent</span>
<span id="cb25-542"><a href="#cb25-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-543"><a href="#cb25-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-544"><a href="#cb25-544" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb25-545"><a href="#cb25-545" aria-hidden="true" tabindex="-1"></a>test</span>
<span id="cb25-546"><a href="#cb25-546" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-547"><a href="#cb25-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-550"><a href="#cb25-550" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-551"><a href="#cb25-551" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-552"><a href="#cb25-552" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot27</span></span>
<span id="cb25-553"><a href="#cb25-553" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-554"><a href="#cb25-554" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-555"><a href="#cb25-555" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Relationship between various concepts of convergence in the context of estimators"</span></span>
<span id="cb25-556"><a href="#cb25-556" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb25-557"><a href="#cb25-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-558"><a href="#cb25-558" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/relating_convergence.png"</span>)</span>
<span id="cb25-559"><a href="#cb25-559" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-560"><a href="#cb25-560" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb25-561"><a href="#cb25-561" aria-hidden="true" tabindex="-1"></a><span class="fu">## Law of Large Numbers</span></span>
<span id="cb25-562"><a href="#cb25-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-563"><a href="#cb25-563" aria-hidden="true" tabindex="-1"></a>In most examples until now, the properties of estimators were implicitly a function of the underlying model the data is generated from. We established that $\bar X$ and $S^2$ are consistent estimators for $\mu$ and $\sigma^2$, respectively, *when* $X_i \iid N(\mu,\sigma^2)$. We know the distribution of $\bar X$, *when* $X_i \iid N(\mu,\sigma^2)$. In an ideal world, we would be able to establish desirable properties of estimators under more robust settings where our specified model may include a wide array of distributions. Our first step in doing this will be introducing one of the most important results in all of probability -- the law of large numbers. Proving this requires an inequality similar to @lem-markovineq.</span>
<span id="cb25-564"><a href="#cb25-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-565"><a href="#cb25-565" aria-hidden="true" tabindex="-1"></a>:::{#lem-}</span>
<span id="cb25-566"><a href="#cb25-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-567"><a href="#cb25-567" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chebyshev's Inequality</span></span>
<span id="cb25-568"><a href="#cb25-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-569"><a href="#cb25-569" aria-hidden="true" tabindex="-1"></a>If $X$ is a random variable with an expected value $\mu$ and variance $\sigma^2$, then for all $a &gt; 0$</span>
<span id="cb25-570"><a href="#cb25-570" aria-hidden="true" tabindex="-1"></a>$$\Pr(|X - \mu| \ge k) \le \frac{\sigma^2}{k^2}.$$</span>
<span id="cb25-571"><a href="#cb25-571" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-572"><a href="#cb25-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-573"><a href="#cb25-573" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-574"><a href="#cb25-574" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-575"><a href="#cb25-575" aria-hidden="true" tabindex="-1"></a>\Pr(|X - \mu| \ge k) &amp;= \Pr((X - \mu)^2 \ge k^2)<span class="sc">\\</span></span>
<span id="cb25-576"><a href="#cb25-576" aria-hidden="true" tabindex="-1"></a>&amp; \le \frac{\E{(X-\mu)^2}}{k^2} &amp; (\text{Markov's Inequality})<span class="sc">\\</span></span>
<span id="cb25-577"><a href="#cb25-577" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\sigma^2}{k^2}</span>
<span id="cb25-578"><a href="#cb25-578" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-579"><a href="#cb25-579" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-580"><a href="#cb25-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-581"><a href="#cb25-581" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-582"><a href="#cb25-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-583"><a href="#cb25-583" aria-hidden="true" tabindex="-1"></a><span class="fu">## (Khinchine's Weak) Law of Large Numbers (LLN)</span></span>
<span id="cb25-584"><a href="#cb25-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-585"><a href="#cb25-585" aria-hidden="true" tabindex="-1"></a>If $(X_1,\ldots, X_n)$ are a set of iid random variables where $\E{X_i} = \mu$, then $\bar X \pto \mu$. </span>
<span id="cb25-586"><a href="#cb25-586" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-587"><a href="#cb25-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-588"><a href="#cb25-588" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-589"><a href="#cb25-589" aria-hidden="true" tabindex="-1"></a>Recall that $\var{\bar X} = \sigma^2/n$. By Chebyshev's Inequality,</span>
<span id="cb25-590"><a href="#cb25-590" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-591"><a href="#cb25-591" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\Pr(|X_n - \mu| \ge \varepsilon) \le \lim_{n\to\infty}\frac{(\sigma^2/n)}{\varepsilon^2} =   \lim_{n\to\infty} \frac{\sigma^2}{n\varepsilon} = 0.</span>
<span id="cb25-592"><a href="#cb25-592" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-593"><a href="#cb25-593" aria-hidden="true" tabindex="-1"></a>Therefore, $\bar X\pto \mu$.</span>
<span id="cb25-594"><a href="#cb25-594" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-595"><a href="#cb25-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-596"><a href="#cb25-596" aria-hidden="true" tabindex="-1"></a>Another way of thinking about the LLN is that we can approximate $\mu$ with $\bar X$, and this approximation gets better as $n\to\infty$.</span>
<span id="cb25-597"><a href="#cb25-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-598"><a href="#cb25-598" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-599"><a href="#cb25-599" aria-hidden="true" tabindex="-1"></a>To illustrate the LLN, let's simulate realizations of iid random variables from a series of different distributions and show that regardless of the distribution, $\bar X \to \mu$. We will use the following distributions:</span>
<span id="cb25-600"><a href="#cb25-600" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-601"><a href="#cb25-601" aria-hidden="true" tabindex="-1"></a>X_i &amp; \iid \text{Exp}(1/\mu)<span class="sc">\\</span></span>
<span id="cb25-602"><a href="#cb25-602" aria-hidden="true" tabindex="-1"></a>X_i &amp; \iid \chi_\mu^2<span class="sc">\\</span></span>
<span id="cb25-603"><a href="#cb25-603" aria-hidden="true" tabindex="-1"></a>X_i &amp; \iid \text{Uni}(0, 2\mu)<span class="sc">\\</span></span>
<span id="cb25-604"><a href="#cb25-604" aria-hidden="true" tabindex="-1"></a>X_i &amp; \iid \text{Gamma}(2\mu, 2)<span class="sc">\\</span></span>
<span id="cb25-605"><a href="#cb25-605" aria-hidden="true" tabindex="-1"></a>X_i &amp; \iid \text{HyperGeo}(10, 20, 15\mu)</span>
<span id="cb25-606"><a href="#cb25-606" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-607"><a href="#cb25-607" aria-hidden="true" tabindex="-1"></a>All these distributions have been selected such that $\E{X_i} = \mu$. For our simulations, we will take $\mu = 5$. If we plot the value of the sample mean versus the sample size $n$, we see that the values converge to the true value $\mu = 5$ regardless of the distribution of $X_i$.</span>
<span id="cb25-608"><a href="#cb25-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-611"><a href="#cb25-611" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-612"><a href="#cb25-612" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-613"><a href="#cb25-613" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot28</span></span>
<span id="cb25-614"><a href="#cb25-614" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-615"><a href="#cb25-615" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-616"><a href="#cb25-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-617"><a href="#cb25-617" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample mean of all samples converges to the population mean by the LLN"</span></span>
<span id="cb25-618"><a href="#cb25-618" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-619"><a href="#cb25-619" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb25-620"><a href="#cb25-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-621"><a href="#cb25-621" aria-hidden="true" tabindex="-1"></a>max_n <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb25-622"><a href="#cb25-622" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">5</span>, <span class="at">nrow =</span> max_n<span class="sc">/</span><span class="dv">10</span>)</span>
<span id="cb25-623"><a href="#cb25-623" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Exponential"</span>, <span class="st">"Chi-Squared"</span>, <span class="st">"Uniform"</span>, <span class="st">"Gamma"</span>, <span class="st">"Hypergeometric"</span>)</span>
<span id="cb25-624"><a href="#cb25-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-625"><a href="#cb25-625" aria-hidden="true" tabindex="-1"></a>exp_sample <span class="ot">&lt;-</span> <span class="fu">rexp</span>(max_n, <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb25-626"><a href="#cb25-626" aria-hidden="true" tabindex="-1"></a>chisq_sample <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(max_n, <span class="dv">5</span>)</span>
<span id="cb25-627"><a href="#cb25-627" aria-hidden="true" tabindex="-1"></a>unif_sample <span class="ot">&lt;-</span> <span class="fu">runif</span>(max_n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb25-628"><a href="#cb25-628" aria-hidden="true" tabindex="-1"></a>gamma_sample <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(max_n, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb25-629"><a href="#cb25-629" aria-hidden="true" tabindex="-1"></a>hyper_sample <span class="ot">&lt;-</span> <span class="fu">rhyper</span>(max_n, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">15</span>)</span>
<span id="cb25-630"><a href="#cb25-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-631"><a href="#cb25-631" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(max_n<span class="sc">/</span><span class="dv">10</span>)) {</span>
<span id="cb25-632"><a href="#cb25-632" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(exp_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb25-633"><a href="#cb25-633" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(chisq_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb25-634"><a href="#cb25-634" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(unif_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb25-635"><a href="#cb25-635" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(gamma_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb25-636"><a href="#cb25-636" aria-hidden="true" tabindex="-1"></a>  store_estimates[n,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(hyper_sample[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">*</span><span class="dv">10</span>)])</span>
<span id="cb25-637"><a href="#cb25-637" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-638"><a href="#cb25-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-639"><a href="#cb25-639" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb25-640"><a href="#cb25-640" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-641"><a href="#cb25-641" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-642"><a href="#cb25-642" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(key) <span class="sc">%&gt;%</span> </span>
<span id="cb25-643"><a href="#cb25-643" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb25-644"><a href="#cb25-644" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, value, <span class="at">color =</span> key)) <span class="sc">+</span></span>
<span id="cb25-645"><a href="#cb25-645" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb25-646"><a href="#cb25-646" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-647"><a href="#cb25-647" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>, <span class="at">color =</span> <span class="st">"Distribution of iid Random Sample"</span>) <span class="sc">+</span></span>
<span id="cb25-648"><a href="#cb25-648" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb25-649"><a href="#cb25-649" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb25-650"><a href="#cb25-650" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">4.5</span>, <span class="fl">5.5</span>)</span>
<span id="cb25-651"><a href="#cb25-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-652"><a href="#cb25-652" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-653"><a href="#cb25-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-654"><a href="#cb25-654" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-655"><a href="#cb25-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-656"><a href="#cb25-656" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-657"><a href="#cb25-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-658"><a href="#cb25-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## Monte Carlo Simulations</span></span>
<span id="cb25-659"><a href="#cb25-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-660"><a href="#cb25-660" aria-hidden="true" tabindex="-1"></a>In @exm-var we performed a Monte Carlo simulation to illustrate the bias of $\hat\theta(\X) = \sum_{i=1}^n (X_i - \bar X)/n$ and unbiasedness of $S^2$. We did this by fixing $n=20$, drawing a random sample, recording estimates $\hat\theta(\x)$ and $S^2(\x)$, and repeating this $k$ times. This is nothing more than drawing $j=1,\ldots,k$ observations from the random variables $\hat\theta(\X)$ and $S^2(\X)$. By the LLN, </span>
<span id="cb25-661"><a href="#cb25-661" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-662"><a href="#cb25-662" aria-hidden="true" tabindex="-1"></a>\frac{1}{k}\sum_{i=1}^k \hat\theta_j(\x) &amp;\pto \E{\hat\theta(\X)},<span class="sc">\\</span></span>
<span id="cb25-663"><a href="#cb25-663" aria-hidden="true" tabindex="-1"></a>\frac{1}{k}\sum_{i=1}^k S_j^2(\x) &amp;\pto \E{S^2(\X)},</span>
<span id="cb25-664"><a href="#cb25-664" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-665"><a href="#cb25-665" aria-hidden="true" tabindex="-1"></a>so for a large enough $k$, we can approximate the expected value with its sample counterpart. </span>
<span id="cb25-666"><a href="#cb25-666" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-667"><a href="#cb25-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-668"><a href="#cb25-668" aria-hidden="true" tabindex="-1"></a>The crucial assumption made by the LLN is that $\bar X$ is calculated with an iid random sample. If we drop this assumption, then $\bar X$ needn't estimate $\mu$ consistently.</span>
<span id="cb25-669"><a href="#cb25-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-670"><a href="#cb25-670" aria-hidden="true" tabindex="-1"></a>:::{#exm-noiid}</span>
<span id="cb25-671"><a href="#cb25-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-672"><a href="#cb25-672" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN Failing with Non-IID Data</span></span>
<span id="cb25-673"><a href="#cb25-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-674"><a href="#cb25-674" aria-hidden="true" tabindex="-1"></a>Suppose $\X = (X_1, \ldots, X_n)$ where $X_i \sim N(-1, i)$ if $i$ is odd, and $X_i \sim N(1,i)$ is $i$ is even. The data is independent, but not identically distributed. If some LLN would hold here, we would suspect that $\bar X$ would converge $0$, the average of the underlying population means $1$ and $-1$.Let's simulate $\bar X$ for $n$ ranging from 1 to 100,000.</span>
<span id="cb25-675"><a href="#cb25-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-678"><a href="#cb25-678" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-679"><a href="#cb25-679" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="fl">1e5</span>)</span>
<span id="cb25-680"><a href="#cb25-680" aria-hidden="true" tabindex="-1"></a>X_bar <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="fl">1e5</span>)</span>
<span id="cb25-681"><a href="#cb25-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-682"><a href="#cb25-682" aria-hidden="true" tabindex="-1"></a><span class="co">#draw realizations for each X_i s</span></span>
<span id="cb25-683"><a href="#cb25-683" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(X)) {</span>
<span id="cb25-684"><a href="#cb25-684" aria-hidden="true" tabindex="-1"></a>  <span class="co">#set mu modular arithmetic</span></span>
<span id="cb25-685"><a href="#cb25-685" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb25-686"><a href="#cb25-686" aria-hidden="true" tabindex="-1"></a>  X[i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu, i)</span>
<span id="cb25-687"><a href="#cb25-687" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-688"><a href="#cb25-688" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate sample mean </span></span>
<span id="cb25-689"><a href="#cb25-689" aria-hidden="true" tabindex="-1"></a>  X_bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[<span class="dv">1</span><span class="sc">:</span>i])</span>
<span id="cb25-690"><a href="#cb25-690" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-691"><a href="#cb25-691" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-692"><a href="#cb25-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-693"><a href="#cb25-693" aria-hidden="true" tabindex="-1"></a>We see that our estimates very much do not converge to any particular value.</span>
<span id="cb25-694"><a href="#cb25-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-697"><a href="#cb25-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-698"><a href="#cb25-698" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-699"><a href="#cb25-699" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot29</span></span>
<span id="cb25-700"><a href="#cb25-700" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-701"><a href="#cb25-701" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-702"><a href="#cb25-702" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-703"><a href="#cb25-703" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample mean of non-IID data does not satisfy the LLN"</span></span>
<span id="cb25-704"><a href="#cb25-704" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-705"><a href="#cb25-705" aria-hidden="true" tabindex="-1"></a>X_bar <span class="sc">%&gt;%</span> </span>
<span id="cb25-706"><a href="#cb25-706" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-707"><a href="#cb25-707" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb25-708"><a href="#cb25-708" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, X_bar)) <span class="sc">+</span></span>
<span id="cb25-709"><a href="#cb25-709" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb25-710"><a href="#cb25-710" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-711"><a href="#cb25-711" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span>
<span id="cb25-712"><a href="#cb25-712" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-713"><a href="#cb25-713" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-714"><a href="#cb25-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-715"><a href="#cb25-715" aria-hidden="true" tabindex="-1"></a>The proof of the LLN relied on Chebyshev's equality and the fact that $\var{\bar X} = \sigma^2/n \to 0$ when $\var{X_i} = \sigma^2$. Perhaps if we added an assumption regarding the variance of a non-iid sample, then we could salvage a result similar to the LLN. This is precisely what Chebyshev's LLN does. </span>
<span id="cb25-716"><a href="#cb25-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-717"><a href="#cb25-717" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-718"><a href="#cb25-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-719"><a href="#cb25-719" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chebyshev's (Weak) Law of Large Numbers </span></span>
<span id="cb25-720"><a href="#cb25-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-721"><a href="#cb25-721" aria-hidden="true" tabindex="-1"></a>Suppose $(X_1,\ldots, X_n)$ are a sample such that $\E{X_i} = \mu_i$, $\cov{X_i, X_j} = \sigma_{ij}^2$, and  $\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_{ij}^2 =0$. If $\frac{1}{n}\sum_{i=1}^n\mu_i \pto \mu$, then $\bar X \pto \mu$.</span>
<span id="cb25-722"><a href="#cb25-722" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-723"><a href="#cb25-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-724"><a href="#cb25-724" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-725"><a href="#cb25-725" aria-hidden="true" tabindex="-1"></a>The expected value of $\bar X$ is </span>
<span id="cb25-726"><a href="#cb25-726" aria-hidden="true" tabindex="-1"></a>$$\E{\bar X} = \frac{1}{n}\sum_{i=1}^n\E{X_i}= \frac{1}{n}\sum_{i=1}^n\mu_i \pto \mu.$$</span>
<span id="cb25-727"><a href="#cb25-727" aria-hidden="true" tabindex="-1"></a>The variance is </span>
<span id="cb25-728"><a href="#cb25-728" aria-hidden="true" tabindex="-1"></a>$$ \var{\bar X} = \var{\frac{1}{n}\sum_{i=1}^nX_i} = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\cov{X_i,X_j} = \frac{1}{n}\sum_{i=1}^n \sigma_{ij}^2\to 0.$$ By Proposition @prp-mse3, $\bar X\ms \mu$, so $\bar X \pto \mu$.</span>
<span id="cb25-729"><a href="#cb25-729" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-730"><a href="#cb25-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-731"><a href="#cb25-731" aria-hidden="true" tabindex="-1"></a>:::{.corrolary}</span>
<span id="cb25-732"><a href="#cb25-732" aria-hidden="true" tabindex="-1"></a>Suppose $(X_1,\ldots, X_n)$ are an independent sample such that $\E{X_i} = \mu_i$, $\var{X_i} = \sigma_{i}^2$, and  $\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 =0$. If $\frac{1}{n}\sum_{i=1}^n\mu_i \pto \mu$, then $\bar X \pto \mu$.</span>
<span id="cb25-733"><a href="#cb25-733" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-734"><a href="#cb25-734" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-735"><a href="#cb25-735" aria-hidden="true" tabindex="-1"></a>If the sample is independent, then $$\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\cov{X_i,X_j} =\frac{1}{n^2}\sum_{i=1}^n\var{X_i}. $$</span>
<span id="cb25-736"><a href="#cb25-736" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-737"><a href="#cb25-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-738"><a href="#cb25-738" aria-hidden="true" tabindex="-1"></a>The reason our non-iid sample in Example @exm-noiid did not converge was because</span>
<span id="cb25-739"><a href="#cb25-739" aria-hidden="true" tabindex="-1"></a>$$ \frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac{n(n+1)}{2} = \frac{n^2 + n}{2n^2} \to \frac{1}{2} \neq 0.$$ Let's modify it slightly so the sum of the variances does converge to zero.</span>
<span id="cb25-740"><a href="#cb25-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-741"><a href="#cb25-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-742"><a href="#cb25-742" aria-hidden="true" tabindex="-1"></a>:::{#exm-noiid}</span>
<span id="cb25-743"><a href="#cb25-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-744"><a href="#cb25-744" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN with Non-IID Data</span></span>
<span id="cb25-745"><a href="#cb25-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-746"><a href="#cb25-746" aria-hidden="true" tabindex="-1"></a>Suppose $\X = (X_1, \ldots, X_n)$ where $X_i \sim N(-1,i^{-1})$ if $i$ is odd, and $X_i \sim N(1,i^{-1})$ is $i$ is even. Now we have </span>
<span id="cb25-747"><a href="#cb25-747" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \left<span class="co">[</span><span class="ot">\lim_{n\to\infty}\frac{1}{n^2}\right</span><span class="co">]</span>\sum_{i=1}^\infty i^{-1} \to 0 .$$</span>
<span id="cb25-750"><a href="#cb25-750" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-751"><a href="#cb25-751" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">200</span>)</span>
<span id="cb25-752"><a href="#cb25-752" aria-hidden="true" tabindex="-1"></a>X_bar <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">200</span>)</span>
<span id="cb25-753"><a href="#cb25-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-754"><a href="#cb25-754" aria-hidden="true" tabindex="-1"></a><span class="co">#draw realizations for each X_i s</span></span>
<span id="cb25-755"><a href="#cb25-755" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(X)) {</span>
<span id="cb25-756"><a href="#cb25-756" aria-hidden="true" tabindex="-1"></a>  <span class="co">#set mu modular arithmetic</span></span>
<span id="cb25-757"><a href="#cb25-757" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb25-758"><a href="#cb25-758" aria-hidden="true" tabindex="-1"></a>  X[i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu, i<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb25-759"><a href="#cb25-759" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-760"><a href="#cb25-760" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate sample mean </span></span>
<span id="cb25-761"><a href="#cb25-761" aria-hidden="true" tabindex="-1"></a>  X_bar[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[<span class="dv">1</span><span class="sc">:</span>i])</span>
<span id="cb25-762"><a href="#cb25-762" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-763"><a href="#cb25-763" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-764"><a href="#cb25-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-765"><a href="#cb25-765" aria-hidden="true" tabindex="-1"></a>Now we see that $\bar X$ is converging to $0$, and doing so rather quickly. </span>
<span id="cb25-766"><a href="#cb25-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-769"><a href="#cb25-769" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-770"><a href="#cb25-770" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-771"><a href="#cb25-771" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot210</span></span>
<span id="cb25-772"><a href="#cb25-772" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-773"><a href="#cb25-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-774"><a href="#cb25-774" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-775"><a href="#cb25-775" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way."</span></span>
<span id="cb25-776"><a href="#cb25-776" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-777"><a href="#cb25-777" aria-hidden="true" tabindex="-1"></a>X_bar <span class="sc">%&gt;%</span> </span>
<span id="cb25-778"><a href="#cb25-778" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-779"><a href="#cb25-779" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">row_number</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb25-780"><a href="#cb25-780" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, value)) <span class="sc">+</span></span>
<span id="cb25-781"><a href="#cb25-781" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb25-782"><a href="#cb25-782" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-783"><a href="#cb25-783" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span>
<span id="cb25-784"><a href="#cb25-784" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-785"><a href="#cb25-785" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-786"><a href="#cb25-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-787"><a href="#cb25-787" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Continuous Mapping Theorem and Slutsky's Theorem</span></span>
<span id="cb25-788"><a href="#cb25-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-789"><a href="#cb25-789" aria-hidden="true" tabindex="-1"></a>At first glance, the LLN may not seem especially useful as it only applies to the sample mean. However, when paired with two key results about convergence, the LLN becomes an indispensable tool to show the convergence of many random variables and estimators. The first of these is an extension of a key result in real analysis. A useful, and defining property, of continuous functions is that they preserve limits of numeric sequences. If $<span class="sc">\{</span>a_n<span class="sc">\}</span>$ is a numeric sequence, then </span>
<span id="cb25-790"><a href="#cb25-790" aria-hidden="true" tabindex="-1"></a>$$\lim_{n \to\infty} f(a_n) = f\left(\lim_{n\to\infty} a_n\right) \iff f\text{ continuous}.$$</span>
<span id="cb25-791"><a href="#cb25-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-792"><a href="#cb25-792" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-793"><a href="#cb25-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-794"><a href="#cb25-794" aria-hidden="true" tabindex="-1"></a><span class="fu">## Continuous Mapping Theorem I</span></span>
<span id="cb25-795"><a href="#cb25-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-796"><a href="#cb25-796" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \pto X$, and let $g$ be a continuous function. Then </span>
<span id="cb25-797"><a href="#cb25-797" aria-hidden="true" tabindex="-1"></a>$$g(X_n)\pto g(X).$$ In other words we are able to interchange the $\plim$ operator with a continuous function:</span>
<span id="cb25-798"><a href="#cb25-798" aria-hidden="true" tabindex="-1"></a>$$\plim g(X_n) = g\left(\plim X_n\right).$$</span>
<span id="cb25-799"><a href="#cb25-799" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-800"><a href="#cb25-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-801"><a href="#cb25-801" aria-hidden="true" tabindex="-1"></a>The proof of this result can be found in @van2000asymptotic. An immediate corollary follows from the fact that convergence in probability implies convergence in distribution.</span>
<span id="cb25-802"><a href="#cb25-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-803"><a href="#cb25-803" aria-hidden="true" tabindex="-1"></a>:::{#cor-}</span>
<span id="cb25-804"><a href="#cb25-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-805"><a href="#cb25-805" aria-hidden="true" tabindex="-1"></a><span class="fu">## Continuous Mapping Theorem II</span></span>
<span id="cb25-806"><a href="#cb25-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-807"><a href="#cb25-807" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \pto X$, and let $g$ be a continuous function. Then </span>
<span id="cb25-808"><a href="#cb25-808" aria-hidden="true" tabindex="-1"></a>$$g(X_n)\dto g(X)$$</span>
<span id="cb25-809"><a href="#cb25-809" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-810"><a href="#cb25-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-811"><a href="#cb25-811" aria-hidden="true" tabindex="-1"></a>An equally useful result involves the limit of a sums and products of convergent random variables. </span>
<span id="cb25-812"><a href="#cb25-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-813"><a href="#cb25-813" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-814"><a href="#cb25-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-815"><a href="#cb25-815" aria-hidden="true" tabindex="-1"></a><span class="fu">## Slusky's Theorem</span></span>
<span id="cb25-816"><a href="#cb25-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-817"><a href="#cb25-817" aria-hidden="true" tabindex="-1"></a>Let $X_n$ and $Y_n$ be sequences of random variables. If $X_n\dto X$ for some random variable $X$, and $Y_n\pto c$ for some constant $c$, then </span>
<span id="cb25-818"><a href="#cb25-818" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-819"><a href="#cb25-819" aria-hidden="true" tabindex="-1"></a>X_n + Y_n &amp;\dto X + c<span class="sc">\\</span></span>
<span id="cb25-820"><a href="#cb25-820" aria-hidden="true" tabindex="-1"></a>X_nY_n &amp; \dto Xc.</span>
<span id="cb25-821"><a href="#cb25-821" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-822"><a href="#cb25-822" aria-hidden="true" tabindex="-1"></a>Furthermore, if $c\neq 0$, </span>
<span id="cb25-823"><a href="#cb25-823" aria-hidden="true" tabindex="-1"></a>$$ X_n/Y_n \dto X/c.$$</span>
<span id="cb25-824"><a href="#cb25-824" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-825"><a href="#cb25-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-826"><a href="#cb25-826" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb25-827"><a href="#cb25-827" aria-hidden="true" tabindex="-1"></a>Define a random vector to be $\mathbf Z_n = (X_n,Y_n)$. We have $\mathbf Z_n \dto (X,c)$ as $X_n\dto X$ and $Y_n \dto c$ (convergence in probability implies convergence in distribution). We can apply the continuous mapping theorem to $g(x,y) = x + y$, $g(x,y)=xy$, and $g(x/y)$ to establish the result.  </span>
<span id="cb25-828"><a href="#cb25-828" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-829"><a href="#cb25-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-830"><a href="#cb25-830" aria-hidden="true" tabindex="-1"></a>Slutsky's theorem can be a bit hard to remember because it involves a sequence of random variable which converges in distribution to a random variable, and a sequence of random variables which converges in probability to a constant. These asymmetries in mode of convergence and the type of limit are essential, otherwise the result will not hold. Fortunately, the result does hold if we replace all convergences in distribution with convergence in probability (as the later implies the prior).</span>
<span id="cb25-831"><a href="#cb25-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-832"><a href="#cb25-832" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-833"><a href="#cb25-833" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \sim\text{Uni}(0,1)$ and $Y_n = - X_n$. We have $X_n \dto \text{Uni}(0,1)$ and $Y_n \dto \text{Uni}(-1,0)$. Despite this $X_n + Y_n = 0 \not\dto \text{Uni}(0,1) + \text{Uni}(-1,0).$ </span>
<span id="cb25-834"><a href="#cb25-834" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-835"><a href="#cb25-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-836"><a href="#cb25-836" aria-hidden="true" tabindex="-1"></a>:::{#exm-convar}</span>
<span id="cb25-837"><a href="#cb25-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-838"><a href="#cb25-838" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency of Sample Variance</span></span>
<span id="cb25-839"><a href="#cb25-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-840"><a href="#cb25-840" aria-hidden="true" tabindex="-1"></a>@exm-consvarnorm showed that $S^2$ is a consistent estimator for $\sigma^2$ when $X_i\iid N(\mu,\sigma^2)$. We can use the continuous mapping theorem, Slutsky's theorem, and the LLN to show that $S^2$ is consistent regardless of the distribution of our iid sample. Suppose $\E{X_i} = \mu$, $\E{X_i^2}=\mu_2$, and $\E{X_i^4}=\mu_4$ for all $i$. If we define our continuous function to be $g(x) = x^2$, then</span>
<span id="cb25-841"><a href="#cb25-841" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-842"><a href="#cb25-842" aria-hidden="true" tabindex="-1"></a>S^2 &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i - \frac{1}{n-1} \sum_{i=1}^n\bar X^2 <span class="sc">\\</span></span>
<span id="cb25-843"><a href="#cb25-843" aria-hidden="true" tabindex="-1"></a>    &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i^2 + \frac{n}{n-1}\bar X^2  <span class="sc">\\</span></span>
<span id="cb25-844"><a href="#cb25-844" aria-hidden="true" tabindex="-1"></a>    &amp; = \frac{n}{n-1}\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^{n}X_i^2 - \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2\right</span><span class="co">]</span></span>
<span id="cb25-845"><a href="#cb25-845" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-846"><a href="#cb25-846" aria-hidden="true" tabindex="-1"></a>The first term in the brackets is an unbiased estimator of $\mu_2$ with vanishing variance, so by @prp-consbias it is a consistent estimator for $\E{X^2}$:</span>
<span id="cb25-847"><a href="#cb25-847" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-848"><a href="#cb25-848" aria-hidden="true" tabindex="-1"></a>\E{\frac{1}{n}\sum_{i=1}^{n}X_i^2} &amp;= \frac{1}{n}\left(n \mu_2\right) = \mu^2<span class="sc">\\</span></span>
<span id="cb25-849"><a href="#cb25-849" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\var{\frac{1}{n}\sum_{i=1}^{n}X_i^2} &amp; = \lim_{n\to\infty}\frac{1}{n^2}n\left(\E{X_i^4} - \E{X_i^2}^2 \right) = \lim_{n\to\infty}\frac{\mu_4 + \mu_2^2}{n} = 0</span>
<span id="cb25-850"><a href="#cb25-850" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-851"><a href="#cb25-851" aria-hidden="true" tabindex="-1"></a>The second term in the brackets can be written as $g(\bar X)$, so by the continuous mapping theorem and the LLN, </span>
<span id="cb25-852"><a href="#cb25-852" aria-hidden="true" tabindex="-1"></a>$$ g(\bar X) = \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2 \pto \mu^2 = g(\mu).$$</span>
<span id="cb25-853"><a href="#cb25-853" aria-hidden="true" tabindex="-1"></a>So </span>
<span id="cb25-854"><a href="#cb25-854" aria-hidden="true" tabindex="-1"></a>$$S^2= \underbrace{\frac{n}{n-1}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^{n}X_i^2}_{\pto \mu_2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2}_{\pto \mu^2} \Bigg].$$</span>
<span id="cb25-855"><a href="#cb25-855" aria-hidden="true" tabindex="-1"></a>We can apply Slutsky's theorem to the sum of sequences of random variables which converge in probability to constants, so</span>
<span id="cb25-856"><a href="#cb25-856" aria-hidden="true" tabindex="-1"></a>$$ S^2 \pto \mu_2 - \mu^2 = \sigma^2,$$ making $S^2$ consistent.</span>
<span id="cb25-857"><a href="#cb25-857" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-858"><a href="#cb25-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-859"><a href="#cb25-859" aria-hidden="true" tabindex="-1"></a><span class="fu">## Central Limit Theorem </span></span>
<span id="cb25-860"><a href="#cb25-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-861"><a href="#cb25-861" aria-hidden="true" tabindex="-1"></a>The LLN told us that our favorite estimator for $\mu$, $\bar X$, is consistent. We know turn to what is perhaps an even more important result regarding $\bar X$, one that may in fact be the most important results in all of probability. </span>
<span id="cb25-862"><a href="#cb25-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-863"><a href="#cb25-863" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-864"><a href="#cb25-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-865"><a href="#cb25-865" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lindeberg-Lévy Central Limit Theorem (CLT)</span></span>
<span id="cb25-866"><a href="#cb25-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-867"><a href="#cb25-867" aria-hidden="true" tabindex="-1"></a>Suppose $\X=(X_1,\ldots, X_n)$ is a sequence of random variables with $\E{X_i}=\mu$ and $\var{X_i}=\sigma^2$. Then $$\sqrt{n}(\bar X - \mu) \dto N(0,\sigma^2),$$ which is also often written as </span>
<span id="cb25-868"><a href="#cb25-868" aria-hidden="true" tabindex="-1"></a>$$\bar X\dto N(\mu, \sigma^2/n),$$ or </span>
<span id="cb25-869"><a href="#cb25-869" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^n X_i \dto N(n\mu, \sqrt n \sigma^2) $$</span>
<span id="cb25-870"><a href="#cb25-870" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-871"><a href="#cb25-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-872"><a href="#cb25-872" aria-hidden="true" tabindex="-1"></a>The Lindeberg-Lévy central limit theorem is often referred to as *the* central limit theorem, and is the form which most are familiar with. The proof is a bit technical and requires some measure-theoretic based probability theory. It can be found in @billingsley2008probability or @durrett2019probability. </span>
<span id="cb25-873"><a href="#cb25-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-874"><a href="#cb25-874" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-875"><a href="#cb25-875" aria-hidden="true" tabindex="-1"></a>Suppose we have an iid sample from $\text{Exp}(1)$. If we simulate 1000 realizations of $\sqrt n(\bar X - \mu)$ for various sample sizes, we should see that the distribution of our realizations becomes approximately normal as we increase the sample size.</span>
<span id="cb25-876"><a href="#cb25-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-879"><a href="#cb25-879" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-880"><a href="#cb25-880" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">100</span>, <span class="dv">1000</span>)</span>
<span id="cb25-881"><a href="#cb25-881" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb25-882"><a href="#cb25-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-883"><a href="#cb25-883" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb25-884"><a href="#cb25-884" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"n ="</span>, sample_sizes)</span>
<span id="cb25-885"><a href="#cb25-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-886"><a href="#cb25-886" aria-hidden="true" tabindex="-1"></a>col_index <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb25-887"><a href="#cb25-887" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb25-888"><a href="#cb25-888" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> col_index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb25-889"><a href="#cb25-889" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb25-890"><a href="#cb25-890" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">rexp</span>(n, <span class="dv">1</span>)</span>
<span id="cb25-891"><a href="#cb25-891" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb25-892"><a href="#cb25-892" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-893"><a href="#cb25-893" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-894"><a href="#cb25-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-895"><a href="#cb25-895" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-896"><a href="#cb25-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-897"><a href="#cb25-897" aria-hidden="true" tabindex="-1"></a>Even for modest values of $n$, we can see that $\sqrt n(\bar X - \mu) \asim N(0, \sigma^2)$.</span>
<span id="cb25-898"><a href="#cb25-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-901"><a href="#cb25-901" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-902"><a href="#cb25-902" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-903"><a href="#cb25-903" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot211</span></span>
<span id="cb25-904"><a href="#cb25-904" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-905"><a href="#cb25-905" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-906"><a href="#cb25-906" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-907"><a href="#cb25-907" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal."</span></span>
<span id="cb25-908"><a href="#cb25-908" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-909"><a href="#cb25-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-910"><a href="#cb25-910" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb25-911"><a href="#cb25-911" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-912"><a href="#cb25-912" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() </span>
<span id="cb25-913"><a href="#cb25-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-914"><a href="#cb25-914" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>key <span class="ot">&lt;-</span> <span class="fu">factor</span>(df<span class="sc">$</span>key, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">'n = 1'</span>,<span class="st">'n = 5'</span>,<span class="st">'n = 10'</span>,<span class="st">'n = 25'</span>, <span class="st">'n = 100'</span>, <span class="st">'n = 1000'</span>))</span>
<span id="cb25-915"><a href="#cb25-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-916"><a href="#cb25-916" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb25-917"><a href="#cb25-917" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span></span>
<span id="cb25-918"><a href="#cb25-918" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb25-919"><a href="#cb25-919" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb25-920"><a href="#cb25-920" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-921"><a href="#cb25-921" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span>
<span id="cb25-922"><a href="#cb25-922" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-923"><a href="#cb25-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-924"><a href="#cb25-924" aria-hidden="true" tabindex="-1"></a>An alternate way to visually test whether our estimates are normally distributed is with a quantile-quantile plot (QQ-plot), which graphs the observed quantiles of our estimates against the theoretical quantiles of a normal distribution (or those of any distribution we suspect our data is drawn from). If our estimates are (approximately) normally distributed, then the observed quantiles should be approximately equal to the theoretical quantiles of a normal distribution, forming a 45-degree line.</span>
<span id="cb25-925"><a href="#cb25-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-928"><a href="#cb25-928" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-929"><a href="#cb25-929" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-930"><a href="#cb25-930" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot212</span></span>
<span id="cb25-931"><a href="#cb25-931" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-932"><a href="#cb25-932" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-933"><a href="#cb25-933" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-934"><a href="#cb25-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The QQ-plot for the simulated distribution of the adjusted sample mean"</span></span>
<span id="cb25-935"><a href="#cb25-935" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-936"><a href="#cb25-936" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb25-937"><a href="#cb25-937" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> value)) <span class="sc">+</span></span>
<span id="cb25-938"><a href="#cb25-938" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb25-939"><a href="#cb25-939" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb25-940"><a href="#cb25-940" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key) <span class="sc">+</span> </span>
<span id="cb25-941"><a href="#cb25-941" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-942"><a href="#cb25-942" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span>
<span id="cb25-943"><a href="#cb25-943" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-944"><a href="#cb25-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-945"><a href="#cb25-945" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-946"><a href="#cb25-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-947"><a href="#cb25-947" aria-hidden="true" tabindex="-1"></a>The central limit theorem is similar to the LLN insofar that they only concern the estimator $\bar X$, so how useful can they really be? Well with the continuous mapping theorem and Slutsky's theorem, the answer is *very useful*!</span>
<span id="cb25-948"><a href="#cb25-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-949"><a href="#cb25-949" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-950"><a href="#cb25-950" aria-hidden="true" tabindex="-1"></a>In Example @exm-tdist we illustrated the fact that $X_n \dto N(0,1)$ where $X_n \sim t_n$, but we didn't actually prove it. Directly proving this result is a matter of verify that </span>
<span id="cb25-951"><a href="#cb25-951" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} F_{X_n}(x) =\lim_{n\to\infty}\int_{-\infty}^x \frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)\sqrt{n\pi}}\left(1 + \frac{x^2}{n}\right)^{-\frac{n+1}{2}} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-x^2/2} = F_X(x)$$</span>
<span id="cb25-952"><a href="#cb25-952" aria-hidden="true" tabindex="-1"></a>where $\Gamma$ is the gamma function defined as </span>
<span id="cb25-953"><a href="#cb25-953" aria-hidden="true" tabindex="-1"></a>$$\Gamma(t) = \int_0^\infty s^{t-1}e^{-s}\ ds.$$ A much easier way to prove that $X_n \dto N(0,1)$, is by using Slutsky's theorem and the continuous mapping theorem. First recall that $$\frac{\bar X - \mu}{S/\sqrt n} \sim t_n,$$ so we can write $X_n$ as  $X_n = \frac{\bar X - \mu}{S/\sqrt n}$ due to the fact that random variables are uniquely determined by their distributions. From Example @exm-consvar, we know $S^2 \pto \sigma^2$. By the continuous mapping theorem $$ \sqrt{S^2} = S \pto \sigma = \sqrt{\sigma^2}.$$ So we have </span>
<span id="cb25-954"><a href="#cb25-954" aria-hidden="true" tabindex="-1"></a>$$ X_n = \frac{\bar X - \mu}{S/\sqrt n}= \underbrace{\sqrt{n}(\bar X - \mu)}_{\dto N(0, \sigma^2)}\underbrace{\frac{1}{s}}_{\pto \sigma}.$$ By Slutsky's theorem, </span>
<span id="cb25-955"><a href="#cb25-955" aria-hidden="true" tabindex="-1"></a>$$X_n \dto \frac{N(0,\sigma^2)}{\sigma} = N(0,1).$$ </span>
<span id="cb25-956"><a href="#cb25-956" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-957"><a href="#cb25-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-958"><a href="#cb25-958" aria-hidden="true" tabindex="-1"></a>The CLT can be generalized to samples of random vectors $\X_i$.</span>
<span id="cb25-959"><a href="#cb25-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-960"><a href="#cb25-960" aria-hidden="true" tabindex="-1"></a>:::{#thm- name="CLT in Higher Dimmensions"}</span>
<span id="cb25-961"><a href="#cb25-961" aria-hidden="true" tabindex="-1"></a>Suppose $(\X_1,\ldots, \X_n)$ is a sequence of iid random vectors with $\E{\X_i}=\boldsymbol\mu$ and $\var{\X_i}=\boldsymbol\Sigma$. Then $$\sqrt{n}(\bar {\X }- \boldsymbol\mu) \dto N(\mathbf{0},\boldsymbol\Sigma).$$</span>
<span id="cb25-962"><a href="#cb25-962" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-963"><a href="#cb25-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-964"><a href="#cb25-964" aria-hidden="true" tabindex="-1"></a>We can also generalize the CLT to the case where variables are independent but not necessarily identically distributed.</span>
<span id="cb25-965"><a href="#cb25-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-966"><a href="#cb25-966" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-967"><a href="#cb25-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-968"><a href="#cb25-968" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lindeberg-Feller CLT</span></span>
<span id="cb25-969"><a href="#cb25-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-970"><a href="#cb25-970" aria-hidden="true" tabindex="-1"></a>Suppose $\X=(X_1,\ldots, X_n)$ is a sequence of independent random variables with $\E{X_i}=\mu_i$ and $\var{X_i}=\sigma_i^2$, and define </span>
<span id="cb25-971"><a href="#cb25-971" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-972"><a href="#cb25-972" aria-hidden="true" tabindex="-1"></a>\bar \mu = \frac{1}{n}\sum_{i=1}^n\mu_i;<span class="sc">\\</span></span>
<span id="cb25-973"><a href="#cb25-973" aria-hidden="true" tabindex="-1"></a>\bar \sigma_n^2 = \frac{1}{n}\sum_{i=1}^n\sigma_i^2.</span>
<span id="cb25-974"><a href="#cb25-974" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-975"><a href="#cb25-975" aria-hidden="true" tabindex="-1"></a>If the collection of variances $\sigma_i^2$ satisfies:</span>
<span id="cb25-976"><a href="#cb25-976" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-977"><a href="#cb25-977" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} &amp;\frac{\max<span class="sc">\{</span>\sigma_i<span class="sc">\}</span>}{n\bar\sigma} = 0;<span class="sc">\\</span></span>
<span id="cb25-978"><a href="#cb25-978" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} &amp;\bar\sigma_n^2 = \bar\sigma^2,</span>
<span id="cb25-979"><a href="#cb25-979" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-980"><a href="#cb25-980" aria-hidden="true" tabindex="-1"></a>then $\sqrt n (\bar X-\mu)\dto N(0, \bar\sigma^2).$</span>
<span id="cb25-981"><a href="#cb25-981" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-982"><a href="#cb25-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-983"><a href="#cb25-983" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb25-984"><a href="#cb25-984" aria-hidden="true" tabindex="-1"></a>The Lindeberg-Feller conditions stipulates that $\lim_{n\to\infty}\bar\sigma_n^2 = \bar\sigma^2$ and $\lim_{n\to\infty} \frac{\max<span class="sc">\{</span>\sigma_i<span class="sc">\}</span>}{n\bar\sigma} = 0$, a condition known as the **_Lindeberg's condition_**. The condition is often presented in more general terms, but the intuition remains the same. For the CLT to hold for random variables that with different variances, we need to makes sure that no single term $\sigma_i$ dominates the standard deviation. We can think about the sample mean $\bar X$ as "mixing" many random variables $X_i.$ After mixing these random variables, we hope to have a normal distribution, but that only happens if tails of the various distributions of $X_i$ are negligible as $n\to\infty$, giving us the trademark tails of a normal distribution which tapper off. Let's consider a counterexample. Suppose $X_i$ is defined on the sample space $<span class="sc">\{</span>-i,0,i<span class="sc">\}</span>$ is distributed such that </span>
<span id="cb25-985"><a href="#cb25-985" aria-hidden="true" tabindex="-1"></a>$$\Pr(X_i = k) = \begin{cases} 1/2i^2 &amp; k = -i^2 <span class="sc">\\</span> 1/2i^2 &amp; k = i^2 <span class="sc">\\</span> 1 - 1/i^2&amp;k=0\end{cases}.$$</span>
<span id="cb25-986"><a href="#cb25-986" aria-hidden="true" tabindex="-1"></a>Graphing the density function for a few values of $i$ gives us a better sense of how $X_i$ behaves.</span>
<span id="cb25-987"><a href="#cb25-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-990"><a href="#cb25-990" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-991"><a href="#cb25-991" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-992"><a href="#cb25-992" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot213</span></span>
<span id="cb25-993"><a href="#cb25-993" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-994"><a href="#cb25-994" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-995"><a href="#cb25-995" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-996"><a href="#cb25-996" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Density of X_i for i = 1,...,5."</span></span>
<span id="cb25-997"><a href="#cb25-997" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-998"><a href="#cb25-998" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">x =</span> <span class="sc">-</span><span class="dv">100</span><span class="sc">:</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-999"><a href="#cb25-999" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">==</span> i<span class="sc">^</span><span class="dv">2</span> <span class="sc">|</span> x <span class="sc">==</span> <span class="sc">-</span>i<span class="sc">^</span><span class="dv">2</span><span class="sc">|</span> x<span class="sc">==</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1000"><a href="#cb25-1000" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(x <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(i<span class="sc">^</span><span class="dv">2</span>), <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>i<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1001"><a href="#cb25-1001" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb25-1002"><a href="#cb25-1002" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb25-1003"><a href="#cb25-1003" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>i, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb25-1004"><a href="#cb25-1004" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"k"</span>) <span class="sc">+</span></span>
<span id="cb25-1005"><a href="#cb25-1005" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> y)) <span class="sc">+</span></span>
<span id="cb25-1006"><a href="#cb25-1006" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb25-1007"><a href="#cb25-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1008"><a href="#cb25-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1009"><a href="#cb25-1009" aria-hidden="true" tabindex="-1"></a>As $i\to\infty$, nearly all the probability is concentrated as $k = 0$. The remaining probability is at the extreme tails of the distribution $\pm i^2$, and these tails become more and more extreme (quadritically so) as $i\to\infty$. This is the exact type of behavior Lindeberg's condition rules out. The expectation, expectation squared, and variance of $X_i$ are:</span>
<span id="cb25-1010"><a href="#cb25-1010" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1011"><a href="#cb25-1011" aria-hidden="true" tabindex="-1"></a>\E{X_i} &amp; = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0<span class="sc">\\</span></span>
<span id="cb25-1012"><a href="#cb25-1012" aria-hidden="true" tabindex="-1"></a>\E{X_i^2} &amp; = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2<span class="sc">\\</span></span>
<span id="cb25-1013"><a href="#cb25-1013" aria-hidden="true" tabindex="-1"></a>\var{X_i} &amp; = i^2 - 0^2 = i^2</span>
<span id="cb25-1014"><a href="#cb25-1014" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb25-1015"><a href="#cb25-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1016"><a href="#cb25-1016" aria-hidden="true" tabindex="-1"></a>We can verify that Lindeberg's condition does not hold.</span>
<span id="cb25-1017"><a href="#cb25-1017" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1018"><a href="#cb25-1018" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\bar \sigma_n = \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n i^2 = \lim_{n\to\infty}\frac{(n+1)(2n+1)}{6} \to\infty</span>
<span id="cb25-1019"><a href="#cb25-1019" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb25-1020"><a href="#cb25-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1021"><a href="#cb25-1021" aria-hidden="true" tabindex="-1"></a>To simulate realizations of this random variable, we can define $X_i$ using $U_i\sim\text{Uni}(0,1)$.^<span class="co">[</span><span class="ot">We can actually draw observations of *any* random variable using the uniform distribution on $[0,1]$ via [**_inverse transform sampling_**](https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions). This is one of the primary ways computers generate random observations from a given distribution.</span><span class="co">]</span> </span>
<span id="cb25-1022"><a href="#cb25-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1023"><a href="#cb25-1023" aria-hidden="true" tabindex="-1"></a>$$X_i = \begin{cases}-i^2 &amp; U_i\in<span class="co">[</span><span class="ot">0, 1/2i^2)\\ i^2 &amp; U_i\in[1/2i^2, 1/i^2) \\ 0 &amp; U_i\in [1/i^2,1</span><span class="co">]</span>\end{cases}$$</span>
<span id="cb25-1024"><a href="#cb25-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1027"><a href="#cb25-1027" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1028"><a href="#cb25-1028" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb25-1029"><a href="#cb25-1029" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb25-1030"><a href="#cb25-1030" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb25-1031"><a href="#cb25-1031" aria-hidden="true" tabindex="-1"></a>  prob_pos_i <span class="ot">&lt;-</span> (U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb25-1032"><a href="#cb25-1032" aria-hidden="true" tabindex="-1"></a>  prob_neg_i <span class="ot">&lt;-</span> (U <span class="sc">&gt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">&amp;</span> U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>((<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb25-1033"><a href="#cb25-1033" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_pos_i <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_neg_i</span>
<span id="cb25-1034"><a href="#cb25-1034" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-1035"><a href="#cb25-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1036"><a href="#cb25-1036" aria-hidden="true" tabindex="-1"></a>store_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> N_sim, <span class="at">ncol =</span> <span class="fu">length</span>(sample_sizes))</span>
<span id="cb25-1037"><a href="#cb25-1037" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(store_estimates) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"n ="</span>, sample_sizes)</span>
<span id="cb25-1038"><a href="#cb25-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1039"><a href="#cb25-1039" aria-hidden="true" tabindex="-1"></a>col_index <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb25-1040"><a href="#cb25-1040" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> sample_sizes) {</span>
<span id="cb25-1041"><a href="#cb25-1041" aria-hidden="true" tabindex="-1"></a>  col_index <span class="ot">&lt;-</span> col_index <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb25-1042"><a href="#cb25-1042" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb25-1043"><a href="#cb25-1043" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(n)</span>
<span id="cb25-1044"><a href="#cb25-1044" aria-hidden="true" tabindex="-1"></a>    store_estimates[k, col_index] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb25-1045"><a href="#cb25-1045" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-1046"><a href="#cb25-1046" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-1047"><a href="#cb25-1047" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1048"><a href="#cb25-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1051"><a href="#cb25-1051" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1052"><a href="#cb25-1052" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-1053"><a href="#cb25-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot214</span></span>
<span id="cb25-1054"><a href="#cb25-1054" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1055"><a href="#cb25-1055" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-1056"><a href="#cb25-1056" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-1057"><a href="#cb25-1057" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Histograms of estimates as sample size increases."</span></span>
<span id="cb25-1058"><a href="#cb25-1058" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-1059"><a href="#cb25-1059" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> store_estimates <span class="sc">%&gt;%</span> </span>
<span id="cb25-1060"><a href="#cb25-1060" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-1061"><a href="#cb25-1061" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>() </span>
<span id="cb25-1062"><a href="#cb25-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1063"><a href="#cb25-1063" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>key <span class="ot">&lt;-</span> <span class="fu">factor</span>(df<span class="sc">$</span>key, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">'n = 1'</span>,<span class="st">'n = 5'</span>,<span class="st">'n = 10'</span>,<span class="st">'n = 25'</span>, <span class="st">'n = 100'</span>, <span class="st">'n = 1000'</span>))</span>
<span id="cb25-1064"><a href="#cb25-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1065"><a href="#cb25-1065" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb25-1066"><a href="#cb25-1066" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value)) <span class="sc">+</span></span>
<span id="cb25-1067"><a href="#cb25-1067" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb25-1068"><a href="#cb25-1068" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb25-1069"><a href="#cb25-1069" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-1070"><a href="#cb25-1070" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span>
<span id="cb25-1071"><a href="#cb25-1071" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1072"><a href="#cb25-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1073"><a href="#cb25-1073" aria-hidden="true" tabindex="-1"></a>The apparent distribution of our estimates is not converging to a normal distribution, as we always have a few outliers that are too plentiful relative to their distance from the mean to be drawn from a normal distribution. As $n\to\infty$ these outliers become even more extreme. This is also evident from QQ-plots, where the points far away from the 45-degree line are drawn even farther away as $n\to\infty$.</span>
<span id="cb25-1074"><a href="#cb25-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1077"><a href="#cb25-1077" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1078"><a href="#cb25-1078" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-1079"><a href="#cb25-1079" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot215</span></span>
<span id="cb25-1080"><a href="#cb25-1080" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1081"><a href="#cb25-1081" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-1082"><a href="#cb25-1082" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-1083"><a href="#cb25-1083" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The QQ-plot for the simulated distribution of the adjusted sample mean"</span></span>
<span id="cb25-1084"><a href="#cb25-1084" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-1085"><a href="#cb25-1085" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb25-1086"><a href="#cb25-1086" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> value)) <span class="sc">+</span></span>
<span id="cb25-1087"><a href="#cb25-1087" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb25-1088"><a href="#cb25-1088" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb25-1089"><a href="#cb25-1089" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb25-1090"><a href="#cb25-1090" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-1091"><a href="#cb25-1091" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span>
<span id="cb25-1092"><a href="#cb25-1092" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1093"><a href="#cb25-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1094"><a href="#cb25-1094" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1095"><a href="#cb25-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1096"><a href="#cb25-1096" aria-hidden="true" tabindex="-1"></a>While theoretically important, Lindeberg's condition can be a bit hard to verify. It is much more common to appeal to a stronger assumption which gives rise to a second CLT that holds for independent, but not necessarily identically distributed, random variables. This final CLT may not be as general as the Lindeberg-Feller CLT, but it is much easier to work with.</span>
<span id="cb25-1097"><a href="#cb25-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1098"><a href="#cb25-1098" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-1099"><a href="#cb25-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1100"><a href="#cb25-1100" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lyapunov CLT</span></span>
<span id="cb25-1101"><a href="#cb25-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1102"><a href="#cb25-1102" aria-hidden="true" tabindex="-1"></a>Suppose $\X=(X_1,\ldots, X_n)$ is a sequence of independent random variables with $\E{X_i}=\mu_i$ and $\var{X_i}=\sigma_i^2$. If $\E{\abs{X_i - \mu_i}^{2+\kappa}}$ is finite for some $\kappa &gt; 0,$ then $\sqrt n (\bar X-\mu)\dto N(0, \bar\sigma^2)$, where $\bar\sigma = (1/n)\sum_{i=1}^n \sigma_i.$</span>
<span id="cb25-1103"><a href="#cb25-1103" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1104"><a href="#cb25-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1105"><a href="#cb25-1105" aria-hidden="true" tabindex="-1"></a><span class="fu">## Delta Method</span></span>
<span id="cb25-1106"><a href="#cb25-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1107"><a href="#cb25-1107" aria-hidden="true" tabindex="-1"></a>Slutsky's theorem and the continuous mapping theorem in tandem with the LLN give us the ability to prove that certain functions of sample means are convergent. Is it possible that we can do something similar with the CLT to find the asymptotic distribution of functions of sample means? </span>
<span id="cb25-1108"><a href="#cb25-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1109"><a href="#cb25-1109" aria-hidden="true" tabindex="-1"></a>Suppose $g$ is a function of $\bar X$, where a CLT applies to the random sample $\X$. We know $\sqrt n(\bar X-\mu)\asim N(0, \sigma^2)$. Is it possible to conclude that $\sqrt n(g(\bar X)-g(\mu))\asim N(0, \tilde\sigma^2)$ for some $\tilde\sigma^2$? Furthermore, can we determine $\tilde\mu$ and $\tilde\sigma^2$ only knowing $g$, $\mu$, and $\sigma^2$? </span>
<span id="cb25-1110"><a href="#cb25-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1111"><a href="#cb25-1111" aria-hidden="true" tabindex="-1"></a>We have information about the difference $\bar X-\mu$ and want information about the difference $g(\bar X)-g(\mu)$. Situations where we know something about behavior in the domain of a function and want to relate it to the function's behavior in the codomain are common place in math, but in particular in real analysis. This is where the mean value theorem saves the day. Assuming $g$ is continuously differentiable and fixing $n$, there exists some $T_n$ in between $\bar X$ and $\theta$ ($\bar X&lt; T_n &lt; \mu$ or $\mu &lt; T_n &lt; \bar X$) such that: </span>
<span id="cb25-1112"><a href="#cb25-1112" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1113"><a href="#cb25-1113" aria-hidden="true" tabindex="-1"></a>&amp; \frac{g(\bar X)-g(\mu)}{\bar X - \mu} = g'(T_n)<span class="sc">\\</span></span>
<span id="cb25-1114"><a href="#cb25-1114" aria-hidden="true" tabindex="-1"></a>\implies &amp; g(\bar X) = g(\mu) + g'(T_n)(\bar X-\mu)<span class="sc">\\</span></span>
<span id="cb25-1115"><a href="#cb25-1115" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> = g'(T_n)\sqrt{n} (\bar X-\mu)</span>
<span id="cb25-1116"><a href="#cb25-1116" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-1117"><a href="#cb25-1117" aria-hidden="true" tabindex="-1"></a>If we let $n$ vary, we have a sequence of random variables $<span class="sc">\{</span>T_n<span class="sc">\}</span>$ such that $\abs{T_n -\mu} &lt; \abs{\bar X - \mu}$. By the LLN $\abs{\bar X - \mu} \pto 0$, so $\abs{T_n -\mu} \pto 0$, which is equivalent to $T_n \pto \mu$. By the continuous mapping theorem, $g(T_n) \pto g(\mu)$. This means </span>
<span id="cb25-1118"><a href="#cb25-1118" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> = \underbrace{g'(T_n)}_{\pto g(\mu)}\cdot\underbrace{\sqrt{n} (\bar X-\mu)}_{\dto N(0, \sigma^2)},$$ so Slutsky's theorem gives </span>
<span id="cb25-1119"><a href="#cb25-1119" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> \dto g(\mu)\cdot N(0,\sigma^2) = N(0, \sigma^2<span class="co">[</span><span class="ot">g(\mu)</span><span class="co">]</span>^2).$$ This all is contingent on $g$ not being a function of $n$, otherwise things fall apart when we apply limiting processes.</span>
<span id="cb25-1120"><a href="#cb25-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1121"><a href="#cb25-1121" aria-hidden="true" tabindex="-1"></a>This result is known as the delta method, and applies to any sequence of random variables that is asymptotically normal. It is also readily generalized to higher dimensions where $\mathbf g$ is a vector valued function. </span>
<span id="cb25-1122"><a href="#cb25-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1123"><a href="#cb25-1123" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb25-1124"><a href="#cb25-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1125"><a href="#cb25-1125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Delta Method</span></span>
<span id="cb25-1126"><a href="#cb25-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1127"><a href="#cb25-1127" aria-hidden="true" tabindex="-1"></a>Suppose $(\X_1,\ldots, \X_n)$ is a sequence of random vectors such that $\sqrt n (\X_n -  \mathbf t) \dto N(\mathbf 0, \boldsymbol\Sigma)$ for some vector $\mathbf t$ in the interior of $\mathcal X$. If $\mathbf g(\X_n)$ is a vector valued function that:</span>
<span id="cb25-1128"><a href="#cb25-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1129"><a href="#cb25-1129" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>is continuously differentiable,</span>
<span id="cb25-1130"><a href="#cb25-1130" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>does not involve $n$,</span>
<span id="cb25-1131"><a href="#cb25-1131" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t) \neq 0$,^<span class="co">[</span><span class="ot">$\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)$ is the $\dim(\mathbf g(\X_n)) \times \dim(\mathbf X_n)$ Jacobian matrix comprised of the partial derivatives of the components of $\mathbf g$.</span><span class="co">]</span></span>
<span id="cb25-1132"><a href="#cb25-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1133"><a href="#cb25-1133" aria-hidden="true" tabindex="-1"></a>then, </span>
<span id="cb25-1134"><a href="#cb25-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1135"><a href="#cb25-1135" aria-hidden="true" tabindex="-1"></a>$$ \sqrt n \left<span class="co">[</span><span class="ot">\mathbf g(\X_n) - \mathbf g(\mathbf t)\right</span><span class="co">]</span> \dto N\left(\mathbf 0, \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right</span><span class="co">]</span>\boldsymbol\Sigma\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right</span><span class="co">]</span>'\right)$$</span>
<span id="cb25-1136"><a href="#cb25-1136" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1137"><a href="#cb25-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1138"><a href="#cb25-1138" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-1139"><a href="#cb25-1139" aria-hidden="true" tabindex="-1"></a>Return to the example where $X_i\iid\text{Exp}(1)$, giving $\E{X_i} = 1$ and $\var{X_i} = 1$. By the CLT, $\sqrt n(\bar X - 1)\dto N(0,1)$. If $g(t) = t^2 +3$, what is the asymptotic distribution of $\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(1)</span><span class="co">]</span>$? According to the delta method we have </span>
<span id="cb25-1140"><a href="#cb25-1140" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1141"><a href="#cb25-1141" aria-hidden="true" tabindex="-1"></a>&amp;\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(1)</span><span class="co">]</span>  \asim N(0, 1<span class="co">[</span><span class="ot">g'(1)</span><span class="co">]</span>^2),<span class="sc">\\</span></span>
<span id="cb25-1142"><a href="#cb25-1142" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}\left<span class="co">[</span><span class="ot">\bar X^2 - 1\right</span><span class="co">]</span> \asim N(0, 4).</span>
<span id="cb25-1143"><a href="#cb25-1143" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-1144"><a href="#cb25-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1147"><a href="#cb25-1147" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1148"><a href="#cb25-1148" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb25-1149"><a href="#cb25-1149" aria-hidden="true" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb25-1150"><a href="#cb25-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1151"><a href="#cb25-1151" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(t){</span>
<span id="cb25-1152"><a href="#cb25-1152" aria-hidden="true" tabindex="-1"></a>  t<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb25-1153"><a href="#cb25-1153" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-1154"><a href="#cb25-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1155"><a href="#cb25-1155" aria-hidden="true" tabindex="-1"></a>g_of_mean <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>N_sim, <span class="cf">function</span>(x) <span class="fu">g</span>(<span class="fu">mean</span>(<span class="fu">rexp</span>(sample_size))))</span>
<span id="cb25-1156"><a href="#cb25-1156" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(sample_size)<span class="sc">*</span>(g_of_mean <span class="sc">-</span> <span class="fu">g</span>(<span class="dv">1</span>))</span>
<span id="cb25-1157"><a href="#cb25-1157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1158"><a href="#cb25-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1159"><a href="#cb25-1159" aria-hidden="true" tabindex="-1"></a>We can plot a histogram of our estimates and overlay the distribution $N(0,4)$.</span>
<span id="cb25-1160"><a href="#cb25-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1163"><a href="#cb25-1163" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1164"><a href="#cb25-1164" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-1165"><a href="#cb25-1165" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot216</span></span>
<span id="cb25-1166"><a href="#cb25-1166" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1167"><a href="#cb25-1167" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-1168"><a href="#cb25-1168" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-1169"><a href="#cb25-1169" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method"</span></span>
<span id="cb25-1170"><a href="#cb25-1170" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-1171"><a href="#cb25-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1172"><a href="#cb25-1172" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> estimates) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1173"><a href="#cb25-1173" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x)) <span class="sc">+</span></span>
<span id="cb25-1174"><a href="#cb25-1174" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">colour =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb25-1175"><a href="#cb25-1175" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Estimates of √n(g(X) - g(1)) "</span>) <span class="sc">+</span></span>
<span id="cb25-1176"><a href="#cb25-1176" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-1177"><a href="#cb25-1177" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>), <span class="at">color =</span> <span class="st">"red"</span>)</span>
<span id="cb25-1178"><a href="#cb25-1178" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1179"><a href="#cb25-1179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1180"><a href="#cb25-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1181"><a href="#cb25-1181" aria-hidden="true" tabindex="-1"></a>The big takeaway from the delta method is that estimators which are nice functions of sample means will be asymptotically distributed according to a normal distribution. Furthermore, any nice function of such an estimator will also have a normal asymptotic distribution! </span>
<span id="cb25-1182"><a href="#cb25-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1185"><a href="#cb25-1185" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1186"><a href="#cb25-1186" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1187"><a href="#cb25-1187" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot217</span></span>
<span id="cb25-1188"><a href="#cb25-1188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-1189"><a href="#cb25-1189" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-1190"><a href="#cb25-1190" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A mediocre meme"</span></span>
<span id="cb25-1191"><a href="#cb25-1191" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb25-1192"><a href="#cb25-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1193"><a href="#cb25-1193" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/meme.png"</span>)</span>
<span id="cb25-1194"><a href="#cb25-1194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1195"><a href="#cb25-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1196"><a href="#cb25-1196" aria-hidden="true" tabindex="-1"></a><span class="fu">## Little $o_p$, Big $O_p$, and Taylor Expansions</span></span>
<span id="cb25-1197"><a href="#cb25-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1198"><a href="#cb25-1198" aria-hidden="true" tabindex="-1"></a>We've talked a lot about whether or not random variables converge, and how they converge, but not the rate at which they converge. We can introduce some notation that allows us to quantify this rate.</span>
<span id="cb25-1199"><a href="#cb25-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1200"><a href="#cb25-1200" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb25-1201"><a href="#cb25-1201" aria-hidden="true" tabindex="-1"></a>Given a sequence of random variables $X_n$, we say $X_n$ is  <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_little "O.P" of $n^k$_**<span class="kw">&lt;/span&gt;</span>, denoted $X_n = o_p(n^k)$, if $X_n / n^k\pto 0$.</span>
<span id="cb25-1202"><a href="#cb25-1202" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1203"><a href="#cb25-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1204"><a href="#cb25-1204" aria-hidden="true" tabindex="-1"></a>Note that $X_n\pto 0$ implies that $X_n = o_p(1)$. The use of "=" is a bit misleading in this definition, as $X_n = o_p(n^k)$ does not establish any equality, instead referring to how $X_n$ behaves asymptotically. For instance, if we have two sequences of random variables $X_n$ and $Y_n$ such that $X_n\pto X$ and $Y_n\pto 0$, we have $ X_n + Y_n \pto X + 0 $, but could write $X_n + Y_n$ as $X_n + o_p(1)$. This emphasizes the sequence $X_n$, and frames $Y_n$ as some negligible remainder term that tends to zero.</span>
<span id="cb25-1205"><a href="#cb25-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1206"><a href="#cb25-1206" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb25-1207"><a href="#cb25-1207" aria-hidden="true" tabindex="-1"></a>Given a sequence of random variables $X_n$, we say $X_n$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_big "O.P" of $n^k$_**&lt;/span&gt;, denoted $X_n = O_p(n^k)$,  if for all $\varepsilon &gt; 0$, there exists some $\delta$ and $N$ such that $\Pr(|X_n/n^k| \ge \delta) &lt;\varepsilon$ for all $n &gt; N$. In other words, $X_n/n^k$ is &lt;span style="color:red"&gt;**_bounded in probability_**<span class="kw">&lt;/span&gt;</span>.</span>
<span id="cb25-1208"><a href="#cb25-1208" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1209"><a href="#cb25-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1210"><a href="#cb25-1210" aria-hidden="true" tabindex="-1"></a>We are most interested in the case where $X_n = O_p(1)$. If this is the case, then as $n\to\infty$, we can bound the area in the tails of $f_{X_n}$ by some constant $\delta$ such that the area is negligible (less than $\varepsilon$).</span>
<span id="cb25-1211"><a href="#cb25-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1212"><a href="#cb25-1212" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-1213"><a href="#cb25-1213" aria-hidden="true" tabindex="-1"></a>We know that $\bar X \dto N(\mu, \sigma^2/n)$ when $\X$ is an iid sample. We have that $\bar X = O_p(1)$. We have </span>
<span id="cb25-1214"><a href="#cb25-1214" aria-hidden="true" tabindex="-1"></a>$$\Pr(|\bar X/1| \ge \delta) = \Pr(-\bar X\ge -\delta \text{ and }\delta \le \bar X) = 2\cdot\Pr(\bar X\ge \delta) = 2\left<span class="co">[</span><span class="ot">1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right</span><span class="co">]</span>.$$ If we take the limit of this as $n\to \infty$ we have $$ \lim_{n\to \infty}\Pr(|X\bar X/1| \ge \delta) = \lim_{n\to \infty}2\left<span class="co">[</span><span class="ot">1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right</span><span class="co">]</span> = 0.$$ By the definition of a limit, there must exists some $N$ such that $\Pr(|\bar X/1| \ge \delta) &lt; \varepsilon$ for any $n &gt; N$, so $\bar X = O_p(1)$. </span>
<span id="cb25-1215"><a href="#cb25-1215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1216"><a href="#cb25-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1217"><a href="#cb25-1217" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb25-1218"><a href="#cb25-1218" aria-hidden="true" tabindex="-1"></a>If $X_n \dto X$, then $X_n = O_p(1)$.</span>
<span id="cb25-1219"><a href="#cb25-1219" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1220"><a href="#cb25-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1221"><a href="#cb25-1221" aria-hidden="true" tabindex="-1"></a>:::{#cor-}</span>
<span id="cb25-1222"><a href="#cb25-1222" aria-hidden="true" tabindex="-1"></a>If $X_n = o_p(1)$, then $X_n = O_p(1)$.</span>
<span id="cb25-1223"><a href="#cb25-1223" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1224"><a href="#cb25-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1225"><a href="#cb25-1225" aria-hidden="true" tabindex="-1"></a>A common place to encounter $o_p$ is when performing Taylor expansions.</span>
<span id="cb25-1226"><a href="#cb25-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1227"><a href="#cb25-1227" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb25-1228"><a href="#cb25-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1229"><a href="#cb25-1229" aria-hidden="true" tabindex="-1"></a><span class="fu">## Taylor's Theorem</span></span>
<span id="cb25-1230"><a href="#cb25-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1231"><a href="#cb25-1231" aria-hidden="true" tabindex="-1"></a>Taylor's theorem, as given in @rudin1976principles, tells us that if $f:\mathbb R\to\mathbb R$ is $k-$times differentiable at a point $a\in \mathbb R$, then there is some element $c\in (a,b)$ such that </span>
<span id="cb25-1232"><a href="#cb25-1232" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1233"><a href="#cb25-1233" aria-hidden="true" tabindex="-1"></a>f(b) &amp; = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{k!}(b-a)^j + \frac{f^{(n)}(c)}{k!}(b-a)^k.</span>
<span id="cb25-1234"><a href="#cb25-1234" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-1235"><a href="#cb25-1235" aria-hidden="true" tabindex="-1"></a>For $n = 2$, we have the mean value theorem:</span>
<span id="cb25-1236"><a href="#cb25-1236" aria-hidden="true" tabindex="-1"></a>$$ f(b)= f(a) + f'(c)(b-a).$$</span>
<span id="cb25-1237"><a href="#cb25-1237" aria-hidden="true" tabindex="-1"></a>If we let $a\to b$, then </span>
<span id="cb25-1238"><a href="#cb25-1238" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1239"><a href="#cb25-1239" aria-hidden="true" tabindex="-1"></a>&amp;\lim_{a\to b}f(b)  = \sum_{j=0}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k.<span class="sc">\\</span></span>
<span id="cb25-1240"><a href="#cb25-1240" aria-hidden="true" tabindex="-1"></a>\implies &amp; f(b)  =\lim_{a\to b} f(a) + \sum_{j=1}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j +  \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k<span class="sc">\\</span></span>
<span id="cb25-1241"><a href="#cb25-1241" aria-hidden="true" tabindex="-1"></a>\implies &amp; f(b)  = f(b) + \underbrace{\sum_{j=1}^{k-1}\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k<span class="sc">\\</span></span>
<span id="cb25-1242"><a href="#cb25-1242" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!}(b-a)^k = 0<span class="sc">\\</span></span>
<span id="cb25-1243"><a href="#cb25-1243" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!(b-a)^k} = 0<span class="sc">\\</span></span>
<span id="cb25-1244"><a href="#cb25-1244" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{f^{(k)}(c)}{k!} = o(|a-b|^k)<span class="sc">\\</span></span>
<span id="cb25-1245"><a href="#cb25-1245" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-1246"><a href="#cb25-1246" aria-hidden="true" tabindex="-1"></a>Here, $o$ is the deterministic counterpart of $o_p$ (we're not working with random variables just yet). This means we can write Taylor's theorem as </span>
<span id="cb25-1247"><a href="#cb25-1247" aria-hidden="true" tabindex="-1"></a>$$ f(b) = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).$$ In the event $f$ is infinitely differentiable we can make this approximation arbitrarily accurate, giving rise toa function's Taylor series.</span>
<span id="cb25-1248"><a href="#cb25-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1249"><a href="#cb25-1249" aria-hidden="true" tabindex="-1"></a>Now suppose $f_n(X)$ is a sequence of functions of random variables, where the subscript $n$ emphasizes that $f_n$ is a random variable. IF we apply Taylor's theorem to $f_n(X)$ we have </span>
<span id="cb25-1250"><a href="#cb25-1250" aria-hidden="true" tabindex="-1"></a>$$f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)$$ for realizations of the random variable $a,b,c\in\mathcal X$ where $c\in(a,b)$. Assuming $k \ge 1$, then $o_p(|a-b|^k)$ implies $o_p(1)$, so </span>
<span id="cb25-1251"><a href="#cb25-1251" aria-hidden="true" tabindex="-1"></a>$$f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).$$ </span>
<span id="cb25-1252"><a href="#cb25-1252" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1253"><a href="#cb25-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1254"><a href="#cb25-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1255"><a href="#cb25-1255" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotically Normal Estimators </span></span>
<span id="cb25-1256"><a href="#cb25-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1257"><a href="#cb25-1257" aria-hidden="true" tabindex="-1"></a>When putting our asymptotic tools to work on an estimator of interest, we will almost always find that it converges to a normal distribution, is consistent, and that the rate of convergence is linked to $\sqrt{n}$.</span>
<span id="cb25-1258"><a href="#cb25-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1259"><a href="#cb25-1259" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb25-1260"><a href="#cb25-1260" aria-hidden="true" tabindex="-1"></a>An estimator $\hat{\thet}$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_$\sqrt{n}-$consistent asymptotically normal (root-n CAN)_**&lt;/span&gt;, if $$\sqrt{n}(\hat{\thet}- \thet) \dto N(\mathbf 0, \mathbf V)$$ for a PSD matrix $\mathbf V$. Equivalently, $$ \hat{\thet}\asim N(\thet, \mathbf V/n).$$ We refer to $\mathbf V/n$ as the &lt;span style="color:red"&gt;**_asymptotic variance_**<span class="kw">&lt;/span&gt;</span> of $\hat{\thet}$ and write $\avar{\hat{\thet}} = \mathbf V /n$.</span>
<span id="cb25-1261"><a href="#cb25-1261" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1262"><a href="#cb25-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1263"><a href="#cb25-1263" aria-hidden="true" tabindex="-1"></a>"$\sqrt n-$" emphasizes the fact that $\sqrt{n}(\hat{\thet} - \thet) = O_p(1)$, which is equivalent to $\hat{\thet} = \thet + O_p(n^{-1/2})$. In other words, as $n\to\infty$ the error term associated with our estimate decreases at a rate of $n^{1/2}$. A fourfold increase in observations results in half the error. We also have that $\sqrt{n}(\hat{\thet} - \thet) = o_p(1)$, so $\hat{\thet}\pto \thet$, hence the "consistent" in the previous definition. We also have that $\hat{\thet}$ is asymptotically unbiased if it is root-n CAN, as $\E{\hat{\thet}}\to \thet$. This will be the one of, if not the, **_most important property_** an estimator can posses. </span>
<span id="cb25-1264"><a href="#cb25-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1265"><a href="#cb25-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1266"><a href="#cb25-1266" aria-hidden="true" tabindex="-1"></a>Our final example highlights some interesting properties of root-N CAN estimators in the context of the mean value theorem.  This will be especially important in Section \@ref(extremum-estimators).</span>
<span id="cb25-1267"><a href="#cb25-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1268"><a href="#cb25-1268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Thinking Beyond $\mathbb{R}^k$</span></span>
<span id="cb25-1269"><a href="#cb25-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1270"><a href="#cb25-1270" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normed Vector Spaces and Metric Spaces</span></span>
<span id="cb25-1271"><a href="#cb25-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1272"><a href="#cb25-1272" aria-hidden="true" tabindex="-1"></a>A **_normed vector space_** $V$ defined over a field $F$ is, as the name implies, a vector space equipped with a **_norm_** $\norm{\cdot}:V\mapsto [0,\infty)$ satisfying:</span>
<span id="cb25-1273"><a href="#cb25-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1274"><a href="#cb25-1274" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\norm{v} = 0 \iff v = 0$, where $0$ is the additive identity;</span>
<span id="cb25-1275"><a href="#cb25-1275" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\norm{av} = \abs{a}\norm{v}$ for all $a\in F$ and $v\in V$;</span>
<span id="cb25-1276"><a href="#cb25-1276" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\norm{w + v} \le \norm{w} + \norm{v}$ for all $w,v\in V$.</span>
<span id="cb25-1277"><a href="#cb25-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1278"><a href="#cb25-1278" aria-hidden="true" tabindex="-1"></a>A norm measures the the "distance" an element of $V$ is from the origin $0$. A **_metric space_** $(X,d)$ is some set $X$ with a **_metric_** $d:X\times X\to [0,\infty)$ such that:</span>
<span id="cb25-1279"><a href="#cb25-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1280"><a href="#cb25-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$d(x,y) = 0 \iff x=y$ for $x,y\in X$;</span>
<span id="cb25-1281"><a href="#cb25-1281" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$d(y,x)=d(x,y$ for all $x,y\in X$; </span>
<span id="cb25-1282"><a href="#cb25-1282" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$d(x,z) \le d(x,y) + d(y,z)$.</span>
<span id="cb25-1283"><a href="#cb25-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1284"><a href="#cb25-1284" aria-hidden="true" tabindex="-1"></a>A metric measures the distance between any two points in a metric space. Any normed vector space is a metric space if we define </span>
<span id="cb25-1285"><a href="#cb25-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1286"><a href="#cb25-1286" aria-hidden="true" tabindex="-1"></a>$$d(x,y)= \norm{x-y}$$</span>
<span id="cb25-1287"><a href="#cb25-1287" aria-hidden="true" tabindex="-1"></a>for $x,y\in V$.</span>
<span id="cb25-1288"><a href="#cb25-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1289"><a href="#cb25-1289" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1290"><a href="#cb25-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1291"><a href="#cb25-1291" aria-hidden="true" tabindex="-1"></a><span class="fu">## Euclidean Space</span></span>
<span id="cb25-1292"><a href="#cb25-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1293"><a href="#cb25-1293" aria-hidden="true" tabindex="-1"></a>$\R^k$ is a vector space, where the norm is given as </span>
<span id="cb25-1294"><a href="#cb25-1294" aria-hidden="true" tabindex="-1"></a>$$ \norm{\x} = \left(\sum_{i=1}^k x_i^2\right)^{1/2},$$ and is called the **_Euclidean norm_**. To measure the distance between two vectors in  $\R^k$ we use a familiar formula. </span>
<span id="cb25-1295"><a href="#cb25-1295" aria-hidden="true" tabindex="-1"></a>$$ \norm{\x -\y} = \left(\sum_{i=1}^k (x_i-y_i)^2\right)^{1/2}.$$</span>
<span id="cb25-1296"><a href="#cb25-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1297"><a href="#cb25-1297" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1298"><a href="#cb25-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1299"><a href="#cb25-1299" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1300"><a href="#cb25-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1301"><a href="#cb25-1301" aria-hidden="true" tabindex="-1"></a><span class="fu">## Euclidean Space with the $p$-norm</span></span>
<span id="cb25-1302"><a href="#cb25-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1303"><a href="#cb25-1303" aria-hidden="true" tabindex="-1"></a>We could also define the vector space $\R^k$ with the norm </span>
<span id="cb25-1304"><a href="#cb25-1304" aria-hidden="true" tabindex="-1"></a>$$\norm{\x}_p = \left(\sum_{i=1}^k \abs{x_i}^p\right)^{1/p}.$$ If $p = 1$, then $$\norm{\x - \y}_1 = \sum_{i=1}^n \abs{x_i - y_i},$$ whose name follows from how you would define distance if you were driving between two points in a city with a grid layout. If $p=2$ we have the Euclidean norm. Why bother considering  values of $p$ other than $2$? We can think of $p$ as the way we weight the component-wise "distances" from the origin $\abs{x_1},\abs{x_2},\ldots \abs{x_k}$. For $p = 2$, we square each of these distances (meaning the result is always positive rendering the absolute value moot), assigning more weight to components where $x_i$ is relatively large. What happens as $p\to\infty?$? Let's look at some plots.</span>
<span id="cb25-1305"><a href="#cb25-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1308"><a href="#cb25-1308" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1309"><a href="#cb25-1309" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb25-1310"><a href="#cb25-1310" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot2213</span></span>
<span id="cb25-1311"><a href="#cb25-1311" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1312"><a href="#cb25-1312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb25-1313"><a href="#cb25-1313" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb25-1314"><a href="#cb25-1314" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "p norm"</span></span>
<span id="cb25-1315"><a href="#cb25-1315" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb25-1316"><a href="#cb25-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1317"><a href="#cb25-1317" aria-hidden="true" tabindex="-1"></a>p_norm <span class="ot">&lt;-</span> <span class="cf">function</span>(p, x){</span>
<span id="cb25-1318"><a href="#cb25-1318" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>(<span class="fu">abs</span>(x)<span class="sc">^</span>p))<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>p)</span>
<span id="cb25-1319"><a href="#cb25-1319" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-1320"><a href="#cb25-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1321"><a href="#cb25-1321" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb25-1322"><a href="#cb25-1322" aria-hidden="true" tabindex="-1"></a>       <span class="at">x2 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb25-1323"><a href="#cb25-1323" aria-hidden="true" tabindex="-1"></a>       <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb25-1324"><a href="#cb25-1324" aria-hidden="true" tabindex="-1"></a>       <span class="at">x4 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">4</span>)</span>
<span id="cb25-1325"><a href="#cb25-1325" aria-hidden="true" tabindex="-1"></a>       ) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1326"><a href="#cb25-1326" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vector =</span> <span class="fu">ifelse</span>(x4 <span class="sc">==</span> <span class="sc">-</span><span class="dv">4</span>, <span class="st">"x = (1,2,3,-4)"</span>, <span class="fu">ifelse</span>(x3 <span class="sc">==</span> <span class="dv">3</span>, <span class="st">"x = (1,2,3)"</span>, <span class="fu">ifelse</span>(x2 <span class="sc">==</span> <span class="dv">2</span>, <span class="st">"x = (1,2)"</span>, <span class="st">"x = 1"</span>))))  <span class="sc">%&gt;%</span> </span>
<span id="cb25-1327"><a href="#cb25-1327" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand_grid</span>(<span class="at">p =</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>, <span class="at">length =</span> <span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1328"><a href="#cb25-1328" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-1329"><a href="#cb25-1329" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">norm =</span> <span class="fu">p_norm</span>(p, <span class="fu">c</span>(x1, x2, x3, x4))) <span class="sc">%&gt;%</span> </span>
<span id="cb25-1330"><a href="#cb25-1330" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, norm)) <span class="sc">+</span></span>
<span id="cb25-1331"><a href="#cb25-1331" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb25-1332"><a href="#cb25-1332" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>vector, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb25-1333"><a href="#cb25-1333" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-1334"><a href="#cb25-1334" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p"</span>, <span class="at">y =</span> <span class="st">"p-norm"</span>)</span>
<span id="cb25-1335"><a href="#cb25-1335" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-1336"><a href="#cb25-1336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1337"><a href="#cb25-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1338"><a href="#cb25-1338" aria-hidden="true" tabindex="-1"></a>It appears that </span>
<span id="cb25-1339"><a href="#cb25-1339" aria-hidden="true" tabindex="-1"></a>$$ \lim_{p\to\infty}\norm{\x}_p= \max_{x_i}\abs{x_i}.$$ For this reason we define the limiting case of the $p-$norm as </span>
<span id="cb25-1340"><a href="#cb25-1340" aria-hidden="true" tabindex="-1"></a>$$ \norm{\x}_\infty= \max_{x_i}\abs{x_i}.$$</span>
<span id="cb25-1341"><a href="#cb25-1341" aria-hidden="true" tabindex="-1"></a>A useful illustration is of the unit circle in $(\R^k,\norm{\cdot}_p)$ for various values of $p$</span>
<span id="cb25-1342"><a href="#cb25-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1345"><a href="#cb25-1345" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb25-1346"><a href="#cb25-1346" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb25-1347"><a href="#cb25-1347" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb25-1348"><a href="#cb25-1348" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot1241123</span></span>
<span id="cb25-1349"><a href="#cb25-1349" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.5</span></span>
<span id="cb25-1350"><a href="#cb25-1350" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 0.8</span></span>
<span id="cb25-1351"><a href="#cb25-1351" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "test"</span></span>
<span id="cb25-1352"><a href="#cb25-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1353"><a href="#cb25-1353" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/lp_norm.png"</span>)</span>
<span id="cb25-1354"><a href="#cb25-1354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-1355"><a href="#cb25-1355" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1356"><a href="#cb25-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1357"><a href="#cb25-1357" aria-hidden="true" tabindex="-1"></a>We want to think about a vector space formed by random variables where each vector is its own random variable. Before jumping straight to this case, let's consider a vector space formed by real functions, and then worry about randomness later on.</span>
<span id="cb25-1358"><a href="#cb25-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1359"><a href="#cb25-1359" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1360"><a href="#cb25-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1361"><a href="#cb25-1361" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Space of Real Continuous Bounded Functions</span></span>
<span id="cb25-1362"><a href="#cb25-1362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1363"><a href="#cb25-1363" aria-hidden="true" tabindex="-1"></a>Define the space $C(X)$ as </span>
<span id="cb25-1364"><a href="#cb25-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1365"><a href="#cb25-1365" aria-hidden="true" tabindex="-1"></a>$$C(X) = <span class="sc">\{</span>f:X\to\R \mid X\subset\R, f\text{ continuous and bounded}<span class="sc">\}</span>.$$ This is a vector space over the field $\R$ where addition and scalar multiplication are defined as:</span>
<span id="cb25-1366"><a href="#cb25-1366" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb25-1367"><a href="#cb25-1367" aria-hidden="true" tabindex="-1"></a>(f +g)(x)&amp;=f(x)+g(x),<span class="sc">\\</span></span>
<span id="cb25-1368"><a href="#cb25-1368" aria-hidden="true" tabindex="-1"></a>(cf)(x) &amp; = c\times f(x).</span>
<span id="cb25-1369"><a href="#cb25-1369" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb25-1370"><a href="#cb25-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1371"><a href="#cb25-1371" aria-hidden="true" tabindex="-1"></a>How would we define a norm on a space of functions though? Firstly, by the definition of addition of vectors, the $0$ elements is $f(x) = 0$. So how do we measure the distance between some $f\in C(X)$ and $f(x) = 0$? In the event $X$ is an interval of $\R$, it contains uncountably many points, so we cannot use the $p$-norm and sum $|f(x)|$ over all the values of $x$. We can however integrate over all such values, as one interpretation of the integral is as an uncountably infinite counterpart to a countable sum. We define the $p-$norm on $C(X)$ as </span>
<span id="cb25-1372"><a href="#cb25-1372" aria-hidden="true" tabindex="-1"></a>$$ \norm{f}_p = \left<span class="co">[</span><span class="ot">\int_X \abs{f(x)}^p\ dx\right</span><span class="co">]</span>^{1/p}.$$</span>
<span id="cb25-1373"><a href="#cb25-1373" aria-hidden="true" tabindex="-1"></a>In the event we want to take $p\to\infty$, we have </span>
<span id="cb25-1374"><a href="#cb25-1374" aria-hidden="true" tabindex="-1"></a>$$ \norm{f}_\infty = \sup_{x\in X}\abs{f(x)}.$$ We know can use this norm to define the distance between two functions in $C(X)$, </span>
<span id="cb25-1375"><a href="#cb25-1375" aria-hidden="true" tabindex="-1"></a>$$ \norm{f}_\infty = \sup_{x\in X}\abs{f(x) - g(x)}.$$ This gives the distance between two functions as the distance between their evaluated values where this distance is its largest. We refer to $\norm{\cdot}_\infty$ on $C(X)$ as the **_uniform norm_** or **_supremum norm_**.</span>
<span id="cb25-1376"><a href="#cb25-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1377"><a href="#cb25-1377" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1378"><a href="#cb25-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1379"><a href="#cb25-1379" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence and Continuity</span></span>
<span id="cb25-1380"><a href="#cb25-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1381"><a href="#cb25-1381" aria-hidden="true" tabindex="-1"></a>Given a metric space $(X,d)$, a sequence of elements $<span class="sc">\{</span>x_n<span class="sc">\}</span>\subset X$ **_converges_** to an element $x$ if *in* $X$ for all $\varepsilon &gt; 0$, there exists some integer $N\in \Z^+$ such that $d(x_n,x)&lt;\varepsilon$ for all $n&gt;N$. If $<span class="sc">\{</span>x_n<span class="sc">\}</span>$ converges to $x$, we write $x_n\to x$ or $\lim_{n\to\infty}x_n=x$. It's important to remember that convergence is defined in relation to the metric space $X$. A sequence $<span class="sc">\{</span>x_n<span class="sc">\}</span>$ may converge in one metric space but not another, so it's important to remember what space we're working in, and the metric the space is equipped with.</span>
<span id="cb25-1382"><a href="#cb25-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1383"><a href="#cb25-1383" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1384"><a href="#cb25-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1385"><a href="#cb25-1385" aria-hidden="true" tabindex="-1"></a><span class="fu">## Euclidean Space</span></span>
<span id="cb25-1386"><a href="#cb25-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1387"><a href="#cb25-1387" aria-hidden="true" tabindex="-1"></a>A sequence of vectors $<span class="sc">\{</span>\x_n<span class="sc">\}</span>\subset \R^k$ converges to $\x$ if for all $\varepsilon &gt; 0$, there exists some integer $N\in \mathbb{Z}^+$ such that $$\norm{x_n -x} =\left(\sum_{i=1}^k (x_{n,i} - x_i)^2\right)^{1/2} &lt;\varepsilon$$ for all $n&gt;N$. It can be shown that a sequence of vectors only converges to a limit if and only if each of its components converges to their respective limits. </span>
<span id="cb25-1388"><a href="#cb25-1388" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1389"><a href="#cb25-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1390"><a href="#cb25-1390" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1391"><a href="#cb25-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1392"><a href="#cb25-1392" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pointwise Convergence </span></span>
<span id="cb25-1393"><a href="#cb25-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1394"><a href="#cb25-1394" aria-hidden="true" tabindex="-1"></a>Suppose we have a sequence of real functions $<span class="sc">\{</span>f_n<span class="sc">\}</span>$ where $f_n:X\to \R$ for some $X\subset \R$. One way to think about $f_n$ converging to some limit is by considering $<span class="sc">\{</span>f_n(x)<span class="sc">\}</span>\subset \R$ as a sequence in Euclidean space for each $x\in X$. If $f_n(x)\to f(x)$ for all $x\in X$, then we say $f$ **_converges pointwise_** to $f$, and write $f_n\to f$. In other words, $f_n\to f$ if for all $\varepsilon &gt; 0$, there exists some $N_x$ such that $\abs{f_n(x) - f(x)}&lt; \varepsilon$ for all $n&gt; N$, for each $x\in X$.</span>
<span id="cb25-1395"><a href="#cb25-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1396"><a href="#cb25-1396" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1397"><a href="#cb25-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1398"><a href="#cb25-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1399"><a href="#cb25-1399" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1400"><a href="#cb25-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1401"><a href="#cb25-1401" aria-hidden="true" tabindex="-1"></a><span class="fu">## Uniform Convergence</span></span>
<span id="cb25-1402"><a href="#cb25-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1403"><a href="#cb25-1403" aria-hidden="true" tabindex="-1"></a>In the definition of pointwise convergence, note that $N_x$ depends on $x$. This means that while $\abs{f_n(x_1) - f(x_1)} &lt; \varepsilon$, it may not be the case that $\abs{f_n(x_2) - f(x_2)} &lt; \varepsilon$. We can strengthen pointwise convergence by insuring that $N$ does not depend on $x$, and $<span class="sc">\{</span>f_n<span class="sc">\}</span>$ gets arbitrarily close to $f$ uniformly on $X$. We say $f$ **_converges uniformly_** to $f$, and write $f_n\overset{uni}\to f$ if for all $\varepsilon &gt;0$, there exists some $N$ (idenpendent of $x$) such that $\abs{f_n(x) - f(x)}&lt; \varepsilon$ for all $n&gt; N$ and any $x\in X$. Uniform convergence has some great properties that pointwise convergence lacks,^<span class="co">[</span><span class="ot">It preserves continuity, integration, and under an additional weak condition it also preserves differentiation.</span><span class="co">]</span> making it much more attractive, albeit a much stricter condition. A more natural formulation of uniform convergence arises by considering the vector space of bounded continuous functions $f:X\to \R$, $C(X)$, equipped with the supremum norm $\norm{\cdot}_\infty$. In this case, $f_n\to f$ (in $(C(X),\norm{\cdot}_\infty)$) if for all $\varepsilon &gt; 0$ there exists some $N\in \mathbb Z^+$ such that $$d(f_n,f) = \norm{f_n-f} = \sup_{x\in X}\abs{f_n-f} &lt; \varepsilon$$ for all $n &gt; N$. By the definition of $\norm{\cdot}_\infty$, </span>
<span id="cb25-1404"><a href="#cb25-1404" aria-hidden="true" tabindex="-1"></a>$$\abs{f_n(x)-f(x)} \le \sup_{x\in X}\abs{f_n-f} \le \varepsilon$$ for all $x\in X$ when $n&gt;N$.</span>
<span id="cb25-1405"><a href="#cb25-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1406"><a href="#cb25-1406" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1407"><a href="#cb25-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1408"><a href="#cb25-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1409"><a href="#cb25-1409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1410"><a href="#cb25-1410" aria-hidden="true" tabindex="-1"></a>Recall from real analysis that a function $f:X\to Y$ defined on metric spaces $X$ and $Y$, each equipped with metrics $d_X(\cdot,\cdot)$ and $d_Y(\cdot,\cdot)$, is continuous at $x\in X$ if for all $\varepsilon&gt;0$, there exists some $\delta_x$ such that </span>
<span id="cb25-1411"><a href="#cb25-1411" aria-hidden="true" tabindex="-1"></a>$$ d_Y(f(x),f(p)) &lt; \varepsilon$$ when $d_X(x,p) &lt; \delta_x$. That is, for any arbitrarily small distance $\varepsilon$, we can find some point $p$ which is arbitrarily close (within a distance of $\delta_x$ to be exact) to $x$ such that the distance between $f(x)$ and $f(p)$ is less than $\varepsilon$. If $f$ is continuous for all $p$ in some subset $E\subset X$, then we say $f$ is continuous on $E$. </span>
<span id="cb25-1412"><a href="#cb25-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1413"><a href="#cb25-1413" aria-hidden="true" tabindex="-1"></a>One of the more subtle parts about the definition of continuity is that $\delta_x$ depends on $x$. If we want to ensure that a single value of $\delta$ can be used for all $x\in E$, then we need to strengthen the definition of continuity. We say $f$ is uniformly continuous on $E$ if for every $\varepsilon &gt; 0$, there exists some $\delta$ independent of $x$ such that </span>
<span id="cb25-1414"><a href="#cb25-1414" aria-hidden="true" tabindex="-1"></a>$$d_Y(f(x),f(p)) &lt; \varepsilon$$ whenever $d(x,p) &lt;\delta$ for all $x,p\in E$. Now we can use one uniform $\delta$ on the entire set $x\in E$. Uniform continuity is *much* stronger then continuity, but it naturally arises in many common settings.^<span class="co">[</span><span class="ot">Any continuous function on a compact set is uniformly continuous, because compactness allows you to "chop up" $E$ into discrete points and then take $\delta$ to be the smallest $\delta_x$ associated with the discrete points. Intuitively, there must be some $\delta_x$ that is the "strictest" among all $x\in E$, and that one will work for all the other points in $E$. This intuition is what makes the proof of this result so beautiful.</span><span class="co">]</span> </span>
<span id="cb25-1415"><a href="#cb25-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1416"><a href="#cb25-1416" aria-hidden="true" tabindex="-1"></a>Now suppose we have a collection of a functions $\mathcal F$, each of which is uniformly continuous on $E\subset X$. This means for all $f\in\mathcal F$ and $\varepsilon &gt; 0$, there exists a $\delta_{f}$  independent of $x$ such that </span>
<span id="cb25-1417"><a href="#cb25-1417" aria-hidden="true" tabindex="-1"></a>$$d_Y(f(x),f(p)) &lt; \varepsilon$$ when $d_X(x,p) &lt; \delta_f$. Of course $\delta_f$ depends on the function $f\in\mathcal F$ here. Once we start looking at multiple functions, we need to adjust $\delta_f$ accordingly. Even if $\delta_f$ is uniform across $x\in E$ for each separate $f$, it isn't guaranteed it will be uniform across $x\in E$ uniformly across $f\in \mathcal F$. A special case we're interested in is when $\mathcal F$ is some sequence sequence of functions indexed by $n$, $\mathcal F=<span class="sc">\{</span>f_n\mid n\in\Z^+<span class="sc">\}</span>$.  </span>
<span id="cb25-1418"><a href="#cb25-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1419"><a href="#cb25-1419" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb25-1420"><a href="#cb25-1420" aria-hidden="true" tabindex="-1"></a>Define the sequence of functions $f_n:<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>\to \R$ where $f_n(x)=x^n$ and let our collection of functions be this sequence. We will focus our attention on $x = 1$. The function $f_n$ is continuous on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ for all $n$. In fact, one can show that for all $\varepsilon &gt; 0$, </span>
<span id="cb25-1421"><a href="#cb25-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1422"><a href="#cb25-1422" aria-hidden="true" tabindex="-1"></a>$$\abs{f_n(x) - f_n(p)} &lt; \varepsilon$$ whenever </span>
<span id="cb25-1423"><a href="#cb25-1423" aria-hidden="true" tabindex="-1"></a>$$ \delta_{n,x} = \frac{\varepsilon}{n(x + 1)^{n-1}}.$$ We can go a step further and conclude that $f_n$ is uniformly continuous on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, by setting </span>
<span id="cb25-1424"><a href="#cb25-1424" aria-hidden="true" tabindex="-1"></a>$$ \delta_{n} = \inf_{x\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>}\delta_{n,x} = \frac{\varepsilon}{n(1 + 1)^{n-1}} = \frac{\varepsilon}{n2^{n-1}}.$$ By construction $\delta_{n}$ will now work on the entire interval $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ and doesn't depend on $x$. Unfortunately, $\delta_n$ still depends on the function $f_n$ (hence the subscript $n$), and we cannot perform the same "trick" to get a $\delta$ common to all $n$. </span>
<span id="cb25-1425"><a href="#cb25-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1426"><a href="#cb25-1426" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence of Stochastic Process</span></span>
<span id="cb25-1427"><a href="#cb25-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1428"><a href="#cb25-1428" aria-hidden="true" tabindex="-1"></a><span class="fu">### Functional Central Limit Theorem</span></span>
<span id="cb25-1429"><a href="#cb25-1429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1430"><a href="#cb25-1430" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-1431"><a href="#cb25-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1432"><a href="#cb25-1432" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb25-1433"><a href="#cb25-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-1434"><a href="#cb25-1434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Eric Zivot's <span class="co">[</span><span class="ot">primer on asymptotics</span><span class="co">](http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf)</span></span>
<span id="cb25-1435"><a href="#cb25-1435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@wooldridge2010econometric, Chapter 3</span>
<span id="cb25-1436"><a href="#cb25-1436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@greene2003econometric, Appendix D</span>
<span id="cb25-1437"><a href="#cb25-1437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@van2000asymptotic</span>
<span id="cb25-1438"><a href="#cb25-1438" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@white2014asymptotic</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>