{"title":"Preliminaries","markdown":{"headingText":"Preliminaries","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\\DeclareMathOperator{\\plim}{plim}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\newcommand{\\var}[1]{\\text{Var}\\left(#1\\right)}\n\\newcommand{\\avar}[1]{\\text{Avar}\\left(#1\\right)}\n\\newcommand{\\E}[1]{\\text{E}\\left[#1\\right]}\n\\newcommand{\\cov}[1]{\\text{Cov}\\left(#1\\right)}\n\\newcommand{\\mse}[1]{\\text{MSE}\\left(#1\\right)}\n\\newcommand{\\se}[1]{\\text{se}\\left(#1\\right)}\n\\newcommand{\\limfunc}{lim} \n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\Xm}{\\mathbb{X}}\n\\newcommand{\\EER}{\\bar{\\thet}_\\text{EE}}\n\\newcommand{\\NLS}{\\hat{\\bet}_\\text{NLLS}}\n\\newcommand{\\z}{\\mathbf{z}}\n\\newcommand{\\rr}{\\mathbf{r}}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\Pe}{\\mathbf{P}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\xm}{\\mathbb{x}}\n\\newcommand{\\Zm}{\\mathbb{Z}}\n\\newcommand{\\Wm}{\\mathbb{W}}\n\\newcommand{\\Hm}{\\mathbb{H}}\n\\newcommand{\\W}{\\mathbf{W}}\n\\newcommand{\\Z}{\\mathbf{Z}}\n\\newcommand{\\Hess}{\\mathbf{H}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\Score}{\\mathbf{S}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\A}{\\mathbf{A}}\n\\newcommand{\\h}{\\mathbf{h}}\n\\newcommand{\\Q}{\\mathbf{Q}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\G}{\\mathbf{G}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\renewcommand{\\D}{\\mathbf{D}}\n\\renewcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\OLS}{\\hat{\\boldsymbol\\beta}_\\text{OLS} }\n\\newcommand{\\OLSOV}{\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} }\n\\newcommand{\\OLSME}{\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} }\n\\newcommand{\\EE}{\\hat{\\boldsymbol\\theta}_\\text{EX} }\n\\newcommand{\\ME}{\\hat{\\boldsymbol\\theta}_\\text{M} }\n\\newcommand{\\MDE}{\\hat{\\boldsymbol\\theta}_\\text{MDE} }\n\\newcommand{\\IV}{\\hat{\\boldsymbol\\beta}_\\text{IV} }\n\\newcommand{\\TSLS}{\\hat{\\boldsymbol\\beta}_\\text{2SLS} }\n\\newcommand{\\thet}{\\boldsymbol{\\theta}}\n\\newcommand{\\et}{\\boldsymbol{\\eta}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Sig}{\\boldsymbol{\\Sigma}}\n\\newcommand{\\ep}{\\boldsymbol{\\varepsilon}}\n\\newcommand{\\Omeg}{\\boldsymbol{\\Omega}}\n\\newcommand{\\Thet}{\\boldsymbol{\\Theta}}\n\\newcommand{\\bet}{\\boldsymbol{\\beta}}\n\\newcommand{\\rk}{rank}\n\\newcommand{\\tsum}{\\sum}\n\\newcommand{\\tr}{tr}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\newcommand{\\ms}{\\overset{ms}{\\to}}\n\\newcommand{\\pto}{\\overset{p}{\\to}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\dto}{\\overset{d}{\\to}}\n\\newcommand{\\asim}{\\overset{a}{\\sim}}\n\n\n-   Multivariable Calculus\n    -   partial derivatives, the gradient, Jacobian matrix, Hessian matrix\n    -   optimization\n-   Linear Algebra\n    -   matrices and vectors\n    -   linear transformations\n    -   projections\n    -   PSD matrices\n-   Probability\n    -   random variables\n    -   distribution and density of RVs\n    -   expectation and variance \n    -   moments \n    -   common distributions\n        -   normal distribution\n        -   \"friends\" of the normal distribution: chi-squared, student's\n            t, F distribution\n-   Mathematical Statistics\n    -   Estimation\n    -   Hypothesis testing\n-   Basic [Real Analysis](https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf)\n    -   infimum and supremum \n    -   metric spaces \n    -   compact sets in $\\mathbb R$\n    -   mean value theorem\n    -   convergence of sequences\n    -   pointwise versus uniform convergence\n    -   Taylor series\n-   Basic Numerical Optimization \n    -   \"numerical\" vs. \"analytic\"\n\n## Probablitiy Theory\n\n\\indent We'll briefly go over the basics of probability theory. A rigorous treatment can be found in @durrett2019probability or @billingsley2008probability. For an even more general discussion, see @folland1999real, @royden1988real, and/or @rudin.\n\n\n::: {#def-}\nA [***measure space***]{style=\"color:red\"} is a triple $(X, \\mathcal F, \\mu)$ comprised of:\n\n1. A set $\\mathcal X$.\n2. A collection of subsets of $X$, $\\mathcal F\\subseteq 2^{X}$. This collection of sets satisfies the following properties: $A^c\\in \\mathcal F$ if $A\\in \\mathcal F$, and $\\cup_{\\alpha} A_\\alpha \\subseteq \\mathcal F$ for all countable collections of sets $\\{A_\\alpha\\}\\subseteq \\mathcal F$. Such a collection of sets is known as a [***sigma-algebra***]{style=\"color:red\"}.^[We cannot define measure over the entire power set $2^{\\mathcal X}$, or else we run into some technical problems.] \n3. A [***measure***]{style=\"color:red\"} $\\mu:\\mathcal F\\to [0,\\infty]$ satisfying $\\mu(\\emptyset) = 0$, and $\\mu(\\sum_\\alpha A_\\alpha)= \\sum_\\alpha \\mu(A_\\alpha)$ for all countable collections of *disjoint* sets $\\{A_\\alpha\\}\\subseteq \\mathcal F$.\n\nWe also refer to just the pair $(X,\\mathcal F)$ as a [***measurable space***]{style=\"color:red\"}.\n:::\n\n\n::: {#exm-}\n## Lebesgue Measure\nThe most important such space is used to measure subsets of the real line (and more generally euclidean spaces). If we want to measure subsets of $\\mathbb R$, we define the associated sigma-algebra as the collection of all compliments and countable unions of the open intervals (along with the subsequent sets generated), denoted $\\mathcal B(\\mathbb R) = \\{(a,b)\\subset \\mathbb R\\mid a,b\\in\\mathbb R\\}$.^[Anytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra] The measure of some interval $I = (a,b)\\subset \\mathbb R$ is defined as $$m(I)=b -a,$$ which is fairly reasonable. This measure is known as the Lebesgue measure.\n:::\n\n::: {#exm-}\n## Counting Measure\nAnother nice example of a measure space is $(X, 2^{X}, \\mu)$ where $$\\mu(A) = \\begin{cases}|A|& A\\text{ finite}\\\\ \\infty& A\\text{ infinite}\\end{cases}.$$ This measure simply assigns each set its cardinality, and is known as the counting measure.  \n:::\n\n       One of the main motivations of measure theory is to make the operation of integration more robust. We can do this by defining a property of functions that will make integration with respect to a measure possible. \n\n::: {#def-}\nLet $(X, \\mathcal F)$ and $(Y, \\mathcal G)$ be two measurable spaces. A function $f:X\\to Y$ is [***measurable***]{style=\"color:red\"} if \n\\begin{align*}\n& f^{-1}(G) \\in \\mathcal F & (\\forall G\\in\\mathcal G)\n\\end{align*}\nwhere $f^{-1}(G)$ is the preimage of a set $G\\in \\mathcal G$.\n:::\n\n       It turns out that any function that is Riemann integrable is measurable.^[Examples include continuous functions, monotonic functions, and functions that are discontinuous at a countable number of points.] Defining the integral of a measurable function with respect to a measure $\\mu$ requires some intermediate steps that make for a verbose definition. Instead of presenting the technical definition, we'll just introduce the notation:\n$$\\int_{[a,b]} f\\ d\\mu$$ This is the [***Lebesgue integral***]{style=\"color:red\"} of $f$ on $[a,b]\\subseteq \\mathcal F$ with respect to the measure $\\mu$. \n\n::: {#exm-}\n## Integration and Lebesgue Measure\nSuppose we have the measure space $(\\R, \\mathcal B(\\R), m)$. For a measurable function $f:[a,b]\\to\\R$ we have \n$$\\int_{[a,b]}f\\ dm = \\int_a^b f(x)\\ dx$$ where the later integral is the Riemann integral. Since they're equal, it's rare that we need to turn to tools beyond basic calculus to integrate functions.\n:::\n\n::: {#exm-}\n## Integration and Counting Measure\nSuppose we have a measure space $(\\mathbb N, 2^{\\mathbb N}, \\mu)$ where $\\mu$ is the counting measure. If we have some function $f:\\mathbb N\\to\\R$^[Technically this function is a real sequence, as a real sequence is just a mapping from the natural numbers to $\\R$.] and a set $A\\subset \\mathbb N$ on which we want to integrate $f$, then \n$$\\int_A f\\ d\\mu = \\sum_{x\\in A}f(x).$$ Proving why this is the case requires the formal definition of the Lebesgue integral, but the intuition shouldn't be anything new. An integral can be thought of as a continuous version of summation, so if we integrate on a discrete set we should just end up with a summation.\n:::\n\n       The fact that the Lebesgue integral reduces to summation demonstrates why it's such a powerful tool. \n \n      Given a probability space $(\\mathcal X, \\mathcal F, P)$, we can define a function $X:\\mathcal X\\to E$ where $E$ is some set equipped with a sigma-algebra $\\mathcal E$. This function is a **_random variable_** if the preimage of every set in $\\mathcal E$ is in $\\mathcal F$.^[More generally, such functions are called measurable.] \n$$ X^{-1}(I) \\in \\mathcal F \\ \\ \\ \\forall I\\in\\mathcal E.$$ In most cases, we're interested in random variables that take on real values (equipped with the $\\mathcal B(\\mathbb R)$) in which case $X:\\mathcal X\\to \\R$ and \n$$ X^{-1}(I) \\in \\mathcal F \\ \\ \\ \\forall I\\in\\mathcal B(\\R).$$ In other words, if we can measure $I$ using the Lebesgue measure $\\mu$, we are able to measure $X^{-1}(I)$ using the probability measure $P$. This allows us to define a probability measure $P(X^{-1}(I))$ on $\\mathcal B(\\mathbb R)$ which gives us the probability that $X$ is in the set $I\\subset \\mathcal B(\\mathbb R)$. This probability measure is the **_distribution_** of $X$. If $I = (-\\infty,x)$ for $x\\in\\mathcal X$, then the distribution takes the form $P(X^{-1}(I)) = P(X \\in (-\\infty,x)) = P(X\\le x)$. We usually work with the distribution in this form and call it the  **_distribution function_** $F_X(x) = P(X\\le x)$.\n      The chief motivation for defining measure spaces and measurable functions is a general theory of integration. The Riemann integral is achieved by taking the limit of a process which partitions the area under a function into rectangles. The width of these rectangles is the length of their base, which can be thought of as the measure of an interval. When performing Riemann integration, if $I=(a,b)$ is the base of a rectangle, we take the width to be $m(I)=b -a$. That is, we use the Lebesgue measure. This is why you will sometimes see integrals written as \n$$ \\int f(x)\\ dx = \\int f(x) \\ dm.$$ Writing the integral this way emphasize that we are integrating with respect to some measure (notion of length/volume), but it is more than just a notational difference. It is the result of defining integration in an entirely different way giving the **_Lebesgue integral (with respect to $m$)_**. In practice this won't matter a whole lot, because if we can calculate the Riemann integral of a function than we can calculate the Lebesgue integral, and both integrals are equal.^[The converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are *many* functions which are not Riemann integrable but are Lebesgue integrable.] What's important is that the idea of integration with respect to a measure can be generalized to any measure space $(X, \\mathcal N,\\mu)$. In general, the **_Lebesgue integral_** of $f$ over $A\\in \\mathcal N$ is written as $$\\int_A f \\ d\\mu.$$ The rigorous definition of this integral, and how it is calculated using that definition, aren't essential at the moment. That being said, one useful property to know is that if we integrate the constant $1$ on $A$ with respect to a measure $\\mu$, we get $\\mu(A)$. \n$$\\int_A 1\\ d\\mu = \\int_A d\\mu =\\mu(A)$$ This should seem familiar from calculus. If we have a set $I=(a,b)$, then $$\\int_a^b 1\\ dx = b - a$$ using Riemann integration. Lebesgue integration gives \n$$ \\int_I 1\\ dm = m(I) = b-a.$$ Interestingly, if we look at counting measure on the set of real numbers (our measure space is $(\\mathbb R, 2^{\\mathbb R}, \\mu)$), and some set $A = \\{a_1,\\ldots, a_n\\}\\in 2^{\\mathbb R}$ \n$$ \\int_Af\\ d\\mu = \\int_A 1\\ d\\mu = \\mu(A) = |A| = \\sum_{i=1}^n 1=\\sum_{i=1}^n f(a_i).$$ In general, integration with respect to the counting measure is a simple summation. This illustrates one nice consequence of the generality that Lebesgue integration provides -- summation is a special case of integration.^[But is this really new? The Riemann integral is just the limit of a sum.] \n      What happens if we apply Lebesgue integration to the distribution of a random variable $X$ defined on the sample space $\\mathcal X$? Well if we integrate $1$ with respect to some real interval $I=(a,b)\\subset \\mathbb R$, then \n$$ \\int _I 1\\ dP = P(I) = F(b) - F(a).$$ If we instead integrate over all $x\\in\\mathcal X$, we can think of the integral over the measure $P$ as a weighted sum of all $x$, where weights are given by $P$. This is just the **_expected value_** of $X$. $$\\E{X} = \\int_{\\mathcal X}x \\ dP = \\int_{\\mathcal X}x\\ dF_X$$ But how do we calculate this expectation? Even though this is a measure space over real numbers, we really don't know how to calculate integrals with respect to any measure besides Lebesgue measure. In the event that the distribution $F$ satisfies certain standard conditions, we can find some function $f_X$ such that \n$$ \\E{X} = \\int_{\\mathcal X}x\\ dF_X = \\int_{\\mathcal X} xf_X(x)\\ d\\mu = \\int_{\\mathcal X} x f_X(x)\\ dx.$$ This function is called the **_density function_** $f(x)$ of $X$, and is actually given as $$f_X = \\frac{dF_X}{dx}.$$ In a sense, the density function is a \"conversion rate\" between the measure $P$ and the measure $\\mu$.^[In general, this type of construction is called the **_Radon–Nikodym derivative_**, and its existence is outlined by the **_Radon–Nikodym theorem_**.] \n      But what about \"discrete\" random variables? The expectation of a discrete random variable is not an integral...or is it? Just like how the integral with respect to the counting measure reduces to a sum, if our sample space $\\mathcal X$ is countably infinite such that a random variable $X:\\mathcal X \\to \\mathbb R$ takes on one of a discrete number of values, then \n$$ \\E{X} = \\int_{\\mathcal X} x\\ dF_X = \\sum_{\\mathcal x\\in \\mathcal X}x f(x).$$\n\n       Definitions are readily extended to higher dimensions.  $$\n\nIf you couldn't care less about measure theory,^[It won't save your life in emergency situations.] then all this boils down to two facts:\n\n1. All results will be stated in terms of continuous random variables. If you want to show them for discrete random variables, just replace integrals with sums.\n2. Sometimes we will write expectation as $\\int x\\ dF_X$, which is the same as $$\\int xf(x)\\ dx.$$\n\n\n\n\n \n\n## Random Matrices and Vectors\n\n\n::: {#def-} \nA $m$ by $n$ <span style=\"color:red\">**_random matrix_**</span> $\\mathbb{X}$ is a matrix whose entries\nare $m\\times n$ random variables $X_{i,j}$, where\n$\\mathbb{X}_{i,j} = X_{i,j}$.\n\\begin{align*}\n\\mathbb{X}= \\begin{bmatrix}\nX_{11}&\\cdots& X_{1n}\\\\\\vdots&\\ddots&\\vdots\\\\X_{m1}&\\cdots& X_{mn}\n\\end{bmatrix}.\n\\end{align*}\nIn the event that either $m=1$ or $n=1$ we have a\n<span style=\"color:red\">**_random vector_**</span> $\\mathbf{X}$. \n\n:::\n\nWe will often want to write a random matrix $\\Xm$ as a collection of random vector $\\X$. Whether\nthese be column vectors or row vectors depends on the context. If $\\X$ is an $m\\times n$ random matrix\nwhere columns are indexed by $i$ and rows by $j$, we will take $\\X_j$ to be a column vector and $\\X_i$ to be a row vector.\n\\begin{align*}\n\\Xm &= \\begin{bmatrix}\\X_1 & \\cdots & \\X_i &\\cdots & \\X_n\\end{bmatrix}\\\\\n\\Xm &= \\begin{bmatrix}\\X_1 \\\\ \\vdots \\\\ \\X_j \\\\ \\vdots \\\\ \\X_m\\end{bmatrix}\n\\end{align*}\n\n::: {#def-} \nThe <span style=\"color:red\">**_expectation_**</span> of a random matrix $\\mathbb{X}$ is defined as\n\n\\begin{align*}\n\\E{\\Xm} = \\begin{bmatrix}\n\\text{E}[X_{11}]&\\cdots& \\text{E}[X_{1n}]\\\\\\vdots&\\ddots&\\vdots\\\\\\text{E}[X_{m1}]&\\cdots& \\text{E}[X_{mn}]\n\\end{bmatrix}.\n\\end{align*}\n\n:::\n\n\n::: {#prp-}\n\n## Properties of Expectation\nSuppose $\\mathbb{X}$ and $\\mathbb Y$ are $m\\times n$ random matrices,\n$\\mathbf A$ is a $\\ell\\times m$ matrix, $\\mathbf B$ is a $n\\times k$\nmatrix, and $c$ is a scalar. Then:\n\n1.  $\\text{E}\\left[\\mathbb{X}'\\right]= \\text{E}\\left[\\mathbb{X}\\right]'$\n2.  $\\text{E}\\left[c\\mathbb{X}\\right]=c\\text{E}\\left[\\mathbb{X}\\right]$\n3.  $\\text{E}\\left[\\mathbf A \\mathbb{X}\\mathbf B\\right] = \\mathbf A \\text{E}\\left[\\mathbb{X}\\right] \\mathbf B$\n4.  $\\text{E}\\left[\\mathbb{X}+ \\mathbb Y\\right] = \\text{E}\\left[\\mathbb{X}\\right] +\\text{E}\\left[\\mathbb Y\\right]$\n:::\n\n::: {#def-} \nThe <span style=\"color:red\">**_variance/covariance matrix_**</span> of a random column vector\n$\\mathbf{X}= [X_1, \\ldots, X_n]'$ is defined as\n$$ \\text{Var}\\left(\\mathbf{X}\\right) = \\text{E}\\left[(\\mathbf{X}- \\text{E}\\left[\\mathbf{X}\\right])(\\mathbf{X}- \\text{E}\\left[\\mathbf{X}\\right])'\\right]. $$\n:::\n\nThe definition of $\\text{Var}\\left(\\mathbf{X}\\right)$ can be rewritten\nin a much more approachable form:\n\n\\begin{align*}\n\\text{Var}\\left(\\mathbf{X}\\right) &= \\text{E}\\left[(\\mathbf{X}- \\text{E}\\left[\\mathbf{X}\\right])(\\mathbf{X}- \\text{E}\\left[\\mathbf{X}\\right])'\\right]\\\\\\\\\n  & = \\text{E}\\begin{bmatrix}\n(X_1 - \\text{E}\\left[X_1\\right])(X_1 - \\text{E}\\left[X_1\\right])&\\cdots& (X_1 - \\text{E}\\left[X_1\\right])(X_n - \\text{E}\\left[X_n\\right])\\\\\\vdots&\\ddots&\\vdots\\\\(X_n - \\text{E}\\left[X_n\\right])(X_1 - \\text{E}\\left[X_1\\right])&\\cdots& (X_n - \\text{E}\\left[X_n\\right])(X_n - \\text{E}\\left[X_n\\right])\\end{bmatrix}\\\\\\\\\n  & = \\begin{bmatrix}\n\\text{E}\\left[(X_1 - \\text{E}\\left[X_1\\right])^2\\right]&\\cdots& \\text{E}\\left[(X_1 - \\text{E}\\left[X_1\\right])(X_n - \\text{E}\\left[X_n\\right])\\right]\\\\\\vdots&\\ddots&\\vdots\\\\\\text{E}\\left[(X_n - \\text{E}\\left[X_n\\right])(X_1 - \\text{E}\\left[X_1\\right])\\right]&\\cdots& \\text{E}{(X_n - \\text{E}\\left[X_n\\right])^2}\n\\end{bmatrix}\\\\\\\\& = \\begin{bmatrix}\n\\text{Var}\\left(X_1\\right)&\\cdots& \\text{Cov}\\left(X_1, X_n\\right)\\\\\\vdots&\\ddots&\\vdots\\\\\\text{Cov}\\left(X_n, X_1\\right)&\\cdots& \\text{Var}\\left(X_n\\right)\n\\end{bmatrix}\n\\end{align*}\n\nThe diagonal entries capture the dispersion of each random variable\n$X_i$ while the off diagonal entries correspond to the joint dispersion\nof all possible pairs $(X_i,X_j)$.\n\n::: {#def-} \nThe <span style=\"color:red\">**_covariance matrix_**</span> of random column vectors\n$\\mathbf{X}= [X_1, \\ldots, X_n]'$ and $\\mathbf{Y}= [Y_1, \\ldots, Y_n]'$\\\n$$ \\text{Cov}\\left(\\mathbf{X},\\mathbf{Y}\\right) = \\text{E}\\left[(\\mathbf{X}- \\text{E}\\left[\\mathbf{X}\\right])(\\mathbf{Y}- \\text{E}\\left[\\mathbf{Y}\\right])'\\right]. $$\n:::\n\nAlternatively, we can write the covariance matrix between two random\nvectors as:\n\n\\begin{align*}\n\\text{Cov}\\left(\\mathbf{X},\\mathbf{Y}\\right)=\\begin{bmatrix}\n\\text{Cov}\\left(X_1, Y_1\\right)&\\cdots& \\text{Cov}\\left(X_1, Y_n\\right)\\\\\\vdots&\\ddots&\\vdots\\\\\\text{Cov}\\left(X_n, Y_1\\right)&\\cdots& \\text{Cov}\\left(X_n,Y_n\\right)\n\\end{bmatrix} \n\\end{align*}\n\n::: {#prp-}\n\n## Properties of Variance/Covariance\n\nSuppose $\\mathbf{X}_1$ and $\\mathbf{X}_2$ are random column vectors of length $n$, $\\mathbf A$ and $\\mathbf B$ are matrices with $n$ columns, and $\\mathbf c$ and $\\mathbf d$ are column vectors of length $n$. Then:\n\n1.  $\\text{Var}\\left(\\mathbf A\\mathbf{X}+\\mathbf c\\right)= \\mathbf A \\text{Var}\\left(\\mathbf{X}\\right) \\mathbf A'$\n2.  $\\text{Cov}\\left(\\mathbf A\\mathbf{X}+\\mathbf b, \\mathbf B\\mathbf{y}+\\mathbf c\\right) = \\mathbf A \\text{Cov}\\left(\\mathbf{X},\\mathbf{Y}\\right)\\mathbf B'$\n\n:::\n\nThe first part of this proposition is a generalization of the familiar result that\n$\\text{Var}\\left(aX + b\\right)= a^2\\text{Var}\\left(x\\right)$.\n\n## Multivariate Normal Distribution\n\nRecall that if $X\\sim N(\\mu,\\sigma^2)$, then any linear function of $X$\nis also normally distributed. To be precise, if $Y=aX +b$, then\n$Y\\sim N(a\\mu +b,a^2 \\sigma^2)$. This useful properties generalizes to\nrandom vectors.\n\n::: {#prp-}\nSuppose $\\mathbf{X}\\sim N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})$. Then\n$$ \\mathbf A \\mathbf{X}+ \\mathbf b \\sim N(\\mathbf A\\boldsymbol\\mu +\\mathbf b, \\mathbf A\\boldsymbol{\\Sigma}\\mathbf A')$$\nwhere $\\mathbf A$ and $\\mathbf b$ are a matrix and vector with\ncompatible dimensions.\n:::\n\n## Conditional Expectation and Independence\n\nAs put by @wooldridge2010econometric:\n\n> A substantial portion of research in econometric methodology can be\n> interpreted as finding ways to estimate conditional expectations in\n> the numerous settings that arise in economic applications.\n\nFor this reason, properties related to conditional expectation will\nprove useful time and time again. The first of these is the law of\niterated expectation.\n\n::: theorem\nSuppose $X$ and $Y$ are random variables, then\n$$\\text{E}\\left[\\text{E}\\left[X\\mid Y\\right]\\right]= \\text{E}\\left[X\\right]$$\n:::\n\nConditional expectation is related to the idea of independence. As one\nmay remember from a probability course, if $X$ and $Y$ are independent\nrandom variables, then\n$\\text{E}\\left[X\\mid Y\\right]= \\text{E}\\left[\\mathbf{X}\\right]$. But is the converse\ntrue? As it turns out, the converse does not hold, so we have multiple\nnotions of \"independence\".\n\n::: {#def-} \nSuppose $X$ and $Y$ are random variables with densities $f_X(x)$ and\n$f_Y(y)$, respectively, and a joint density of $f_{X,Y}(x,y)$. $X$ and\n$Y$ are <span style=\"color:red\">**_independent_**</span>, written as $X\\perp Y$, if\n$$f_{X,Y}(x,y)=f_X(x)f_Y(y).$$\n:::\n\n::: {#def-} \nSuppose $X$ and $Y$ are random variables. $X$ and $Y$ are <span style=\"color:red\">**_mean\nindependent_**</span> if $$\\text{E}\\left[X\\mid Y\\right]= \\text{E}\\left[X\\right].$$\n:::\n\n::: {#def-} \nSuppose $X$ and $Y$ are random variables. $X$ and $Y$ are\n<span style=\"color:red\">**_uncorrelated_**</span> if\n$$\\text{E}\\left[XY\\right]= \\text{E}\\left[Y\\right]\\text{E}\\left[X\\right],$$ which is equivalent to $\\text{Cov}\\left(X,Y\\right) = 0$.\n:::\n\nOur final result in this brief section of review related these three\ndefinitions.\n\n:::{#thm-}\nSuppose $X$ and $Y$ are random variables. If $X \\perp Y$, then $X$ and $Y$ are mean independent. In turn, if $X$ and $Y$ are mean independent, then $X$ and $Y$ are uncorrelated.\n:::\n\n:::{.proof}\n\nIf $X \\perp Y$, then \\begin{align*}\n\\text{E}\\left[X\\mid Y\\right] & = \\int x f_{X\\mid Y}(s\\mid t)\\ dx \\\\\n              & = \\int x \\frac{f_{X,Y}(x,y)}{f_{Y}(y)}\\ dx & (\\text{def. of conditional probability}) \\\\\n              & = \\int x \\frac{f_X(x)f_Y(y)}{f_{Y}(y)}\\ dx & (X \\perp Y) \\\\\n              & = \\int x f_X(x)\\ dx \\\\\n              & = \\text{E}\\left[X\\right].\n\\end{align*} In turn, if $X$ and $Y$ are mean independent, then\n\\begin{align*}\n\\text{E}\\left[XY\\right] & = \\text{E}\\left[\\text{E}\\left[XY\\mid Y\\right]\\right] & (\\text{Law of Iterated Expectations})\\\\\n       & = \\text{E}\\left[YE\\left[X\\mid Y\\right]\\right] & (Y\\text{ is constant}) \\\\\n       & = \\text{E}\\left[YE\\left[X\\right]\\right] & (\\text{mean independence}) \\\\\n       & = \\text{E}\\left[Y\\right]\\text{E}\\left[X\\right] & (\\text{E}\\left[X\\right]\\text{ is a constant})\n\\end{align*}\n\n:::\n\n## Existence of Expectation\n\nIt's possible that the expectation of a random variable does not exist. It may be the case that \n$$ \\int_{\\mathcal X}x\\ dF_X \\not< \\infty,$$ in which case we cannot assign an expected value to $X$. While this is theoretically interesting, the applications where we need to be careful assuming $\\E{X}$ exists are few and far between. **_We will always assume the expectation, and relevant higher moments, of random variables exist_**. This is done for expositional ease, and to emphasize other assumptions that tend to be much more important.  \n\n## Code\n\nThe majority of examples and simulations will be done in R, mostly because I like R, writing code in RStudio, and the ```tidyverse```. That being said, once we get to topics in machine learning, I'll switch over to Python. Python's ```scikit-learn``` is much more comprehensive than its counterparts in R, and it seems that most machine learning tutorials use Python. I suspect this is because machine learning is applied very often in \"industry\" where code needs to be production ready and implementable by engineers, something Python affords that R does not. Machine learning tools like TensorFlow and Keras are also built with Python in mind. Eventually I may include a section on scientific computing and optimization, in which case I may use Julia. For *excellent* examples of economic modelling in Julia (and Python), see [QuantEcon](https://quantecon.org/). \n\nTwo other softwares/languages that are popular in (academic) economics are Stata and MATLAB. These tools share in one major drawback -- they are not open source. While not as flexible as R or Python, Stata is particularly well-suited for basic econometrics. For example, I find it much easier to work with panel data and basic time series in Stata than in R. A great overview of econometric theory using Stata is provided by @cameron2010microeconometrics (which is a less technical companion to @cameron2005microeconometrics).\n\n## Organization \n\nThe organization of sections roughly follows a handful of econometrics courses I took while at Boston College. \n\n### Part I - Statistics  {-}\n\nWe'll start with a review of mathematical statistics presented at the level of @lehmann2006theory of @lehmann2005testing.^[These are the two standard references used for a PhD-level statistical theory course.] While the concepts will likely be familiar, they may seem a bit more technical when defined in the context of statistical decision theory. The focus is entirely on frequentist statistics, as Bayesian statistics will be covered later on.\n\n2. **Finite Sample Properties of Estimators**: We consider the problem of estimation, and give formal definitions and related notation to nebulous concepts such as: models, parametrizations, identification, statistics, parametric, semi-parametric, non-parametric, and estimators. Then we consider how to assess estimators for a fixed sample size. \n3. **Asymptotic Properties of Estimators**: In most cases, we won't be able to determine finite sample properties of estimators, so we consider the situation where $n\\to \\infty$. Familiar results like the central limit theorem (CLT) and law of large numbers (LLN) will be discussed, but we'll also introduce a handful of essential results (the continuous mapping theorem, Slutsky's theorem, the delta method) that will enable us to use the CLT and LLN to determine the asymptotic behavior of almost every estimator we will consider.   \n4. **Hypothesis Testing**: The problem of estimation is only one part of statistics. The other is inference. We give an overview of inference and hypothesis testing using the Neyman-Pearson framework, and then consider how inference in relation to asymptotics. Two large sample tests (the Wald test and $t-$test) are covered. \n5. **Exponential Families**: A quick treatment of a special type of probability distribution is given. Familiarity with these distributions is not necessary as far as econometrics is concerned, but a cursory understanding will make it possible to illuminate some cool connections between statistics and econometrics. In particular, we'll see exponential families come up when considering maximum likelihood estimation (MLE), generalized linear models (GLMs), and Bayesian estimation. \n\n### Part II - Linear Models  {-}\n\nThe foundation of econometrics is *the* linear model. In this section, we build the classical linear model from scratch adding assumptions as needed until arriving at the Gauss-Markov theorem. Then we \"take the model apart\" by dropping assumptions, and determine how to suitably estimate the subsequent linear models.\n\n6. **Classical Regression Model**: This section is a lengthy treatment of the classic linear model and estimation via ordinary least squares (OLS)\n7. **Endogeneity I**\n8. **Endogeneity II**\n9. **Generalized Least Squares**\n\n### Part III - Estimation Framework  {-}\n\n11. **Extremum Estimators** \n12. **The Generalized Method of Moments** \n13. **Maximum Likelihood Estimation**\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"index.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}