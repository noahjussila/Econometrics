<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanved Econometrics with Examples - 7&nbsp; Extremum Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./binary.html" rel="next">
<link href="./endog.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#definition-and-identification" id="toc-definition-and-identification" class="nav-link active" data-scroll-target="#definition-and-identification"><span class="toc-section-number">7.1</span>  Definition and Identification</a></li>
  <li><a href="#consistency" id="toc-consistency" class="nav-link" data-scroll-target="#consistency"><span class="toc-section-number">7.2</span>  Consistency</a></li>
  <li><a href="#extremum-identification" id="toc-extremum-identification" class="nav-link" data-scroll-target="#extremum-identification"><span class="toc-section-number">7.3</span>  Extremum Identification</a></li>
  <li><a href="#asymptotic-normality" id="toc-asymptotic-normality" class="nav-link" data-scroll-target="#asymptotic-normality"><span class="toc-section-number">7.4</span>  Asymptotic Normality</a></li>
  <li><a href="#the-hypothesis-testing-trinity" id="toc-the-hypothesis-testing-trinity" class="nav-link" data-scroll-target="#the-hypothesis-testing-trinity"><span class="toc-section-number">7.5</span>  The Hypothesis Testing “Trinity”</a></li>
  <li><a href="#m-estimators" id="toc-m-estimators" class="nav-link" data-scroll-target="#m-estimators"><span class="toc-section-number">7.6</span>  M-Estimators</a>
  <ul class="collapse">
  <li><a href="#nonlinear-least-squares" id="toc-nonlinear-least-squares" class="nav-link" data-scroll-target="#nonlinear-least-squares"><span class="toc-section-number">7.6.1</span>  Nonlinear Least Squares</a></li>
  <li><a href="#least-absolute-deviations" id="toc-least-absolute-deviations" class="nav-link" data-scroll-target="#least-absolute-deviations"><span class="toc-section-number">7.6.2</span>  Least Absolute Deviations</a></li>
  </ul></li>
  <li><a href="#minimum-distance-estimators" id="toc-minimum-distance-estimators" class="nav-link" data-scroll-target="#minimum-distance-estimators"><span class="toc-section-number">7.7</span>  Minimum Distance Estimators</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap"><span class="toc-section-number">7.8</span>  Recap</a></li>
  <li><a href="#examplereplication" id="toc-examplereplication" class="nav-link" data-scroll-target="#examplereplication"><span class="toc-section-number">7.9</span>  Example/Replication</a></li>
  <li><a href="#futher-reading" id="toc-futher-reading" class="nav-link" data-scroll-target="#futher-reading"><span class="toc-section-number">7.10</span>  Futher Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell" data-hash="extremum_cache/html/unnamed-chunk-1_f9f1b090248def368602d779554183d5">

</div>
<p>The estimation of linear models turned out to be fairly straightforward, as all our estimators were somehow related to OLS. When moving beyond linear models, we need to consider more general approaches to estimation, such as the generalized method of moments (GMM) or maximum likelihood estimation (MLE). Both of these estimators fall into a very broad class of estimators formalized by <span class="citation" data-cites="takeshi1985advanced">Amemiya (<a href="references.html#ref-takeshi1985advanced" role="doc-biblioref">1985</a>)</span> known as extremum estimators. We’ll establish the properties of this broad class of estimators before considering special cases in Section @ref(generalized-method-of-moments) and Section @ref(maximum-likelihood-estimation). A great deal of this will be based off of <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span>, and requires comfort with <a href="https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf">real analysis</a>. It may be helpful to review the relationship between continuity and limits, sequences of functions, properties of uniform convergence, compactness (and how it relates to extrema), convexity (in the context of sets and functions), and the mean value theorem as it applies to gradients and Hessians.</p>
<section id="definition-and-identification" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="definition-and-identification"><span class="header-section-number">7.1</span> Definition and Identification</h2>
<p>As the name implies, an extremum estimator is one that is defined via optimization.</p>
<div class="definition">
<p>Suppose <span class="math inline">\(\mathbb{W}= [\mathbf{W}_1,\ldots, \mathbf{W}_n]' \sim P_\boldsymbol{\theta}\)</span> where <span class="math inline">\(P_\boldsymbol{\theta}\in\mathcal P\)</span> for a known model <span class="math inline">\(\mathcal P\)</span>. An <span style="color:red"><strong><em>extremum estimator</em></strong></span> <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} :\mathcal W\to \boldsymbol{\Theta}\)</span> is an estimator of the form <span class="math display">\[ \hat{\boldsymbol\theta}_\text{EX} = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}\mid \mathbb{W})\]</span> for an objective function <span class="math inline">\(Q_n(\boldsymbol{\theta}\mid \mathbb{W})\)</span> which depends on realized observations and sample size. A more general definition defines <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> as the value which satisfies <span class="math display">\[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) \ge \sup_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) + o_p(1).\]</span></p>
</div>
<p>We’ve written <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> as the argument which maximizes an objective function, but this implicitly includes the case where <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is the minimizing argument of a function, as <span class="math display">\[ \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) = \mathop{\mathrm{argmin}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} [-Q_n(\boldsymbol{\theta})].\]</span> The idea that an estimate should arise from an optimization problem should feel natural. We generally want an estimator to be “better” than other estimators, and extremum estimators achieve this in terms of some criterion <span class="math inline">\(Q_n(\boldsymbol{\theta})\)</span>.</p>
<div class="example">
<p>The estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is an extremum estimator. <span class="math display">\[\hat{\boldsymbol\beta}_\text{OLS} = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}= \mathop{\mathrm{argmin}}_{\mathbf b \in \mathbb R^k} \sum_{i=1}^n (Y_i - \mathbf{X}_i\mathbf b)^2 = \mathop{\mathrm{argmax}}_{\mathbf b \in \mathbb R^k}\left[ - \sum_{i=1}^n (Y_i - \mathbf{X}_i\mathbf b)^2\right] = \mathop{\mathrm{argmax}}_{\mathbf b \in \mathbb R^k} -\left\lVert\mathbf{Y}- \mathbb{X}\mathbf b\right\rVert^2\]</span> We could also maximize any monotonic transformation of this, giving way to a few other common objectives: <span class="math display">\[\begin{align*}
Q_n(\boldsymbol{\beta}) = -\sum_{i=1}^n (Y_i - \mathbf{X}_i\boldsymbol{\beta})^2 \propto -\frac{1}{n} \sum_{i=1}^n (Y_i - \mathbf{X}_i\boldsymbol{\beta})^2 \propto -\frac{1}{2} \sum_{i=1}^n (Y_i - \mathbf{X}_i\boldsymbol{\beta})^2 = -\frac{1}{2}\left\lVert\mathbf{Y}- \mathbb{X}\mathbf b\right\rVert^2
\end{align*}\]</span></p>
</div>
<p>Before we consider the properties of extremum estimators and give a handful of example, we need to address a major problem from optimization that could plague us. If asked to calculate <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, it’s tempting to inspect the first order condition <span class="math display">\[\frac{\partial Q_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \mathbf{0}.\]</span> Not only does this assume <span class="math inline">\(Q_n\)</span> is differentiable (which it may not be), we may find that the first order condition has several solutions, even if there is a unique maximum. Furthermore that unique maximum could be local and not global. To avoid these complications, we’ll think about <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> as a global maximum, and not a solution to a first order condition.</p>
<div class="hypothesis" name="Numerical Optimization">
<p>In most non-trivial situations, extremum estimators will not have an analytic form, so we need to turn to numerical methods to solve the requisite optimization problem. Numerical optimization is a behemoth subject spanning applied math, computer science, operations research, and engineering. Most standard econometrics references dedicate some time to the subject:</p>
<ul>
<li>Chapter 10 of <span class="citation" data-cites="cameron2005microeconometrics">Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span></li>
<li>Section 7.5 of <span class="citation" data-cites="hayashi2011econometrics">Hayashi (<a href="references.html#ref-hayashi2011econometrics" role="doc-biblioref">2011</a>)</span></li>
<li>Section 12.7 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span></li>
<li>Appendix E of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span></li>
<li>Chapter 12 of <span class="citation" data-cites="hansen2022probability">B. E. Hansen (<a href="references.html#ref-hansen2022probability" role="doc-biblioref">2022</a>)</span></li>
</ul>
<p>A more comprehensive treatment is <span class="citation" data-cites="judd1998numerical">Judd (<a href="references.html#ref-judd1998numerical" role="doc-biblioref">1998</a>)</span>.</p>
</div>
</section>
<section id="consistency" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="consistency"><span class="header-section-number">7.2</span> Consistency</h2>
<p>Despite the definition of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> being wildly general, we can establish that <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}\)</span> under certain conditions. The only defining features of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> are the function being maximized (<span class="math inline">\(Q_n\)</span>), and the space over which it is being maximized (<span class="math inline">\(\boldsymbol{\Theta}\)</span>), so we should expect that we’ll need to impose some conditions on one of (if not both) of these objects. Let’s start by writing the definition of convergence in this case. <span class="math display">\[ \mathop{\mathrm{plim}}\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) = \boldsymbol{\theta}_0\]</span></p>
<p>The fact that we are dealing with the limiting process of <span class="math inline">\(\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta})\)</span> instead of <span class="math inline">\(Q_n\)</span> complicates things. Limiting processes of function do not play well with many of our favorite operators.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="example">
<p>Suppose <span class="math inline">\(\Theta = [-1,\infty]\)</span> and <span class="math inline">\(\theta_0 = -1/2\)</span>. Define functions <span class="math inline">\(Q\)</span>, <span class="math inline">\(g_n\)</span>, and <span class="math inline">\(Q_n\)</span> as: <span class="math display">\[\begin{align*}
Q(\theta) &amp;= \begin{cases}1+\theta &amp; -1\le\theta\le\theta_0 \\ -\theta &amp; \theta_0&lt;\theta\le 0 \\ 0 &amp; \text{otherwise}  \end{cases}\\
g_n(\theta) &amp;= \begin{cases}\theta - n &amp; n\le\theta\le n + 1 \\ n + 2 - \theta &amp; n + 1\le\theta\le n + 2 \\ 0 &amp; \text{otherwise} \end{cases}\\
Q_n(\theta) &amp; = Q(\theta) + g_n(\theta)
\end{align*}\]</span> These functions don’t contain random variables, so the <span class="math inline">\(\mathop{\mathrm{plim}}\)</span> operator coincides with <span class="math inline">\(\lim_{n\to\infty}\)</span> The apparently nonsense function <span class="math inline">\(Q_n\)</span> is actually defined such that we can determine it’s limit and maximum by inspecting a plot.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="extremum_cache/html/unnamed-chunk-2_eba640aa3c53aae73627bc9e419cfff3">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="extremum_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It appears that <span class="math inline">\(\mathop{\mathrm{plim}}Q_n = Q\)</span>, as the height of the the “triangle” formed on <span class="math inline">\(\theta \in [n,n+2]\)</span> shrinks (this is the the contribution of <span class="math inline">\(g_n/n\)</span>), leaving the triangle formed by <span class="math inline">\(f\)</span>. Formally, <span class="math display">\[ \mathop{\mathrm{plim}}Q_n(\theta) = \mathop{\mathrm{plim}}Q(\theta) + \mathop{\mathrm{plim}}g_n(\theta) = Q(\theta)  + 0 = Q(\theta).\]</span> If we compare the limit of the <span class="math inline">\(\mathop{\mathrm{argmax}}\)</span> and the <span class="math inline">\(\mathop{\mathrm{argmax}}\)</span> of the limit, we have <span class="math display">\[\begin{align*}
\mathop{\mathrm{plim}}\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) &amp; = \mathop{\mathrm{plim}}n + 1 = \infty\\
\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} \mathop{\mathrm{plim}}Q_n(\boldsymbol{\theta}) &amp; = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} f = \theta_0
\end{align*}\]</span></p>
</div>
<p>One reason this happens, is because the nature in which <span class="math inline">\(Q_n\)</span> converges. While we have <span class="math inline">\(\mathop{\mathrm{plim}}Q_n(\boldsymbol{\theta}) = Q(\boldsymbol{\theta})\)</span> for all <span class="math inline">\(\boldsymbol{\theta}\in \boldsymbol{\Theta}\)</span>, the function does not converge uniformly in the sense that <span class="math inline">\(Q_n\)</span> does not approaches <span class="math inline">\(Q\)</span> on all of <span class="math inline">\(\boldsymbol{\Theta}\)</span> simultaneously. We need to strengthen our definition of convergence in probability to insure this happens.</p>
<div class="definition">
<p>A sequence of functions random variables <span class="math inline">\(f_n(X)\)</span> <span style="color:red"><strong><em>converges in probability uniformly (on <span class="math inline">\(\mathcal X\)</span>)</em></strong></span> to a function <span class="math inline">\(f\)</span> if for all <span class="math inline">\(\varepsilon &gt; 0\)</span> and <span class="math inline">\(\delta &gt; 0\)</span> there exists a fixed <span class="math inline">\(N\)</span> independent of <span class="math inline">\(X_n\)</span> such that <span class="math display">\[ \Pr(\left\lvert f_n(X) - f\right\rvert &gt; \varepsilon,\ \forall X \in\mathcal X) &lt; \delta,\]</span> which is equivalent to <span class="math display">\[ \lim_{n\to\infty}\Pr(\left\lvert f_n(X) - f\right\rvert &lt; \varepsilon,\ \forall X \in\mathcal X) = 0  \]</span>. Yet another definition is <span class="math display">\[\sup_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}\left\lvert f_n(X) - f\right\rvert \overset{p}{\to}0.\]</span></p>
</div>
<p>The definition using the supremum follows from the fact that if the maximum distance between <span class="math inline">\(f_n(X)\)</span> and <span class="math inline">\(f\)</span> shrinks as <span class="math inline">\(n\to\infty\)</span>, then is must be the case that the distance between <span class="math inline">\(f_n\)</span> and <span class="math inline">\(f\)</span> at any point in the support is shrinking as well. So let’s assume that <span class="math inline">\(Q_n\)</span> converges uniformly to some limit <span class="math inline">\(Q_0\)</span>. Let’s look at another example, but this time define <span class="math inline">\(Q_n\)</span> such that it converges uniformly to some <span class="math inline">\(Q_0\)</span>.</p>
<div class="example">
<p>Suppose <span class="math inline">\(\Theta = [0,\theta_0]\)</span> and <span class="math inline">\(Q_n(\theta) = \theta/n\)</span>. Once again, there are no random variables in the picture, so <span class="math inline">\(\mathop{\mathrm{plim}}\)</span> reduces to <span class="math inline">\(\lim_{n\to\infty}\)</span>. We have <span class="math display">\[ \sup_{\theta \in [0,1]}\left\lvert\theta/n - 0\right\rvert = 1/n \to 0,\]</span> so <span class="math inline">\(Q_n(\theta)\)</span> converges uniformly to <span class="math inline">\(Q_0(\theta) = 0\)</span> on <span class="math inline">\([0,\theta_0]\)</span>. The maximum of <span class="math inline">\(Q_n\)</span> is achieved at <span class="math inline">\(\theta_0\)</span> for all <span class="math inline">\(n\)</span>. Despite the uniform convergence, we have <span class="math display">\[\begin{align*}
\mathop{\mathrm{plim}}\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) &amp; = \mathop{\mathrm{plim}}\theta_0 = \theta_0\\
\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} \mathop{\mathrm{plim}}Q_n(\boldsymbol{\theta}) &amp; = \mathop{\mathrm{argmax}}_{\theta \in[0,\theta_0]} Q_0(\theta) = \mathop{\mathrm{argmax}}_{\theta \in[0,1]} 0 = [0,\theta_0]
\end{align*}\]</span></p>
</div>
<p>We need to rule out situations where the limit of <span class="math inline">\(Q_n\)</span> does not have a unique maximum, or that unique maximum is not achieved at the true value <span class="math inline">\(\boldsymbol{\theta}_0\)</span>. Finally, consider a third example.</p>
<div class="example">
<p>Suppose <span class="math inline">\(\Theta = [-2,1]\)</span> and <span class="math inline">\(\theta_0 = -1\)</span>. Define <span class="math display">\[Q_n(\theta) = \begin{cases}(1-2^{1-n})\theta + 2 - 2^{2-n} &amp; -2\le \theta\le -1
\\(2^{1-n}-1)\theta &amp; -1 &lt; \theta \le 0 \\ \frac{2^{n+1}-2}{2^{n+1}-1}\theta &amp; 0  &lt; \theta\le 1-2^{1-n}\\ \frac{2^{n+1}-2}{2^{n+1}-1}\theta + 2 - 2^{2-n} &amp; 1-2^{1-n}\le \theta &lt; 1\\0 &amp; \theta = 1\end{cases}\]</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="extremum_cache/html/unnamed-chunk-3_cbc1f45cb2e95bb014c2d03827c017fc">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="extremum_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>By inspecting <span class="math inline">\(Q_n\)</span> for <span class="math inline">\(n = 1,\ldots, 6\)</span>, we see that <span class="math display">\[\begin{align*}
\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} \mathop{\mathrm{plim}}Q_n(\boldsymbol{\theta}) &amp; = \mathop{\mathrm{plim}}\theta_0 = \theta_0,\\
\mathop{\mathrm{plim}}\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}  Q_n(\boldsymbol{\theta}) &amp; = 1.
\end{align*}\]</span> The limit <span class="math inline">\(Q_0\)</span> has a discontinuity at the point where <span class="math inline">\(\mathop{\mathrm{plim}}\mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}) = 1\)</span>. If we wanted to insure that <span class="math inline">\(Q_0\)</span> is continuous on all of <span class="math inline">\(\boldsymbol{\Theta}\)</span>, we could redefine <span class="math inline">\(\boldsymbol{\Theta}= [-2,1)\)</span>. Unfortunately, this would mean that the limit of the sequence formed by _{} Q_n() converges to a point outside of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
</div>
<p>This final examples shows that things can go wrong when our function <span class="math inline">\(Q_0\)</span> is not continuous, or when <span class="math inline">\(\boldsymbol{\Theta}\)</span> does not contain the limit of the maximized objective. In this example the limit in question was not in <span class="math inline">\(\boldsymbol{\Theta}\)</span> because <span class="math inline">\(\boldsymbol{\Theta}\)</span> did not contain all its limit points, i.e it isn’t a closed set. Something like this could also happen if <span class="math inline">\(\boldsymbol{\Theta}\)</span> is unbounded and the limit diverges. To ensure that this limit is always in <span class="math inline">\(\boldsymbol{\Theta}\)</span> we need <span class="math inline">\(\boldsymbol{\Theta}\)</span> to be compact.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> If <span class="math inline">\(\boldsymbol{\Theta}\subset \mathbb R^k\)</span>, then this is equivalent to being closed and bounded.</p>
<p>We now have all the building blocks required to show that <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is consistent: continuity of <span class="math inline">\(Q_0\)</span>,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math inline">\(Q_n \overset{p}{\to}Q_0\)</span> uniformly, <span class="math inline">\(\boldsymbol{\Theta}\)</span> compact, and <span class="math inline">\(Q_0(\boldsymbol{\theta})\)</span> is uniquely maximized at <span class="math inline">\(\boldsymbol{\Theta}_0\)</span>. There are a few ways to prove this result, but I’ll follow <span class="citation" data-cites="takeshi1985advanced">Amemiya (<a href="references.html#ref-takeshi1985advanced" role="doc-biblioref">1985</a>)</span>. The first proof seems to be due to <span class="citation" data-cites="amemiya1973regression">Amemiya (<a href="references.html#ref-amemiya1973regression" role="doc-biblioref">1973</a>)</span>. Other great proofs are given by econometrician Xiaoxia Shi in <a href="https://www.ssc.wisc.edu/~xshi/econ715/Lecture_3_consistency.pdf">these notes</a>, and by <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span>.</p>
<div id="excon" class="theorem" name="Consistency of Extremum Estimators I">
<p>Suppose there exists some (non-stochastic) function <span class="math inline">\(Q_0(\boldsymbol{\theta})\)</span> such that:</p>
<ol type="1">
<li><span class="math inline">\(\boldsymbol{\theta}_0 = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}Q_0(\boldsymbol{\theta})\)</span>;</li>
<li><span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact;</li>
<li><span class="math inline">\(Q_0(\boldsymbol{\theta})\)</span> is continuous on <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
<li><span class="math inline">\(Q_n(\boldsymbol{\theta})\)</span> converges uniformly in probability to <span class="math inline">\(Q_0(\boldsymbol{\theta})\)</span>.</li>
</ol>
<p>Then <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(N_r(\boldsymbol{\theta}_0)\)</span> be the open neighborhood/ball centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span> with radius of <span class="math inline">\(r &gt; 0\)</span>, where <span class="math inline">\(r\)</span> is arbitrary. The neighborhood <span class="math inline">\(N_r(\boldsymbol{\theta}_0)\)</span> is not necessarily a subset of <span class="math inline">\(\boldsymbol{\Theta}\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> If <span class="math inline">\(N^c_r(\boldsymbol{\theta}_0)\)</span> is the compliment of <span class="math inline">\(N\)</span>, then <span class="math inline">\(N^c_r(\boldsymbol{\theta}_0)\)</span> is closed, and <span class="math inline">\(N^c_r(\boldsymbol{\theta}_0) \cap \boldsymbol{\Theta}\subset \boldsymbol{\Theta}\)</span> is compact.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Define <span class="math display">\[\boldsymbol{\theta}^* = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in N^c_r(\boldsymbol{\theta}_0) \cap \boldsymbol{\Theta}} Q_0(\boldsymbol{\theta}).\]</span> The point <span class="math inline">\(\boldsymbol{\Theta}^*\)</span> is guaranteed to exists because <span class="math inline">\(Q_0\)</span> is a continuous function and <span class="math inline">\(N^c_r(\boldsymbol{\theta}_0)\cap \boldsymbol{\Theta}\)</span> is a compact set. Define <span class="math inline">\(\varepsilon = Q_0(\boldsymbol{\theta}_0) - \boldsymbol{\theta}^*\)</span>, and let <span class="math inline">\(A_n\)</span> be the event <span class="math inline">\(``|Q_n(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &lt; \varepsilon/2\)</span> for all <span class="math inline">\(\boldsymbol{\theta}"\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Event <span class="math inline">\(A_n\)</span> holds <em>for all</em> <span class="math inline">\(\boldsymbol{\theta}\)</span>, including <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> and <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, so: <span class="math display">\[\begin{align}
&amp; |Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_0(\hat{\boldsymbol\theta}_\text{EX} )| &lt; \varepsilon/2 \\
\implies &amp; Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &lt; \varepsilon/2\\
\implies &amp;  Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &gt; Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - \varepsilon/2 (\#eq:a1)\\\\
&amp; |Q_n(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0)| &lt; \varepsilon/2 \\
\implies &amp; Q_n(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0) &lt; -\varepsilon/2\\
\implies &amp;  Q_n(\boldsymbol{\theta}_0) &gt; Q_0(\boldsymbol{\theta}_0) - \varepsilon/2 (\#eq:a2)
\end{align}\]</span> By the definition of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, <span class="math inline">\(Q_n(\hat{\boldsymbol\theta}_\text{EX} ) \ge Q_n(\boldsymbol{\theta}_0)\)</span>. If we combine this with @ref(eq:a1), we have <span class="math display">\[\begin{equation}
Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &gt; Q_n(\boldsymbol{\theta}_0) - \varepsilon/2 (\#eq:a3).
\end{equation}\]</span> If we add inequalities @ref(eq:a2) and @ref(eq:a3), event <span class="math inline">\(A_n\)</span> implies <span class="math display">\[\begin{align*}
&amp;Q_0(\hat{\boldsymbol\theta}_\text{EX} )  + Q_n(\boldsymbol{\theta}_0) &gt; Q_n(\boldsymbol{\theta}_0) - \varepsilon/2 + Q_0(\boldsymbol{\theta}_0) - \varepsilon/2\\
\implies &amp; Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &gt;  Q_0(\boldsymbol{\theta}_0) - \varepsilon \\
\implies &amp;  Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &gt;  Q_0(\boldsymbol{\theta}_0) - Q_0(\boldsymbol{\theta}_0) - \boldsymbol{\theta}^* &amp; (\varepsilon = Q_0(\boldsymbol{\theta}_0) - \boldsymbol{\theta}^*)\\
\implies &amp;  Q_0(\hat{\boldsymbol\theta}_\text{EX} ) &gt;  \boldsymbol{\theta}^* \\
\implies &amp; \hat{\boldsymbol\theta}_\text{EX} \notin N^c \cap \boldsymbol{\Theta}&amp; \left(\boldsymbol{\theta}^* = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in N^c_r(\boldsymbol{\theta}_0) \cap \boldsymbol{\Theta}} Q_0(\boldsymbol{\theta})\right)\\
\implies &amp; \hat{\boldsymbol\theta}_\text{EX} \in N_r(\boldsymbol{\theta}_0) &amp;(\hat{\boldsymbol\theta}_\text{EX} \in \boldsymbol{\Theta})\\
\implies &amp; \left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r &amp; (\text{definition of }N_r(\boldsymbol{\theta}_0))
\end{align*}\]</span> If the event <span class="math inline">\(A_n\)</span> implies that <span class="math inline">\(\left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r\)</span>, then <span class="math inline">\(\Pr(A_n) \le \Pr\left(\left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r\right)\)</span>. We have assumed <span class="math inline">\(Q_n \overset{p}{\to}Q_0\)</span> uniformly, so <span class="math display">\[\begin{align*}
&amp;\lim_{n\to \infty}\Pr(|Q_n(\boldsymbol{\theta}) - Q_0(\boldsymbol{\theta})| &lt; \varepsilon/2) = 1\\
\implies &amp; \lim_{n\to \infty}\Pr(A_n) = 1\\
\implies &amp; \lim_{n\to \infty}\Pr\left(\left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r\right) = 1 &amp; \left(\Pr(A_n) \le \Pr\left(\left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r\right)\right)
\end{align*}\]</span> We’ve taken the radius <span class="math inline">\(r\)</span> to be arbitrary, so <span class="math display">\[\begin{align*}
&amp; \lim_{n\to \infty}\Pr\left(\left\lvert\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0\right\rvert &lt; r\right) = 1 &amp; (\forall r &gt; 0).
\end{align*}\]</span> This is the definition of convergence in probability, so <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>!</p>
</div>
<p>This theorem applies to any metric space <span class="math inline">\(\boldsymbol{\Theta}\)</span>, not just subsets of Euclidean space.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span> provide slightly weaker conditions under which this theorem holds, the details of which can be studied in <span class="citation" data-cites="aliprantisinfinite">Aliprantis and Border (<a href="references.html#ref-aliprantisinfinite" role="doc-biblioref">n.d.</a>)</span>.</p>
<p>Actually applying Theorem, which we would like to do @ref(thm:excon) in Section @ref(generalized-method-of-moments) and Section @ref(maximum-likelihood-estimation), can be a bit tricky. On particular problem comes with the assumption that <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact. This will not hold whenever <span class="math inline">\(\boldsymbol{\Theta}= \mathbb R^k\)</span>, which is the cases we’ve been most concerned with. Realistically, we could restrict our attention to some closed subset <span class="math inline">\(\boldsymbol{\Theta}' \subset \mathbb R^k\)</span> in most applications. For example, if we’re estimating a linear model where we want to estimate the returns of schooling to log earnings <span class="math inline">\(\beta\)</span>, we can confidently assume <span class="math inline">\(\beta \in [0, 10]\)</span>. There are situations where we cannot do this though, so we need a second consistency result holds in the absence of compactness.</p>
<div id="excon2" class="theorem" name="Consistency of Extremum Estimators II">
<p>Suppose there exists some (non-stochastic) function <span class="math inline">\(Q_0(\boldsymbol{\theta})\)</span> such that:</p>
<ol type="1">
<li><span class="math inline">\(\boldsymbol{\theta}_0 = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}Q_0(\boldsymbol{\theta})\)</span>;</li>
<li><span class="math inline">\(\boldsymbol{\theta}_0\)</span> is an interior point of <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
<li><span class="math inline">\(\boldsymbol{\Theta}\)</span> is a convex set;</li>
<li><span class="math inline">\(Q_n(\boldsymbol{\theta})\)</span> is a concave function on <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
<li><span class="math inline">\(Q_n(\boldsymbol{\theta}) \overset{p}{\to}Q_0(\boldsymbol{\theta})\)</span> (pointwise) on <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</li>
</ol>
<p>Then <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> exists with probability approaching 1, and <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The point <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is an interior point of <span class="math inline">\(\boldsymbol{\Theta}\)</span>, so there exists a compact neighborhood of radius <span class="math inline">\(2\varepsilon\)</span> centered at <span class="math inline">\(\boldsymbol{\theta}_0\)</span> contained in <span class="math inline">\(\boldsymbol{\Theta}\)</span>, <span class="math inline">\(C\subset \boldsymbol{\Theta}\)</span>. The boundary of this neighborhood is <span class="math inline">\(\partial C\)</span>. Appealing to some more obscure real analysis results, we can conclude:</p>
<ol type="1">
<li><span class="math inline">\(Q_0\)</span> is concave (the pointwise limit of concave functions is concave).</li>
<li><span class="math inline">\(Q_0\)</span> is continuous on <span class="math inline">\(C\)</span> (concave functions are continuous on the interior of their domains).</li>
<li><span class="math inline">\(Q_n \overset{p}{\to}Q_0\)</span> uniformly on <span class="math inline">\(C\)</span> (pointwise convergence of concave functions of a dense subset of an open set implies uniform convergence on compact subsets of an open set)</li>
</ol>
<p>Define <span class="math inline">\(\tilde {\boldsymbol{\theta}} = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in C} Q_n(\boldsymbol{\theta})\)</span>. In light of points 2 and 3, we have <span class="math inline">\(\tilde {\boldsymbol{\theta}} \overset{p}{\to}\boldsymbol{\theta}_0\)</span> by @ref(thm:excon). We will now show that <span class="math inline">\(\tilde {\boldsymbol{\theta}}\)</span> not only maximizes <span class="math inline">\(Q_n\)</span> over <span class="math inline">\(C\)</span>, but also over <span class="math inline">\(\boldsymbol{\Theta}\)</span>, so <span class="math inline">\(\tilde{\boldsymbol{\theta}} = \hat{\boldsymbol\theta}_\text{EX} \)</span> and <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>.</p>
<p>If <span class="math inline">\(\tilde{\boldsymbol{\theta}} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>, then <span class="math inline">\(\Pr(\left\lvert\tilde{\boldsymbol{\theta}} - \boldsymbol{\theta}_0\right\rvert &lt; \varepsilon)\to 1\)</span>. This is equivalent to <span class="math display">\[\Pr\left(Q_n(\tilde{\boldsymbol{\theta}}) \ge \max_{\partial C} Q_n(\boldsymbol{\theta})\right)\to 1.\]</span> By the convexity of <span class="math inline">\(\boldsymbol{\Theta}\)</span>, in the event that <span class="math inline">\(Q_n(\tilde{\boldsymbol{\theta}}) \ge \max_{\partial C} Q_n(\boldsymbol{\theta})\)</span>, there exists a convex combination <span class="math inline">\(\alpha \tilde{\boldsymbol{\theta}} + (1-\alpha)\tilde{\boldsymbol{\theta}} \in \partial C\)</span> (<span class="math inline">\(\alpha\)</span> &lt; 1) for any <span class="math inline">\(\boldsymbol{\theta}\notin C\)</span>. If we evaluate <span class="math inline">\(Q_n\)</span> at this convex combination, we have <span class="math display">\[\begin{equation}
Q_n(\tilde{\boldsymbol{\theta}}) \ge Q_n(\alpha \tilde{\boldsymbol{\theta}} + (1-\alpha)\tilde{\boldsymbol{\theta}} ). (\#eq:a4)
\end{equation}\]</span> But <span class="math inline">\(Q_n\)</span> is concave, so <span class="math display">\[\begin{equation}
Q_n(\alpha \tilde{\boldsymbol{\theta}} + (1-\alpha)\tilde{\boldsymbol{\theta}} ) \ge \alpha Q_n( \tilde{\boldsymbol{\theta}}) + (1-\alpha)Q_n( \tilde{\boldsymbol{\theta}}). (\#eq:a5)
\end{equation}\]</span> If we combine inequalities @ref(eq:a4) and @ref(eq:a5), we have <span class="math display">\[(1-\alpha)Q_n(\tilde{\boldsymbol{\theta}}) \ge (1-\alpha)Q_n({\boldsymbol{\theta}}),\]</span> so <span class="math inline">\(\tilde{\boldsymbol{\theta}} = \hat{\boldsymbol\theta}_\text{EX} \)</span>.</p>
</div>
</section>
<section id="extremum-identification" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="extremum-identification"><span class="header-section-number">7.3</span> Extremum Identification</h2>
<p>Perhaps the most important stipulation in Theorems @ref(thm:excon) and @ref(thm:excon2) is that <span class="math inline">\(\mathop{\mathrm{plim}}Q_n = Q_0\)</span> is uniquely maximized at the true value <span class="math inline">\(\boldsymbol{\theta}_0 \in \boldsymbol{\Theta}\)</span>. We can think of <span class="math inline">\(Q_0\)</span> as a “population” counterpart to <span class="math inline">\(Q_n\)</span>. In the event his non-stochastic/true population function <span class="math inline">\(Q_0\)</span> is maximized at multiple values <span class="math inline">\(\{\boldsymbol{\theta}_0, \boldsymbol{\theta}_0'\}\)</span>, then we have no way of knowing if our extremum estimator is consistently estimating <span class="math inline">\(\boldsymbol{\theta}_0\)</span> or <span class="math inline">\(\boldsymbol{\theta}_0'\)</span>. This problem should sound <em>very familiar</em>. It seems to be similar, if not the same, exact problem that arises when a model <span class="math inline">\(\mathcal P\)</span> is unidentified! It happens to be the same exact problem.</p>
<p>Suppose we have a model <span class="math inline">\(\mathcal P\)</span> with a parameterization <span class="math inline">\(\mu : \boldsymbol{\Theta}\to \mathcal P\)</span>, where <span class="math inline">\(\mu\)</span> is injective such that <span class="math inline">\(\mathcal P\)</span> is identified. The true parameter value for <span class="math inline">\(P\)</span> is <span class="math inline">\(\mu^{-1}(P)\)</span> (where this is the left inverse of <span class="math inline">\(\mu\)</span>). Consider the problem of assigning real numbers to combinations of parameters and model values <span class="math inline">\((\boldsymbol{\theta}, P) \in \boldsymbol{\Theta}\times \mathcal P\)</span> such that we can identify each <span class="math inline">\(P\)</span> using these numbers. We can use a function <span class="math inline">\(Q_0:\boldsymbol{\Theta}\times \mathcal P \to \mathbb R\)</span> for this, and define <span class="math inline">\(\phi:\boldsymbol{\Theta}\to \mathcal P\)</span> such that its left inverse is: <span class="math display">\[ \phi^{-1}(P) = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}Q_0(P, \boldsymbol{\theta}).\]</span> If we wanted to reparameterize <span class="math inline">\(\mathcal P\)</span> with <span class="math inline">\(\phi\)</span> and maintain identification, then what conditions would <span class="math inline">\(Q_0\)</span> need to satisfy? It would need to be the case that <span class="math inline">\(Q_0\)</span> has a unique maximum (otherwise <span class="math inline">\(\phi\)</span> would assign multiple parameters to <span class="math inline">\(P\)</span> and we would not have identification), <em>and</em> that unique maximization needs to occur at true parameter <span class="math inline">\(\theta = \mu^{-1}(P)\)</span> associated with <span class="math inline">\(P\)</span>: <span class="math display">\[ \mu^{-1}(P) = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}}Q_0(P, \boldsymbol{\theta}).\]</span> This is precisely the first condition required for consistency! For more details about this, see <span class="citation" data-cites="davidson1993estimation">Davidson and MacKinnon (<a href="references.html#ref-davidson1993estimation" role="doc-biblioref">1993</a>)</span> and <span class="citation" data-cites="lewbel2019identification">Lewbel (<a href="references.html#ref-lewbel2019identification" role="doc-biblioref">2019</a>)</span>, the latter of which coined the term “extremum identification”. A concrete example may illuminate the link between identification and <span class="math inline">\(Q_0\)</span> achieving a unique maximum at <span class="math inline">\(\boldsymbol{\theta}_0\)</span></p>
<div id="olsEX" class="example">
<p>Return to the classic linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> and the estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. <span class="math display">\[ \hat{\boldsymbol\beta}_\text{OLS} = \mathop{\mathrm{argmin}}_{\mathbf b} \sum_{i=1}^n(Y_i-\mathbf{X}_i\mathbf b)^2 = \mathop{\mathrm{argmax}}_{\mathbf b}\underbrace{ -\frac{1}{n}\sum_{i=1}^n(Y_i-\mathbf{X}_i\mathbf b)^2}_{Q_n}.\]</span> By the law of large numbers <span class="math display">\[ \underbrace{ -\frac{1}{n}\sum_{i=1}^n(Y_i-\mathbf{X}_i\mathbf b)^2}_{Q_n}\overset{p}{\to}\underbrace{-\text{E}\left[(Y - \mathbf{X}\mathbf b)^2\right]}_{Q_0}\]</span></p>
<p>Under what conditions is <span class="math inline">\(Q_0\)</span> uniquely maximized at the true parameter value <span class="math inline">\(\boldsymbol{\beta}\)</span>? The first order condition for <span class="math inline">\(\mathop{\mathrm{argmax}}Q_0\)</span> can be reduced to two different equations: <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{X}'Y\right]&amp;=\boldsymbol{\beta}\text{E}\left[\mathbf{X}'\mathbf{X}\right]\\
\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]&amp;=0
\end{align*}\]</span> Therefore <span class="math inline">\(Q_0\)</span> is uniquely maximized at the true parameter value <span class="math inline">\(\boldsymbol{\beta}\)</span> when <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]=0\)</span> and <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible. These are precisely the same conditions which we determined identified <span class="math inline">\(\mathcal P_\text{LM}\)</span>.</p>
</div>
<div class="example">
<p>Consider the linear projection model <span class="math inline">\(\mathcal P_\text{LP}\)</span> where <span class="math inline">\(\boldsymbol{\beta}\)</span> was defined as <span class="math display">\[\boldsymbol{\beta}=\mathop{\mathrm{argmax}}_{\mathbf b}-\text{E}\left[(Y - \mathbf{X}\mathbf b)^2\right]\]</span> to begin with (opposed to <span class="math inline">\(\boldsymbol{\beta}\)</span> having a structural interpretation like in <span class="math inline">\(\mathcal P_\text{LM}\)</span>). The parameterization is already given by a maximization problem here! In this special case, we can think of an extremum estimator as originating from the analog principle, because <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> (when written as the solution to the maximization problem) is the sample analog to <span class="math inline">\(\boldsymbol{\beta}\)</span>. This model is identified when <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, which has the roll of insuring that the maximization problem which defines <span class="math inline">\(\boldsymbol{\beta}\)</span> has a unique solution. Once again, this is condition 1 in @ref(thm:excon).</p>
</div>
<div class="hypothesis" name="Consistency and Identification">
<p>In general, consistency and identification are inherently related. Consider the classical linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> where <span class="math inline">\(\boldsymbol{\beta}= \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{X}'Y\right]\)</span>. The model is identified, so <span class="math inline">\(\boldsymbol{\beta}\)</span> is unique to <span class="math inline">\(P_{\boldsymbol{\beta}, \sigma^2}\in\mathcal P\)</span>. Heuristically, we can think of identification as the ability to estimate the parameter perfectly at the population level with an infinite amount of data. If given an infinite amount of data, we could calculate the moments <span class="math inline">\(\text{E}\left[\mathbf{X}'Y\right]\)</span> and <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span>, enabling us to calculate <span class="math inline">\(\boldsymbol{\beta}\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> We will never have infinite data, so the best we can do is define the sample analog of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>, and appeal to the fact that as our sample size grows and approaches infinity, our estimates become arbitrarily better. This just happens to be consistency. Formally, suppose under an assumed model value <span class="math inline">\(P\in\mathcal P\)</span> we have an estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> with a unique limit in probability <span class="math inline">\(\mathop{\mathrm{plim}}\hat{\boldsymbol{\theta}}\)</span>. If we define the parameterization <span class="math inline">\(\boldsymbol{\theta}\mapsto P_{\boldsymbol{\theta}}\)</span> as <span class="math inline">\(\boldsymbol{\theta}= \mathop{\mathrm{plim}}\hat{\boldsymbol{\theta}}\)</span>, then we’ve leveraged consistency such that our model is identified by construction.</p>
</div>
</section>
<section id="asymptotic-normality" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="asymptotic-normality"><span class="header-section-number">7.4</span> Asymptotic Normality</h2>
<p>Our next technical result is that <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> happens to be root-n CAN under certain conditions. This will require a bit more work now that we aren’t restricting our attention to linear models. Recall the steps we took to prove Theorem @ref(thm:asymols). From the onset of this proof, we had an closed form solution for <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>. Given <span class="math inline">\(Q_n = -\left\lVert\mathbf{Y}- \mathbb{X}\mathbf b\right\rVert^2\)</span>, we were able to solve the associated first order condition for a unique solution in the form of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} = \hat{\boldsymbol\beta}_\text{OLS} = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}\)</span>, because the first order condition was itself a linear equation with one root: <span class="math display">\[ \frac{\partial Q_n}{\partial \mathbf b} = 2\mathbb{X}'(\mathbf{Y}-\mathbb{X}\mathbf b).\]</span> Once we move beyond linear models, it may be the case that the first order condition <span class="math inline">\(\frac{\partial Q_n}{\partial \boldsymbol{\theta}}\)</span> is nonlinear and cannot be solved explicitly for <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, even if we assume the first order condition has a unique root. The easiest way to handle this is using the mean value theorem to approximate the first order condition linearly (i.e a first order Taylor expansion). Doing this requires additional assumptions about <span class="math inline">\(Q_n\)</span>, in particular assumptions about the existence of derivatives. Using the mean value theorem require continuous differentiability, but in our case we’re going to apply it to the derivative <span class="math inline">\(\frac{\partial Q_n}{\partial \boldsymbol{\theta}}\)</span>, so we will need <span class="math inline">\(Q_n\)</span> to be twice continuous differentiability. This means our theorem will involve the hessian <span class="math inline">\(\mathbf H(\boldsymbol{\theta})\)</span> of <span class="math inline">\(Q_n\)</span>.</p>
<div id="exasy" class="theorem" name="Asymptotic Normality of Extremum Estimators">
<p>Suppose that:</p>
<ol type="1">
<li><span class="math inline">\(\frac{\partial Q_n(\hat{\boldsymbol\theta}_\text{EX} )}{\partial \boldsymbol{\theta}} = o_p(n^{-1/2})\)</span> (the FOC holds as <span class="math inline">\(n\to\infty\)</span>);</li>
<li><span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span></li>
<li><span class="math inline">\(\boldsymbol{\Theta}_0\)</span> is in the interior of <span class="math inline">\(\boldsymbol{\Theta}\)</span>;</li>
<li><span class="math inline">\(Q_n(\boldsymbol{\theta})\)</span> is twice continuously differentiable in a neighborhood <span class="math inline">\(N_r(\boldsymbol{\theta}_0)\)</span>;</li>
<li><span class="math inline">\(\sqrt n \frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span>;</li>
<li><span class="math inline">\(\frac{\partial^2 Q_n(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} \overset{p}{\to}\mathbf H(\boldsymbol{\theta})\)</span> <em>uniformly</em> for some <span class="math inline">\(\mathbf H(\boldsymbol{\theta})\)</span> continuous at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>;</li>
<li><span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span> is invertible. Then <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) &amp;\overset{d}{\to}N(\mathbf{0}, \mathbf H(\boldsymbol{\theta}_0)^{-1} \boldsymbol \Omega \mathbf H(\boldsymbol{\theta}_0)^{-1}).
\end{align*}\]</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We will begin by performing a first-order Taylor expansion of <span class="math inline">\(\frac{\partial Q_n(\hat{\boldsymbol\theta}_\text{EX} )}{\partial \boldsymbol{\theta}}\)</span> about <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p><span class="math display">\[\begin{align}
&amp;\frac{\partial Q_n(\hat{\boldsymbol\theta}_\text{EX} )}{\partial \hat{\boldsymbol\theta}_\text{EX} } = o_p(n^{-1/2})\\
\implies &amp; o_p(n^{-1/2})= \frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} + \frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) &amp; (\hat\theta_{\text{EE},j} &lt; \tilde\theta_{j} &lt;  \theta_{0,j} \ \forall j) (\#eq:a7)
\end{align}\]</span> The vector <span class="math inline">\(\tilde{\boldsymbol{\theta}}\)</span> “lies between” <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> and <span class="math inline">\(\boldsymbol{\theta}_0\)</span> (element-wise, where the elements of <span class="math inline">\(\tilde{\boldsymbol{\theta}}\)</span> are given by the <span class="math inline">\(K\)</span> expansions), so <span class="math inline">\(\tilde{\boldsymbol{\theta}}\overset{p}{\to}\boldsymbol{\theta}_0\)</span> as a consequence of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>. Using assumption 6, <span class="math display">\[\begin{align}
&amp;\frac{\partial^2 Q_n(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} \overset{p}{\to}\mathbf H(\boldsymbol{\theta}) &amp; (\forall \boldsymbol{\theta}\in \boldsymbol{\Theta})\\
\implies &amp; \frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} \overset{p}{\to}\mathbf H(\tilde{\boldsymbol{\theta}})\\
\implies &amp; \mathop{\mathrm{plim}}\frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} = \mathop{\mathrm{plim}}\mathbf H(\tilde{\boldsymbol{\theta}})\\
\implies &amp; \mathop{\mathrm{plim}}\frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} = \mathbf H(\mathop{\mathrm{plim}}\tilde{\boldsymbol{\theta}}) &amp; (\mathbf H\text{ continuous at } \boldsymbol{\theta}_0 = \mathop{\mathrm{plim}}\tilde{\boldsymbol{\theta}})\\
\implies &amp; \mathop{\mathrm{plim}}\frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} = \mathbf H(\boldsymbol{\theta}_0) &amp; (\boldsymbol{\theta}_0 = \mathop{\mathrm{plim}}\tilde{\boldsymbol{\theta}})\\
\implies &amp; \frac{\partial^2 Q_n(\tilde{\boldsymbol{\theta}})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'} = \mathbf H(\boldsymbol{\theta}_0) + o_p(1) (\#eq:a8)
\end{align}\]</span> If we substitute equation @ref(eq:a8) into equation @ref(eq:a7), then <span class="math display">\[\begin{align*}
&amp; o_p(n^{-1/2}) = \frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} + (\mathbf H(\boldsymbol{\theta}_0) + o_p(1))(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0)\\
\implies &amp; \underbrace{\sqrt n \cdot o_p(n^{-1/2})}_{o_p(1)} = \sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} + (\mathbf H(\boldsymbol{\theta}_0) + o_p(1))\sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0)\\
\implies &amp;\sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) = -(\mathbf H(\boldsymbol{\theta}_0) + o_p(1))^{-1}\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} - \underbrace{o_p(1)(\mathbf H(\boldsymbol{\theta}_0) + o_p(1))^{-1}}_{o_p(1)}\\
\implies &amp; \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) \overset{p}{\to}-\mathbf H(\boldsymbol{\theta}_0)^{-1} \sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}}\\
\implies &amp; \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) \overset{d}{\to}-\mathbf H(\boldsymbol{\theta}_0)^{-1} \underbrace{\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}}}_{\overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)} &amp; (\overset{p}{\to}\implies \overset{d}{\to})\\
\implies &amp; \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) \overset{d}{\to}-\mathbf H(\boldsymbol{\theta}_0)^{-1} N(\mathbf{0}, \boldsymbol \Omega)\\
\implies &amp; \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) \overset{d}{\to}N(\mathbf{0}, [-\mathbf H(\boldsymbol{\theta}_0)^{-1}]'\boldsymbol \Omega[-\mathbf H(\boldsymbol{\theta}_0)^{-1}]) &amp; (\mathbf H(\boldsymbol{\theta}_0)\text{ invertible})\\
\implies &amp; \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0) \overset{d}{\to}N(\mathbf{0}, \mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega\mathbf H(\boldsymbol{\theta}_0)^{-1}) &amp; (\mathbf H(\boldsymbol{\theta}_0)\text{ symmetric})
\end{align*}\]</span> <span style="color:white">space</span></p>
</div>
<div class="hypothesis" name="Sandwhich Variance/Covariance Matrix">
<p>The asymptotic variance of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> exhibits a nice pattern. It’s the variance <span class="math inline">\(\boldsymbol\Omega\)</span> “sandwiched” between <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)^{-1}\)</span>. This is why a(n) (asymptotic) variance/covariance matrix of this form, <span class="math display">\[\text{Avar}\left(\hat{\boldsymbol{\theta}}\right) = a \mathbf B^{-1}\mathbf A \mathbf B^{-1},\]</span> is called a <strong><em>sandwich variance/covariance matrix</em></strong>. Many root-n CAN estimators have a sandwich variance/covariance matrix because the delta method along with the properties of variance naturally lend themselves to this sandwich form. <span class="math display">\[\begin{align*}
\text{Var}\left(\mathbf B \mathbf{X}+ \mathbf c\right) &amp;= \mathbf B\text{Var}\left(\mathbf{X}\right)\mathbf B'\\
\sqrt{n}[\mathbf g(\mathbf{X}_n) - \mathbf g(\mathbf t)] &amp;\overset{d}{\to}N\left(\mathbf{0}, \left[\frac{\partial \mathbf g}{\partial\mathbf{x}}(\mathbf t)\right]\text{Avar}\left(\mathbf{X}_n\right)\left[\frac{\partial \mathbf g}{\partial\mathbf{x}}(\mathbf t)\right]'\right) &amp; (\mathbf{X}_n \overset{d}{\to}N(\mathbf{0}, \text{Avar}\left(\mathbf{X}_n\right)))
\end{align*}\]</span> In the event that <span class="math inline">\(\text{Avar}\left(\mathbf{X}_n\right)\)</span> is a symmetric matrix <span class="math inline">\(\mathbf B'\)</span>, we have the sandwich form <span class="math inline">\(\mathbf B^{-1}\mathbf A \mathbf B^{-1}\)</span> (which may be scaled by some <span class="math inline">\(a\)</span>). In certain special cases, <span class="math inline">\(a \mathbf B^{-1}\mathbf A \mathbf B^{-1}\)</span> may reduce to <span class="math inline">\(a \mathbf B^{-1}\)</span>.</p>
</div>
</section>
<section id="the-hypothesis-testing-trinity" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="the-hypothesis-testing-trinity"><span class="header-section-number">7.5</span> The Hypothesis Testing “Trinity”</h2>
<p>There are three main approaches to testing hypotheses using <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, one that we are familiar with, and two new ones. In each case, we’ll need a consistent estimator <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{EX} )\)</span>. Unfortunately, Theorem @ref(thm:exasy) is so general that it doesn’t provide much guidance as to estimating <span class="math display">\[\text{Avar}\left(\hat{\boldsymbol\theta}_\text{EX} \right) = \frac{\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega\mathbf H(\boldsymbol{\theta}_0)^{-1}}{n}.\]</span> We only know that <span class="math inline">\(\mathbf H\)</span> is the limit of the Hessian of <span class="math inline">\(Q_n\)</span>, and that <span class="math inline">\(\boldsymbol \Omega\)</span> is the asymptotic variance of the gradient of <span class="math inline">\(\mathbf H\)</span>. Without more information about <span class="math inline">\(Q_n\)</span>, there isn’t much we can do right now to estimate the asymptotic variance of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>. Instead, we’ll derive consistent estimators <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{EX} )\)</span> for special cases of extremum estimators, and assume they exist for now, allowing us to construct test statistics.</p>
<p>Theorem @ref(thm:exasy) establishes that <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is root-n CAN, so we can test a general nonlinear hypothesis <span class="math inline">\(\mathbf h(\boldsymbol{\theta}) = \mathbf{0}\)</span> using a Wald test: <span class="math display">\[ W = \mathbf h(\hat{\boldsymbol\theta}_\text{EX} )'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{EX} )\widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{EX} )\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{EX} )'\right]^{-1}\mathbf h(\hat{\boldsymbol\theta}_\text{EX} ).\]</span> We can also test hypotheses about <span class="math inline">\(\hat\theta_{\text{EX,}j}\)</span> using the <span class="math inline">\(t-\)</span>test <span class="math display">\[ t = \frac{\hat\theta_{\text{EX},j} - \theta_0}{\widehat{\text{se}}(\hat\theta_{\text{EX},j})}.\]</span> These tests are based on the discrepancy between <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> and <span class="math inline">\(\boldsymbol{\theta}_0\)</span> scaled by the precision (asymptotic variance) of our estimator <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>. This is not the only way to measure how “far” estimates are from a null hypothesis.</p>
<p>Consider the definition of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> in general, and under <span class="math inline">\(H_0:\mathbf h(\boldsymbol{\theta}) = \mathbf{0}\)</span>: <span class="math display">\[\begin{align*}
\hat{\boldsymbol\theta}_\text{EX} &amp; = \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} Q_n(\boldsymbol{\theta}),\\
\bar{\boldsymbol{\theta}}_\text{EE} &amp; = \mathop{\mathrm{argmax}}_{\mathbf h(\boldsymbol{\theta}) = \mathbf{0}} Q_n(\boldsymbol{\theta}).
\end{align*}\]</span> We usually refer to an estimator such as <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> as a <strong><em>restricted estimator</em></strong>, because it is calculated under the restriction that the null hypothesis is true.</p>
<p><span class="citation" data-cites="rao1948large">Rao (<a href="references.html#ref-rao1948large" role="doc-biblioref">1948</a>)</span> proposed an alternate statistic based solely on <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span>. Appealing to the method of Lagrange multipliers, the restricted estimator <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> is given as the solution to the following first order conditions (after being scaled by <span class="math inline">\(\sqrt{n}\)</span>): <span class="math display">\[\begin{align*}
\sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}) + \mathbf h(\bar{\boldsymbol{\theta}}_\text{EE})'\sqrt{n}\boldsymbol\lambda  &amp; = \mathbf{0}(\#eq:lm1)\\
\sqrt{n}\boldsymbol\lambda\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}} &amp; = \mathbf{0}(\#eq:lm2)
\end{align*}\]</span></p>
<p>In the event that <span class="math inline">\(H_0\)</span> is true, then the multiplier <span class="math inline">\(\boldsymbol\lambda = \mathbf{0}\)</span>. This suggests testing the equivalent hypothesis <span class="math inline">\(H_0:\boldsymbol\lambda = \mathbf{0}\)</span>. The Wald statistic associated with this problem is <span class="math display">\[ \boldsymbol\lambda'\left[\widehat{\text{Avar}}(\boldsymbol\lambda)\right]^{-1}\boldsymbol\lambda,\]</span> which can be simplified if we find <span class="math inline">\(\text{Avar}\left(\boldsymbol\lambda\right)\)</span>.</p>
<p>We can derive a test statistic from these FOCs assuming <span class="math inline">\(\sqrt n \frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span> and that we are able to perform a mean value expansion on <span class="math inline">\(\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})\)</span> and <span class="math inline">\(\mathbf h\)</span>. For some values <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}&lt;\tilde{\boldsymbol{\theta}}&lt;\boldsymbol{\theta}_0\)</span> and <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}&lt;\boldsymbol{\theta}^\dagger&lt;\boldsymbol{\theta}_0\)</span>,<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> <span class="math display">\[\begin{align*}
\sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}) &amp;= \sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) + \frac{\partial^2 Q_n}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'}(\boldsymbol{\theta}^\dagger)\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)\\
\sqrt{n}\mathbf h(\bar{\boldsymbol{\theta}}_\text{EE}) &amp;= \underbrace{\sqrt{n}\mathbf h(\boldsymbol{\theta}_0)}_\mathbf{0}+ \frac{\partial\mathbf h}{\partial \boldsymbol{\theta}}(\tilde{\boldsymbol{\theta}})\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)
\end{align*}\]</span> If we assume that <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\overset{p}{\to}\boldsymbol{\theta}_0\)</span>, then <span class="math inline">\(\tilde{\boldsymbol{\theta}} \overset{p}{\to}\boldsymbol{\theta}_0\)</span> and <span class="math inline">\(\boldsymbol{\theta}^\dagger\overset{p}{\to}\boldsymbol{\theta}_0\)</span>, because they’re “squeezed” in between <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> and <span class="math inline">\(\boldsymbol{\theta}_0\)</span>. If we take the limit (in probability) of these expansions, we have <span class="math display">\[\begin{align*}
\sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}) &amp;= \sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) +\mathbf H(\boldsymbol{\theta}_0)\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0) + o_p(1) (\#eq:lm3)\\
\sqrt{n}\mathbf h(\bar{\boldsymbol{\theta}}_\text{EE}) &amp;= \frac{\partial\mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0) + o_p(1) (\#eq:lm4)
\end{align*}\]</span> where the definition of <span class="math inline">\(\mathbf H\)</span> is given in Theorem @ref(thm:exasy). If we assume that <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\overset{p}{\to}\boldsymbol{\theta}_0\)</span> under <span class="math inline">\(H_0\)</span>, then <span class="math display">\[\begin{align*}
\mathbf h(\bar{\boldsymbol{\theta}}_\text{EE})'\boldsymbol\lambda = \mathbf h(\boldsymbol{\theta}_0)'\boldsymbol\lambda + o_p(1) (\#eq:lm5)
\end{align*}\]</span> If we substitute equations @ref(eq:lm3), @ref(eq:lm4), and @ref(eq:lm5) into the first order conditions given by equations (#eq:lm1) and (#eq:lm2), we have (after some consolidation into matrices): <span class="math display">\[ \begin{bmatrix} \mathbf H(\boldsymbol{\theta}_0) &amp; \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\\
\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) &amp; \mathbf{0}\end{bmatrix}\begin{bmatrix}\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)\\ \sqrt n \boldsymbol \lambda \end{bmatrix} = \begin{bmatrix} -\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\\ \mathbf{0}\end{bmatrix} + o_p(1)\]</span></p>
<p>If we solve this system we have: <span class="math display">\[\begin{align*}
&amp;\begin{bmatrix}\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)\\ \sqrt n \boldsymbol \lambda \end{bmatrix}  = \begin{bmatrix} \mathbf H(\boldsymbol{\theta}_0) &amp; \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\\
\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) &amp; \mathbf{0}\end{bmatrix}^{-1}\begin{bmatrix} -\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\\ \mathbf{0}\end{bmatrix} + o_p(1)\\
\implies &amp; \sqrt{n}\boldsymbol \lambda = -\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) + o_p(1)\\
&amp; \sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)=-\left[\mathbf H(\boldsymbol{\theta}_0)^{-1} - \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left(\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \mathbf H(\boldsymbol{\theta}_0)^{-1}\right]\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} + o_p(1) (\#eq:lm6)\\
\implies &amp; \sqrt{n}\boldsymbol \lambda \overset{p}{\to}-\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\underbrace{\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) }_{N(\mathbf{0}, \boldsymbol \Omega)}\\
\implies &amp; \sqrt{n}\boldsymbol \lambda \overset{d}{\to}N\left(\mathbf{0},  \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega  \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1} \right)\\
\implies &amp; \text{Avar}\left(\boldsymbol \lambda\right) = \frac{1}{n}\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega  \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}
\end{align*}\]</span> Therefore, the Wald statistic associated with <span class="math inline">\(H_0:\boldsymbol\lambda = \mathbf{0}\)</span> (taking the asymptotic variance to be known at the moment) is <span class="math display">\[\begin{align*}
&amp;\boldsymbol\lambda' \left[\text{Avar}\left(\boldsymbol\lambda\right)\right]^{-1}\boldsymbol\lambda\\
\implies &amp; \boldsymbol\lambda' \left[\frac{1}{n}\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega  \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\right]^{-1}\boldsymbol\lambda\\
\implies &amp; n\boldsymbol\lambda'\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega  \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\boldsymbol\lambda.
\end{align*}\]</span> Under <span class="math inline">\(H_0\)</span> we have <span class="math inline">\(\nabla_\boldsymbol{\theta}Q_n(\bar{\boldsymbol{\theta}}_\text{EE}) = \mathbf{0}\)</span>. Combining this with @ref(eq:lm2) gives <span class="math display">\[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\boldsymbol \lambda =  \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}),\]</span> which can be used to simplify our test statistic.</p>
<p><span class="math display">\[ n\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})'\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol \Omega  \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})'.\]</span> In practice, we only know <span class="math inline">\(H_0:\mathbf h(\boldsymbol{\theta}) = \boldsymbol{\theta}_0\)</span>, <span class="math inline">\(Q_n\)</span>, and <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span>, so we need to estimate <span class="math inline">\(\boldsymbol \Omega\)</span> and <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span>. Once we substitute in suitable estimators, we have our Wald statistic for <span class="math inline">\(H_0:\boldsymbol \lambda = \mathbf{0}\)</span>, which is usually considered it’s own seperate test statistic.</p>
<div class="definition">
<p>Suppose <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is an extremum estimator for which <span class="math inline">\(\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span>, and <span class="math inline">\(\frac{\partial^2 Q_n}{\partial \boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\boldsymbol{\theta}_0) \overset{p}{\to}\mathbf H(\boldsymbol{\theta}_0)\)</span>. If a null hypothesis <span class="math inline">\(H_0\)</span> is summarized by some (possibly nonlinear) function <span class="math inline">\(\mathbf h(\boldsymbol{\theta}) = \mathbf{0}\)</span> the <span style="color:red"><strong><em>Lagrange multiplier statistic</em></strong></span> is defined as <span class="math display">\[ LM = n\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\hat{\boldsymbol \Omega}\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}),\]</span> where <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> is the extremum estimator subject to <span class="math inline">\(H_0\)</span>. The statistic is also known as the <span style="color:red"><strong><em>Rao test statistic</em></strong></span> or <span style="color:red"><strong><em>score test statistic</em></strong></span></p>
</div>
<p>If <span class="math inline">\(\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}) \gg 0\)</span>, then <span class="math inline">\(LM \gg 0\)</span>, and it is likely that <span class="math inline">\(H_0\)</span> is false. An immediate consequence of our derivation of <span class="math inline">\(LM\)</span> is that <span class="math inline">\(LM \overset{d}{\to}\chi_q^2\)</span>, as it is a special case of the Wald statistic. A formal statement of this will be presented after deriving one last test statistic.</p>
<p>A third option to test <span class="math inline">\(H_0\)</span> is by looking at the difference <span class="math inline">\(Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})\)</span> (which is always positive). If <span class="math inline">\(H_0\)</span> is likely to be true then <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \approx \bar{\boldsymbol{\theta}}_\text{EE}\)</span>, so <span class="math inline">\(Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})\approx 0\)</span>. A test statistic based on this criterion would have a major advantage over the Wald and Lagrange multiplier statistics because it would not require us to estimate any asymptotic variances. Unfortunately, as we’ll soon see, this advantage comes at the cost of an assumption about <span class="math inline">\(\boldsymbol \Omega\)</span> and <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span>. First, let’s look at the second-order Taylor expansion of <span class="math inline">\(Q_n(\bar{\boldsymbol{\theta}}_\text{EE})\)</span> about <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>. For some <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} &lt;\tilde{\boldsymbol{\theta}} &lt; \bar{\boldsymbol{\theta}}_\text{EE}\)</span>, <span class="math display">\[\begin{align*}
&amp; Q_n(\bar{\boldsymbol{\theta}}_\text{EE}) =  Q_n(\hat{\boldsymbol\theta}_\text{EX} ) + \frac{\partial Q_n}{\partial\boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} ) + \frac{1}{2}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )'\frac{\partial^2 Q_n}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\tilde{\boldsymbol{\theta}})(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )\\
\implies &amp; 2[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) -  Q_n(\bar{\boldsymbol{\theta}}_\text{EE})] = -\frac{\partial Q_n}{\partial\boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} ) - \frac{1}{2}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )'\frac{\partial^2 Q_n}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\tilde{\boldsymbol{\theta}})(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )
\end{align*}\]</span> Under the assumption that <span class="math inline">\(\sqrt{n}\frac{\partial Q_n}{\partial\boldsymbol{\theta}} \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span>, the first term of this expansion is <span class="math inline">\(o_p(1)\)</span>. <span class="math display">\[\begin{align*}
&amp; 2\sqrt{n}[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})] = -\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )'\frac{\partial^2 Q_n}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\tilde{\boldsymbol{\theta}})(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )+ o_p(1)\\
\implies &amp; 2n[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})] = -[\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )]'\frac{\partial^2 Q_n}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\tilde{\boldsymbol{\theta}})[\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )] + o_p(1)
\end{align*}\]</span> If <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>, then we also have <span class="math inline">\(\tilde{\boldsymbol{\theta}} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>, so <span class="math display">\[2n[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})] = -[\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )]'\mathbf H(\boldsymbol{\theta}_0)[\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )] + o_p(1).\]</span> The term <span class="math inline">\([\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} )]\)</span> looks a lot like something that would be asymptotically normal, in which case <span class="math inline">\(2n[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})]\)</span> would be a function of a quadratic form of (asymptotically) normal vectors meaning it could be distributed according to some <span class="math inline">\(\chi^2\)</span> distribution. From the Taylor expansion used to prove Theorem @ref(thm:exasy) and Equation Theorem @ref(eq:lm6), we have <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0)&amp;-\mathbf H(\boldsymbol{\theta}_0)^{-1}\sqrt{n}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) + o_p(1),\\
\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0)&amp;=-\left[\mathbf H(\boldsymbol{\theta}_0)^{-1} - \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left(\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \mathbf H(\boldsymbol{\theta}_0)^{-1}\right]\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}} + o_p(1).
\end{align*}\]</span> These imply <span class="math display">\[\begin{align*}
\sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}-\hat{\boldsymbol\theta}_\text{EX} ) &amp; = \sqrt{n}(\bar{\boldsymbol{\theta}}_\text{EE}- \boldsymbol{\theta}_0) - \sqrt{n}(\hat{\boldsymbol\theta}_\text{EX} - \boldsymbol{\theta}_0)\\
&amp; = -\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \left[\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}}\right] + o_p(1)
\end{align*}\]</span> If we plug this into our last expression for <span class="math inline">\(2n[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})]\)</span>,<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> we have <span class="math display">\[\begin{align*}
2n[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})] &amp; = \underbrace{\left[\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}}\right]}_{\overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)}' \mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \underbrace{\left[\sqrt{n}\frac{\partial Q_n(\boldsymbol{\theta}_0)}{\partial \boldsymbol{\theta}}\right]}_{\overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)} + o_p(1)\\
&amp;  \overset{d}{\to}\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} N(\mathbf{0}, \boldsymbol \Omega)\right]' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1}\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} N(\mathbf{0}, \boldsymbol \Omega)\right]\\
&amp; = N\left(\mathbf{0},\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \boldsymbol \Omega \mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \right)' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1} N\left(\mathbf{0},\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \boldsymbol \Omega \mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \right)
\end{align*}\]</span> The limiting distribution is a quadratic form of normally distributed variables, <em>but</em> the matrix in the center which “weights” the quadratic form does not correspond properly to the variance of the distributions such that we have a chi-squared distribution. We need this matrix in the center to be equal to the variance of the normal distribution in order to “standardize” it and give us the square of two standard normal distributions (which is the definition of a chi-squared distributed). <span class="math display">\[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \boldsymbol \Omega \mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \neq  \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\]</span> For this relation to hold with equality, we need to make an assumption about the relationship between <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span> and <span class="math inline">\(\boldsymbol \Omega\)</span>.</p>
<div class="definition">
<p>Suppose <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is an extremum estimator for which <span class="math inline">\(\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span>, and <span class="math inline">\(\frac{\partial^2 Q_n}{\partial \boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\boldsymbol{\theta}_0) \overset{p}{\to}\mathbf H(\boldsymbol{\theta}_0)\)</span>. In addition assume that there exists some scalar <span class="math inline">\(c\in \mathbb R\)</span> such that <span class="math display">\[\boldsymbol \Omega = c \mathbf H(\boldsymbol{\theta}_0),\]</span> giving and <span class="math inline">\(\boldsymbol \Omega \propto \mathbf H(\boldsymbol{\theta}_0)\)</span>. The equality associated with this assumption is the <span style="color:red"><strong><em>generalized information matrix equality</em></strong></span>.</p>
</div>
<p>This assumption seems completely arbitrary, but once we start working with concrete examples of extremum estimators, we’ll see it hold in many familiar settings that motivate it holding in this general case.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Also note that as we’ve defined <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, <span class="math inline">\(c &lt; 0\)</span>. If <span class="math inline">\(Q_0\)</span> is uniquely maximized at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, then its Hessian <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span> is negative semi-definite, while <span class="math inline">\(\boldsymbol \Omega\)</span> corresponds to a variance (meaning its positive semi-definite). These facts imply <span class="math inline">\(c&lt; 0\)</span>.</p>
<p>If we assume the generalized information matrix equality holds, then <span class="math display">\[\begin{align*}
\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \boldsymbol \Omega \mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'&amp; = \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} c \mathbf H(\boldsymbol{\theta}_0) \mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'= c\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'
\end{align*}\]</span> All we need to do is divide our statistic by <span class="math inline">\(c\)</span> to account for the proportionality. <span class="math display">\[\frac{2n}{c}[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})]\overset{d}{\to}N\left(\mathbf{0},\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}  \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \right)' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1} N\left(\mathbf{0},\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\mathbf H(\boldsymbol{\theta}_0)^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \right) = \chi_q^2.\]</span></p>
<div class="definition">
<p>Suppose <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> is an extremum estimator for which <span class="math inline">\(\sqrt n \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \overset{d}{\to}N(\mathbf{0}, \boldsymbol \Omega)\)</span>, and <span class="math inline">\(\frac{\partial^2 Q_n}{\partial \boldsymbol{\theta}\partial\boldsymbol{\theta}'}(\boldsymbol{\theta}_0) \overset{p}{\to}\mathbf H(\boldsymbol{\theta}_0)\)</span>, and <span class="math inline">\(\boldsymbol \Omega = c \mathbf H(\boldsymbol{\theta}_0)\)</span> for some <span class="math inline">\(c &lt; 0\)</span>. The <span style="color:red"><strong><em>distance metric statistic</em></strong></span> for the null hypothesis <span class="math inline">\(H_0\)</span> is <span class="math display">\[DM = \frac{2n}{\hat c}[Q_n(\hat{\boldsymbol\theta}_\text{EX} ) - Q_n(\bar{\boldsymbol{\theta}}_\text{EE})],\]</span> where <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> is the extremum estimator subject to <span class="math inline">\(H_0\)</span>.</p>
</div>
<p>Now we can formalize things into a theorem.</p>
<div id="trintest" class="theorem" name="Equivelence of Testing Trinity">
<p>Let <span class="math inline">\(H_0:\mathbf h(\boldsymbol{\theta}) = \mathbf{0}\)</span> be some hypothesis where <span class="math inline">\(\mathbf h:\boldsymbol{\Theta}\to \mathbb R^q\)</span> (<span class="math inline">\(q \le \dim(\boldsymbol{\Theta}) = K\)</span>), <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span> be an extremum estimator for <span class="math inline">\(\boldsymbol{\theta}\)</span>, and <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\)</span> be the restricted extremum estimator for <span class="math inline">\(\boldsymbol{\theta}\)</span> under <span class="math inline">\(H_0\)</span>. Suppose:</p>
<ol type="a">
<li>The function <span class="math inline">\(\mathbf h(\boldsymbol{\theta})\)</span> is continuously differentiable in a neighborhood of <span class="math inline">\(\boldsymbol{\theta}_0\)</span>;</li>
<li>The conditions of Theorem @ref(thm:exasy) are satisfied.</li>
<li><span class="math inline">\(\frac{\partial \mathbf h(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\)</span> is invertible;</li>
<li><span class="math inline">\(\hat {\mathbf H}(\boldsymbol{\theta}_0)\)</span> and <span class="math inline">\(\hat {\boldsymbol\Omega}\)</span> are consistent estimators of <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span> and <span class="math inline">\(\boldsymbol \Omega\)</span>, respectively;</li>
<li><span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE} \overset{p}{\to}\boldsymbol{\theta}_0\)</span> under <span class="math inline">\(H_0\)</span>.</li>
<li>There exists some scalar <span class="math inline">\(c\in \mathbb R\)</span> such that <span class="math inline">\(\boldsymbol \Omega = c \mathbf H(\boldsymbol{\theta}_0)\)</span>, and <span class="math inline">\(\hat c\)</span> is a consistent estimator for <span class="math inline">\(c\)</span>.</li>
</ol>
<p>Then:</p>
<ol type="1">
<li>Under assumptions a-d, <span class="math inline">\(W \overset{d}{\to}\chi_q^2\)</span>;</li>
<li>Under assumptions a-e, <span class="math inline">\(LM \overset{d}{\to}\chi_q^2\)</span>. If assumption f holds, then <span class="math display">\[LM = \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat {\boldsymbol\Omega}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE}) \]</span></li>
<li>Under assumptions a-f, <span class="math inline">\(DM \overset{d}{\to}\chi_q^2\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<ol type="1">
<li>We proved that <span class="math inline">\(W\overset{d}{\to}\chi_q^2\)</span> in Theorem @ref(thm:wald). All we need is a consistent estimator for <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\theta}_\text{EX} \right) = \mathbf H(\boldsymbol{\theta}_0)^{-1}\boldsymbol\Omega\mathbf H(\boldsymbol{\theta}_0)^{-1}\)</span>. By Assumption d, we have this in the form of <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{EX} ) = \hat {\mathbf H}(\boldsymbol{\theta}_0)^{-1} \hat {\boldsymbol\Omega}{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\)</span></li>
<li>We showed that <span class="math inline">\(LM\)</span> is a special case of <span class="math inline">\(W\)</span> with the additional assumption that <span class="math inline">\(\bar{\boldsymbol{\theta}}_\text{EE}\overset{p}{\to}\boldsymbol{\theta}_0\)</span> under <span class="math inline">\(H_0\)</span>. IIf assumption f holds, we have a consistent estimator for <span class="math inline">\(\boldsymbol \Omega\)</span> in the form of <span class="math inline">\(\hat {\boldsymbol\Omega} = \hat c\hat{\mathbf H}(\boldsymbol{\theta}_0)\)</span>: <span class="math display">\[\begin{align*}
LM &amp;= n\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\hat c\hat{\mathbf H}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})\\
&amp; = \frac{n}{\hat c}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})\\
&amp; = \frac{n}{\hat c}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\underbrace{\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\right]^{-1}}_{\mathbf I}\underbrace{\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}\right]^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}}_{\mathbf I}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})\\
&amp; = \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})'\frac{\hat{\mathbf H}(\boldsymbol{\theta}_0)^{-1}}{\hat c} \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})\\
&amp; = \frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})' \hat {\boldsymbol\Omega}^{-1}\frac{\partial Q_n}{\partial \boldsymbol{\theta}}(\bar{\boldsymbol{\theta}}_\text{EE})
\end{align*}\]</span></li>
<li>We already established that the test statistic converges in distribution to <span class="math inline">\(\chi_q^2\)</span> when <span class="math inline">\(c\)</span>, is known. By Slutsky’s theorem, this will still hold for a consistent estimator <span class="math inline">\(\hat c\)</span>.</li>
</ol>
<p><span style="color:white">space</span></p>
</div>
<p>If we assume <span class="math inline">\(\dim(\boldsymbol{\Theta}) = \dim(\mathbf h) = 1\)</span>, we can plot <span class="math inline">\(Q_n\)</span>, <span class="math inline">\(H_0:h(\theta) = 0\)</span> and the distances corresponding to our these three statistics.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="extremum_cache/html/unnamed-chunk-4_6d8685a58e0c7fb14a7c08e24e94a509">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="extremum_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Despite our trio of our test statistics being asymptotically equivalent, there are still some advantages and disadvantages associated with each one. The Wald test and Lagrange multiplier test only require us to estimate the unrestricted model, whereas the distance metric test requires estimating the unrestricted model and restricted model. The distance metric test doesn’t require us to estimate any asymptotic variances, only the constant <span class="math inline">\(c\)</span> where <span class="math inline">\(\boldsymbol \Omega = c \mathbf H(\boldsymbol{\theta}_0)\)</span>. The restricted model may also be much simpler than the unrestricted model (for example <span class="math inline">\(H_0\)</span> could be “the model is linear”), in which case the Lagrange multiplier test is much easier to use.</p>
</section>
<section id="m-estimators" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="m-estimators"><span class="header-section-number">7.6</span> M-Estimators</h2>
<p>A particular class of extremum estimators that we will work with often define <span class="math inline">\(Q_n\)</span> to be a sample average of some function of the data.</p>
<div class="definition">
<p>Suppose <span class="math inline">\(\mathbb{W}= [\mathbf{W}_1,\ldots, \mathbf{W}_n]' \sim P_\boldsymbol{\theta}\)</span> where <span class="math inline">\(P_\boldsymbol{\theta}\in\mathcal P\)</span> for a known model <span class="math inline">\(\mathcal P\)</span>. An <span style="color:red"><strong><em>M-estimator</em></strong></span> <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} :\mathcal W\to \boldsymbol{\Theta}\)</span> is an extremum estimator where <span class="math display">\[\begin{align*}
Q_n(\boldsymbol{\theta}) &amp;= \frac{1}{n}\sum_{i=1}^n m(\boldsymbol{\theta},\mathbf{W}_i),\\
\hat{\boldsymbol\theta}_\text{M} &amp;= \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} \frac{1}{n}\sum_{i=1}^n m(\boldsymbol{\theta}, \mathbf{W}_i).
\end{align*}\]</span></p>
</div>
<p><span class="citation" data-cites="Huber">Huber (<a href="references.html#ref-Huber" role="doc-biblioref">1964</a>)</span> formalize M-estimators in an effort to generalize MLE (hence the letter M). Clearly M-estimators are consistent and asymptotically normal under the conditions of Theorems @ref(thm:excon) and @ref(thm:exasy), but perhaps these conditions simplify in the special case of <span class="math inline">\(Q_n(\boldsymbol{\theta})=n^{-1}\sum_{i=1}^n m(\boldsymbol{\theta}, \mathbf{W}_i)\)</span>. For instance, <span class="math inline">\(Q_n\)</span> is the sum of sample averages, so perhaps we can use some LLN to conclude <span class="math inline">\(Q_n \overset{p}{\to}Q_0\)</span> uniformly instead of directly verifying that <span class="math inline">\(\sup_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} \left\lvert Q_n - Q_0\right\rvert \overset{p}{\to}0\)</span>.</p>
<div id="uwlln" class="lemma" name="Uniform Weak Law of Large Numbers (UWLLN)">
<p>Suppose that <span class="math inline">\(\mathbf{W}_i \overset{iid}{\sim}P_\boldsymbol{\theta}\)</span>, <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact, <span class="math inline">\(m(\boldsymbol{\theta}, \mathbf{W}_i)\)</span> is continuous on <span class="math inline">\(\boldsymbol{\Theta}\)</span> for all <span class="math inline">\(\mathbf{W}_i \in \mathcal W\)</span>, and there exists some <span class="math inline">\(d(\mathbf{W}_i)\)</span> such that <span class="math inline">\(\left\lVert m(\boldsymbol{\theta},\mathbf{W}_i)\right\rVert \le d(\mathbf{W}_i)\)</span> on <span class="math inline">\(\boldsymbol{\Theta}\)</span> where <span class="math inline">\(\text{E}\left[d(\mathbf{W}_i)\right] &lt; \infty\)</span>. Then <span class="math inline">\(\text{E}\left[m(\boldsymbol{\theta}, \mathbf{W}_i)\right]\)</span> is continuous and <span class="math display">\[n^{-1}\sum_{i=1}^n m(\boldsymbol{\theta}, W_i) \overset{p}{\to}\text{E}\left[m(\boldsymbol{\theta}, \mathbf{W}_i)\right]\]</span> uniformly on <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
</div>
<p>While I haven’t been able to locate a formal proof of this result, it seems like a direct application of the Weierstrass M-test (Theorem 7.10 in <span class="citation" data-cites="rudin1976principles">Rudin (<a href="references.html#ref-rudin1976principles" role="doc-biblioref">1976</a>)</span>) and the fact that the uniform limit of continuous functions is continuous. The lemma also includes two of the other conditions from @ref(thm:excon): <span class="math inline">\(\boldsymbol{\Theta}\)</span> is compact, and <span class="math inline">\(Q_0 = \text{E}\left[m(\boldsymbol{\theta}, \mathbf{W}_i)\right]\)</span>.</p>
<div class="corollary" name="Consistency of M-Estimators">
<p>If the conditions of Lemma @ref(lem:uwlln) hold and <span class="math inline">\(\text{E}\left[m(\boldsymbol{\theta}_0, \mathbf W)\right] &gt; \text{E}\left[m(\boldsymbol{\theta}, \mathbf W)\right]\)</span> for all <span class="math inline">\(\boldsymbol{\theta}\neq\boldsymbol{\theta}_0\)</span>, then <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Conditions 2-4 of @ref(thm:excon) are satisfied under the assumption that Lemma @ref(lem:uwlln) holds. Condition 1 is satisfied as well because <span class="math display">\[ \mathop{\mathrm{plim}}Q_n = \mathop{\mathrm{plim}}\frac{1}{n}\sum_{i=1}^n m(\boldsymbol{\theta}, W_i) = \text{E}\left[m(\boldsymbol{\theta}_0, \mathbf W)\right] = Q_0,\]</span> and we’ve assumed <span class="math inline">\(Q_0 = \text{E}\left[m(\boldsymbol{\theta}_0, \mathbf W)\right]\)</span> is uniquely maximized at <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.</p>
</div>
<p>We can also restate Theorem @ref(thm:exasy) in the context of M-estimators. Let’s write the Jacobian and Hessian of <span class="math inline">\(Q_n\)</span> in the event that <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} = \hat{\boldsymbol\theta}_\text{M} \)</span>. <span class="math display">\[\begin{align*}
\frac{\partial Q_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &amp;= \frac{\partial}{\partial \boldsymbol{\theta}}\left[\frac{1}{n}\sum_{i=1}^n m(\boldsymbol{\theta},\mathbf{W}_i)\right] = \frac{1}{n}\sum_{i=1}^n \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}}\\
\frac{\partial Q_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'} &amp; = \frac{1}{n}\sum_{i=1}^n \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'}
\end{align*}\]</span> The derivatives of <span class="math inline">\(Q_n\)</span> are sample averages of the derivatives of <span class="math inline">\(m\)</span>. Following <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span> define the random matrix <span class="math inline">\(\mathbb{H}(\boldsymbol{\theta}, \mathbf{W}_i) = \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'}\)</span> and random vector <span class="math inline">\(\mathbf S(\boldsymbol{\theta}, \mathbf{W}_i)= \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}}\)</span>.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> Theorem @ref(thm:exasy) also involved the probability limits of the Jacobian and Hessian of <span class="math inline">\(Q_n\)</span>, and in the case of M-estimators, these happen to be expected values because of the LLN. <span class="math display">\[\begin{align*}
\frac{\partial Q_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} &amp;= \frac{1}{n}\sum_{i=1}^n \mathbf S(\boldsymbol{\theta}, \mathbf{W}_i) \overset{p}{\to}\text{E}\left[\mathbf S(\boldsymbol{\theta}, \mathbf{W})\right]\\
\frac{\partial Q_n(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'} &amp; = \frac{1}{n}\sum_{i=1}^n \mathbb{H}(\boldsymbol{\theta}, \mathbf{W}_i) \overset{p}{\to}\text{E}\left[\mathbb{H}(\boldsymbol{\theta}, \mathbf{W})\right]
\end{align*}\]</span></p>
<div id="asyM" class="corollary" name="Asymptotic Normality of M-Estimators">
<p>Suppose the conditions of @ref(thm:exasy) are met where <span class="math display">\[\begin{align*}
Q_n(\boldsymbol{\theta}) &amp;= \frac{1}{n}\sum_{i=1}^n m(\boldsymbol{\theta},\mathbf{W}_i),\\
\mathbf S(\boldsymbol{\theta}, \mathbf{W}_i) &amp;= \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}},\\
\mathbb{H}(\boldsymbol{\theta}, \mathbf{W}_i) &amp;= \frac{\partial m(\boldsymbol{\theta},\mathbf{W}_i)}{\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}'}.
\end{align*}\]</span> Then <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\boldsymbol\theta}_\text{M} - \boldsymbol{\theta}) &amp;\overset{d}{\to}N\left(\mathbf{0}, \text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\text{Var}\left(S(\boldsymbol{\theta}_0, \mathbf{W})\right)\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\right),\\
\hat{\boldsymbol\theta}_\text{M} &amp; \overset{a}{\sim}N\left(\boldsymbol{\theta}, \frac{1}{n}\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\text{Var}\left( S(\boldsymbol{\theta}_0, \mathbf{W})\right)\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\right).
\end{align*}\]</span></p>
</div>
<p>We are also able to use Lemma @ref(lem:uwlln) to verify condition 6 of Theorem @ref(thm:exasy). In the case of Corollary @ref(cor:asyM), Condition 5 of Theorem @ref(thm:exasy) establishes that <span class="math inline">\(\text{E}\left[\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})\right] = \mathbf{0}\)</span>, so the expectation of <span class="math inline">\(\mathbf S\)</span> “squared” is the variance, giving <span class="math display">\[ \hat{\boldsymbol\theta}_\text{M} \overset{a}{\sim}N\left(\boldsymbol{\theta}, \frac{1}{n}\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\text{E}\left[\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})'\right]\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\right).\]</span> In general, we didn’t provide an estimator for <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\theta}_\text{EX} \right)\)</span>, but we should be able to use the LLN to find a consistent estimator for <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\theta}_\text{M} \right)\)</span>. Define the Hessian and score functions evaluated for a single observation <span class="math inline">\(\mathbf{W}_i\)</span>: <span class="math display">\[\begin{align*}
\hat{\mathbb{H}}_i &amp;= \mathbb{H}(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i) &amp; (i =1,\ldots,n)\\
\hat{\mathbf S}_i &amp;= \mathbf{S}(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i) &amp; (i =1,\ldots,n)
\end{align*}\]</span> If the LLN holds for these random quantities, then <span class="math display">\[\begin{align*}
\widehat{\text{E}\left[\mathbb{H}\right]} &amp;= \frac{1}{n}\sum_{i=1}^n {\mathbb{H}}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i) \overset{p}{\to}\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]\\
\widehat{\text{E}\left[\mathbf S\mathbf S'\right]} &amp;= \frac{1}{n}\sum_{i=1}^n {\mathbf S}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i){\mathbf S}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i)'  \overset{p}{\to}\text{E}\left[\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})'\right]
\end{align*}\]</span> because <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} \overset{p}{\to}\boldsymbol{\theta}_0\)</span>. The matrices of constants <span class="math inline">\(\boldsymbol \Omega\)</span> and <span class="math inline">\(\mathbf H\)</span> from Theorem @ref(thm:exasy) now correspond to the variance of the mean-zero random vector <span class="math inline">\(\mathbf S\)</span> and expectation of random matrix <span class="math inline">\(\mathbb{H}\)</span>, respectively.</p>
<div class="theorem" name="Estimating Asymptotic Variance for M-Estimators">
<p>Suppose <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} \)</span> is an M-estimator, and define <span class="math display">\[ \widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{M} ) = \frac{1}{n}\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}\left[\mathbf S\mathbf S'\right]}\widehat{\text{E}\left[\mathbb{H}\right]}^{-1} = \left[\sum_{i=1}^n {\mathbb{H}}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i)\right]^{-1}\left[\sum_{i=1}^n {\mathbf S}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i){\mathbf S}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i)'\right]\left[\sum_{i=1}^n {\mathbb{H}}_i(\hat{\boldsymbol\theta}_\text{M} , \mathbf{W}_i)\right]^{-1}.\]</span> Then <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\theta}_\text{M} ) \overset{p}{\to}\text{Avar}\left(\hat{\boldsymbol\theta}_\text{M} \right)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The consistency follows from the above derivation. If you factor out the <span class="math inline">\(n\)</span> terms from <span class="math inline">\(\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\)</span> and <span class="math inline">\(\widehat{\text{E}\left[\mathbf S\mathbf S'\right]}\)</span>, they (including the <span class="math inline">\(1/n\)</span>) will all cancel.</p>
</div>
<div class="example" name="OLS as an M-estimator">
<p>Suppose we are estimating the classical linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. In this case we can partition the random vector <span class="math inline">\(\mathbf{W}_i\)</span> into <span class="math inline">\((\varepsilon_i,\mathbf{X}_i)\)</span>, where the model defines <span class="math inline">\(Y_i = \mathbf{X}_i\boldsymbol{\beta}+ \varepsilon_i\)</span>. This is an M-estimator where <span class="math display">\[m(\boldsymbol{\beta}, Y_i, \mathbf{X}_i) = -(Y_i - \mathbf{X}_i\boldsymbol{\beta})^2.\]</span> We have <span class="math display">\[\begin{align*}
\mathbf S_i(\boldsymbol{\beta}, Y_i, \mathbf{X}_i) &amp; = 2\mathbf{X}_i'(Y_i - \mathbf{X}_i\boldsymbol{\beta}) = 2\mathbf{X}_i\varepsilon_i\\
\mathbb{H}_i(\boldsymbol{\beta}, Y_i, \mathbf{X}_i) &amp; = 2\mathbf{X}_i'\mathbf{X}_i
\end{align*}\]</span></p>
<p>In Example @ref(exm:olsEX), we saw that <span class="math inline">\(Q_0\)</span> is uniquely maximized at <span class="math inline">\(\boldsymbol{\beta}_0\)</span>. The parameter space <span class="math inline">\(\boldsymbol{\Theta}= \mathbb R^k\)</span> is convex, <span class="math inline">\(Q_n\)</span> is concave, <span class="math inline">\(\boldsymbol{\beta}_0\)</span> is an interior point of <span class="math inline">\(\boldsymbol{\Theta}\)</span> (otherwise it would be infinity), and by @ref(lem:uwlln) <span class="math inline">\(Q_n \overset{p}{\to}Q_0\)</span> uniformly, so by Theorem @ref(thm:excon2) <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}_0\)</span>. Note that by the assumptions of <span class="math inline">\(\mathcal P_\text{LM}\)</span>, <span class="math inline">\(\text{E}\left[\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})\right] = \mathbf{0}\)</span> (<span class="math inline">\(\mathbf{X}_i\)</span> is weakly exogenous) and <span class="math inline">\(\text{E}\left[\mathbb{H}(\boldsymbol{\beta}, Y_i, \mathbf{X}_i)\right]\)</span> is invertible (<span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> has full rank), along with all the other assumptions of Corollary @ref(cor:asyM) being met. Therefore we have</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol\beta}_\text{OLS} &amp;\overset{a}{\sim}N\left(\mathbf{0}, \frac{1}{n}\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\text{Var}\left(\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})\mathbf S(\boldsymbol{\theta}_0, \mathbf{W})'\right)\text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf{W})\right]^{-1}\right)\\
&amp; \overset{a}{\sim}N\left(\boldsymbol{\beta}, \frac{1}{n}\text{E}\left[2\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[2\mathbf{X}'\boldsymbol{\varepsilon}[2\mathbf{X}'\boldsymbol{\varepsilon}]'\right]\text{E}\left[2\mathbf{X}'\mathbf{X}\right]^{-1}\right)\\
&amp; \overset{a}{\sim}N\left(\boldsymbol{\beta}, \frac{1}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}\mathbf{X}\right]\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\right)\\
&amp; \overset{a}{\sim}N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{X}'\mathbf{X}\right]\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\right)\\
&amp; \overset{a}{\sim}N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\right).
\end{align*}\]</span></p>
<p>This is the <em>exact</em> same asymptotic distribution from Theorem @ref(thm:asymols)! We can also derive an asymptotic estimator for <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\)</span> using properties of M-estimators. <span class="math display">\[\begin{align*}
\widehat{\text{E}\left[\mathbb{H}\right]} &amp;= \frac{2}{n}\sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\\
\widehat{\text{E}\left[\mathbf S\mathbf S'\right]} &amp;= \frac{4}{n}\sum_{i=1}^n \hat{e}_i^2\mathbf{X}_i'\mathbf{X}_i =  \left(\frac{2}{n}\sum_{i=1}^n \hat e_i^2\right) \left(\frac{2}{n}\sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\right)\\
\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}\left[\mathbf S\mathbf S'\right]} \widehat{\text{E}\left[\mathbb{H}\right]}^{-1} &amp; = \left(\frac{2}{n}\sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\right)^{-1} \left(\frac{2}{n}\sum_{i=1}^n \hat e_i^2\right) \left(\frac{2}{n}\sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\right)\left(\frac{2}{n}\sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\right)^{-1} =\hat{\mathbf{e}}'\hat{\mathbf{e}}\left(\mathbb{X}'\mathbb{X}\right)^{-1}  \\
\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ) &amp; = \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\left(\mathbb{X}'\mathbb{X}\right)^{-1}
\end{align*}\]</span> But this doesn’t look right, because <span class="math inline">\(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n} \neq S^2\)</span>, as it doesn’t have the bias correction of <span class="math inline">\(n-K\)</span>. Technically, this isn’t the end of the world because the estimator for <span class="math inline">\(\sigma^2\)</span> is asymptotically unbiased.</p>
</div>
<p>In general, we weren’t able to estimate the asymptotic variance of <span class="math inline">\(\hat{\boldsymbol\theta}_\text{EX} \)</span>, but we can for <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} \)</span>. This means we can use the trio of test statistics developed in the previous section.</p>
<div id="asyM" class="corollary" name="Testing Trinity, M-Estimators">
<p>If <span class="math inline">\(\hat{\boldsymbol\theta}_\text{M} \)</span> is an M-estimator, the test statistics in Theorem @ref(thm:trintest) become: <span class="math display">\[\begin{align*}
W &amp; = n\mathbf h(\hat{\boldsymbol\theta}_\text{M} )'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}\left[\mathbf S\mathbf S'\right]}\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )'\right]^{-1}   \mathbf h(\hat{\boldsymbol\theta}_\text{M} );\\
LM &amp; =  n\widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]}'\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)' \left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0) \widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}}{[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})']}\widehat{\text{E}\left[\mathbb{H}\right]}^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)'\right]^{-1} \frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]};\\
DM &amp; = \frac{2}{\hat c} \left[\sum_{i=1}^n m(\hat{\boldsymbol\theta}_\text{M} ,\mathbf{W}_i) - \sum_{i=1}^n m(\bar{\boldsymbol{\theta}}_\text{M},\mathbf{W}_i)\right].
\end{align*}\]</span> In the generalized information inequality holds when calculating <span class="math inline">\(LM\)</span>, we have <span class="math display">\[ LM = n\widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]}'\widehat{\text{E}}{[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})']}^{-1} \widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]}\]</span></p>
</div>
<div id="olstests" class="example">
<p>Let’s employ our tests in the context of the classical linear model. In this case, let <span class="math display">\[m_i(\boldsymbol{\beta}, Y_i, \mathbf{X}_i) = -\frac{1}{2} \sum_{i=1}^n (Y_i - \mathbf{X}_i\boldsymbol{\beta})^2,\]</span> such that the 2s are eliminated when differentiating <span class="math inline">\(m_i(\boldsymbol{\beta}, Y_i, \mathbf{X}_i)\)</span>.</p>
<p>First, let’s confirm the generalized information inequality holds and that we can use the distance metric statistic. <span class="math display">\[\begin{align*}
\mathbf H(\boldsymbol{\theta}_0) &amp; = \text{E}\left[\mathbb{H}(\boldsymbol{\theta}_0, \mathbf W)\right] = \text{E}\left[\mathbf{X}'\mathbf{X}\right] \\
\boldsymbol \Omega &amp; = \text{E}\left[\mathbf S(\boldsymbol{\theta}_0, \mathbf W)\mathbf S(\boldsymbol{\theta}_0, \mathbf W)'\right] = \text{E}\left[\mathbf{X}'\varepsilon\varepsilon'\mathbf{X}\right] = \text{E}\left[\varepsilon^2\right]\text{E}\left[\mathbf{X}'\mathbf{X}\right] = \sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right]
\end{align*}\]</span> The inequality holds, as <span class="math inline">\(\boldsymbol \Omega = c\mathbf H(\boldsymbol{\theta}_0)\)</span>, where <span class="math inline">\(c = \sigma^2\)</span>. We could let <span class="math inline">\(\hat c = S^2\)</span>, but here we’ll use <span class="math display">\[\hat c = \frac{\hat{\boldsymbol \Omega}}{\hat{\mathbf H}(\boldsymbol{\theta}_0)} = \frac{1}{n}\sum_{i=1}^n \hat e_i^2 = \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}.\]</span></p>
<p>Our test statistics are <span class="math display">\[\begin{align*}
W &amp; = n\mathbf h(\hat{\boldsymbol\theta}_\text{M} )'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\widehat{\text{E}\left[\mathbf S\mathbf S'\right]}\widehat{\text{E}\left[\mathbb{H}\right]}^{-1}\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )'\right]^{-1}   \mathbf h(\hat{\boldsymbol\theta}_\text{M} ) \\
&amp; = \mathbf h(\hat{\boldsymbol\theta}_\text{M} )'\left[\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )\left[\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\left(\mathbb{X}'\mathbb{X}\right)^{-1}\right]\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\hat{\boldsymbol\theta}_\text{M} )'\right]^{-1}   \mathbf h(\hat{\boldsymbol\theta}_\text{M} ) \\\\
LM &amp;= n\widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]}'\widehat{\text{E}}{[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})']} \widehat{\text{E}\left[\mathbf S(\bar{\boldsymbol{\theta}}_\text{M})\right]}\\
&amp; = n\left[\frac{1}{n}\sum_{i=1}^n  \bar{e}_i\mathbf{X}_i \right]'\left[\frac{1}{n}\sum_{i=1}^n \bar{e}_i^2\mathbf{X}_i'\mathbf{X}_i \right]^{-1} \left[\frac{1}{n}\sum_{i=1}^n \bar e_i\mathbf{X}_i \right]\\
&amp; = \left[\sum_{i=1}^n  \bar{e}_i\mathbf{X}_i \right]'\left[\frac{\bar{\mathbf{e}}'\bar{\mathbf{e}}}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i \right]^{-1} \left[\sum_{i=1}^n \bar e_i\mathbf{X}_i \right]\\
&amp; = \mathbb{X}'\bar{\mathbf{e}}\left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\mathbb{X}'\mathbb{X}\right)^{-1}\bar{\mathbf{e}}'\mathbb{X}\\\\

DM &amp; = \frac{2}{\hat c} \left[\sum_{i=1}^n m(\hat{\boldsymbol\theta}_\text{M} ,\mathbf{W}_i) - \sum_{i=1}^n m(\bar{\boldsymbol{\theta}}_\text{M},\mathbf{W}_i)\right]\\
   &amp; = \frac{2}{\hat{\mathbf{e}}'\hat{\mathbf{e}}/n}\left[\sum_{i=1}^n -\frac{1}{2}(Y_i-\mathbf{X}_i\hat{\boldsymbol\beta}_\text{OLS} )^2 - \sum_{i=1}^n -\frac{1}{2}(Y_i-\mathbf{X}_i\bar{\boldsymbol{\beta}}_\text{OLS})^2\right]\\
   &amp; = \frac{1}{\hat{\mathbf{e}}'\hat{\mathbf{e}}/n}\left[\sum_{i=1}^n \bar e_i^2 - \sum_{i=1}^n \bar e_i^2\right]\\
   &amp; = \frac{1}{\hat{\mathbf{e}}'\hat{\mathbf{e}}/n}\left[\bar{\mathbf{e}}'\bar{\mathbf{e}} - \hat{\mathbf{e}}'\hat{\mathbf{e}}\right]
\end{align*}\]</span> The residuals associated with the unrestricted estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> are <span class="math inline">\(\hat e_i\)</span>, while <span class="math inline">\(\bar e_i\)</span> are the residuals associated with the restricted estimator. If we let <span class="math inline">\(SSR = \hat{\mathbf{e}}'\hat{\mathbf{e}}\)</span> and <span class="math inline">\(SSR_0 = \bar{\mathbf{e}}'\bar{\mathbf{e}}\)</span> denote the sum of squared residuals for the unrestricted and restricted models respectively, then</p>
<p><span class="math display">\[ DM = \frac{SSR_0 - SSR}{SSR/n}.\]</span> Suppose <span class="math inline">\(Y = 1 + 2X + \varepsilon\)</span>, where <span class="math inline">\(\boldsymbol{\beta}= [1,2]'\)</span>. Let’s test the null hypothesis that the parameters coincide with their true underlying values, <span class="math inline">\(H_0: \boldsymbol{\beta}= \boldsymbol{\beta}_0\)</span>. In this case <span class="math display">\[\begin{align*}
\mathbf h(\boldsymbol{\beta}) &amp;= \begin{bmatrix}\beta_1 - \beta_{0,2}\\ \beta_2 - \beta_{0,1} \end{bmatrix}\\
\frac{\partial\mathbf h(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &amp;= \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} = \mathbf I
\end{align*}\]</span> where <span class="math inline">\(H_0: \mathbf h(\boldsymbol{\beta}) = \mathbf{0}\)</span>. The image <span class="math inline">\(\mathbf h(\boldsymbol{\Theta})\)</span> is a singleton, so when we maximize <span class="math inline">\(Q_n\)</span> such that <span class="math inline">\(\mathbf h(\boldsymbol{\theta}) = \mathbf{0}\)</span> to get our restricted estimate, we trivially have <span class="math inline">\(\bar{\boldsymbol{\beta}}_\text{OLS} = \boldsymbol{\beta}_0\)</span>. For this hypothesis we have <span class="math display">\[\begin{align*}
W &amp; = (\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0)'\left[\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\left(\mathbb{X}'\mathbb{X}\right)^{-1}\right]^{-1}(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0)\\
  &amp; = \left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\right)^{-1}(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0)'\left(\mathbb{X}'\mathbb{X}\right)(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0) \\
  &amp; = \left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\right)^{-1}(\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} - \mathbb{X}\boldsymbol{\beta}_0)'(\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} - \mathbb{X}\boldsymbol{\beta}_0)\\
  &amp; = \left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\right)^{-1}((\mathbf{Y}- \hat{\mathbf{e}}) - (\mathbf{Y}- \bar{\mathbf{e}}))'((\mathbf{Y}- \hat{\mathbf{e}}) - (\mathbf{Y}- \bar{\mathbf{e}})) &amp; (\mathbf{Y}= \mathbf{X}\hat{\boldsymbol\beta}_\text{OLS} + \hat{\mathbf{e}},\ \mathbf{Y}= \mathbf{X}\boldsymbol{\beta}_0 + \bar{\mathbf{e}})\\
  &amp; = \left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\right)^{-1}(\bar{\mathbf{e}} - \hat{\mathbf{e}})'(\bar{\mathbf{e}} - \hat{\mathbf{e}})\\
  &amp; = \left(\frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n}\right)^{-1}(\bar{\mathbf{e}}'\bar{\mathbf{e}} - \bar{\mathbf{e}}'\hat{\mathbf{e}}  - \hat{\mathbf{e}}'\bar{\mathbf{e}} + \hat{\mathbf{e}}'\hat{\mathbf{e}})
\end{align*}\]</span> It happens to be the case that <span class="math inline">\(\bar{\mathbf{e}}'\hat{\mathbf{e}} = \hat{\mathbf{e}}'\bar{\mathbf{e}} = \hat{\mathbf{e}}'\hat{\mathbf{e}}\)</span>,<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> so we have <span class="math display">\[\begin{align*}
W  =\frac{\bar{\mathbf{e}}'\bar{\mathbf{e}} - \hat{\mathbf{e}}'\hat{\mathbf{e}}}{\hat{\mathbf{e}}'\hat{\mathbf{e}}/n} = DM.
\end{align*}\]</span></p>
<div class="cell" data-hash="extremum_cache/html/unnamed-chunk-5_c346c1ae166d15293e31cf8c3f0134c2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#estimate model and calculate OLS residuals and restricted OLS residual</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> y <span class="sc">-</span> X <span class="sc">%*%</span> OLS</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>res_0 <span class="ot">&lt;-</span> y <span class="sc">-</span> X <span class="sc">%*%</span> beta_0</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Wald stat</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ee_n <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>avar <span class="ot">&lt;-</span> ee_n <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">t</span>(OLS <span class="sc">-</span> beta_0) <span class="sc">%*%</span> <span class="fu">solve</span>(avar) <span class="sc">%*%</span> (OLS <span class="sc">-</span> beta_0)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">#LM stat</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>LM <span class="ot">&lt;-</span> <span class="fu">t</span>(res_0) <span class="sc">%*%</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(ee_n <span class="sc">*</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> res_0</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">#DM stat</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>SSR <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">t</span>(res) <span class="sc">%*%</span> res)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>SSR_0 <span class="ot">&lt;-</span>  <span class="fu">as.numeric</span>(<span class="fu">t</span>(res_0) <span class="sc">%*%</span> res_0)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>DM <span class="ot">&lt;-</span> (SSR_0 <span class="sc">-</span> SSR) <span class="sc">/</span> (ee_n)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(W, LM, DM)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.199298 3.199298 3.199298</code></pre>
</div>
</div>
</div>
<section id="nonlinear-least-squares" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="nonlinear-least-squares"><span class="header-section-number">7.6.1</span> Nonlinear Least Squares</h3>
<p>We’ve generalized the classical linear model <span class="math inline">\(\mathcal P_\text{LM}\)</span> several ways. Section @ref(endogeniety-i-iv-and-2sls) dropped the assumption <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]=\mathbf{0}\)</span> and introduced the model <span class="math inline">\(\mathcal P_\text{IV}\)</span>. Section @ref(generalized-least-squares) dropped the assumption <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\right] =\sigma^2 \mathbf I\)</span> and introduced the general linear regression model <span class="math inline">\(\mathcal P_\text{GLRM}\)</span>. Section @ref(endogeniety-ii-simultaneous-equation-models) extended the linear model by allowing for multiple “seemingly unrelated” linear models, <span class="math inline">\(\mathcal P_\text{SUR}\)</span>. In each of these case, we assumed the structural relationship between the dependent variable and regressors was linear. Let’s now drop this assumptions.</p>
<p>Suppose we posit that <span class="math inline">\(Y\)</span> is related to regressors <span class="math inline">\(\mathbf{X}\)</span> and parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> through some deterministic relationship given by a function <span class="math inline">\(f(\mathbf{X}, \boldsymbol{\beta})\)</span>. We can add a structural error such that <span class="math inline">\(Y_i = g(\mathbf{X}_i,\boldsymbol{\beta}) + \varepsilon_i\)</span>, or <span class="math display">\[ \mathbf{Y}= \mathbf g(\mathbb{X},\boldsymbol{\beta}) + \boldsymbol{\varepsilon}\]</span> where <span class="math inline">\(\mathbf g\)</span> is a vector-valued function of <span class="math inline">\(n\)</span> copies of <span class="math inline">\(f\)</span>. An element of our model <span class="math inline">\(\mathcal P\)</span> will contain elements <span class="math inline">\(P\)</span> which are collections of joint distributions <span class="math inline">\(F_{\mathbb{X},\boldsymbol{\varepsilon}}\)</span> satisfying some common conditions. We also want our model to reduce to <span class="math inline">\(\mathcal P_\text{LM}\)</span> in the event that <span class="math inline">\(\mathbf g(\mathbb{X},\boldsymbol{\beta}) = \mathbb{X}\boldsymbol{\beta}\)</span>. Two of the assumptions of the <span class="math inline">\(\mathcal P_\text{LM}\)</span> are pretty easy as we can replace <span class="math inline">\(\mathbb{X}\)</span> with $g(,) $:</p>
<ul>
<li>Strict exogeneity, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbf g(\mathbb{X},\boldsymbol{\beta})\right] = \mathbf{0}\implies \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span> when <span class="math inline">\(\mathbf g(\mathbb{X},\boldsymbol{\beta}) = \mathbb{X}\boldsymbol{\beta}\)</span>. This assumption also implies weak exogeneity <span class="math inline">\(\text{E}\left[\mathbf g(\mathbb{X},\boldsymbol{\beta})'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>.</li>
<li>Spherical errors, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbf g(\mathbb{X},\boldsymbol{\beta})\right] = \sigma^2\mathbf I \implies \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \sigma^2\mathbf I\)</span> when <span class="math inline">\(\mathbf g(\mathbb{X},\boldsymbol{\beta}) = \mathbb{X}\boldsymbol{\beta}\)</span></li>
</ul>
<p>The one assumption of <span class="math inline">\(\mathcal P_\text{LM}\)</span> that takes some thought to generalize is <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>. What role did this assumption play in the linear model? It made sure that <span class="math inline">\(\boldsymbol{\beta}\neq \boldsymbol{\beta}'\)</span> whenever <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\neq \mathbf{X}\boldsymbol{\beta}'\)</span>, as <span class="math inline">\(\boldsymbol{\beta}\)</span> satisfies <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \boldsymbol{\beta}\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> in this case. In the event <span class="math inline">\(g(\mathbf{X}_i,\boldsymbol{\beta}) = \mathbf{X}_i\boldsymbol{\beta}\)</span> for <span class="math inline">\(i = 1,\ldots,n\)</span>, then this condition ensures that <span class="math inline">\(\boldsymbol{\beta}\neq \boldsymbol{\beta}'\)</span> implies <span class="math inline">\(g(\mathbf{X}_i,\boldsymbol{\beta}) \neq g(\mathbf{X}_i,\boldsymbol{\beta}')\)</span> for all <span class="math inline">\(i\)</span>. This will be the analogous condition – <span class="math inline">\(\boldsymbol{\beta}\neq \boldsymbol{\beta}'\)</span> implies <span class="math inline">\(g(\mathbf{X}_i,\boldsymbol{\beta}) \neq g(\mathbf{X}_i,\boldsymbol{\beta}')\)</span>.</p>
<div class="definition">
<p>The <span style="color:red"><strong><em>(classical) nonlinear model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{NLM} = \{P_{\boldsymbol{\beta},\sigma^2} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \sigma^2\in\mathbb R\}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\sigma^2} &amp;= \{F_{\mathbb{X},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbf g(\mathbb{X},\boldsymbol{\beta}) + \boldsymbol{\varepsilon}, \ \text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbf g(\mathbb{X},\boldsymbol{\beta})\right]=\sigma^2\mathbf I, \ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i}, \boldsymbol{\beta}\neq\boldsymbol{\beta}' \implies \mathbf g(\mathbb{X},\boldsymbol{\beta}) \neq \mathbf g(\mathbb{X},\boldsymbol{\beta}'),\ \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbf g(\mathbb{X},\boldsymbol{\beta})\right] = \mathbf{0}\},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n],
\end{align*}\]</span> for a known function <span class="math inline">\(\mathbf g(\mathbb{X}, \boldsymbol{\beta})\)</span>.</p>
</div>
<p>This definition of the model assumes that we’ve correctly specified <span class="math inline">\(\mathbf g(\mathbb{X}, \boldsymbol{\beta})\)</span>, just like how the linear model assumed that the relationship between <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbb{X}\)</span> was actually <span class="math inline">\(\mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span>. For <span class="math inline">\(\mathcal P_\text{NLM}\)</span> this assumption is much stronger. We are not only assuming that the model contains the correct independent variables, but we are also assuming the relationship <span class="math inline">\(\mathbf g(\mathbb{X}, \boldsymbol{\beta})\)</span> is correct. The natural estimator for the nonlinear model is nonlinear least squares.</p>
<div class="definition">
<p>The <span style="color:red"><strong><em>nonlinear least squares estimator</em></strong></span> is defined as <span class="math display">\[ \hat{\boldsymbol{\beta}}_\text{NLLS}(\mathbb{X}, \mathbf{Y}) = \mathop{\mathrm{argmin}}_\boldsymbol{\beta}\frac{1}{n}\sum_{i=1}^n(Y_i - g(\mathbf{X}_i,\boldsymbol{\beta}))^2 \]</span></p>
</div>
<p>This estimator is an M-estimator where <span class="math inline">\(m(\boldsymbol{\beta}, Y_i, \mathbf{X}_i) = -\(Y_i - g(\mathbf{X}_i,\boldsymbol{\beta}))^2\)</span>, and <span class="math display">\[Q_0 = \mathop{\mathrm{plim}}-\frac{1}{n}\sum_{i=1}^n(Y_i - g(\mathbf{X}_i,\boldsymbol{\beta}))^2= - \text{E}\left[Y - g(\mathbf{X}, \boldsymbol{\beta})\right]\]</span></p>
</section>
<section id="least-absolute-deviations" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="least-absolute-deviations"><span class="header-section-number">7.6.2</span> Least Absolute Deviations</h3>
</section>
</section>
<section id="minimum-distance-estimators" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="minimum-distance-estimators"><span class="header-section-number">7.7</span> Minimum Distance Estimators</h2>
<div class="definition">
<p>Suppose <span class="math inline">\(\mathbb{W}= [\mathbf{W}_1,\ldots, \mathbf{W}_n]' \sim P_\boldsymbol{\theta}\)</span> where <span class="math inline">\(P_\boldsymbol{\theta}\in\mathcal P\)</span> for a known model <span class="math inline">\(\mathcal P\)</span>. A <span style="color:red"><strong><em>minimum distance estimator</em></strong></span> <span class="math inline">\(\hat{\boldsymbol\theta}_\text{MDE} :\mathcal W\to \boldsymbol{\Theta}\)</span> is an extremum estimator where <span class="math display">\[\begin{align*}
Q_n(\boldsymbol{\theta}) &amp;= -[\hat {\boldsymbol \pi} - \mathbf g(\boldsymbol{\theta})]'\boldsymbol \Phi [\hat {\boldsymbol \pi}- \mathbf g(\boldsymbol{\theta})],\\
\hat{\boldsymbol\theta}_\text{MDE} &amp;= \mathop{\mathrm{argmax}}_{\boldsymbol{\theta}\in \boldsymbol{\Theta}} -[\hat {\boldsymbol \pi} - \mathbf g(\boldsymbol{\theta})]'\boldsymbol \Phi [\hat {\boldsymbol \pi}- \mathbf g(\boldsymbol{\theta})],
\end{align*}\]</span> for some sample statistic <span class="math inline">\(\hat {\boldsymbol \pi}\)</span> satisfying <span class="math inline">\(\hat {\boldsymbol \pi} \overset{p}{\to}\mathbf g(\boldsymbol{\theta})\)</span> and a weighting matrix <span class="math inline">\(\boldsymbol \Phi\)</span>.</p>
</div>
</section>
<section id="recap" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="recap"><span class="header-section-number">7.8</span> Recap</h2>
</section>
<section id="examplereplication" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="examplereplication"><span class="header-section-number">7.9</span> Example/Replication</h2>
</section>
<section id="futher-reading" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="futher-reading"><span class="header-section-number">7.10</span> Futher Reading</h2>
<p><strong><em>Extremum estimators</em></strong>: <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span>, Chapter 4 of <span class="citation" data-cites="takeshi1985advanced">Amemiya (<a href="references.html#ref-takeshi1985advanced" role="doc-biblioref">1985</a>)</span>, <a href="https://www.ssc.wisc.edu/~xshi/econ715/Lecture_6_testing2.pdf">these</a> <a href="https://www.ssc.wisc.edu/~xshi/econ715/Lecture_3_consistency.pdf">awesome</a> <a href="https://www.ssc.wisc.edu/~xshi/econ715/Lecture_4_normality.pdf">slides</a> posted by Xiaoxia Shi, Chapter Chapter 7 of <span class="citation" data-cites="hayashi2011econometrics">Hayashi (<a href="references.html#ref-hayashi2011econometrics" role="doc-biblioref">2011</a>)</span>, Chapter 12 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span></p>
<p><strong><em>Trinity of Hypothesis Tests</em></strong>: <span class="citation" data-cites="engle1984wald">Engle (<a href="references.html#ref-engle1984wald" role="doc-biblioref">1984</a>)</span>, Section 12.4 of <span class="citation" data-cites="lehmann2005testing">Romano and Lehmann (<a href="references.html#ref-lehmann2005testing" role="doc-biblioref">2005</a>)</span>, Chapter 16 of <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="references.html#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span></p>
<p><strong><em>M-estimators</em></strong>: Chapter 12 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 22 of <span class="citation" data-cites="hansen2022econometrics">B. Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span>, Chapter 12 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Chapter 7 of <span class="citation" data-cites="hayashi2011econometrics">Hayashi (<a href="references.html#ref-hayashi2011econometrics" role="doc-biblioref">2011</a>)</span>, Chapter 17 of <span class="citation" data-cites="davidson1993estimation">Davidson and MacKinnon (<a href="references.html#ref-davidson1993estimation" role="doc-biblioref">1993</a>)</span></p>
<p><strong><em>Minimum distances estimators</em></strong>:</p>
<p><strong><em>NLLS</em></strong>: Chapter 2 of <span class="citation" data-cites="davidson1993estimation">Davidson and MacKinnon (<a href="references.html#ref-davidson1993estimation" role="doc-biblioref">1993</a>)</span>, Chapter 7 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Chapter 12 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 23 of <span class="citation" data-cites="hansen2022econometrics">B. Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="mizon1977inferential">Mizon (<a href="references.html#ref-mizon1977inferential" role="doc-biblioref">1977</a>)</span>, Chapter 5 of <span class="citation" data-cites="cameron2005microeconometrics">Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span></p>
<p><strong><em>LAD</em></strong>:</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-aliprantisinfinite" class="csl-entry" role="doc-biblioentry">
Aliprantis, Charalambos D, and Kim C Border. n.d. <span>“Infinite Dimensional Analysis a Hitchhiker’s Guide.”</span>
</div>
<div id="ref-amemiya1973regression" class="csl-entry" role="doc-biblioentry">
Amemiya, Takeshi. 1973. <span>“Regression Analysis When the Dependent Variable Is Truncated Normal.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 997–1016.
</div>
<div id="ref-takeshi1985advanced" class="csl-entry" role="doc-biblioentry">
———. 1985. <em>Advanced Econometrics</em>. Harvard university press.
</div>
<div id="ref-cameron2005microeconometrics" class="csl-entry" role="doc-biblioentry">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-davidson1993estimation" class="csl-entry" role="doc-biblioentry">
Davidson, Russell, and James G MacKinnon. 1993. <em>Estimation and Inference in Econometrics</em>. Vol. 63. Oxford New York.
</div>
<div id="ref-engle1984wald" class="csl-entry" role="doc-biblioentry">
Engle, Robert F. 1984. <span>“Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics.”</span> <em>Handbook of Econometrics</em> 2: 775–826.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-hansen2022econometrics" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce. 2022. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-hansen2022probability" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce E. 2022. <em>Probability and Statistics for Economists</em>. Princeton University Press.
</div>
<div id="ref-hayashi2011econometrics" class="csl-entry" role="doc-biblioentry">
Hayashi, Fumio. 2011. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-Huber" class="csl-entry" role="doc-biblioentry">
Huber, Peter J. 1964. <span>“Robust Estimation of a Location Parameter.”</span> <em>The Annals of Mathematical Statistics</em> 35 (1): 73–101. <a href="http://www.jstor.org/stable/2238020">http://www.jstor.org/stable/2238020</a>.
</div>
<div id="ref-judd1998numerical" class="csl-entry" role="doc-biblioentry">
Judd, Kenneth L. 1998. <em>Numerical Methods in Economics</em>. MIT press.
</div>
<div id="ref-lewbel2019identification" class="csl-entry" role="doc-biblioentry">
Lewbel, Arthur. 2019. <span>“The Identification Zoo: Meanings of Identification in Econometrics.”</span> <em>Journal of Economic Literature</em> 57 (4): 835–903.
</div>
<div id="ref-mizon1977inferential" class="csl-entry" role="doc-biblioentry">
Mizon, Grayham E. 1977. <span>“Inferential Procedures in Nonlinear Models: An Application in a UK Industrial Cross Section Study of Factor Substitution and Returns to Scale.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 1221–42.
</div>
<div id="ref-newey1994large" class="csl-entry" role="doc-biblioentry">
Newey, Whitney K, and Daniel McFadden. 1994. <span>“Large Sample Estimation and Hypothesis Testing.”</span> <em>Handbook of Econometrics</em> 4: 2111–2245.
</div>
<div id="ref-rao1948large" class="csl-entry" role="doc-biblioentry">
Rao, C Radhakrishna. 1948. <span>“Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.”</span> In <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 44:50–57. 1. Cambridge University Press.
</div>
<div id="ref-lehmann2005testing" class="csl-entry" role="doc-biblioentry">
Romano, Joseph P, and EL Lehmann. 2005. <em>Testing Statistical Hypotheses</em>. Vol. 3. Springer.
</div>
<div id="ref-rudin1976principles" class="csl-entry" role="doc-biblioentry">
Rudin, Walter. 1976. <em>Principles of Mathematical Analysis</em>. Vol. 3. McGraw-hill New York.
</div>
<div id="ref-van2000asymptotic" class="csl-entry" role="doc-biblioentry">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="doc-biblioentry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>If you’re a real analysis nerd, you likely see what assumption we’re heading towards.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A set is compact is every open cover submits a finite open subcover.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Assuming <span class="math inline">\(Q_n\overset{p}{\to}Q_0\)</span> uniformly, we could also have that <span class="math inline">\(Q_n\)</span> is continuous for all <span class="math inline">\(n\)</span>, as the uniform limit of continuous functions is continuous.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>In the event that <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is an interior point of <span class="math inline">\(\boldsymbol{\Theta}\)</span>, then <span class="math inline">\(N_r(\boldsymbol{\theta}_0) \subset \boldsymbol{\Theta}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The intersection of a closed set and compact set is compact.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>“Event” in the probabilistic sense, as <span class="math inline">\(Q_n(\boldsymbol{\theta}\mid \mathbb{W})\)</span> is a random variable.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="references.html#ref-newey1994large" role="doc-biblioref">1994</a>)</span> asserts it holds for any topological space, but the concept of uniform convergence not only requires a notion of proximity (given by open sets in a topology), but also some concept of distance (given by a metric).<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>This is why <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span> define parameterization in the context of linear models as “[the parameter] can be written in terms of population moments in observable variables”.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Technically, we’re performing <span class="math inline">\(\dim(\boldsymbol{\Theta}) = K\)</span> different expansions and just writing them succinctly in matrix form.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Once again, we’re actually performing mean value expansions in several variables at once but consolidating it using vectors and matrices. This means that these inequalities hold element-wise.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Many of the terms involving <span class="math inline">\(\frac{\partial \mathbf h}{\partial \boldsymbol{\theta}}(\boldsymbol{\theta}_0)\)</span> and <span class="math inline">\(\mathbf H(\boldsymbol{\theta}_0)\)</span> will cancel. <span class="math display">\[ \]</span><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>We could also weaken this assumption by assuming there is a sequence of constants <span class="math inline">\(c_n\)</span> whose limit gives the proportion.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><span class="math inline">\(\mathbf H\)</span> as in “Hessian of <span class="math inline">\(m\)</span>”, <span class="math inline">\(\mathbf S\)</span> as in “score function” (lending from the terminology for MLE).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><span class="math display">\[\begin{align*} \bar{\mathbf{e}}'\hat{\mathbf{e}} &amp; = (\mathbf{Y}- \mathbb{X}\boldsymbol{\beta}_0)'(\mathbf{Y}- \mathbb{X}\boldsymbol{\beta}_0) \\ &amp; = \mathbf{Y}'\mathbf{Y}- \mathbf{Y}'\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0'\mathbb{X}\mathbf{Y}+ \boldsymbol{\beta}_0'\mathbb{X}'\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} \\ &amp; = \mathbf{Y}'\mathbf{Y}- \mathbf{Y}'\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}- \boldsymbol{\beta}_0'\mathbb{X}\mathbf{Y}+ \boldsymbol{\beta}_0'\mathbb{X}'\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}\\
&amp; = \mathbf{Y}'[\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}']\mathbf{Y}- \boldsymbol{\beta}_0'\mathbb{X}\mathbf{Y}+ \boldsymbol{\beta}_0'\mathbb{X}'\mathbf{Y}\\
&amp; = \mathbf{Y}'\mathbb M\mathbf{Y}\\
&amp; = (\mathbf{X}\hat{\boldsymbol\beta}_\text{OLS} + \hat{\mathbf{e}})'\mathbb M\mathbf{Y}\\
&amp; = \hat{\mathbf{e}}'\underbrace{\mathbf M \mathbf{Y}}_{\hat{\mathbf{e}}} + \hat{\boldsymbol\beta}_\text{OLS} '\underbrace{\mathbf{X}\mathbb M}_\mathbf{0}\mathbf{Y}\\
&amp; = \hat{\mathbf{e}}'\hat{\mathbf{e}} \\
\hat{\mathbf{e}}'\bar{\mathbf{e}} &amp; = (\bar{\mathbf{e}}'\hat{\mathbf{e}})'\\
&amp; = (\hat{\mathbf{e}}'\hat{\mathbf{e}})'\\
&amp; = \hat{\mathbf{e}}'\hat{\mathbf{e}}
\end{align*}\]</span><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./endog.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./binary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb3" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># Extremum Estimators </span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>The estimation of linear models turned out to be fairly straightforward, as all our estimators were somehow related to OLS. When moving beyond linear models, we need to consider more general approaches to estimation, such as the generalized method of moments (GMM) or maximum likelihood estimation (MLE). Both of these estimators fall into a very broad class of estimators formalized by @takeshi1985advanced known as extremum estimators. We'll establish the properties of this broad class of estimators before considering special cases in Section \@ref(generalized-method-of-moments) and Section \@ref(maximum-likelihood-estimation). A great deal of this will be based off of @newey1994large, and requires comfort with <span class="co">[</span><span class="ot">real analysis</span><span class="co">](https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf)</span>. It may be helpful to review the relationship between continuity and limits, sequences of functions, properties of uniform convergence, compactness (and how it relates to extrema), convexity (in the context of sets and functions), and the mean value theorem as it applies to gradients and Hessians.</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Definition and Identification</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>As the name implies, an extremum estimator is one that is defined via optimization.</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>Suppose $\Wm = <span class="co">[</span><span class="ot">\W_1,\ldots, \W_n</span><span class="co">]</span>' \sim P_\thet$ where $P_\thet\in\mathcal P$ for a known model $\mathcal P$. An <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_extremum estimator_**<span class="kw">&lt;/span&gt;</span> $\EE:\mathcal W\to \Thet$ is an estimator of the form </span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>$$ \EE = \argmax_{\thet \in \Thet} Q_n(\thet \mid \Wm)$$ for an objective function $Q_n(\thet \mid \Wm)$ which depends on realized observations and sample size. A more general definition defines $\EE$ as the value which satisfies </span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>$$Q_n(\EE) \ge \sup_{\thet \in \Thet} Q_n(\thet) + o_p(1).$$</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>We've written $\EE$ as the argument which maximizes an objective function, but this implicitly includes the case where $\EE$ is the minimizing argument of a function, as </span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>$$ \argmax_{\thet \in \Thet} Q_n(\thet) = \argmin_{\thet \in \Thet} <span class="co">[</span><span class="ot">-Q_n(\thet)</span><span class="co">]</span>.$$ The idea that an estimate should arise from an optimization problem should feel natural. We generally want an estimator to be "better" than other estimators, and extremum estimators achieve this in terms of some criterion $Q_n(\thet)$. </span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>The estimator $\OLS$ is an extremum estimator.</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>$$\OLS = (\Xm'\Xm)^{-1}\Xm'\Y = \argmin_{\mathbf b \in \mathbb R^k} \sum_{i=1}^n (Y_i - \X_i\mathbf b)^2 = \argmax_{\mathbf b \in \mathbb R^k}\left<span class="co">[</span><span class="ot"> - \sum_{i=1}^n (Y_i - \X_i\mathbf b)^2\right</span><span class="co">]</span> = \argmax_{\mathbf b \in \mathbb R^k} -\norm{\Y - \Xm\mathbf b}^2$$ We could also maximize any monotonic transformation of this, giving way to a few other common objectives:</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>Q_n(\bet) = -\sum_{i=1}^n (Y_i - \X_i\bet)^2 \propto -\frac{1}{n} \sum_{i=1}^n (Y_i - \X_i\bet)^2 \propto -\frac{1}{2} \sum_{i=1}^n (Y_i - \X_i\bet)^2 = -\frac{1}{2}\norm{\Y - \Xm\mathbf b}^2</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>Before we consider the properties of extremum estimators and give a handful of example, we need to address a major problem from optimization that could plague us. If asked to calculate $\EE$, it's tempting to inspect the first order condition</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial Q_n(\thet)}{\partial \thet} = \zer.$$ Not only does this assume $Q_n$ is differentiable (which it may not be), we may find that the first order condition has several solutions, even if there is a unique maximum. Furthermore that unique maximum could be local and not global. To avoid these complications, we'll think about $\EE$ as a global maximum, and not a solution to a first order condition.</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>::: {.hypothesis name="Numerical Optimization"}</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>In most non-trivial situations, extremum estimators will not have an analytic form, so we need to turn to numerical methods to solve the requisite optimization problem. Numerical optimization is a behemoth subject spanning applied math, computer science, operations research, and engineering. Most standard econometrics references dedicate some time to the subject:</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 10 of @cameron2005microeconometrics</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Section 7.5 of @hayashi2011econometrics</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Section 12.7 of @wooldridge2010econometric</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Appendix E of @greene2003econometric</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 12 of @hansen2022probability</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>A more comprehensive treatment is @judd1998numerical. </span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>Despite the definition of $\EE$ being wildly general, we can establish that $\EE\pto \thet$ under certain conditions. The only defining features of $\EE$ are the function being maximized ($Q_n$), and the space over which it is being maximized ($\Thet$), so we should expect that we'll need to impose some conditions on one of (if not both) of these objects. Let's start by writing the definition of convergence in this case.</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>$$ \plim \argmax_{\thet \in \Thet} Q_n(\thet) = \thet_0$$</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>The fact that we are dealing with the limiting process of $\argmax_{\thet \in \Thet} Q_n(\thet)$ instead of $Q_n$ complicates things. Limiting processes of function do not play well with many of our favorite operators.^<span class="co">[</span><span class="ot">If you're a real analysis nerd, you likely see what assumption we're heading towards.</span><span class="co">]</span> </span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>Suppose $\Theta = <span class="co">[</span><span class="ot">-1,\infty</span><span class="co">]</span>$ and $\theta_0 = -1/2$. Define functions $Q$, $g_n$, and $Q_n$ as: </span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>Q(\theta) &amp;= \begin{cases}1+\theta &amp; -1\le\theta\le\theta_0 <span class="sc">\\</span> -\theta &amp; \theta_0&lt;\theta\le 0 <span class="sc">\\</span> 0 &amp; \text{otherwise}  \end{cases}<span class="sc">\\</span></span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>g_n(\theta) &amp;= \begin{cases}\theta - n &amp; n\le\theta\le n + 1 <span class="sc">\\</span> n + 2 - \theta &amp; n + 1\le\theta\le n + 2 <span class="sc">\\</span> 0 &amp; \text{otherwise} \end{cases}<span class="sc">\\</span></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>Q_n(\theta) &amp; = Q(\theta) + g_n(\theta)</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>These functions don't contain random variables, so the $\plim$ operator coincides with $\lim_{n\to\infty}$ The apparently nonsense function $Q_n$ is actually defined such that we can determine it's limit and maximum by inspecting a plot. </span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}</span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="cf">function</span>(t){</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (t <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">0.5</span>) {</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="sc">+</span> t</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (t <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span> <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> <span class="dv">0</span> ) {</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>      <span class="sc">-</span>t</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>      <span class="dv">0</span></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (n <span class="sc">&lt;=</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> n<span class="sc">+</span> <span class="dv">1</span>) {</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>    t <span class="sc">-</span> n</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (n <span class="sc">+</span> <span class="dv">1</span> <span class="sc">&lt;=</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> n <span class="sc">+</span> <span class="dv">2</span>) {</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>      n <span class="sc">+</span> <span class="dv">2</span> <span class="sc">-</span> t</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>      <span class="dv">0</span></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>QH <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>  <span class="fu">Q</span>(t) <span class="sc">+</span> <span class="fu">H</span>(t,n)</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">t =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">22</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>,<span class="dv">20</span>))</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df), <span class="cf">function</span>(i)<span class="fu">QH</span>(df[i,<span class="dv">1</span>], df[i,<span class="dv">2</span>]) )</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t,y)) <span class="sc">+</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"θ"</span>,</span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Qn"</span>,</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"n"</span>) <span class="sc">+</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span><span class="fu">as.factor</span>(n), <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>It appears that $\plim Q_n = Q$, as the height of the the "triangle" formed on $\theta \in <span class="co">[</span><span class="ot">n,n+2</span><span class="co">]</span>$ shrinks (this is the the contribution of $g_n/n$), leaving the triangle formed by $f$. Formally, </span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>$$ \plim Q_n(\theta) = \plim Q(\theta) + \plim g_n(\theta) = Q(\theta)  + 0 = Q(\theta).$$</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>If we compare the limit of the $\argmax$ and the $\argmax$ of the limit, we have </span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>\plim \argmax_{\thet \in \Thet} Q_n(\thet) &amp; = \plim n + 1 = \infty<span class="sc">\\</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>\argmax_{\thet \in \Thet} \plim Q_n(\thet) &amp; = \argmax_{\thet \in \Thet} f = \theta_0</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>One reason this happens, is because the nature in which $Q_n$ converges. While we have $\plim Q_n(\thet) = Q(\thet)$ for all $\thet \in \Thet$, the function does not converge uniformly in the sense that $Q_n$ does not approaches $Q$ on all of $\Thet$ simultaneously. We need to strengthen our definition of convergence in probability to insure this happens.</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a>A sequence of functions random variables $f_n(X)$ <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_converges in probability uniformly (on $\mathcal X$)_**<span class="kw">&lt;/span&gt;</span> to a function $f$ if for all $\varepsilon &gt; 0$ and $\delta &gt; 0$ there exists a fixed $N$ independent of $X_n$ such that </span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>$$ \Pr(\abs{f_n(X) - f} &gt; \varepsilon,\ \forall X \in\mathcal X) &lt; \delta,$$ which is equivalent to </span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>$$ \lim_{n\to\infty}\Pr(\abs{f_n(X) - f} &lt; \varepsilon,\ \forall X \in\mathcal X) = 0  $$. Yet another definition is </span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>$$\sup_{\thet\in \Thet}\abs{f_n(X) - f} \pto 0.$$</span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>The definition using the supremum follows from the fact that if the maximum distance between $f_n(X)$ and $f$ shrinks as $n\to\infty$, then is must be the case that the distance between $f_n$ and $f$ at any point in the support is shrinking as well. So let's assume that $Q_n$ converges uniformly to some limit $Q_0$. Let's look at another example, but this time define $Q_n$ such that it converges uniformly to some $Q_0$. </span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>Suppose $\Theta = <span class="co">[</span><span class="ot">0,\theta_0</span><span class="co">]</span>$ and $Q_n(\theta) = \theta/n$. Once again, there are no random variables in the picture, so $\plim$ reduces to $\lim_{n\to\infty}$. We have </span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>$$ \sup_{\theta \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>}\abs{\theta/n - 0} = 1/n \to 0,$$ so $Q_n(\theta)$ converges uniformly to $Q_0(\theta) = 0$ on $<span class="co">[</span><span class="ot">0,\theta_0</span><span class="co">]</span>$. The maximum of $Q_n$ is achieved at $\theta_0$ for all $n$. Despite the uniform convergence, we have </span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a>\plim \argmax_{\thet \in \Thet} Q_n(\thet) &amp; = \plim \theta_0 = \theta_0<span class="sc">\\</span></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>\argmax_{\thet \in \Thet} \plim Q_n(\thet) &amp; = \argmax_{\theta \in<span class="co">[</span><span class="ot">0,\theta_0</span><span class="co">]</span>} Q_0(\theta) = \argmax_{\theta \in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>} 0 = <span class="co">[</span><span class="ot">0,\theta_0</span><span class="co">]</span></span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>We need to rule out situations where the limit of $Q_n$ does not have a unique maximum, or that unique maximum is not achieved at the true value $\thet_0$. Finally, consider a third example.</span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>Suppose $\Theta = <span class="co">[</span><span class="ot">-2,1</span><span class="co">]</span>$ and $\theta_0 = -1$. Define </span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>$$Q_n(\theta) = \begin{cases}(1-2^{1-n})\theta + 2 - 2^{2-n} &amp; -2\le \theta\le -1</span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>(2^{1-n}-1)\theta &amp; -1 &lt; \theta \le 0 <span class="sc">\\</span> \frac{2^{n+1}-2}{2^{n+1}-1}\theta &amp; 0  &lt; \theta\le 1-2^{1-n}<span class="sc">\\</span> \frac{2^{n+1}-2}{2^{n+1}-1}\theta + 2 - 2^{2-n} &amp; 1-2^{1-n}\le \theta &lt; 1<span class="sc">\\</span>0 &amp; \theta = 1\end{cases}$$.</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE}</span></span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a>Q1 <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>  (<span class="sc">-</span><span class="dv">2</span> <span class="sc">&lt;=</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> <span class="sc">-</span><span class="dv">1</span>)<span class="sc">*</span>( (<span class="dv">1-2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">-</span>n))<span class="sc">*</span>t <span class="sc">+</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">^</span>(<span class="dv">2</span><span class="sc">-</span>n) )</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>Q2 <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>  (<span class="sc">-</span><span class="dv">1</span> <span class="sc">&lt;=</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> <span class="dv">0</span>)<span class="sc">*</span>(<span class="sc">-</span>(<span class="dv">1-2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">-</span>n))<span class="sc">*</span>t)</span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a>Q3 <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">0</span> <span class="sc">&lt;</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;=</span> (<span class="dv">1-2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">-</span>n)))<span class="sc">*</span>(  ((<span class="dv">2</span><span class="sc">^</span>(n<span class="sc">+</span><span class="dv">1</span>) <span class="sc">-</span> <span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">^</span>(n<span class="sc">+</span><span class="dv">1</span>) <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">*</span>t )</span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>Q4 <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a>  ((<span class="dv">1-2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">-</span>n)) <span class="sc">&lt;</span> t <span class="sc">&amp;</span> t <span class="sc">&lt;</span> <span class="dv">1</span>)<span class="sc">*</span>(  ((<span class="dv">2</span><span class="sc">^</span>(n<span class="sc">+</span><span class="dv">1</span>) <span class="sc">-</span> <span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">^</span>(n<span class="sc">+</span><span class="dv">1</span>) <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">*</span>t <span class="sc">+</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">-</span>n) )</span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>Q5 <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>  (t <span class="sc">==</span> <span class="dv">1</span>)<span class="sc">*</span>(<span class="dv">0</span>)</span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="cf">function</span>(t,n){</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>  <span class="fu">Q1</span>(t,n) <span class="sc">+</span> <span class="fu">Q2</span>(t,n) <span class="sc">+</span> <span class="fu">Q3</span>(t,n) <span class="sc">+</span> <span class="fu">Q4</span>(t,n) <span class="sc">+</span> <span class="fu">Q4</span>(t,n) <span class="sc">+</span> <span class="fu">Q5</span>(t,n)</span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">t =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">1</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df), <span class="cf">function</span>(i)<span class="fu">Q</span>(df[i,<span class="dv">1</span>], df[i,<span class="dv">2</span>]) )</span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span> </span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">n =</span> <span class="fu">paste</span>(<span class="st">"n="</span>, n, <span class="at">sep=</span><span class="st">""</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t,y)) <span class="sc">+</span></span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"θ"</span>,</span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Qn"</span>) <span class="sc">+</span></span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>n) <span class="sc">+</span></span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a>By inspecting $Q_n$ for $n = 1,\ldots, 6$, we see that </span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a> \argmax_{\thet \in \Thet} \plim Q_n(\thet) &amp; = \plim \theta_0 = \theta_0,<span class="sc">\\</span></span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a>\plim \argmax_{\thet \in \Thet}  Q_n(\thet) &amp; = 1.</span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a>The limit $Q_0$ has a discontinuity at the point where $\plim \argmax_{\thet \in \Thet} Q_n(\thet) = 1$. If we wanted to insure that $Q_0$ is continuous on all of $\Thet$, we could redefine $\Thet = [-2,1)$. Unfortunately, this would mean that the limit of the sequence formed by \argmax_{\thet \in \Thet}  Q_n(\thet) converges to a point outside of $\Thet$.</span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a>This final examples shows that things can go wrong when our function $Q_0$ is not continuous, or when $\Thet$ does not contain the limit of the maximized objective. In this example the limit in question was not in $\Thet$ because $\Thet$ did not contain all its limit points, i.e it isn't a closed set. Something like this could also happen if $\Thet$ is unbounded and the limit diverges. To ensure that this limit is always in $\Thet$ we need $\Thet$ to be compact.^<span class="co">[</span><span class="ot">A set is compact is every open cover submits a finite open subcover.</span><span class="co">]</span> If $\Thet \subset \mathbb R^k$, then this is equivalent to being closed and bounded. </span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a>We now have all the building blocks required to show that $\EE$ is consistent: continuity of $Q_0$,^<span class="co">[</span><span class="ot">Assuming $Q_n\pto Q_0$ uniformly, we could also have that $Q_n$ is continuous for all $n$, as the uniform limit of continuous functions is continuous.</span><span class="co">]</span> $Q_n \pto Q_0$ uniformly, $\Thet$ compact, and $Q_0(\thet)$ is uniquely maximized at $\Thet_0$. There are a few ways to prove this result, but I'll follow @takeshi1985advanced. The first proof seems to be due to @amemiya1973regression. Other great proofs are given by econometrician Xiaoxia Shi in <span class="co">[</span><span class="ot">these notes</span><span class="co">](https://www.ssc.wisc.edu/~xshi/econ715/Lecture_3_consistency.pdf)</span>, and by @newey1994large.</span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Consistency of Extremum Estimators I" #excon}</span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a>Suppose there exists some (non-stochastic) function $Q_0(\thet)$ such that:</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\thet_0 = \argmax_{\thet \in \Thet}Q_0(\thet)$;</span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\Thet$ is compact;</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$Q_0(\thet)$ is continuous on $\Thet$;</span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$Q_n(\thet)$ converges uniformly in probability to $Q_0(\thet)$.</span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a>Then $\EE \pto \thet_0$.</span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a>Let $N_r(\thet_0)$ be the open neighborhood/ball centered at $\thet_0$ with radius of $r &gt; 0$, where $r$ is arbitrary. The neighborhood $N_r(\thet_0)$ is not necessarily a subset of $\Thet$.^<span class="co">[</span><span class="ot">In the event that $\thet_0$ is an interior point of $\Thet$, then $N_r(\thet_0) \subset \Thet$.</span><span class="co">]</span> If $N^c_r(\thet_0)$ is the compliment of $N$, then $N^c_r(\thet_0)$ is closed, and $N^c_r(\thet_0) \cap \Thet \subset \Thet$ is compact.^<span class="co">[</span><span class="ot">The intersection of a closed set and compact set is compact.</span><span class="co">]</span> Define $$\thet^* = \argmax_{\thet \in N^c_r(\thet_0) \cap \Thet} Q_0(\thet).$$  The point $\Thet^*$ is guaranteed to exists because $Q_0$ is a continuous function and $N^c_r(\thet_0)\cap \Thet$ is a compact set. Define $\varepsilon = Q_0(\thet_0) - \thet^*$, and let $A_n$ be the event $``|Q_n(\thet) - Q_0(\thet)| &lt; \varepsilon/2$ for all $\thet"$.^["Event" in the probabilistic sense, as $Q_n(\thet \mid \Wm)$ is a random variable.]  Event $A_n$ holds *for all* $\thet$, including $\EE$ and $\thet_0$, so:</span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a>&amp; |Q_n(\EE) - Q_0(\EE)| &lt; \varepsilon/2 <span class="sc">\\</span></span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>\implies &amp; Q_n(\EE) - Q_0(\EE) &lt; \varepsilon/2<span class="sc">\\</span></span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a>\implies &amp;  Q_0(\EE) &gt; Q_n(\EE) - \varepsilon/2 (<span class="sc">\#</span>eq:a1)<span class="sc">\\\\</span></span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>&amp; |Q_n(\thet_0) - Q_0(\thet_0)| &lt; \varepsilon/2 <span class="sc">\\</span></span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a>\implies &amp; Q_n(\thet_0) - Q_0(\thet_0) &lt; -\varepsilon/2<span class="sc">\\</span></span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>\implies &amp;  Q_n(\thet_0) &gt; Q_0(\thet_0) - \varepsilon/2 (<span class="sc">\#</span>eq:a2)</span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>By the definition of $\EE$, $Q_n(\EE) \ge Q_n(\thet_0)$. If we combine this with \@ref(eq:a1), we have </span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a>Q_0(\EE) &gt; Q_n(\thet_0) - \varepsilon/2 (<span class="sc">\#</span>eq:a3).</span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a>If we add inequalities \@ref(eq:a2) and \@ref(eq:a3), event $A_n$ implies </span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a>&amp;Q_0(\EE)  + Q_n(\thet_0) &gt; Q_n(\thet_0) - \varepsilon/2 + Q_0(\thet_0) - \varepsilon/2<span class="sc">\\</span></span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a>\implies &amp; Q_0(\EE) &gt;  Q_0(\thet_0) - \varepsilon <span class="sc">\\</span> </span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a>\implies &amp;  Q_0(\EE) &gt;  Q_0(\thet_0) - Q_0(\thet_0) - \thet^* &amp; (\varepsilon = Q_0(\thet_0) - \thet^*)<span class="sc">\\</span></span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a>\implies &amp;  Q_0(\EE) &gt;  \thet^* <span class="sc">\\</span></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a>\implies &amp; \EE \notin N^c \cap \Thet &amp; \left(\thet^* = \argmax_{\thet \in N^c_r(\thet_0) \cap \Thet} Q_0(\thet)\right)<span class="sc">\\</span></span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a>\implies &amp; \EE \in N_r(\thet_0) &amp;(\EE \in \Thet)<span class="sc">\\</span></span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a>\implies &amp; \abs{\EE - \thet_0} &lt; r &amp; (\text{definition of }N_r(\thet_0))</span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a>If the event $A_n$ implies that $\abs{\EE - \thet_0} &lt; r$, then $\Pr(A_n) \le \Pr\left(\abs{\EE - \thet_0} &lt; r\right)$. We have assumed $Q_n \pto Q_0$ uniformly, so </span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a>&amp;\lim_{n\to \infty}\Pr(|Q_n(\thet) - Q_0(\thet)| &lt; \varepsilon/2) = 1<span class="sc">\\</span></span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to \infty}\Pr(A_n) = 1<span class="sc">\\</span></span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to \infty}\Pr\left(\abs{\EE - \thet_0} &lt; r\right) = 1 &amp; \left(\Pr(A_n) \le \Pr\left(\abs{\EE - \thet_0} &lt; r\right)\right)</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>We've taken the radius $r$ to be arbitrary, so </span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>&amp; \lim_{n\to \infty}\Pr\left(\abs{\EE - \thet_0} &lt; r\right) = 1 &amp; (\forall r &gt; 0).</span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>This is the definition of convergence in probability, so $\EE \pto \thet_0$!</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>This theorem applies to any metric space $\Thet$, not just subsets of Euclidean space.^<span class="co">[</span><span class="ot">@newey1994large asserts it holds for any topological space, but the concept of uniform convergence not only requires a notion of proximity (given by open sets in a topology), but also some concept of distance (given by a metric).</span><span class="co">]</span> @newey1994large provide slightly weaker conditions under which this theorem holds, the details of which can be studied in @aliprantisinfinite. </span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a>Actually applying Theorem, which we would like to do \@ref(thm:excon) in Section \@ref(generalized-method-of-moments) and Section \@ref(maximum-likelihood-estimation), can be a bit tricky. On particular problem comes with the assumption that $\Thet$ is compact. This will not hold whenever $\Thet = \mathbb R^k$, which is the cases we've been most concerned with. Realistically, we could restrict our attention to some closed subset $\Thet' \subset \mathbb R^k$ in most applications. For example, if we're estimating a linear model where we want to estimate the returns of schooling to log earnings $\beta$, we can confidently assume $\beta \in <span class="co">[</span><span class="ot">0, 10</span><span class="co">]</span>$. There are situations where we cannot do this though, so we need a second consistency result holds in the absence of compactness.  </span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Consistency of Extremum Estimators II" #excon2}</span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a>Suppose there exists some (non-stochastic) function $Q_0(\thet)$ such that:</span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\thet_0 = \argmax_{\thet \in \Thet}Q_0(\thet)$;</span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\thet_0$ is an interior point of $\Thet$;</span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\Thet$ is a convex set;</span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$Q_n(\thet)$ is a concave function on $\Thet$;</span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>$Q_n(\thet) \pto Q_0(\thet)$ (pointwise) on $\Thet$.</span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a>Then $\EE$ exists with probability approaching 1, and $\EE \pto \thet_0$.</span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>The point $\thet_0$ is an interior point of $\Thet$, so there exists a compact neighborhood of radius $2\varepsilon$ centered at $\thet_0$ contained in $\Thet$, $C\subset \Thet$. The boundary of this neighborhood is $\partial C$. Appealing to some more obscure real analysis results, we can conclude:</span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$Q_0$ is concave (the pointwise limit of concave functions is concave).</span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$Q_0$ is continuous on $C$ (concave functions are continuous on the interior of their domains).</span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$Q_n \pto Q_0$ uniformly on $C$ (pointwise convergence of concave functions of a dense subset of an open set implies uniform convergence on compact subsets of an open set)</span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a>Define $\tilde {\thet} = \argmax_{\thet\in C} Q_n(\thet)$. In light of points 2 and 3, we have $\tilde {\thet} \pto \thet_0$ by  \@ref(thm:excon). We will now show that $\tilde {\thet}$ not only maximizes $Q_n$ over $C$, but also over $\Thet$, so $\tilde{\thet} = \EE$ and $\EE \pto \thet_0$.</span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>If $\tilde{\thet} \pto \thet_0$, then $\Pr(\abs{\tilde{\thet} - \thet_0} &lt; \varepsilon)\to 1$. This is equivalent to $$\Pr\left(Q_n(\tilde{\thet}) \ge \max_{\partial C} Q_n(\thet)\right)\to 1.$$ By the convexity of $\Thet$, in the event that $Q_n(\tilde{\thet}) \ge \max_{\partial C} Q_n(\thet)$, there exists a convex combination $\alpha \tilde{\thet} + (1-\alpha)\tilde{\thet} \in \partial C$ ($\alpha$ &lt; 1) for any $\thet \notin C$. If we evaluate $Q_n$ at this convex combination, we have </span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a>Q_n(\tilde{\thet}) \ge Q_n(\alpha \tilde{\thet} + (1-\alpha)\tilde{\thet} ). (<span class="sc">\#</span>eq:a4)</span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a> But $Q_n$ is concave, so </span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a> \begin{equation}</span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a> Q_n(\alpha \tilde{\thet} + (1-\alpha)\tilde{\thet} ) \ge \alpha Q_n( \tilde{\thet}) + (1-\alpha)Q_n( \tilde{\thet}). (<span class="sc">\#</span>eq:a5)</span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a>If we combine inequalities \@ref(eq:a4) and \@ref(eq:a5), we have </span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a>$$(1-\alpha)Q_n(\tilde{\thet}) \ge (1-\alpha)Q_n({\thet}),$$</span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>so $\tilde{\thet} = \EE$.</span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a><span class="fu">## Extremum Identification</span></span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a>Perhaps the most important stipulation in Theorems  \@ref(thm:excon) and \@ref(thm:excon2) is that $\plim Q_n = Q_0$ is uniquely maximized at the true value $\thet_0 \in \Thet$. We can think of $Q_0$ as a "population" counterpart to $Q_n$. In the event his non-stochastic/true population function $Q_0$ is maximized at multiple values $<span class="sc">\{</span>\thet_0, \thet_0'<span class="sc">\}</span>$, then we have no way of knowing if our extremum estimator is consistently estimating $\thet_0$ or $\thet_0'$. This problem should sound *very familiar*. It seems to be similar, if not the same, exact problem that arises when a model $\mathcal P$ is unidentified! It happens to be the same exact problem. </span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a>Suppose we have a model $\mathcal P$ with a parameterization $\mu : \Thet \to \mathcal P$, where $\mu$ is injective such that $\mathcal P$ is identified. The true parameter value for $P$ is $\mu^{-1}(P)$ (where this is the left inverse of $\mu$). Consider the problem of assigning real numbers to combinations of parameters and model values $(\thet, P) \in \Thet \times \mathcal P$ such that we can identify each $P$ using these numbers. We can use a function $Q_0:\Thet \times \mathcal P \to \mathbb R$ for this, and define $\phi:\Thet \to \mathcal P$ such that its left inverse is:</span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a>$$ \phi^{-1}(P) = \argmax_{\thet \in \Thet}Q_0(P, \thet).$$ If we wanted to reparameterize $\mathcal P$ with $\phi$ and maintain identification, then what conditions would $Q_0$ need to satisfy? It would need to be the case that $Q_0$ has a unique maximum (otherwise $\phi$ would assign multiple parameters to $P$ and we would not have identification), *and* that unique maximization needs to occur at true parameter $\theta = \mu^{-1}(P)$ associated with $P$:</span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a>$$ \mu^{-1}(P) = \argmax_{\thet \in \Thet}Q_0(P, \thet).$$ This is precisely the first condition required for consistency! For more details about this, see @davidson1993estimation and @lewbel2019identification, the latter of which coined the term "extremum identification". A concrete example may illuminate the link between identification and $Q_0$ achieving a unique maximum at $\thet_0$</span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a>:::{.example #olsEX}</span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a>Return to the classic linear model $\mathcal P_\text{LM}$ and the estimator $\OLS$. </span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a>$$ \OLS = \argmin_{\mathbf b} \sum_{i=1}^n(Y_i-\X_i\mathbf b)^2 = \argmax_{\mathbf b}\underbrace{ -\frac{1}{n}\sum_{i=1}^n(Y_i-\X_i\mathbf b)^2}_{Q_n}.$$</span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a>By the law of large numbers $$ \underbrace{ -\frac{1}{n}\sum_{i=1}^n(Y_i-\X_i\mathbf b)^2}_{Q_n}\pto \underbrace{-\E{(Y - \X\mathbf b)^2}}_{Q_0}$$</span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a>Under what conditions is $Q_0$ uniquely maximized at the true parameter value $\bet$? The first order condition for $\argmax Q_0$ can be reduced to two different equations: </span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a>\E{\X'Y}&amp;=\bet\E{\X'\X}<span class="sc">\\</span></span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a>\E{\X'\ep}&amp;=0</span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a>Therefore $Q_0$ is uniquely maximized at the true parameter value $\bet$ when $\E{\X'\ep}=0$ and $\E{\X'\X}$ is invertible. These are precisely the same conditions which we determined identified $\mathcal P_\text{LM}$.</span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>Consider the linear projection model $\mathcal P_\text{LP}$ where $\bet$ was defined as </span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>$$\bet =\argmax_{\mathbf b}-\E{(Y - \X\mathbf b)^2}$$ to begin with (opposed to $\bet$ having a structural interpretation like in $\mathcal P_\text{LM}$). The parameterization is already given by a maximization problem here! In this special case, we can think of an extremum estimator as originating from the analog principle, because $\OLS$ (when written as the solution to the maximization problem) is the sample analog to $\bet$. This model is identified when $\text{rank}\left(\E{\X'\X}\right) = K$, which has the roll of insuring that the maximization problem which defines $\bet$ has a unique solution. Once again, this is condition 1 in \@ref(thm:excon). </span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>::: {.hypothesis name="Consistency and Identification"}</span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a>In general, consistency and identification are inherently related. Consider the classical linear model $\mathcal P_\text{LM}$ where $\bet = \E{\X'\X}^{-1}\E{\X'Y}$. The model is identified, so $\bet$ is unique to $P_{\bet, \sigma^2}\in\mathcal P$. Heuristically, we can think of identification as the ability to estimate the parameter perfectly at the population level with an infinite amount of data. If given an infinite amount of data, we could calculate the moments  $\E{\X'Y}$ and $\E{\X'\X}$, enabling us to calculate $\bet$.^<span class="co">[</span><span class="ot">This is why @wooldridge2010econometric define parameterization in the context of linear models as "[the parameter] can be written in terms of population moments in observable variables".</span><span class="co">]</span> We will never have infinite data, so the best we can do is define the sample analog of $\bet$ and $\OLS$, and appeal to the fact that as our sample size grows and approaches infinity, our estimates become arbitrarily better. This just happens to be consistency.  Formally, suppose under an assumed model value $P\in\mathcal P$ we have an estimator $\hat{\thet}$ with a unique limit in probability $\plim \hat{\thet}$. If we define the parameterization $\thet \mapsto P_{\thet}$ as $\thet = \plim \hat{\thet}$, then we've leveraged consistency such that  our model is identified by construction.</span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotic Normality </span></span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a>Our next technical result is that $\EE$ happens to be root-n CAN under certain conditions. This will require a bit more work now that we aren't restricting our attention to linear models. Recall the steps we took to prove Theorem \@ref(thm:asymols). From the onset of this proof, we had an closed form solution for $\EE$. Given $Q_n =  -\norm{\Y - \Xm\mathbf b}^2$, we were able to solve the associated first order condition for a unique solution in the form of $\EE = \OLS = (\Xm'\Xm)^{-1}\Xm'\Y$, because the first order condition was itself a linear equation with one root:</span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>$$ \frac{\partial Q_n}{\partial \mathbf b} = 2\Xm'(\Y -\Xm \mathbf b).$$ Once we move beyond linear models, it may be the case that the first order condition $\frac{\partial Q_n}{\partial \thet}$ is nonlinear and cannot be solved explicitly for $\EE$, even if we assume the first order condition has a unique root. The easiest way to handle this is using the mean value theorem to approximate the first order condition linearly (i.e a first order Taylor expansion). Doing this requires additional assumptions about $Q_n$, in particular assumptions about the existence of derivatives. Using the mean value theorem require continuous differentiability, but in our case we're going to apply it to the derivative $\frac{\partial Q_n}{\partial \thet}$, so we will need $Q_n$ to be twice continuous differentiability. This means our theorem will involve the hessian $\mathbf H(\thet)$ of $Q_n$.    </span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Asymptotic Normality of Extremum Estimators" #exasy}</span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>Suppose that:</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\frac{\partial Q_n(\EE)}{\partial \thet} = o_p(n^{-1/2})$ (the FOC holds as $n\to\infty$);</span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\EE \pto \thet_0$</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\Thet_0$ is in the interior of $\Thet$;</span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$Q_n(\thet)$ is twice continuously differentiable in a neighborhood $N_r(\thet_0)$;</span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>$\sqrt n \frac{\partial Q_n(\thet_0)}{\partial \thet} \dto N(\zer, \boldsymbol \Omega)$;</span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>$\frac{\partial^2 Q_n(\thet)}{\partial\thet\partial\thet'} \pto \mathbf H(\thet)$ *uniformly* for some $\mathbf H(\thet)$ continuous at $\thet_0$;</span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>$\mathbf H(\thet_0)$ is invertible.</span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>Then </span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\EE - \thet_0) &amp;\dto N(\zer, \mathbf H(\thet_0)^{-1} \boldsymbol \Omega \mathbf H(\thet_0)^{-1}).</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a>We will begin by performing a first-order Taylor expansion of $\frac{\partial Q_n(\EE)}{\partial \thet}$ about $\thet_0$.^<span class="co">[</span><span class="ot">Technically, we're performing $\dim(\Thet) = K$ different expansions and just writing them succinctly in matrix form.</span><span class="co">]</span></span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\partial Q_n(\EE)}{\partial \EE} = o_p(n^{-1/2})<span class="sc">\\</span></span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a>\implies &amp; o_p(n^{-1/2})= \frac{\partial Q_n(\thet_0)}{\partial \thet} + \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'}(\EE - \thet_0) &amp; (\hat\theta_{\text{EE},j} &lt; \tilde\theta_{j} &lt;  \theta_{0,j} \ \forall j) (<span class="sc">\#</span>eq:a7)</span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb3-404"><a href="#cb3-404" aria-hidden="true" tabindex="-1"></a>The vector $\tilde{\thet}$ "lies between" $\EE$ and $\thet_0$ (element-wise, where the elements of $\tilde{\thet}$ are given by the $K$ expansions), so $\tilde{\thet}\pto \thet_0$ as a consequence of $\EE \pto \thet_0$. Using assumption 6, </span>
<span id="cb3-405"><a href="#cb3-405" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb3-406"><a href="#cb3-406" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\partial^2 Q_n(\thet)}{\partial\thet\partial\thet'} \pto \mathbf H(\thet) &amp; (\forall \thet\in \Thet)<span class="sc">\\</span></span>
<span id="cb3-407"><a href="#cb3-407" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'} \pto \mathbf H(\tilde{\thet})<span class="sc">\\</span></span>
<span id="cb3-408"><a href="#cb3-408" aria-hidden="true" tabindex="-1"></a>\implies &amp; \plim \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'} = \plim  \mathbf H(\tilde{\thet})<span class="sc">\\</span> </span>
<span id="cb3-409"><a href="#cb3-409" aria-hidden="true" tabindex="-1"></a>\implies &amp; \plim \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'} = \mathbf H(\plim\tilde{\thet}) &amp; (\mathbf H\text{ continuous at } \thet_0 = \plim \tilde{\thet})<span class="sc">\\</span></span>
<span id="cb3-410"><a href="#cb3-410" aria-hidden="true" tabindex="-1"></a>\implies &amp; \plim \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'} = \mathbf H(\thet_0) &amp; (\thet_0 = \plim \tilde{\thet})<span class="sc">\\</span></span>
<span id="cb3-411"><a href="#cb3-411" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial^2 Q_n(\tilde{\thet})}{\partial\thet\partial\thet'} = \mathbf H(\thet_0) + o_p(1) (<span class="sc">\#</span>eq:a8)</span>
<span id="cb3-412"><a href="#cb3-412" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb3-413"><a href="#cb3-413" aria-hidden="true" tabindex="-1"></a>If we substitute equation \@ref(eq:a8) into equation \@ref(eq:a7), then </span>
<span id="cb3-414"><a href="#cb3-414" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-415"><a href="#cb3-415" aria-hidden="true" tabindex="-1"></a>&amp; o_p(n^{-1/2}) = \frac{\partial Q_n(\thet_0)}{\partial \thet} + (\mathbf H(\thet_0) + o_p(1))(\EE - \thet_0)<span class="sc">\\</span></span>
<span id="cb3-416"><a href="#cb3-416" aria-hidden="true" tabindex="-1"></a>\implies &amp; \underbrace{\sqrt n \cdot o_p(n^{-1/2})}_{o_p(1)} = \sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet} + (\mathbf H(\thet_0) + o_p(1))\sqrt{n}(\EE - \thet_0)<span class="sc">\\</span></span>
<span id="cb3-417"><a href="#cb3-417" aria-hidden="true" tabindex="-1"></a>\implies &amp;\sqrt{n}(\EE - \thet_0) = -(\mathbf H(\thet_0) + o_p(1))^{-1}\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet} - \underbrace{o_p(1)(\mathbf H(\thet_0) + o_p(1))^{-1}}_{o_p(1)}<span class="sc">\\</span></span>
<span id="cb3-418"><a href="#cb3-418" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}(\EE - \thet_0) \pto -\mathbf H(\thet_0)^{-1} \sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet}<span class="sc">\\</span></span>
<span id="cb3-419"><a href="#cb3-419" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}(\EE - \thet_0) \dto -\mathbf H(\thet_0)^{-1} \underbrace{\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet}}_{\dto N(\zer, \boldsymbol \Omega)} &amp; (\pto \implies \dto)<span class="sc">\\</span></span>
<span id="cb3-420"><a href="#cb3-420" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}(\EE - \thet_0) \dto -\mathbf H(\thet_0)^{-1} N(\zer, \boldsymbol \Omega)<span class="sc">\\</span></span>
<span id="cb3-421"><a href="#cb3-421" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}(\EE - \thet_0) \dto N(\zer, <span class="co">[</span><span class="ot">-\mathbf H(\thet_0)^{-1}</span><span class="co">]</span>'\boldsymbol \Omega<span class="co">[</span><span class="ot">-\mathbf H(\thet_0)^{-1}</span><span class="co">]</span>) &amp; (\mathbf H(\thet_0)\text{ invertible})<span class="sc">\\</span></span>
<span id="cb3-422"><a href="#cb3-422" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}(\EE - \thet_0) \dto N(\zer, \mathbf H(\thet_0)^{-1}\boldsymbol \Omega\mathbf H(\thet_0)^{-1}) &amp; (\mathbf H(\thet_0)\text{ symmetric})</span>
<span id="cb3-423"><a href="#cb3-423" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-424"><a href="#cb3-424" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span></span>
<span id="cb3-425"><a href="#cb3-425" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-426"><a href="#cb3-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-427"><a href="#cb3-427" aria-hidden="true" tabindex="-1"></a>::: {.hypothesis name="Sandwhich Variance/Covariance Matrix"}</span>
<span id="cb3-428"><a href="#cb3-428" aria-hidden="true" tabindex="-1"></a>The asymptotic variance of $\EE$ exhibits a nice pattern. It's the variance $\boldsymbol\Omega$ "sandwiched" between $\mathbf H(\thet_0)^{-1}$. This is why a(n) (asymptotic) variance/covariance matrix of this form, </span>
<span id="cb3-429"><a href="#cb3-429" aria-hidden="true" tabindex="-1"></a>$$\avar{\hat{\thet}} = a \mathbf B^{-1}\mathbf A \mathbf B^{-1},$$ is called a **_sandwich variance/covariance matrix_**. Many root-n CAN estimators have a sandwich variance/covariance matrix because the delta method along with the properties of variance naturally lend themselves to this sandwich form. </span>
<span id="cb3-430"><a href="#cb3-430" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-431"><a href="#cb3-431" aria-hidden="true" tabindex="-1"></a>\var{\mathbf B \X + \mathbf c} &amp;= \mathbf B\var{\X}\mathbf B'<span class="sc">\\</span></span>
<span id="cb3-432"><a href="#cb3-432" aria-hidden="true" tabindex="-1"></a>\sqrt{n}<span class="co">[</span><span class="ot">\mathbf g(\X_n) - \mathbf g(\mathbf t)</span><span class="co">]</span> &amp;\dto N\left(\zer, \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\x}(\mathbf t)\right</span><span class="co">]</span>\avar{\X_n}\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\x}(\mathbf t)\right</span><span class="co">]</span>'\right) &amp; (\X_n \dto N(\zer, \avar{\X_n}))</span>
<span id="cb3-433"><a href="#cb3-433" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-434"><a href="#cb3-434" aria-hidden="true" tabindex="-1"></a>In the event that $\avar{\X_n}$ is a symmetric matrix $\mathbf B'$, we have the sandwich form $\mathbf B^{-1}\mathbf A \mathbf B^{-1}$ (which may be scaled by some $a$). In certain special cases, $a \mathbf B^{-1}\mathbf A \mathbf B^{-1}$ may reduce to $a \mathbf B^{-1}$.</span>
<span id="cb3-435"><a href="#cb3-435" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-436"><a href="#cb3-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-437"><a href="#cb3-437" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Hypothesis Testing "Trinity"</span></span>
<span id="cb3-438"><a href="#cb3-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-439"><a href="#cb3-439" aria-hidden="true" tabindex="-1"></a>There are three main approaches to testing hypotheses using $\EE$, one that we are familiar with, and two new ones. In each case, we'll need a consistent estimator $\widehat{\text{Avar}}(\EE)$. Unfortunately, Theorem \@ref(thm:exasy) is so general that it doesn't provide much guidance as to estimating $$\avar{\EE} = \frac{\mathbf H(\thet_0)^{-1}\boldsymbol \Omega\mathbf H(\thet_0)^{-1}}{n}.$$ We only know that $\mathbf H$ is the limit of the Hessian of $Q_n$, and that $\boldsymbol \Omega$ is the asymptotic variance of the gradient of $\mathbf H$. Without more information about $Q_n$, there isn't much we can do right now to estimate the asymptotic variance of $\EE$. Instead, we'll derive consistent estimators $\widehat{\text{Avar}}(\EE)$ for special cases of extremum estimators, and assume they exist for now, allowing us to construct test statistics.   </span>
<span id="cb3-440"><a href="#cb3-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-441"><a href="#cb3-441" aria-hidden="true" tabindex="-1"></a>Theorem \@ref(thm:exasy) establishes that $\EE$ is root-n CAN, so we can test a general nonlinear hypothesis $\mathbf h(\thet) = \zer$ using a Wald test:</span>
<span id="cb3-442"><a href="#cb3-442" aria-hidden="true" tabindex="-1"></a>$$ W = \mathbf h(\EE)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\EE)\widehat{\text{Avar}}(\EE)\frac{\partial \mathbf h}{\partial \thet}(\EE)'\right</span><span class="co">]</span>^{-1}\mathbf h(\EE).$$ We can also test hypotheses about $\hat\theta_{\text{EX,}j}$ using the $t-$test </span>
<span id="cb3-443"><a href="#cb3-443" aria-hidden="true" tabindex="-1"></a>$$ t = \frac{\hat\theta_{\text{EX},j} - \theta_0}{\widehat{\text{se}}(\hat\theta_{\text{EX},j})}.$$ These tests are based on the discrepancy between $\EE$ and $\thet_0$ scaled by the precision (asymptotic variance) of our estimator $\EE$. This is not the only way to measure how "far" estimates are from a null hypothesis. </span>
<span id="cb3-444"><a href="#cb3-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-445"><a href="#cb3-445" aria-hidden="true" tabindex="-1"></a>Consider the definition of $\EE$ in general, and under $H_0:\mathbf h(\thet) = \zer$:</span>
<span id="cb3-446"><a href="#cb3-446" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-447"><a href="#cb3-447" aria-hidden="true" tabindex="-1"></a>\EE &amp; = \argmax_{\thet \in \Thet} Q_n(\thet),<span class="sc">\\</span></span>
<span id="cb3-448"><a href="#cb3-448" aria-hidden="true" tabindex="-1"></a>\bar{\thet}_\text{EE} &amp; = \argmax_{\mathbf h(\thet) = \zer} Q_n(\thet).</span>
<span id="cb3-449"><a href="#cb3-449" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-450"><a href="#cb3-450" aria-hidden="true" tabindex="-1"></a>We usually refer to an estimator such as $\bar{\thet}_\text{EE}$ as a **_restricted estimator_**, because it is calculated under the restriction that the null hypothesis is true.</span>
<span id="cb3-451"><a href="#cb3-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-452"><a href="#cb3-452" aria-hidden="true" tabindex="-1"></a>@rao1948large proposed an alternate statistic based solely on $\bar{\thet}_\text{EE}$. Appealing to the method of Lagrange multipliers, the restricted estimator $\bar{\thet}_\text{EE}$ is given as the solution to the following first order conditions (after being scaled by $\sqrt{n}$):</span>
<span id="cb3-453"><a href="#cb3-453" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-454"><a href="#cb3-454" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}) + \mathbf h(\bar{\thet}_\text{EE})'\sqrt{n}\boldsymbol\lambda  &amp; = \zer (<span class="sc">\#</span>eq:lm1)<span class="sc">\\</span></span>
<span id="cb3-455"><a href="#cb3-455" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\boldsymbol\lambda\frac{\partial \mathbf h}{\partial \thet} &amp; = \zer(<span class="sc">\#</span>eq:lm2)</span>
<span id="cb3-456"><a href="#cb3-456" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-457"><a href="#cb3-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-458"><a href="#cb3-458" aria-hidden="true" tabindex="-1"></a>In the event that $H_0$ is true, then the multiplier $\boldsymbol\lambda = \zer$. This suggests testing the equivalent hypothesis $H_0:\boldsymbol\lambda = \zer$. The Wald statistic associated with this problem is </span>
<span id="cb3-459"><a href="#cb3-459" aria-hidden="true" tabindex="-1"></a>$$ \boldsymbol\lambda'\left<span class="co">[</span><span class="ot">\widehat{\text{Avar}}(\boldsymbol\lambda)\right</span><span class="co">]</span>^{-1}\boldsymbol\lambda,$$ which can be simplified if we find $\avar{\boldsymbol\lambda}$.</span>
<span id="cb3-460"><a href="#cb3-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-461"><a href="#cb3-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-462"><a href="#cb3-462" aria-hidden="true" tabindex="-1"></a>We can derive a test statistic from these FOCs assuming $\sqrt n \frac{\partial Q_n(\thet_0)}{\partial \thet} \dto N(\zer, \boldsymbol \Omega)$ and that we are able to perform a mean value expansion on $\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})$ and $\mathbf h$. For some values $\EER&lt;\tilde{\thet}&lt;\thet_0$ and $\EER&lt;\thet^\dagger&lt;\thet_0$,^<span class="co">[</span><span class="ot">Once again, we're actually performing mean value expansions in several variables at once but consolidating it using vectors and matrices. This means that these inequalities hold element-wise.</span><span class="co">]</span></span>
<span id="cb3-463"><a href="#cb3-463" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-464"><a href="#cb3-464" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}) &amp;= \sqrt{n}\frac{\partial Q_n}{\partial \thet}(\thet_0) + \frac{\partial^2 Q_n}{\partial \thet\partial \thet'}(\thet^\dagger)\sqrt{n}(\EER - \thet_0)<span class="sc">\\</span></span>
<span id="cb3-465"><a href="#cb3-465" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\mathbf h(\EER) &amp;= \underbrace{\sqrt{n}\mathbf h(\thet_0)}_\zer + \frac{\partial\mathbf h}{\partial \thet}(\tilde{\thet})\sqrt{n}(\EER - \thet_0)</span>
<span id="cb3-466"><a href="#cb3-466" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-467"><a href="#cb3-467" aria-hidden="true" tabindex="-1"></a>If we assume that $\EER\pto \thet_0$, then $\tilde{\thet} \pto\thet_0$ and $\thet^\dagger\pto\thet_0$, because they're "squeezed" in between $\EER$ and $\thet_0$. If we take the limit (in probability) of these expansions, we have </span>
<span id="cb3-468"><a href="#cb3-468" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-469"><a href="#cb3-469" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}) &amp;= \sqrt{n}\frac{\partial Q_n}{\partial \thet}(\thet_0) +\mathbf H(\thet_0)\sqrt{n}(\EER - \thet_0) + o_p(1) (<span class="sc">\#</span>eq:lm3)<span class="sc">\\</span></span>
<span id="cb3-470"><a href="#cb3-470" aria-hidden="true" tabindex="-1"></a>\sqrt{n}\mathbf h(\EER) &amp;= \frac{\partial\mathbf h}{\partial \thet}(\thet_0)\sqrt{n}(\EER - \thet_0) + o_p(1) (<span class="sc">\#</span>eq:lm4)</span>
<span id="cb3-471"><a href="#cb3-471" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-472"><a href="#cb3-472" aria-hidden="true" tabindex="-1"></a>where the definition of $\mathbf H$ is given in Theorem \@ref(thm:exasy). If we assume that $\EER \pto \thet_0$ under $H_0$, then </span>
<span id="cb3-473"><a href="#cb3-473" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-474"><a href="#cb3-474" aria-hidden="true" tabindex="-1"></a>\mathbf h(\bar{\thet}_\text{EE})'\boldsymbol\lambda = \mathbf h(\thet_0)'\boldsymbol\lambda + o_p(1) (<span class="sc">\#</span>eq:lm5)</span>
<span id="cb3-475"><a href="#cb3-475" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-476"><a href="#cb3-476" aria-hidden="true" tabindex="-1"></a>If we substitute equations \@ref(eq:lm3), \@ref(eq:lm4), and \@ref(eq:lm5) into the first order conditions given by equations (<span class="sc">\#</span>eq:lm1) and (<span class="sc">\#</span>eq:lm2), we have (after some consolidation into matrices):</span>
<span id="cb3-477"><a href="#cb3-477" aria-hidden="true" tabindex="-1"></a>$$ \begin{bmatrix} \mathbf H(\thet_0) &amp; \frac{\partial \mathbf h}{\partial \thet}(\thet_0)'<span class="sc">\\</span></span>
<span id="cb3-478"><a href="#cb3-478" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathbf h}{\partial \thet}(\thet_0) &amp; \zer\end{bmatrix}\begin{bmatrix}\sqrt{n}(\EER - \thet_0)<span class="sc">\\</span> \sqrt n \boldsymbol \lambda \end{bmatrix} = \begin{bmatrix} -\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0)<span class="sc">\\</span> \zer \end{bmatrix} + o_p(1)$$</span>
<span id="cb3-479"><a href="#cb3-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-480"><a href="#cb3-480" aria-hidden="true" tabindex="-1"></a>If we solve this system we have:</span>
<span id="cb3-481"><a href="#cb3-481" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-482"><a href="#cb3-482" aria-hidden="true" tabindex="-1"></a>&amp;\begin{bmatrix}\sqrt{n}(\EER - \thet_0)<span class="sc">\\</span> \sqrt n \boldsymbol \lambda \end{bmatrix}  = \begin{bmatrix} \mathbf H(\thet_0) &amp; \frac{\partial \mathbf h}{\partial \thet}(\thet_0)'<span class="sc">\\</span></span>
<span id="cb3-483"><a href="#cb3-483" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathbf h}{\partial \thet}(\thet_0) &amp; \zer\end{bmatrix}^{-1}\begin{bmatrix} -\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0)<span class="sc">\\</span> \zer \end{bmatrix} + o_p(1)<span class="sc">\\</span></span>
<span id="cb3-484"><a href="#cb3-484" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}\boldsymbol \lambda = -\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0) + o_p(1)<span class="sc">\\</span></span>
<span id="cb3-485"><a href="#cb3-485" aria-hidden="true" tabindex="-1"></a>&amp; \sqrt{n}(\EER - \thet_0)=-\left<span class="co">[</span><span class="ot">\mathbf H(\thet_0)^{-1} - \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left(\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0) \mathbf H(\thet_0)^{-1}\right</span><span class="co">]</span>\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet} + o_p(1) (<span class="sc">\#</span>eq:lm6)<span class="sc">\\</span></span>
<span id="cb3-486"><a href="#cb3-486" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}\boldsymbol \lambda \pto -\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\underbrace{\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0) }_{N(\zer, \boldsymbol \Omega)}<span class="sc">\\</span></span>
<span id="cb3-487"><a href="#cb3-487" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}\boldsymbol \lambda \dto N\left(\zer,  \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\boldsymbol \Omega  \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1} \right)<span class="sc">\\</span></span>
<span id="cb3-488"><a href="#cb3-488" aria-hidden="true" tabindex="-1"></a>\implies &amp; \avar{\boldsymbol \lambda} = \frac{1}{n}\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\boldsymbol \Omega  \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}</span>
<span id="cb3-489"><a href="#cb3-489" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-490"><a href="#cb3-490" aria-hidden="true" tabindex="-1"></a>Therefore, the Wald statistic associated with $H_0:\boldsymbol\lambda = \zer$ (taking the asymptotic variance to be known at the moment) is </span>
<span id="cb3-491"><a href="#cb3-491" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-492"><a href="#cb3-492" aria-hidden="true" tabindex="-1"></a>&amp;\boldsymbol\lambda' \left<span class="co">[</span><span class="ot">\avar{\boldsymbol\lambda}\right</span><span class="co">]</span>^{-1}\boldsymbol\lambda<span class="sc">\\</span></span>
<span id="cb3-493"><a href="#cb3-493" aria-hidden="true" tabindex="-1"></a>\implies &amp; \boldsymbol\lambda' \left<span class="co">[</span><span class="ot">\frac{1}{n}\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\boldsymbol \Omega  \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right]^{-1}\right</span><span class="co">]</span>^{-1}\boldsymbol\lambda<span class="sc">\\</span></span>
<span id="cb3-494"><a href="#cb3-494" aria-hidden="true" tabindex="-1"></a>\implies &amp; n\boldsymbol\lambda'\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\boldsymbol \Omega  \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\boldsymbol\lambda.</span>
<span id="cb3-495"><a href="#cb3-495" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-496"><a href="#cb3-496" aria-hidden="true" tabindex="-1"></a>Under $H_0$ we have $\nabla_\thet Q_n(\EER) = \zer$. Combining this with \@ref(eq:lm2) gives $$\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\boldsymbol \lambda =  \frac{\partial Q_n}{\partial \thet}(\EER),$$ which can be used to simplify our test statistic.</span>
<span id="cb3-497"><a href="#cb3-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-498"><a href="#cb3-498" aria-hidden="true" tabindex="-1"></a>$$ n\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})'\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\boldsymbol \Omega  \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})'.$$</span>
<span id="cb3-499"><a href="#cb3-499" aria-hidden="true" tabindex="-1"></a>In practice, we only know $H_0:\mathbf h(\thet) = \thet_0$, $Q_n$, and $\EER$, so we need to estimate $\boldsymbol \Omega$ and $\mathbf H(\thet_0)$. Once we substitute in suitable estimators, we have our Wald statistic for $H_0:\boldsymbol \lambda = \zer$, which is usually considered it's own seperate test statistic. </span>
<span id="cb3-500"><a href="#cb3-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-501"><a href="#cb3-501" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-502"><a href="#cb3-502" aria-hidden="true" tabindex="-1"></a>Suppose $\EE$ is an extremum estimator for which $\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0) \dto N(\zer, \boldsymbol \Omega)$, and $\frac{\partial^2 Q_n}{\partial \thet\partial\thet'}(\thet_0) \pto \mathbf H(\thet_0)$.  If a null hypothesis $H_0$ is summarized by some (possibly nonlinear) function $\mathbf h(\thet) = \zer$ the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_Lagrange multiplier statistic_**<span class="kw">&lt;/span&gt;</span> is defined as </span>
<span id="cb3-503"><a href="#cb3-503" aria-hidden="true" tabindex="-1"></a>$$ LM = n\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\hat{\boldsymbol \Omega}\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}),$$ where $\EER$ is the extremum estimator subject to $H_0$.  The statistic is also known as the &lt;span style="color:red"&gt;**_Rao test statistic_**&lt;/span&gt; or &lt;span style="color:red"&gt;**_score test statistic_**<span class="kw">&lt;/span&gt;</span></span>
<span id="cb3-504"><a href="#cb3-504" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-505"><a href="#cb3-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-506"><a href="#cb3-506" aria-hidden="true" tabindex="-1"></a>If $\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}) \gg 0$, then $LM \gg 0$, and it is likely that $H_0$ is false. An immediate consequence of our derivation of $LM$ is that $LM \dto \chi_q^2$, as it is a special case of the Wald statistic. A formal statement of this will be presented after deriving one last test statistic. </span>
<span id="cb3-507"><a href="#cb3-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-508"><a href="#cb3-508" aria-hidden="true" tabindex="-1"></a>A third option to test $H_0$ is by looking at the difference $Q_n(\EE) - Q_n(\EER)$ (which is always positive). If $H_0$ is likely to be true then $\EE \approx \EER$, so $Q_n(\EE) - Q_n(\EER)\approx 0$. A test statistic based on this criterion would have a major advantage over the Wald and Lagrange multiplier statistics because it would not require us to estimate any asymptotic variances. Unfortunately, as we'll soon see, this advantage comes at the cost of an assumption about $\boldsymbol \Omega$ and $\mathbf H(\thet_0)$. First, let's look at the second-order Taylor expansion of $Q_n(\EER)$ about $\EE$. For some $\EE &lt;\tilde{\thet} &lt; \EER$,</span>
<span id="cb3-509"><a href="#cb3-509" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-510"><a href="#cb3-510" aria-hidden="true" tabindex="-1"></a>&amp; Q_n(\EER) =  Q_n(\EE) + \frac{\partial Q_n}{\partial\thet}(\EER-\EE) + \frac{1}{2}(\EER-\EE)'\frac{\partial^2 Q_n}{\partial\thet\partial\thet'}(\tilde{\thet})(\EER-\EE)<span class="sc">\\</span></span>
<span id="cb3-511"><a href="#cb3-511" aria-hidden="true" tabindex="-1"></a>\implies &amp; 2<span class="co">[</span><span class="ot">Q_n(\EE) -  Q_n(\EER)</span><span class="co">]</span> = -\frac{\partial Q_n}{\partial\thet}(\EER-\EE) - \frac{1}{2}(\EER-\EE)'\frac{\partial^2 Q_n}{\partial\thet\partial\thet'}(\tilde{\thet})(\EER-\EE)</span>
<span id="cb3-512"><a href="#cb3-512" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-513"><a href="#cb3-513" aria-hidden="true" tabindex="-1"></a>Under the assumption that $\sqrt{n}\frac{\partial Q_n}{\partial\thet} \dto N(\zer, \boldsymbol \Omega)$, the first term of this expansion is $o_p(1)$.</span>
<span id="cb3-514"><a href="#cb3-514" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-515"><a href="#cb3-515" aria-hidden="true" tabindex="-1"></a>&amp; 2\sqrt{n}<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span> = -\sqrt{n}(\EER-\EE)'\frac{\partial^2 Q_n}{\partial\thet\partial\thet'}(\tilde{\thet})(\EER-\EE)+ o_p(1)<span class="sc">\\</span></span>
<span id="cb3-516"><a href="#cb3-516" aria-hidden="true" tabindex="-1"></a>\implies &amp; 2n<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span> = -<span class="co">[</span><span class="ot">\sqrt{n}(\EER-\EE)</span><span class="co">]</span>'\frac{\partial^2 Q_n}{\partial\thet\partial\thet'}(\tilde{\thet})<span class="co">[</span><span class="ot">\sqrt{n}(\EER-\EE)</span><span class="co">]</span> + o_p(1)</span>
<span id="cb3-517"><a href="#cb3-517" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-518"><a href="#cb3-518" aria-hidden="true" tabindex="-1"></a>If $\EE \pto \thet_0$, then we also have $\tilde{\thet} \pto \thet_0$, so </span>
<span id="cb3-519"><a href="#cb3-519" aria-hidden="true" tabindex="-1"></a>$$2n<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span> = -<span class="co">[</span><span class="ot">\sqrt{n}(\EER-\EE)</span><span class="co">]</span>'\mathbf H(\thet_0)<span class="co">[</span><span class="ot">\sqrt{n}(\EER-\EE)</span><span class="co">]</span> + o_p(1).$$ The term $<span class="co">[</span><span class="ot">\sqrt{n}(\EER-\EE)</span><span class="co">]</span>$ looks a lot like something that would be asymptotically normal, in which case $2n<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span>$ would be a function of a quadratic form of (asymptotically) normal vectors meaning it could be distributed according to some $\chi^2$ distribution. From the Taylor expansion used to prove Theorem \@ref(thm:exasy) and Equation Theorem \@ref(eq:lm6), we have </span>
<span id="cb3-520"><a href="#cb3-520" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-521"><a href="#cb3-521" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\EE - \thet_0)&amp;-\mathbf H(\thet_0)^{-1}\sqrt{n}\frac{\partial Q_n}{\partial \thet}(\thet_0) + o_p(1),<span class="sc">\\</span></span>
<span id="cb3-522"><a href="#cb3-522" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\EER - \thet_0)&amp;=-\left<span class="co">[</span><span class="ot">\mathbf H(\thet_0)^{-1} - \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left(\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0) \mathbf H(\thet_0)^{-1}\right</span><span class="co">]</span>\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet} + o_p(1).</span>
<span id="cb3-523"><a href="#cb3-523" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-524"><a href="#cb3-524" aria-hidden="true" tabindex="-1"></a>These imply </span>
<span id="cb3-525"><a href="#cb3-525" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-526"><a href="#cb3-526" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\EER-\EE) &amp; = \sqrt{n}(\EER - \thet_0) - \sqrt{n}(\EE - \thet_0)<span class="sc">\\</span></span>
<span id="cb3-527"><a href="#cb3-527" aria-hidden="true" tabindex="-1"></a>&amp; = -\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \left<span class="co">[</span><span class="ot">\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet}\right</span><span class="co">]</span> + o_p(1)</span>
<span id="cb3-528"><a href="#cb3-528" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-529"><a href="#cb3-529" aria-hidden="true" tabindex="-1"></a>If we plug this into our last expression for $2n<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span>$,^<span class="co">[</span><span class="ot">Many of the terms involving $\frac{\partial \mathbf h}{\partial \thet}(\thet_0)$ and $\mathbf H(\thet_0)$ will cancel. $$ $$</span><span class="co">]</span> we have </span>
<span id="cb3-530"><a href="#cb3-530" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-531"><a href="#cb3-531" aria-hidden="true" tabindex="-1"></a>2n<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span> &amp; = \underbrace{\left<span class="co">[</span><span class="ot">\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet}\right</span><span class="co">]</span>}_{\dto N(\zer, \boldsymbol \Omega)}' \mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \underbrace{\left[\sqrt{n}\frac{\partial Q_n(\thet_0)}{\partial \thet}\right]}_{\dto N(\zer, \boldsymbol \Omega)} + o_p(1)<span class="sc">\\</span></span>
<span id="cb3-532"><a href="#cb3-532" aria-hidden="true" tabindex="-1"></a>&amp;  \dto \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} N(\zer, \boldsymbol \Omega)\right</span><span class="co">]</span>' \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1}\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} N(\zer, \boldsymbol \Omega)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-533"><a href="#cb3-533" aria-hidden="true" tabindex="-1"></a>&amp; = N\left(\zer,\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \boldsymbol \Omega \mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \right)' \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1} N\left(\zer,\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \boldsymbol \Omega \mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \right)</span>
<span id="cb3-534"><a href="#cb3-534" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-535"><a href="#cb3-535" aria-hidden="true" tabindex="-1"></a>The limiting distribution is a quadratic form of normally distributed variables, *but* the matrix in the center which "weights" the quadratic form does not correspond properly to the variance of the distributions such that we have a chi-squared distribution. We need this matrix in the center to be equal to the variance of the normal distribution in order to "standardize" it and give us the square of two standard normal distributions (which is the definition of a chi-squared distributed). </span>
<span id="cb3-536"><a href="#cb3-536" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \boldsymbol \Omega \mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \neq  \frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'$$</span>
<span id="cb3-537"><a href="#cb3-537" aria-hidden="true" tabindex="-1"></a>For this relation to hold with equality, we need to make an assumption about the relationship between $\mathbf H(\thet_0)$ and $\boldsymbol \Omega$. </span>
<span id="cb3-538"><a href="#cb3-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-539"><a href="#cb3-539" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-540"><a href="#cb3-540" aria-hidden="true" tabindex="-1"></a>Suppose $\EE$ is an extremum estimator for which $\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0) \dto N(\zer, \boldsymbol \Omega)$, and $\frac{\partial^2 Q_n}{\partial \thet\partial\thet'}(\thet_0) \pto \mathbf H(\thet_0)$. In addition assume that there exists some scalar $c\in \mathbb R$ such that</span>
<span id="cb3-541"><a href="#cb3-541" aria-hidden="true" tabindex="-1"></a>$$\boldsymbol \Omega = c \mathbf H(\thet_0),$$ giving and $\boldsymbol \Omega \propto \mathbf H(\thet_0)$. The equality associated with this assumption is the </span>
<span id="cb3-542"><a href="#cb3-542" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_generalized information matrix equality_**<span class="kw">&lt;/span&gt;</span>. </span>
<span id="cb3-543"><a href="#cb3-543" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-544"><a href="#cb3-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-545"><a href="#cb3-545" aria-hidden="true" tabindex="-1"></a>This assumption seems completely arbitrary, but once we start working with concrete examples of extremum estimators, we'll see it hold in many familiar settings that motivate it holding in this general case.^<span class="co">[</span><span class="ot">We could also weaken this assumption by assuming there is a sequence of constants $c_n$ whose limit gives the proportion.</span><span class="co">]</span> Also note that as we've defined $\EE$, $c &lt; 0$. If $Q_0$ is uniquely maximized at $\thet_0$, then its Hessian $\mathbf H(\thet_0)$ is negative semi-definite, while $\boldsymbol \Omega$ corresponds to a variance (meaning its positive semi-definite). These facts imply $c&lt; 0$. </span>
<span id="cb3-546"><a href="#cb3-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-547"><a href="#cb3-547" aria-hidden="true" tabindex="-1"></a>If we assume the generalized information matrix equality holds, then </span>
<span id="cb3-548"><a href="#cb3-548" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-549"><a href="#cb3-549" aria-hidden="true" tabindex="-1"></a>\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \boldsymbol \Omega \mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)'&amp; = \frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} c \mathbf H(\thet_0) \mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)'= c\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'</span>
<span id="cb3-550"><a href="#cb3-550" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-551"><a href="#cb3-551" aria-hidden="true" tabindex="-1"></a>All we need to do is divide our statistic by $c$ to account for the proportionality. </span>
<span id="cb3-552"><a href="#cb3-552" aria-hidden="true" tabindex="-1"></a>$$\frac{2n}{c}<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span>\dto N\left(\zer,\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}  \frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \right)' \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right</span><span class="co">]</span>^{-1} N\left(\zer,\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\mathbf H(\thet_0)^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \right) = \chi_q^2.$$</span>
<span id="cb3-553"><a href="#cb3-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-554"><a href="#cb3-554" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-555"><a href="#cb3-555" aria-hidden="true" tabindex="-1"></a>Suppose $\EE$ is an extremum estimator for which $\sqrt n \frac{\partial Q_n}{\partial \thet}(\thet_0) \dto N(\zer, \boldsymbol \Omega)$, and $\frac{\partial^2 Q_n}{\partial \thet\partial\thet'}(\thet_0) \pto \mathbf H(\thet_0)$, and $\boldsymbol \Omega = c \mathbf H(\thet_0)$ for some $c &lt; 0$. The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_distance metric statistic_**<span class="kw">&lt;/span&gt;</span> for the null hypothesis $H_0$ is </span>
<span id="cb3-556"><a href="#cb3-556" aria-hidden="true" tabindex="-1"></a>$$DM = \frac{2n}{\hat c}<span class="co">[</span><span class="ot">Q_n(\EE) - Q_n(\EER)</span><span class="co">]</span>,$$ where $\EER$ is the extremum estimator subject to $H_0$.</span>
<span id="cb3-557"><a href="#cb3-557" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-558"><a href="#cb3-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-559"><a href="#cb3-559" aria-hidden="true" tabindex="-1"></a>Now we can formalize things into a theorem.</span>
<span id="cb3-560"><a href="#cb3-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-561"><a href="#cb3-561" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Equivelence of Testing Trinity" #trintest}</span>
<span id="cb3-562"><a href="#cb3-562" aria-hidden="true" tabindex="-1"></a>Let $H_0:\mathbf h(\thet) = \zer$ be some hypothesis where $\mathbf h:\Thet \to \mathbb R^q$ ($q \le \dim(\Thet) = K$), $\EE$ be an extremum estimator for $\thet$, and $\bar{\thet}_\text{EE}$ be the restricted extremum estimator for $\thet$ under $H_0$. Suppose:</span>
<span id="cb3-563"><a href="#cb3-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-564"><a href="#cb3-564" aria-hidden="true" tabindex="-1"></a>a. The function $\mathbf h(\thet)$ is continuously differentiable in a neighborhood of $\thet_0$;</span>
<span id="cb3-565"><a href="#cb3-565" aria-hidden="true" tabindex="-1"></a>b. The conditions of  Theorem \@ref(thm:exasy) are satisfied. </span>
<span id="cb3-566"><a href="#cb3-566" aria-hidden="true" tabindex="-1"></a>c. $\frac{\partial \mathbf h(\thet)}{\partial \thet}$ is invertible;</span>
<span id="cb3-567"><a href="#cb3-567" aria-hidden="true" tabindex="-1"></a>d. $\hat {\mathbf H}(\thet_0)$ and $\hat {\boldsymbol\Omega}$ are consistent estimators of $\mathbf H(\thet_0)$ and $\boldsymbol \Omega$, respectively;</span>
<span id="cb3-568"><a href="#cb3-568" aria-hidden="true" tabindex="-1"></a>e. $\bar{\thet}_\text{EE} \pto \thet_0$ under $H_0$.</span>
<span id="cb3-569"><a href="#cb3-569" aria-hidden="true" tabindex="-1"></a>f. There exists some scalar $c\in \mathbb R$ such that $\boldsymbol \Omega = c \mathbf H(\thet_0)$, and $\hat c$ is a consistent estimator for $c$.</span>
<span id="cb3-570"><a href="#cb3-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-571"><a href="#cb3-571" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb3-572"><a href="#cb3-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-573"><a href="#cb3-573" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Under assumptions a-d, $W \dto \chi_q^2$;</span>
<span id="cb3-574"><a href="#cb3-574" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Under assumptions a-e, $LM \dto \chi_q^2$. If assumption f holds, then</span>
<span id="cb3-575"><a href="#cb3-575" aria-hidden="true" tabindex="-1"></a>$$LM = \frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat {\boldsymbol\Omega}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE}) $$</span>
<span id="cb3-576"><a href="#cb3-576" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Under assumptions a-f, $DM \dto \chi_q^2$. </span>
<span id="cb3-577"><a href="#cb3-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-578"><a href="#cb3-578" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-579"><a href="#cb3-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-580"><a href="#cb3-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-581"><a href="#cb3-581" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-582"><a href="#cb3-582" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span></span>
<span id="cb3-583"><a href="#cb3-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-584"><a href="#cb3-584" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We proved that $W\dto \chi_q^2$ in Theorem \@ref(thm:wald). All we need is a consistent estimator for $\avar{\EE} = \mathbf H(\thet_0)^{-1}\boldsymbol\Omega\mathbf H(\thet_0)^{-1}$. By Assumption d, we have this in the form of $\widehat{\text{Avar}}(\EE) = \hat {\mathbf H}(\thet_0)^{-1} \hat {\boldsymbol\Omega}{\mathbf H}(\thet_0)^{-1}$</span>
<span id="cb3-585"><a href="#cb3-585" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We showed that $LM$ is a special case of $W$ with the additional assumption that $\EER \pto \thet_0$ under $H_0$. IIf assumption f holds, we have a consistent estimator for $\boldsymbol \Omega$ in the form of $\hat {\boldsymbol\Omega} = \hat c\hat{\mathbf H}(\thet_0)$:</span>
<span id="cb3-586"><a href="#cb3-586" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-587"><a href="#cb3-587" aria-hidden="true" tabindex="-1"></a>LM &amp;= n\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\hat c\hat{\mathbf H}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})<span class="sc">\\</span></span>
<span id="cb3-588"><a href="#cb3-588" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{n}{\hat c}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})<span class="sc">\\</span></span>
<span id="cb3-589"><a href="#cb3-589" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{n}{\hat c}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat{\mathbf H}(\thet_0)^{-1}\underbrace{\frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\right]^{-1}}_{\mathbf I}\underbrace{\left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}\right]^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)\hat{\mathbf H}(\thet_0)^{-1}}_{\mathbf I}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})<span class="sc">\\</span></span>
<span id="cb3-590"><a href="#cb3-590" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})'\frac{\hat{\mathbf H}(\thet_0)^{-1}}{\hat c} \frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})<span class="sc">\\</span></span>
<span id="cb3-591"><a href="#cb3-591" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})' \hat {\boldsymbol\Omega}^{-1}\frac{\partial Q_n}{\partial \thet}(\bar{\thet}_\text{EE})</span>
<span id="cb3-592"><a href="#cb3-592" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-593"><a href="#cb3-593" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We already established that the test statistic converges in distribution to $\chi_q^2$ when $c$, is known. By Slutsky's theorem, this will still hold for a consistent estimator $\hat c$. </span>
<span id="cb3-594"><a href="#cb3-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-595"><a href="#cb3-595" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span></span>
<span id="cb3-596"><a href="#cb3-596" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-597"><a href="#cb3-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-598"><a href="#cb3-598" aria-hidden="true" tabindex="-1"></a>If we assume $\dim(\Thet) = \dim(\mathbf h) = 1$, we can plot $Q_n$, $H_0:h(\theta) = 0$ and the distances corresponding to our these three statistics.</span>
<span id="cb3-599"><a href="#cb3-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-600"><a href="#cb3-600" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, fig.align='center', fig.asp = 0.7, fig.width = 8, fig.cap ="test", warning=FALSE, message=FALSE}</span></span>
<span id="cb3-601"><a href="#cb3-601" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">9</span>,<span class="at">length =</span> <span class="dv">10000</span>)</span>
<span id="cb3-602"><a href="#cb3-602" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">x =</span> x) <span class="sc">%&gt;%</span> </span>
<span id="cb3-603"><a href="#cb3-603" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">`</span><span class="at">Objective function Qn</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">10</span> <span class="sc">-</span> (x <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb3-604"><a href="#cb3-604" aria-hidden="true" tabindex="-1"></a>         <span class="st">`</span><span class="at">Derivative of Qn</span><span class="st">`</span> <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">10</span>,</span>
<span id="cb3-605"><a href="#cb3-605" aria-hidden="true" tabindex="-1"></a>         <span class="st">`</span><span class="at">Null Hypothesis h</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="dv">30</span><span class="sc">*</span>x<span class="dv">-80</span>) <span class="sc">-</span> <span class="dv">6</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb3-606"><a href="#cb3-606" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="st">"group"</span>, <span class="st">"value"</span>, <span class="sc">-</span>x) <span class="sc">%&gt;%</span> </span>
<span id="cb3-607"><a href="#cb3-607" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,value, <span class="at">color =</span> group)) <span class="sc">+</span></span>
<span id="cb3-608"><a href="#cb3-608" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb3-609"><a href="#cb3-609" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-610"><a href="#cb3-610" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x=</span><span class="fu">element_blank</span>(),</span>
<span id="cb3-611"><a href="#cb3-611" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb3-612"><a href="#cb3-612" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb3-613"><a href="#cb3-613" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">""</span>,</span>
<span id="cb3-614"><a href="#cb3-614" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"θ"</span>,</span>
<span id="cb3-615"><a href="#cb3-615" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb3-616"><a href="#cb3-616" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb3-617"><a href="#cb3-617" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">5</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">5</span>, <span class="at">yend =</span> <span class="dv">10</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb3-618"><a href="#cb3-618" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">yend =</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>) <span class="sc">+</span> <span class="dv">10</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span> </span>
<span id="cb3-619"><a href="#cb3-619" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">yend =</span> <span class="dv">10</span> <span class="sc">-</span> (<span class="dv">116</span><span class="sc">/</span><span class="dv">30</span> <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span> </span>
<span id="cb3-620"><a href="#cb3-620" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">y =</span> <span class="dv">10</span> <span class="sc">-</span> (<span class="dv">116</span><span class="sc">/</span><span class="dv">30</span> <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span>, <span class="at">xend =</span> <span class="dv">5</span>, <span class="at">yend =</span> <span class="dv">10</span> <span class="sc">-</span> (<span class="dv">116</span><span class="sc">/</span><span class="dv">30</span> <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb3-621"><a href="#cb3-621" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">5</span>, <span class="at">y =</span> <span class="dv">10</span> <span class="sc">-</span> (<span class="dv">116</span><span class="sc">/</span><span class="dv">30</span> <span class="sc">-</span> <span class="dv">5</span>)<span class="sc">^</span><span class="dv">2</span>, <span class="at">xend =</span> <span class="dv">5</span>, <span class="at">yend =</span> <span class="dv">10</span>), <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"purple"</span>) <span class="sc">+</span></span>
<span id="cb3-622"><a href="#cb3-622" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span>  <span class="dv">116</span><span class="sc">/</span><span class="dv">30</span>, <span class="at">yend =</span> <span class="dv">10</span> <span class="sc">-</span> <span class="dv">116</span><span class="sc">/</span><span class="dv">15</span>), <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"purple"</span>) <span class="sc">+</span></span>
<span id="cb3-623"><a href="#cb3-623" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">5</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> <span class="dv">5</span>, <span class="at">yend =</span> <span class="fu">sqrt</span>(<span class="dv">30</span><span class="sc">*</span><span class="dv">5-80</span>) <span class="sc">-</span> <span class="dv">6</span>), <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"purple"</span>) <span class="sc">+</span> </span>
<span id="cb3-624"><a href="#cb3-624" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">5.3</span>, <span class="at">y =</span> <span class="fl">1.25</span>, <span class="at">label =</span> <span class="st">"Wald"</span>, <span class="at">color =</span> <span class="st">"purple"</span>) <span class="sc">+</span></span>
<span id="cb3-625"><a href="#cb3-625" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">4.1</span>, <span class="at">y =</span> <span class="fl">1.25</span>, <span class="at">label =</span> <span class="st">"LM"</span>, <span class="at">color =</span> <span class="st">"purple"</span>) <span class="sc">+</span></span>
<span id="cb3-626"><a href="#cb3-626" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">5.3</span>, <span class="at">y =</span> <span class="fl">9.3</span>, <span class="at">label =</span> <span class="st">"DM"</span>, <span class="at">color =</span> <span class="st">"purple"</span>)</span>
<span id="cb3-627"><a href="#cb3-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-628"><a href="#cb3-628" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-629"><a href="#cb3-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-630"><a href="#cb3-630" aria-hidden="true" tabindex="-1"></a>Despite our trio of our test statistics being asymptotically equivalent, there are still some advantages and disadvantages associated with each one. The Wald test and Lagrange multiplier test only require us to estimate the unrestricted model, whereas the distance metric test requires estimating the unrestricted model and restricted model. The distance metric test doesn't require us to estimate any asymptotic variances, only the constant $c$ where $\boldsymbol \Omega = c \mathbf H(\thet_0)$. The restricted model may also be much simpler than the unrestricted model (for example $H_0$ could be "the model is linear"), in which case the Lagrange multiplier test is much easier to use.    </span>
<span id="cb3-631"><a href="#cb3-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-632"><a href="#cb3-632" aria-hidden="true" tabindex="-1"></a><span class="fu">## M-Estimators</span></span>
<span id="cb3-633"><a href="#cb3-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-634"><a href="#cb3-634" aria-hidden="true" tabindex="-1"></a>A particular class of extremum estimators that we will work with often define $Q_n$ to be a sample average of some function of the data.</span>
<span id="cb3-635"><a href="#cb3-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-636"><a href="#cb3-636" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-637"><a href="#cb3-637" aria-hidden="true" tabindex="-1"></a>Suppose $\Wm = <span class="co">[</span><span class="ot">\W_1,\ldots, \W_n</span><span class="co">]</span>' \sim P_\thet$ where $P_\thet\in\mathcal P$ for a known model $\mathcal P$. An <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_M-estimator_**<span class="kw">&lt;/span&gt;</span> $\ME:\mathcal W\to \Thet$ is an extremum estimator where </span>
<span id="cb3-638"><a href="#cb3-638" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-639"><a href="#cb3-639" aria-hidden="true" tabindex="-1"></a> Q_n(\thet) &amp;= \frac{1}{n}\sum_{i=1}^n m(\thet,\W_i),<span class="sc">\\</span></span>
<span id="cb3-640"><a href="#cb3-640" aria-hidden="true" tabindex="-1"></a>\ME &amp;= \argmax_{\thet \in \Thet} \frac{1}{n}\sum_{i=1}^n m(\thet, \W_i).</span>
<span id="cb3-641"><a href="#cb3-641" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-642"><a href="#cb3-642" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-643"><a href="#cb3-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-644"><a href="#cb3-644" aria-hidden="true" tabindex="-1"></a>@Huber formalize M-estimators in an effort to generalize MLE (hence the letter M). Clearly M-estimators are consistent and asymptotically normal under the conditions of Theorems \@ref(thm:excon) and \@ref(thm:exasy), but perhaps these conditions simplify in the special case of $Q_n(\thet)=n^{-1}\sum_{i=1}^n m(\thet, \W_i)$. For instance, $Q_n$ is the sum of sample averages, so perhaps we can use some LLN to conclude $Q_n \pto Q_0$ uniformly instead of directly verifying that $\sup_{\thet \in \Thet} \abs{Q_n - Q_0} \pto 0$. </span>
<span id="cb3-645"><a href="#cb3-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-646"><a href="#cb3-646" aria-hidden="true" tabindex="-1"></a>:::{.lemma name="Uniform Weak Law of Large Numbers (UWLLN)" #uwlln}</span>
<span id="cb3-647"><a href="#cb3-647" aria-hidden="true" tabindex="-1"></a>Suppose that $\W_i \iid P_\thet$, $\Thet$ is compact, $m(\thet, \W_i)$ is continuous on $\Thet$ for all $\W_i \in \mathcal W$, and there exists some $d(\W_i)$ such that $\norm{m(\thet,\W_i)} \le d(\W_i)$ on $\Thet$ where $\E{d(\W_i)} &lt; \infty$. Then $\E{m(\thet, \W_i)}$ is continuous and $$n^{-1}\sum_{i=1}^n m(\thet, W_i) \pto \E{m(\thet, \W_i)}$$ uniformly on $\Thet$. </span>
<span id="cb3-648"><a href="#cb3-648" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-649"><a href="#cb3-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-650"><a href="#cb3-650" aria-hidden="true" tabindex="-1"></a>While I haven't been able to locate a formal proof of this result, it seems like a direct application of the Weierstrass M-test (Theorem 7.10 in @rudin1976principles) and the fact that the uniform limit of continuous functions is continuous. The lemma also includes two of the other conditions from \@ref(thm:excon): $\Thet$ is compact, and $Q_0 = \E{m(\thet, \W_i)}$.</span>
<span id="cb3-651"><a href="#cb3-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-652"><a href="#cb3-652" aria-hidden="true" tabindex="-1"></a>:::{.corollary name="Consistency of M-Estimators"}</span>
<span id="cb3-653"><a href="#cb3-653" aria-hidden="true" tabindex="-1"></a>If the conditions of Lemma \@ref(lem:uwlln) hold and $\E{m(\thet_0, \mathbf W)} &gt; \E{m(\thet, \mathbf W)}$ for all $\thet\neq\thet_0$, then $\ME \pto \thet_0$.</span>
<span id="cb3-654"><a href="#cb3-654" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-655"><a href="#cb3-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-656"><a href="#cb3-656" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-657"><a href="#cb3-657" aria-hidden="true" tabindex="-1"></a>Conditions 2-4 of \@ref(thm:excon) are satisfied under the assumption that  Lemma \@ref(lem:uwlln) holds. Condition 1 is satisfied as well because </span>
<span id="cb3-658"><a href="#cb3-658" aria-hidden="true" tabindex="-1"></a>$$ \plim Q_n = \plim \frac{1}{n}\sum_{i=1}^n m(\thet, W_i) = \E{m(\thet_0, \mathbf W)} = Q_0,$$ and we've assumed $Q_0 = \E{m(\thet_0, \mathbf W)}$ is uniquely maximized at $\thet_0$.</span>
<span id="cb3-659"><a href="#cb3-659" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-660"><a href="#cb3-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-661"><a href="#cb3-661" aria-hidden="true" tabindex="-1"></a>We can also restate Theorem \@ref(thm:exasy) in the context of M-estimators. Let's write the Jacobian and Hessian of $Q_n$ in the event that $\EE = \ME$.</span>
<span id="cb3-662"><a href="#cb3-662" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-663"><a href="#cb3-663" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q_n(\thet)}{\partial \thet} &amp;= \frac{\partial}{\partial \thet}\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^n m(\thet,\W_i)\right</span><span class="co">]</span> = \frac{1}{n}\sum_{i=1}^n \frac{\partial m(\thet,\W_i)}{\partial \thet}<span class="sc">\\</span></span>
<span id="cb3-664"><a href="#cb3-664" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q_n(\thet)}{\partial \thet \partial \thet'} &amp; = \frac{1}{n}\sum_{i=1}^n \frac{\partial m(\thet,\W_i)}{\partial \thet \partial \thet'}</span>
<span id="cb3-665"><a href="#cb3-665" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-666"><a href="#cb3-666" aria-hidden="true" tabindex="-1"></a>The derivatives of $Q_n$ are sample averages of the derivatives of $m$. Following @wooldridge2010econometric define the random matrix</span>
<span id="cb3-667"><a href="#cb3-667" aria-hidden="true" tabindex="-1"></a>$\Hm(\thet, \W_i) = \frac{\partial m(\thet,\W_i)}{\partial \thet \partial \thet'}$ and random vector</span>
<span id="cb3-668"><a href="#cb3-668" aria-hidden="true" tabindex="-1"></a>$\mathbf S(\thet, \W_i)= \frac{\partial m(\thet,\W_i)}{\partial \thet}$.^<span class="co">[</span><span class="ot">$\mathbf H$ as in "Hessian of $m$", $\mathbf S$ as in "score function" (lending from the terminology for MLE).</span><span class="co">]</span> Theorem \@ref(thm:exasy) also involved the probability limits of the Jacobian and Hessian of $Q_n$, and in the case of M-estimators, these happen to be expected values because of the LLN. </span>
<span id="cb3-669"><a href="#cb3-669" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-670"><a href="#cb3-670" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q_n(\thet)}{\partial \thet} &amp;= \frac{1}{n}\sum_{i=1}^n \mathbf S(\thet, \W_i) \pto \E{\mathbf S(\thet, \W)}<span class="sc">\\</span></span>
<span id="cb3-671"><a href="#cb3-671" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q_n(\thet)}{\partial \thet \partial \thet'} &amp; = \frac{1}{n}\sum_{i=1}^n \Hm(\thet, \W_i) \pto \E{\Hm(\thet, \W)}</span>
<span id="cb3-672"><a href="#cb3-672" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-673"><a href="#cb3-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-674"><a href="#cb3-674" aria-hidden="true" tabindex="-1"></a>:::{.corollary name="Asymptotic Normality of M-Estimators" #asyM}</span>
<span id="cb3-675"><a href="#cb3-675" aria-hidden="true" tabindex="-1"></a>Suppose the conditions of \@ref(thm:exasy) are met where </span>
<span id="cb3-676"><a href="#cb3-676" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-677"><a href="#cb3-677" aria-hidden="true" tabindex="-1"></a>Q_n(\thet) &amp;= \frac{1}{n}\sum_{i=1}^n m(\thet,\W_i),<span class="sc">\\</span></span>
<span id="cb3-678"><a href="#cb3-678" aria-hidden="true" tabindex="-1"></a>\mathbf S(\thet, \W_i) &amp;= \frac{\partial m(\thet,\W_i)}{\partial \thet},<span class="sc">\\</span></span>
<span id="cb3-679"><a href="#cb3-679" aria-hidden="true" tabindex="-1"></a>\Hm(\thet, \W_i) &amp;= \frac{\partial m(\thet,\W_i)}{\partial \thet \partial \thet'}.</span>
<span id="cb3-680"><a href="#cb3-680" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-681"><a href="#cb3-681" aria-hidden="true" tabindex="-1"></a>Then </span>
<span id="cb3-682"><a href="#cb3-682" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-683"><a href="#cb3-683" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\ME - \thet) &amp;\dto N\left(\zer, \E{\Hm(\thet_0, \W)}^{-1}\var{S(\thet_0, \W)}\E{\Hm (\thet_0, \W)}^{-1}\right),<span class="sc">\\</span></span>
<span id="cb3-684"><a href="#cb3-684" aria-hidden="true" tabindex="-1"></a>\ME &amp; \asim N\left(\thet, \frac{1}{n}\E{\Hm(\thet_0, \W)}^{-1}\var{ S(\thet_0, \W)}\E{\Hm (\thet_0, \W)}^{-1}\right).</span>
<span id="cb3-685"><a href="#cb3-685" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-686"><a href="#cb3-686" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-687"><a href="#cb3-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-688"><a href="#cb3-688" aria-hidden="true" tabindex="-1"></a>We are also able to use Lemma \@ref(lem:uwlln) to verify condition 6 of Theorem \@ref(thm:exasy). In the case of Corollary \@ref(cor:asyM), Condition 5 of Theorem \@ref(thm:exasy) establishes that $\E{\mathbf S(\thet_0, \W)} = \zer$, so the expectation of $\mathbf S$ "squared" is the variance, giving</span>
<span id="cb3-689"><a href="#cb3-689" aria-hidden="true" tabindex="-1"></a>$$ \ME  \asim N\left(\thet, \frac{1}{n}\E{\Hm (\thet_0, \W)}^{-1}\E{\mathbf S(\thet_0, \W)\mathbf S(\thet_0, \W)'}\E{\Hm (\thet_0, \W)}^{-1}\right).$$</span>
<span id="cb3-690"><a href="#cb3-690" aria-hidden="true" tabindex="-1"></a>In general, we didn't provide an estimator for $\avar{\EE}$, but we should be able to use the LLN to find a consistent estimator for $\avar{\ME}$. Define the Hessian and score functions evaluated for a single observation $\W_i$:</span>
<span id="cb3-691"><a href="#cb3-691" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-692"><a href="#cb3-692" aria-hidden="true" tabindex="-1"></a>\hat{\Hm}_i &amp;= \Hm(\ME, \W_i) &amp; (i =1,\ldots,n)<span class="sc">\\</span></span>
<span id="cb3-693"><a href="#cb3-693" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf S}_i &amp;= \mathbf{S}(\ME, \W_i) &amp; (i =1,\ldots,n)</span>
<span id="cb3-694"><a href="#cb3-694" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-695"><a href="#cb3-695" aria-hidden="true" tabindex="-1"></a>If the LLN holds for these random quantities, then </span>
<span id="cb3-696"><a href="#cb3-696" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-697"><a href="#cb3-697" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\Hm}} &amp;= \frac{1}{n}\sum_{i=1}^n {\Hm}_i(\ME, \W_i) \pto \E{\Hm(\thet_0, \W)}<span class="sc">\\</span></span>
<span id="cb3-698"><a href="#cb3-698" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\mathbf S\mathbf S'}} &amp;= \frac{1}{n}\sum_{i=1}^n {\mathbf S}_i(\ME, \W_i){\mathbf S}_i(\ME, \W_i)'  \pto \E{\mathbf S(\thet_0, \W)\mathbf S(\thet_0, \W)'}</span>
<span id="cb3-699"><a href="#cb3-699" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-700"><a href="#cb3-700" aria-hidden="true" tabindex="-1"></a>because $\ME \pto \thet_0$. The matrices of constants $\boldsymbol \Omega$ and $\mathbf H$ from Theorem \@ref(thm:exasy) now correspond to the variance of the mean-zero random vector $\mathbf S$ and expectation of random matrix $\Hm$, respectively. </span>
<span id="cb3-701"><a href="#cb3-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-702"><a href="#cb3-702" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Estimating Asymptotic Variance for M-Estimators"}</span>
<span id="cb3-703"><a href="#cb3-703" aria-hidden="true" tabindex="-1"></a>Suppose $\ME$ is an M-estimator, and define </span>
<span id="cb3-704"><a href="#cb3-704" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\text{Avar}}(\ME) = \frac{1}{n}\widehat{\E{\Hm}}^{-1}\widehat{\E{\mathbf S\mathbf S'}}\widehat{\E{\Hm}}^{-1} = \left<span class="co">[</span><span class="ot">\sum_{i=1}^n {\Hm}_i(\ME, \W_i)\right</span><span class="co">]</span>^{-1}\left<span class="co">[</span><span class="ot">\sum_{i=1}^n {\mathbf S}_i(\ME, \W_i){\mathbf S}_i(\ME, \W_i)'\right</span><span class="co">]</span>\left<span class="co">[</span><span class="ot">\sum_{i=1}^n {\Hm}_i(\ME, \W_i)\right</span><span class="co">]</span>^{-1}.$$ Then $\widehat{\text{Avar}}(\ME) \pto \avar{\ME}$.</span>
<span id="cb3-705"><a href="#cb3-705" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-706"><a href="#cb3-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-707"><a href="#cb3-707" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb3-708"><a href="#cb3-708" aria-hidden="true" tabindex="-1"></a>The consistency follows from the above derivation. If you factor out the $n$ terms from $\widehat{\E{\Hm}}^{-1}$ and $\widehat{\E{\mathbf S\mathbf S'}}$, they (including the $1/n$) will all cancel. </span>
<span id="cb3-709"><a href="#cb3-709" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-710"><a href="#cb3-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-711"><a href="#cb3-711" aria-hidden="true" tabindex="-1"></a>:::{.example name="OLS as an M-estimator"}</span>
<span id="cb3-712"><a href="#cb3-712" aria-hidden="true" tabindex="-1"></a>Suppose we are estimating the classical linear model $\mathcal P_\text{LM}$ with $\OLS$. In this case we can partition the random vector $\W_i$ into $(\varepsilon_i,\X_i)$, where the model defines $Y_i = \X_i\bet + \varepsilon_i$. This is an M-estimator where </span>
<span id="cb3-713"><a href="#cb3-713" aria-hidden="true" tabindex="-1"></a>$$m(\bet, Y_i, \X_i) = -(Y_i - \X_i\bet)^2.$$</span>
<span id="cb3-714"><a href="#cb3-714" aria-hidden="true" tabindex="-1"></a>We have </span>
<span id="cb3-715"><a href="#cb3-715" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-716"><a href="#cb3-716" aria-hidden="true" tabindex="-1"></a>\mathbf S_i(\bet, Y_i, \X_i) &amp; = 2\X_i'(Y_i - \X_i\bet) = 2\X_i\varepsilon_i<span class="sc">\\</span></span>
<span id="cb3-717"><a href="#cb3-717" aria-hidden="true" tabindex="-1"></a>\Hm_i(\bet, Y_i, \X_i) &amp; = 2\X_i'\X_i</span>
<span id="cb3-718"><a href="#cb3-718" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-719"><a href="#cb3-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-720"><a href="#cb3-720" aria-hidden="true" tabindex="-1"></a>In Example \@ref(exm:olsEX), we saw that $Q_0$ is uniquely maximized at $\bet_0$. The parameter space $\Thet = \mathbb R^k$ is convex, $Q_n$ is concave, $\bet_0$ is an interior point of $\Thet$ (otherwise it would be infinity), and by \@ref(lem:uwlln) $Q_n \pto Q_0$ uniformly, so by Theorem \@ref(thm:excon2) $\OLS \pto \bet_0$. Note that by the assumptions of $\mathcal P_\text{LM}$, $\E{\mathbf S(\thet_0, \W)} = \zer$ ($\X_i$ is weakly exogenous) and $\E{\Hm(\bet, Y_i, \X_i)}$ is invertible ($\E{\X'\X}$ has full rank), along with all the other assumptions of Corollary \@ref(cor:asyM) being met. Therefore we have </span>
<span id="cb3-721"><a href="#cb3-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-722"><a href="#cb3-722" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-723"><a href="#cb3-723" aria-hidden="true" tabindex="-1"></a>\OLS &amp;\asim N\left(\zer, \frac{1}{n}\E{\Hm(\thet_0, \W)}^{-1}\var{\mathbf S(\thet_0, \W)\mathbf S(\thet_0, \W)'}\E{\Hm(\thet_0, \W)}^{-1}\right)<span class="sc">\\</span></span>
<span id="cb3-724"><a href="#cb3-724" aria-hidden="true" tabindex="-1"></a>&amp; \asim  N\left(\bet, \frac{1}{n}\E{2\X'\X}^{-1}\E{2\X'\ep<span class="co">[</span><span class="ot">2\X'\ep</span><span class="co">]</span>'}\E{2\X'\X}^{-1}\right)<span class="sc">\\</span></span>
<span id="cb3-725"><a href="#cb3-725" aria-hidden="true" tabindex="-1"></a>&amp; \asim N\left(\bet, \frac{1}{n}\E{\X'\X}^{-1}\E{\X'\ep\ep \X}\E{\X'\X}^{-1}\right)<span class="sc">\\</span></span>
<span id="cb3-726"><a href="#cb3-726" aria-hidden="true" tabindex="-1"></a>&amp; \asim N\left(\bet, \frac{\sigma^2}{n}\E{\X'\X}^{-1}\E{\X'\X}\E{\X'\X}^{-1}\right)<span class="sc">\\</span></span>
<span id="cb3-727"><a href="#cb3-727" aria-hidden="true" tabindex="-1"></a>&amp; \asim  N\left(\bet, \frac{\sigma^2}{n}\E{\X'\X}^{-1}\right).</span>
<span id="cb3-728"><a href="#cb3-728" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-729"><a href="#cb3-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-730"><a href="#cb3-730" aria-hidden="true" tabindex="-1"></a>This is the *exact* same asymptotic distribution from Theorem \@ref(thm:asymols)! We can also derive an asymptotic estimator for $\avar{\OLS}$ using properties of M-estimators.</span>
<span id="cb3-731"><a href="#cb3-731" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-732"><a href="#cb3-732" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\Hm}} &amp;= \frac{2}{n}\sum_{i=1}^n \X_i'\X_i<span class="sc">\\</span></span>
<span id="cb3-733"><a href="#cb3-733" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\mathbf S\mathbf S'}} &amp;= \frac{4}{n}\sum_{i=1}^n \hat{e}_i^2\X_i'\X_i =  \left(\frac{2}{n}\sum_{i=1}^n \hat e_i^2\right) \left(\frac{2}{n}\sum_{i=1}^n \X_i'\X_i\right)<span class="sc">\\</span></span>
<span id="cb3-734"><a href="#cb3-734" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\Hm}}^{-1}\widehat{\E{\mathbf S\mathbf S'}} \widehat{\E{\Hm}}^{-1} &amp; = \left(\frac{2}{n}\sum_{i=1}^n \X_i'\X_i\right)^{-1} \left(\frac{2}{n}\sum_{i=1}^n \hat e_i^2\right) \left(\frac{2}{n}\sum_{i=1}^n \X_i'\X_i\right)\left(\frac{2}{n}\sum_{i=1}^n \X_i'\X_i\right)^{-1} =\hat{\e}'\hat{\e}\left(\Xm'\Xm\right)^{-1}  <span class="sc">\\</span></span>
<span id="cb3-735"><a href="#cb3-735" aria-hidden="true" tabindex="-1"></a>\widehat{\text{Avar}}(\OLS) &amp; = \frac{\hat{\e}'\hat{\e}}{n}\left(\Xm'\Xm\right)^{-1}</span>
<span id="cb3-736"><a href="#cb3-736" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-737"><a href="#cb3-737" aria-hidden="true" tabindex="-1"></a>But this doesn't look right, because $\frac{\hat{\e}'\hat{\e}}{n} \neq S^2$, as it doesn't have the bias correction of $n-K$. Technically, this isn't the end of the world because the estimator for $\sigma^2$ is asymptotically unbiased. </span>
<span id="cb3-738"><a href="#cb3-738" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-739"><a href="#cb3-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-740"><a href="#cb3-740" aria-hidden="true" tabindex="-1"></a>In general, we weren't able to estimate the asymptotic variance of $\EE$, but we can for $\ME$. This means we can use the trio of test statistics developed in the previous section. </span>
<span id="cb3-741"><a href="#cb3-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-742"><a href="#cb3-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-743"><a href="#cb3-743" aria-hidden="true" tabindex="-1"></a>:::{.corollary name="Testing Trinity, M-Estimators" #asyM}</span>
<span id="cb3-744"><a href="#cb3-744" aria-hidden="true" tabindex="-1"></a>If $\ME$ is an M-estimator, the test statistics in Theorem \@ref(thm:trintest) become: </span>
<span id="cb3-745"><a href="#cb3-745" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-746"><a href="#cb3-746" aria-hidden="true" tabindex="-1"></a>W &amp; = n\mathbf h(\ME)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\ME)\widehat{\E{\Hm}}^{-1}\widehat{\E{\mathbf S\mathbf S'}}\widehat{\E{\Hm}}^{-1}\frac{\partial \mathbf h}{\partial \thet}(\ME)'\right</span><span class="co">]</span>^{-1}   \mathbf h(\ME);<span class="sc">\\</span></span>
<span id="cb3-747"><a href="#cb3-747" aria-hidden="true" tabindex="-1"></a>LM &amp; =  n\widehat{\E{\mathbf S(\bar{\thet}_\text{M})}}'\widehat{\E{\Hm}}^{-1}\frac{\partial \mathbf h}{\partial \thet}(\thet_0)' \left[\frac{\partial \mathbf h}{\partial \thet}(\thet_0) \widehat{\E{\Hm}}^{-1}\widehat{\text{E}}{[\mathbf S(\bar{\thet}_\text{M})\mathbf S(\bar{\thet}_\text{M})']}\widehat{\E{\Hm}}^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)'\right]^{-1} \frac{\partial \mathbf h}{\partial \thet}(\thet_0)\widehat{\E{\Hm}}^{-1}\widehat{\E{\mathbf S(\bar{\thet}_\text{M})}};<span class="sc">\\</span></span>
<span id="cb3-748"><a href="#cb3-748" aria-hidden="true" tabindex="-1"></a>DM &amp; = \frac{2}{\hat c} \left<span class="co">[</span><span class="ot">\sum_{i=1}^n m(\ME,\W_i) - \sum_{i=1}^n m(\bar{\thet}_\text{M},\W_i)\right</span><span class="co">]</span>.</span>
<span id="cb3-749"><a href="#cb3-749" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-750"><a href="#cb3-750" aria-hidden="true" tabindex="-1"></a>In the generalized information inequality holds when calculating $LM$, we have </span>
<span id="cb3-751"><a href="#cb3-751" aria-hidden="true" tabindex="-1"></a>$$ LM = n\widehat{\E{\mathbf S(\bar{\thet}_\text{M})}}'\widehat{\text{E}}{[\mathbf S(\bar{\thet}_\text{M})\mathbf S(\bar{\thet}_\text{M})']}^{-1} \widehat{\E{\mathbf S(\bar{\thet}_\text{M})}}$$</span>
<span id="cb3-752"><a href="#cb3-752" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-753"><a href="#cb3-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-754"><a href="#cb3-754" aria-hidden="true" tabindex="-1"></a>:::{.example #olstests}</span>
<span id="cb3-755"><a href="#cb3-755" aria-hidden="true" tabindex="-1"></a>Let's employ our tests in the context of the classical linear model. In this case, let </span>
<span id="cb3-756"><a href="#cb3-756" aria-hidden="true" tabindex="-1"></a>$$m_i(\bet, Y_i, \X_i) = -\frac{1}{2} \sum_{i=1}^n (Y_i - \X_i\bet)^2,$$ such that the 2s are eliminated when differentiating $m_i(\bet, Y_i, \X_i)$.</span>
<span id="cb3-757"><a href="#cb3-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-758"><a href="#cb3-758" aria-hidden="true" tabindex="-1"></a>First, let's confirm the generalized information inequality holds and that we can use the distance metric statistic. </span>
<span id="cb3-759"><a href="#cb3-759" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-760"><a href="#cb3-760" aria-hidden="true" tabindex="-1"></a>\mathbf H(\thet_0) &amp; = \E{\Hm(\thet_0, \mathbf W)} = \E{\X'\X} <span class="sc">\\</span></span>
<span id="cb3-761"><a href="#cb3-761" aria-hidden="true" tabindex="-1"></a>\boldsymbol \Omega &amp; = \E{\mathbf S(\thet_0, \mathbf W)\mathbf S(\thet_0, \mathbf W)'} = \E{\X'\varepsilon\varepsilon'\X} = \E{\varepsilon^2}\E{\X'\X} = \sigma^2\E{\X'\X} </span>
<span id="cb3-762"><a href="#cb3-762" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-763"><a href="#cb3-763" aria-hidden="true" tabindex="-1"></a>The inequality holds, as $\boldsymbol \Omega = c\mathbf H(\thet_0)$, where $c = \sigma^2$. We could let $\hat c = S^2$, but here we'll use $$\hat c = \frac{\hat{\boldsymbol \Omega}}{\hat{\mathbf H}(\thet_0)} = \frac{1}{n}\sum_{i=1}^n \hat e_i^2 = \frac{\hat{\e}'\hat{\e}}{n}.$$</span>
<span id="cb3-764"><a href="#cb3-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-765"><a href="#cb3-765" aria-hidden="true" tabindex="-1"></a>Our test statistics are</span>
<span id="cb3-766"><a href="#cb3-766" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-767"><a href="#cb3-767" aria-hidden="true" tabindex="-1"></a>W &amp; = n\mathbf h(\ME)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\ME)\widehat{\E{\Hm}}^{-1}\widehat{\E{\mathbf S\mathbf S'}}\widehat{\E{\Hm}}^{-1}\frac{\partial \mathbf h}{\partial \thet}(\ME)'\right</span><span class="co">]</span>^{-1}   \mathbf h(\ME) <span class="sc">\\</span></span>
<span id="cb3-768"><a href="#cb3-768" aria-hidden="true" tabindex="-1"></a>&amp; = \mathbf h(\ME)'\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial \thet}(\ME)\left[\frac{\hat{\e}'\hat{\e}}{n}\left(\Xm'\Xm\right)^{-1}\right]\frac{\partial \mathbf h}{\partial \thet}(\ME)'\right</span><span class="co">]</span>^{-1}   \mathbf h(\ME) <span class="sc">\\\\</span></span>
<span id="cb3-769"><a href="#cb3-769" aria-hidden="true" tabindex="-1"></a>LM &amp;= n\widehat{\E{\mathbf S(\bar{\thet}_\text{M})}}'\widehat{\text{E}}{[\mathbf S(\bar{\thet}_\text{M})\mathbf S(\bar{\thet}_\text{M})']} \widehat{\E{\mathbf S(\bar{\thet}_\text{M})}}<span class="sc">\\</span></span>
<span id="cb3-770"><a href="#cb3-770" aria-hidden="true" tabindex="-1"></a>&amp; = n\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^n  \bar{e}_i\X_i \right</span><span class="co">]</span>'\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^n \bar{e}_i^2\X_i'\X_i \right</span><span class="co">]</span>^{-1} \left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^n \bar e_i\X_i \right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-771"><a href="#cb3-771" aria-hidden="true" tabindex="-1"></a>&amp; = \left<span class="co">[</span><span class="ot">\sum_{i=1}^n  \bar{e}_i\X_i \right</span><span class="co">]</span>'\left<span class="co">[</span><span class="ot">\frac{\bar{\e}'\bar{\e}}{n}\sum_{i=1}^n\X_i'\X_i \right</span><span class="co">]</span>^{-1} \left<span class="co">[</span><span class="ot">\sum_{i=1}^n \bar e_i\X_i \right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-772"><a href="#cb3-772" aria-hidden="true" tabindex="-1"></a>&amp; = \Xm'\bar{\e}\left(\frac{\hat{\e}'\hat{\e}}{n}\Xm'\Xm\right)^{-1}\bar{\e}'\Xm<span class="sc">\\\\</span></span>
<span id="cb3-773"><a href="#cb3-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-774"><a href="#cb3-774" aria-hidden="true" tabindex="-1"></a>DM &amp; = \frac{2}{\hat c} \left<span class="co">[</span><span class="ot">\sum_{i=1}^n m(\ME,\W_i) - \sum_{i=1}^n m(\bar{\thet}_\text{M},\W_i)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-775"><a href="#cb3-775" aria-hidden="true" tabindex="-1"></a>   &amp; = \frac{2}{\hat{\e}'\hat{\e}/n}\left<span class="co">[</span><span class="ot">\sum_{i=1}^n -\frac{1}{2}(Y_i-\X_i\OLS)^2 - \sum_{i=1}^n -\frac{1}{2}(Y_i-\X_i\bar{\bet}_\text{OLS})^2\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-776"><a href="#cb3-776" aria-hidden="true" tabindex="-1"></a>   &amp; = \frac{1}{\hat{\e}'\hat{\e}/n}\left<span class="co">[</span><span class="ot">\sum_{i=1}^n \bar e_i^2 - \sum_{i=1}^n \bar e_i^2\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb3-777"><a href="#cb3-777" aria-hidden="true" tabindex="-1"></a>   &amp; = \frac{1}{\hat{\e}'\hat{\e}/n}\left<span class="co">[</span><span class="ot">\bar{\e}'\bar{\e} - \hat{\e}'\hat{\e}\right</span><span class="co">]</span></span>
<span id="cb3-778"><a href="#cb3-778" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-779"><a href="#cb3-779" aria-hidden="true" tabindex="-1"></a>The residuals associated with the unrestricted estimator $\OLS$ are $\hat e_i$, while $\bar e_i$ are the residuals associated with the restricted estimator. If we let $SSR =  \hat{\e}'\hat{\e}$ and $SSR_0 = \bar{\e}'\bar{\e}$ denote the sum of squared residuals for the unrestricted and restricted models respectively, then </span>
<span id="cb3-780"><a href="#cb3-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-781"><a href="#cb3-781" aria-hidden="true" tabindex="-1"></a>$$ DM = \frac{SSR_0 - SSR}{SSR/n}.$$</span>
<span id="cb3-782"><a href="#cb3-782" aria-hidden="true" tabindex="-1"></a>Suppose $Y = 1 + 2X + \varepsilon$, where $\bet = <span class="co">[</span><span class="ot">1,2</span><span class="co">]</span>'$. Let's test the null hypothesis that the parameters coincide with their true underlying values, $H_0: \bet = \bet_0$. In this case </span>
<span id="cb3-783"><a href="#cb3-783" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-784"><a href="#cb3-784" aria-hidden="true" tabindex="-1"></a>\mathbf h(\bet) &amp;= \begin{bmatrix}\beta_1 - \beta_{0,2}<span class="sc">\\</span> \beta_2 - \beta_{0,1} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb3-785"><a href="#cb3-785" aria-hidden="true" tabindex="-1"></a>\frac{\partial\mathbf h(\bet)}{\partial \bet} &amp;= \begin{bmatrix}1 &amp; 0 <span class="sc">\\</span> 0 &amp; 1 \end{bmatrix} = \mathbf I</span>
<span id="cb3-786"><a href="#cb3-786" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-787"><a href="#cb3-787" aria-hidden="true" tabindex="-1"></a>where $H_0: \mathbf h(\bet) = \zer$. The image $\mathbf h(\Thet)$ is a singleton, so when we maximize $Q_n$ such that $\mathbf h(\thet) = \zer$ to get our restricted estimate, we trivially have $\bar{\bet}_\text{OLS} = \bet_0$. For this hypothesis we have </span>
<span id="cb3-788"><a href="#cb3-788" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-789"><a href="#cb3-789" aria-hidden="true" tabindex="-1"></a>W &amp; = (\OLS - \bet_0)'\left<span class="co">[</span><span class="ot">\frac{\hat{\e}'\hat{\e}}{n}\left(\Xm'\Xm\right)^{-1}\right</span><span class="co">]</span>^{-1}(\OLS - \bet_0)<span class="sc">\\</span></span>
<span id="cb3-790"><a href="#cb3-790" aria-hidden="true" tabindex="-1"></a>  &amp; = \left(\frac{\hat{\e}'\hat{\e}}{n}\right)^{-1}(\OLS - \bet_0)'\left(\Xm'\Xm\right)(\OLS - \bet_0) <span class="sc">\\</span></span>
<span id="cb3-791"><a href="#cb3-791" aria-hidden="true" tabindex="-1"></a>  &amp; = \left(\frac{\hat{\e}'\hat{\e}}{n}\right)^{-1}(\Xm\OLS - \Xm\bet_0)'(\Xm\OLS - \Xm\bet_0)<span class="sc">\\</span></span>
<span id="cb3-792"><a href="#cb3-792" aria-hidden="true" tabindex="-1"></a>  &amp; = \left(\frac{\hat{\e}'\hat{\e}}{n}\right)^{-1}((\Y - \hat{\e}) - (\Y - \bar{\e}))'((\Y - \hat{\e}) - (\Y - \bar{\e})) &amp; (\Y = \X\OLS + \hat{\e},\ \Y = \X\bet_0 + \bar{\e})<span class="sc">\\</span></span>
<span id="cb3-793"><a href="#cb3-793" aria-hidden="true" tabindex="-1"></a>  &amp; = \left(\frac{\hat{\e}'\hat{\e}}{n}\right)^{-1}(\bar{\e} - \hat{\e})'(\bar{\e} - \hat{\e})<span class="sc">\\</span></span>
<span id="cb3-794"><a href="#cb3-794" aria-hidden="true" tabindex="-1"></a>  &amp; = \left(\frac{\hat{\e}'\hat{\e}}{n}\right)^{-1}(\bar{\e}'\bar{\e} - \bar{\e}'\hat{\e}  - \hat{\e}'\bar{\e} + \hat{\e}'\hat{\e})</span>
<span id="cb3-795"><a href="#cb3-795" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-796"><a href="#cb3-796" aria-hidden="true" tabindex="-1"></a>It happens to be the case that $\bar{\e}'\hat{\e}  = \hat{\e}'\bar{\e} = \hat{\e}'\hat{\e}$,^[\begin{align*} \bar{\e}'\hat{\e} &amp; = (\Y- \Xm\bet_0)'(\Y- \Xm\bet_0) <span class="sc">\\</span> &amp; = \Y'\Y - \Y'\Xm\OLS - \bet_0'\Xm\Y + \bet_0'\Xm'\Xm\OLS <span class="sc">\\</span> &amp; = \Y'\Y - \Y'\Xm(\Xm'\Xm)^{-1}\Xm'\Y- \bet_0'\Xm\Y + \bet_0'\Xm'\Xm(\Xm'\Xm)^{-1}\Xm'\Y<span class="sc">\\</span></span>
<span id="cb3-797"><a href="#cb3-797" aria-hidden="true" tabindex="-1"></a>&amp; = \Y'<span class="co">[</span><span class="ot">\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm'</span><span class="co">]</span>\Y- \bet_0'\Xm\Y + \bet_0'\Xm'\Y<span class="sc">\\</span></span>
<span id="cb3-798"><a href="#cb3-798" aria-hidden="true" tabindex="-1"></a>&amp; = \Y'\mathbb M\Y<span class="sc">\\</span></span>
<span id="cb3-799"><a href="#cb3-799" aria-hidden="true" tabindex="-1"></a>&amp; = (\X\OLS + \hat{\e})'\mathbb M\Y <span class="sc">\\</span></span>
<span id="cb3-800"><a href="#cb3-800" aria-hidden="true" tabindex="-1"></a>&amp; = \hat{\e}'\underbrace{\mathbf M \Y}_{\hat{\e}} + \OLS'\underbrace{\X\mathbb M}_\zer \Y <span class="sc">\\</span></span>
<span id="cb3-801"><a href="#cb3-801" aria-hidden="true" tabindex="-1"></a>&amp; = \hat{\e}'\hat{\e} <span class="sc">\\</span></span>
<span id="cb3-802"><a href="#cb3-802" aria-hidden="true" tabindex="-1"></a>\hat{\e}'\bar{\e} &amp; = (\bar{\e}'\hat{\e})'<span class="sc">\\</span></span>
<span id="cb3-803"><a href="#cb3-803" aria-hidden="true" tabindex="-1"></a>&amp; = (\hat{\e}'\hat{\e})'<span class="sc">\\</span></span>
<span id="cb3-804"><a href="#cb3-804" aria-hidden="true" tabindex="-1"></a>&amp; = \hat{\e}'\hat{\e}</span>
<span id="cb3-805"><a href="#cb3-805" aria-hidden="true" tabindex="-1"></a>\end{align*}] so we have</span>
<span id="cb3-806"><a href="#cb3-806" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-807"><a href="#cb3-807" aria-hidden="true" tabindex="-1"></a>W  =\frac{\bar{\e}'\bar{\e} - \hat{\e}'\hat{\e}}{\hat{\e}'\hat{\e}/n} = DM.</span>
<span id="cb3-808"><a href="#cb3-808" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-809"><a href="#cb3-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-810"><a href="#cb3-810" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning=FALSE}</span></span>
<span id="cb3-811"><a href="#cb3-811" aria-hidden="true" tabindex="-1"></a><span class="co">#generate data</span></span>
<span id="cb3-812"><a href="#cb3-812" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb3-813"><a href="#cb3-813" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb3-814"><a href="#cb3-814" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb3-815"><a href="#cb3-815" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-816"><a href="#cb3-816" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb3-817"><a href="#cb3-817" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb3-818"><a href="#cb3-818" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb3-819"><a href="#cb3-819" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb3-820"><a href="#cb3-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-821"><a href="#cb3-821" aria-hidden="true" tabindex="-1"></a><span class="co">#estimate model and calculate OLS residuals and restricted OLS residual</span></span>
<span id="cb3-822"><a href="#cb3-822" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb3-823"><a href="#cb3-823" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> y <span class="sc">-</span> X <span class="sc">%*%</span> OLS</span>
<span id="cb3-824"><a href="#cb3-824" aria-hidden="true" tabindex="-1"></a>res_0 <span class="ot">&lt;-</span> y <span class="sc">-</span> X <span class="sc">%*%</span> beta_0</span>
<span id="cb3-825"><a href="#cb3-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-826"><a href="#cb3-826" aria-hidden="true" tabindex="-1"></a><span class="co">#Wald stat</span></span>
<span id="cb3-827"><a href="#cb3-827" aria-hidden="true" tabindex="-1"></a>ee_n <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n))</span>
<span id="cb3-828"><a href="#cb3-828" aria-hidden="true" tabindex="-1"></a>avar <span class="ot">&lt;-</span> ee_n <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb3-829"><a href="#cb3-829" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">t</span>(OLS <span class="sc">-</span> beta_0) <span class="sc">%*%</span> <span class="fu">solve</span>(avar) <span class="sc">%*%</span> (OLS <span class="sc">-</span> beta_0)</span>
<span id="cb3-830"><a href="#cb3-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-831"><a href="#cb3-831" aria-hidden="true" tabindex="-1"></a><span class="co">#LM stat</span></span>
<span id="cb3-832"><a href="#cb3-832" aria-hidden="true" tabindex="-1"></a>LM <span class="ot">&lt;-</span> <span class="fu">t</span>(res_0) <span class="sc">%*%</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(ee_n <span class="sc">*</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> X)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> res_0</span>
<span id="cb3-833"><a href="#cb3-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-834"><a href="#cb3-834" aria-hidden="true" tabindex="-1"></a><span class="co">#DM stat</span></span>
<span id="cb3-835"><a href="#cb3-835" aria-hidden="true" tabindex="-1"></a>SSR <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">t</span>(res) <span class="sc">%*%</span> res)</span>
<span id="cb3-836"><a href="#cb3-836" aria-hidden="true" tabindex="-1"></a>SSR_0 <span class="ot">&lt;-</span>  <span class="fu">as.numeric</span>(<span class="fu">t</span>(res_0) <span class="sc">%*%</span> res_0)</span>
<span id="cb3-837"><a href="#cb3-837" aria-hidden="true" tabindex="-1"></a>DM <span class="ot">&lt;-</span> (SSR_0 <span class="sc">-</span> SSR) <span class="sc">/</span> (ee_n)</span>
<span id="cb3-838"><a href="#cb3-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-839"><a href="#cb3-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-840"><a href="#cb3-840" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(W, LM, DM)</span>
<span id="cb3-841"><a href="#cb3-841" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-842"><a href="#cb3-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-843"><a href="#cb3-843" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-844"><a href="#cb3-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-845"><a href="#cb3-845" aria-hidden="true" tabindex="-1"></a><span class="fu">### Nonlinear Least Squares</span></span>
<span id="cb3-846"><a href="#cb3-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-847"><a href="#cb3-847" aria-hidden="true" tabindex="-1"></a>We've generalized the classical linear model $\mathcal P_\text{LM}$ several ways. Section \@ref(endogeniety-i-iv-and-2sls) dropped the assumption $\E{\X'\ep}=\zer$ and introduced the model $\mathcal P_\text{IV}$. Section \@ref(generalized-least-squares) dropped the assumption $\E{\ep'\ep} =\sigma^2 \mathbf I$ and introduced the general linear regression model $\mathcal P_\text{GLRM}$. Section \@ref(endogeniety-ii-simultaneous-equation-models) extended the linear model by allowing for multiple "seemingly unrelated" linear models, $\mathcal P_\text{SUR}$. In each of these case, we assumed the structural relationship between the dependent variable and regressors was linear. Let's now drop this assumptions. </span>
<span id="cb3-848"><a href="#cb3-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-849"><a href="#cb3-849" aria-hidden="true" tabindex="-1"></a>Suppose we posit that $Y$ is related to regressors $\X$ and parameters $\bet$ through some deterministic relationship given by a function $f(\X, \bet)$. We can add a structural error such that $Y_i = g(\X_i,\bet) + \varepsilon_i$, or $$ \Y = \mathbf g(\Xm,\bet) + \ep$$ where $\mathbf g$ is a vector-valued function of $n$ copies of $f$. An element of our model $\mathcal P$ will contain elements $P$ which are collections of joint distributions $F_{\Xm,\ep}$ satisfying some common conditions. We also want our model to reduce to $\mathcal P_\text{LM}$ in the event that $\mathbf g(\Xm,\bet) = \Xm\bet$. Two of the assumptions of the $\mathcal P_\text{LM}$ are pretty easy as we can replace $\Xm$ with $\mathbf g(\Xm,\bet) $:</span>
<span id="cb3-850"><a href="#cb3-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-851"><a href="#cb3-851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strict exogeneity, $\E{\ep \mid \mathbf g(\Xm,\bet)} = \zer \implies \E{\ep \mid \Xm} = \zer$ when $\mathbf g(\Xm,\bet) = \Xm\bet$. This assumption also implies weak exogeneity $\E{\mathbf g(\Xm,\bet)'\ep} = \zer$.</span>
<span id="cb3-852"><a href="#cb3-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spherical errors, $\E{\ep'\ep \mid \mathbf g(\Xm,\bet)} = \sigma^2\mathbf I \implies \E{\ep \mid \Xm} = \sigma^2\mathbf I$ when $\mathbf g(\Xm,\bet) = \Xm\bet$</span>
<span id="cb3-853"><a href="#cb3-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-854"><a href="#cb3-854" aria-hidden="true" tabindex="-1"></a>The one assumption of $\mathcal P_\text{LM}$ that takes some thought to generalize is $\text{rank}\left(\E{\X'\X}\right) = K$. What role did this assumption play in the linear model? It made sure that $\bet \neq \bet'$ whenever $\X\bet \neq \X\bet'$, as $\bet$ satisfies $\E{\X'\ep} = \bet \E{\X'\X}$ in this case. In the event $g(\X_i,\bet) = \X_i\bet$ for $i = 1,\ldots,n$, then this condition ensures that $\bet \neq \bet'$ implies $g(\X_i,\bet) \neq g(\X_i,\bet')$ for all $i$. This will be the analogous condition -- $\bet \neq \bet'$ implies $g(\X_i,\bet) \neq g(\X_i,\bet')$. </span>
<span id="cb3-855"><a href="#cb3-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-856"><a href="#cb3-856" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-857"><a href="#cb3-857" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(classical) nonlinear model_**<span class="kw">&lt;/span&gt;</span> is defined as $\mathcal P_\text{NLM} = <span class="sc">\{</span>P_{\bet,\sigma^2} \mid \bet \in \mathbb R^{K}, \sigma^2\in\mathbb R<span class="sc">\}</span>$, where </span>
<span id="cb3-858"><a href="#cb3-858" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-859"><a href="#cb3-859" aria-hidden="true" tabindex="-1"></a>P_{\bet,\sigma^2} &amp;= <span class="sc">\{</span>F_{\Xm,\ep} \mid \Y = \mathbf g(\Xm,\bet) + \ep, \ \E{\ep'\ep\mid \mathbf g(\Xm,\bet)}=\sigma^2\mathbf I, \ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i}, \bet\neq\bet' \implies \mathbf g(\Xm,\bet) \neq \mathbf g(\Xm,\bet'),\ \E{\ep \mid \mathbf g(\Xm,\bet)} = \zer<span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb3-860"><a href="#cb3-860" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_j, \cdots \X_K</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_i, \cdots \X_n</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb3-861"><a href="#cb3-861" aria-hidden="true" tabindex="-1"></a>\Y &amp; = <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>,</span>
<span id="cb3-862"><a href="#cb3-862" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-863"><a href="#cb3-863" aria-hidden="true" tabindex="-1"></a>for a known function $\mathbf g(\Xm, \bet)$.</span>
<span id="cb3-864"><a href="#cb3-864" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-865"><a href="#cb3-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-866"><a href="#cb3-866" aria-hidden="true" tabindex="-1"></a>This definition of the model assumes that we've correctly specified $\mathbf g(\Xm, \bet)$, just like how the linear model assumed that the relationship between $\Y$ and $\Xm$ was actually $\Y = \Xm\bet + \ep$. For $\mathcal P_\text{NLM}$ this assumption is much stronger. We are not only assuming that the model contains the correct independent variables, but we are also assuming the relationship $\mathbf g(\Xm, \bet)$ is correct. The natural estimator for the nonlinear model is nonlinear least squares. </span>
<span id="cb3-867"><a href="#cb3-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-868"><a href="#cb3-868" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-869"><a href="#cb3-869" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_nonlinear least squares estimator_**<span class="kw">&lt;/span&gt;</span> is defined as </span>
<span id="cb3-870"><a href="#cb3-870" aria-hidden="true" tabindex="-1"></a>$$ \NLS(\Xm, \Y) = \argmin_\bet\frac{1}{n}\sum_{i=1}^n(Y_i - g(\X_i,\bet))^2 $$</span>
<span id="cb3-871"><a href="#cb3-871" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-872"><a href="#cb3-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-873"><a href="#cb3-873" aria-hidden="true" tabindex="-1"></a>This estimator is an M-estimator where $m(\bet, Y_i, \X_i) = -<span class="sc">\(</span>Y_i - g(\X_i,\bet))^2$, and $$Q_0 = \plim -\frac{1}{n}\sum_{i=1}^n(Y_i - g(\X_i,\bet))^2= - \E{Y - g(\X, \bet)}$$</span>
<span id="cb3-874"><a href="#cb3-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-875"><a href="#cb3-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-876"><a href="#cb3-876" aria-hidden="true" tabindex="-1"></a><span class="fu">### Least Absolute Deviations</span></span>
<span id="cb3-877"><a href="#cb3-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-878"><a href="#cb3-878" aria-hidden="true" tabindex="-1"></a><span class="fu">## Minimum Distance Estimators</span></span>
<span id="cb3-879"><a href="#cb3-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-880"><a href="#cb3-880" aria-hidden="true" tabindex="-1"></a>:::{.definition}</span>
<span id="cb3-881"><a href="#cb3-881" aria-hidden="true" tabindex="-1"></a>Suppose $\Wm = <span class="co">[</span><span class="ot">\W_1,\ldots, \W_n</span><span class="co">]</span>' \sim P_\thet$ where $P_\thet\in\mathcal P$ for a known model $\mathcal P$. A <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_minimum distance estimator_**<span class="kw">&lt;/span&gt;</span> $\MDE:\mathcal W\to \Thet$ is an extremum estimator where </span>
<span id="cb3-882"><a href="#cb3-882" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb3-883"><a href="#cb3-883" aria-hidden="true" tabindex="-1"></a>Q_n(\thet) &amp;= -<span class="co">[</span><span class="ot">\hat {\boldsymbol \pi} - \mathbf g(\thet)</span><span class="co">]</span>'\boldsymbol \Phi <span class="co">[</span><span class="ot">\hat {\boldsymbol \pi}- \mathbf g(\thet)</span><span class="co">]</span>,<span class="sc">\\</span></span>
<span id="cb3-884"><a href="#cb3-884" aria-hidden="true" tabindex="-1"></a>\MDE &amp;= \argmax_{\thet \in \Thet} -<span class="co">[</span><span class="ot">\hat {\boldsymbol \pi} - \mathbf g(\thet)</span><span class="co">]</span>'\boldsymbol \Phi <span class="co">[</span><span class="ot">\hat {\boldsymbol \pi}- \mathbf g(\thet)</span><span class="co">]</span>,</span>
<span id="cb3-885"><a href="#cb3-885" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb3-886"><a href="#cb3-886" aria-hidden="true" tabindex="-1"></a>for some sample statistic $\hat {\boldsymbol \pi}$ satisfying $\hat {\boldsymbol \pi} \pto \mathbf g(\thet)$ and a weighting matrix $\boldsymbol \Phi$.</span>
<span id="cb3-887"><a href="#cb3-887" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb3-888"><a href="#cb3-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-889"><a href="#cb3-889" aria-hidden="true" tabindex="-1"></a><span class="fu">## Recap</span></span>
<span id="cb3-890"><a href="#cb3-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-891"><a href="#cb3-891" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example/Replication </span></span>
<span id="cb3-892"><a href="#cb3-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-893"><a href="#cb3-893" aria-hidden="true" tabindex="-1"></a><span class="fu">## Futher Reading</span></span>
<span id="cb3-894"><a href="#cb3-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-895"><a href="#cb3-895" aria-hidden="true" tabindex="-1"></a>**_Extremum estimators_**: @newey1994large, Chapter 4 of @takeshi1985advanced, <span class="co">[</span><span class="ot">these</span><span class="co">](https://www.ssc.wisc.edu/~xshi/econ715/Lecture_6_testing2.pdf)</span> <span class="co">[</span><span class="ot">awesome</span><span class="co">]</span>(https://www.ssc.wisc.edu/~xshi/econ715/Lecture_3_consistency.pdf</span>
<span id="cb3-896"><a href="#cb3-896" aria-hidden="true" tabindex="-1"></a>) <span class="co">[</span><span class="ot">slides</span><span class="co">](https://www.ssc.wisc.edu/~xshi/econ715/Lecture_4_normality.pdf)</span> posted by Xiaoxia Shi, Chapter Chapter 7 of @hayashi2011econometrics, Chapter 12 of @greene2003econometric</span>
<span id="cb3-897"><a href="#cb3-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-898"><a href="#cb3-898" aria-hidden="true" tabindex="-1"></a>**_Trinity of Hypothesis Tests_**: @engle1984wald, Section 12.4 of @lehmann2005testing, Chapter 16 of @van2000asymptotic</span>
<span id="cb3-899"><a href="#cb3-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-900"><a href="#cb3-900" aria-hidden="true" tabindex="-1"></a>**_M-estimators_**: Chapter 12 of @wooldridge2010econometric, Chapter 22 of @hansen2022econometrics, Chapter 12 of @greene2003econometric, Chapter 7 of @hayashi2011econometrics, Chapter 17 of @davidson1993estimation </span>
<span id="cb3-901"><a href="#cb3-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-902"><a href="#cb3-902" aria-hidden="true" tabindex="-1"></a>**_Minimum distances estimators_**:</span>
<span id="cb3-903"><a href="#cb3-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-904"><a href="#cb3-904" aria-hidden="true" tabindex="-1"></a>**_NLLS_**: Chapter 2 of @davidson1993estimation, Chapter 7 of @greene2003econometric, Chapter 12 of @wooldridge2010econometric, Chapter 23 of @hansen2022econometrics, @mizon1977inferential, Chapter 5 of @cameron2005microeconometrics</span>
<span id="cb3-905"><a href="#cb3-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-906"><a href="#cb3-906" aria-hidden="true" tabindex="-1"></a>**_LAD_**:</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>