<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanved Econometrics with Examples - 5&nbsp; The Classical Linear Model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./endog.html" rel="next">
<link href="./exp_fam.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#identification-estimation-and-inference" id="toc-identification-estimation-and-inference" class="nav-link active" data-scroll-target="#identification-estimation-and-inference"><span class="toc-section-number">5.1</span>  Identification, Estimation, and Inference</a></li>
  <li><a href="#conditional-expectation-and-linear-projection" id="toc-conditional-expectation-and-linear-projection" class="nav-link" data-scroll-target="#conditional-expectation-and-linear-projection"><span class="toc-section-number">5.2</span>  Conditional Expectation and Linear Projection</a></li>
  <li><a href="#structural-models-and-the-linear-model" id="toc-structural-models-and-the-linear-model" class="nav-link" data-scroll-target="#structural-models-and-the-linear-model"><span class="toc-section-number">5.3</span>  Structural Models and <em>The</em> Linear Model</a></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares"><span class="toc-section-number">5.4</span>  Ordinary Least Squares</a></li>
  <li><a href="#properties-of-ols" id="toc-properties-of-ols" class="nav-link" data-scroll-target="#properties-of-ols"><span class="toc-section-number">5.5</span>  Properties of OLS</a></li>
  <li><a href="#gauss-markov-theorem" id="toc-gauss-markov-theorem" class="nav-link" data-scroll-target="#gauss-markov-theorem"><span class="toc-section-number">5.6</span>  Gauss-Markov Theorem</a></li>
  <li><a href="#asymptotic-distribution-of-the-ols-estimator" id="toc-asymptotic-distribution-of-the-ols-estimator" class="nav-link" data-scroll-target="#asymptotic-distribution-of-the-ols-estimator"><span class="toc-section-number">5.7</span>  Asymptotic Distribution of the OLS Estimator</a></li>
  <li><a href="#estimating-textavarlefthatboldsymbolbeta_textols-right" id="toc-estimating-textavarlefthatboldsymbolbeta_textols-right" class="nav-link" data-scroll-target="#estimating-textavarlefthatboldsymbolbeta_textols-right"><span class="toc-section-number">5.8</span>  Estimating <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\)</span></a></li>
  <li><a href="#basic-model-selection-and-inference" id="toc-basic-model-selection-and-inference" class="nav-link" data-scroll-target="#basic-model-selection-and-inference"><span class="toc-section-number">5.9</span>  Basic Model Selection and Inference</a></li>
  <li><a href="#partialmarginal-effects-linear-projection-revisited" id="toc-partialmarginal-effects-linear-projection-revisited" class="nav-link" data-scroll-target="#partialmarginal-effects-linear-projection-revisited"><span class="toc-section-number">5.10</span>  Partial/Marginal Effects, Linear Projection Revisited</a></li>
  <li><a href="#frischwaughlovell-theorem" id="toc-frischwaughlovell-theorem" class="nav-link" data-scroll-target="#frischwaughlovell-theorem"><span class="toc-section-number">5.11</span>  Frisch–Waugh–Lovell Theorem</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap"><span class="toc-section-number">5.12</span>  Recap</a></li>
  <li><a href="#rep1" id="toc-rep1" class="nav-link" data-scroll-target="#rep1"><span class="toc-section-number">5.13</span>  Example/Replication</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="toc-section-number">5.14</span>  Further Reading</a></li>
  <li><a href="#sec-proj" id="toc-sec-proj" class="nav-link" data-scroll-target="#sec-proj"><span class="toc-section-number">5.15</span>  Math Appendix: Projection</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell" data-hash="ols_cache/html/unnamed-chunk-1_055196ca5917eef0faaf923b87efe41b">

</div>
<p>Now that we’re equipped with all the tools necessary to consider our first econometric model – the classic linear model.</p>
<section id="identification-estimation-and-inference" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="identification-estimation-and-inference"><span class="header-section-number">5.1</span> Identification, Estimation, and Inference</h2>
<p>Before we cover our first, and arguably most important, model in econometrics, it’s worth reiterating how the problems of identification, estimation, and inference are related. It’s also important to consider whether these problems occur in the context of a model/population, or a sample drawn from the model/population. <a href="https://scholar.harvard.edu/files/kasy/files/ia-causalityslides-iv.pdf">These slides</a> provide a nice diagram which provides some insight.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.5" data-hash="ols_cache/html/fig-plot51_4ef991aab28ea0e2cbffdd9560736225">
<div class="cell-output-display">
<div id="fig-plot51" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/identification.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.1: The relationship between models, parameters, and observed data.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>When we posit a model <span class="math inline">\(\mathcal P\)</span> (which is just a collection of probability distributions), the first thing we need to do is parameterize our model. <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (<a href="references.html#ref-angrist2008mostly" role="doc-biblioref">2008</a>)</span> refer to this as a “population first” approach in which we “define the objects [parameters] of interest before we can use data to study [estimate/make inferences] about them.” We then must address the question of identification – for a parameterization <span class="math inline">\(\boldsymbol{\theta}\mapsto P_\boldsymbol{\theta}\)</span>, does <span class="math inline">\(\boldsymbol{\theta}\)</span> uniquely determine <span class="math inline">\(P_\boldsymbol{\theta}\)</span>? If <span class="math inline">\(\boldsymbol{\theta}= \boldsymbol{\theta}'\)</span>, then is it necessarily the case that <span class="math inline">\(P_\boldsymbol{\theta}= P_{\boldsymbol{\theta}'}\)</span>? For this to be the case, we will usually need to amend our initial model by adding one or more assumptions. Once this is done, we can tackle the problem of estimation/inference given a sample drawn from the population knowing that once we’ve made a decision (whether that be an estimate or rejecting a null hypothesis) regarding the parameter space <span class="math inline">\(\boldsymbol\Theta\)</span>, that it will be equivalent to a decision about the model <span class="math inline">\(\mathcal P\)</span> via identification.</p>
<p>Another way to think about identification is in terms of some “perfect” estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span>. Imagine that you had an infinite amount of data such that it was guaranteed that <span class="math inline">\(\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}\)</span>. If <span class="math inline">\(\boldsymbol{\theta}\)</span> is not identified, then our perfect estimate <span class="math inline">\(\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}\)</span> could correspond to multiple <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span>, so it is impossible to know the which model value <span class="math inline">\(P_\boldsymbol{\theta}\)</span> generated the infinite amount of data which gave us our estimate. This speaks to how fundamental the problem of identification is. We usually like to focus on all the nice properties an estimator has, but even if that estimator checks all the boxes, it is meaningless if our parameters/model isn’t identified.</p>
<p>The term “identification” can sometimes be the cause of confusion because it appears in a wide array of contexts, and definitions of identification sometimes only apply to a specific model.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> For an <em>excellent</em> survey of identification in econometrics see <span class="citation" data-cites="lewbel2019identification">Lewbel (<a href="references.html#ref-lewbel2019identification" role="doc-biblioref">2019</a>)</span>.</p>
</section>
<section id="conditional-expectation-and-linear-projection" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="conditional-expectation-and-linear-projection"><span class="header-section-number">5.2</span> Conditional Expectation and Linear Projection</h2>
<p>We will begin with an example owing to <span class="citation" data-cites="galton1886regression">Galton (<a href="references.html#ref-galton1886regression" role="doc-biblioref">1886</a>)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 </strong></span>Suppose we are interested in how the height of two parents is related to their child’s height. Let <span class="math inline">\(X\)</span> be a random variable associated with the average height of two parents, and <span class="math inline">\(Y\)</span> be a random variable associated with the height of a child. Furthermore, assume the joint distribution of <span class="math inline">\((X,Y)\)</span> is: <span class="math display">\[\begin{align*}
(X,Y) &amp;\sim N(\boldsymbol \mu, \boldsymbol \Sigma),\\
\boldsymbol \mu &amp; = [68, 68]',\\
\boldsymbol \Sigma &amp; = \begin{bmatrix}8 &amp; 4\\ 4 &amp; 6\end{bmatrix}.
\end{align*}\]</span> As a consequence, <span class="math inline">\(X \sim N(68, 8)\)</span> and <span class="math inline">\(Y ~ N(68,6)\)</span> have the same marginal density of <span class="math inline">\(N(68, 8)\)</span>. In other words, the average height of individuals is the same across generations. The variance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are given as <span class="math inline">\(\sigma_X^2=\boldsymbol{\Sigma}_{11}\)</span> and <span class="math inline">\(\sigma_Y^2 = \boldsymbol{\Sigma}_{22}\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/fig-plot52_bc5fa702bca2bf398ab86fae0ac69339">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">68</span>, <span class="dv">68</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">key =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Parents"</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="st">"Child"</span>, <span class="dv">1000</span>))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">68</span>, <span class="fu">sqrt</span>(<span class="dv">8</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Height (in)"</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"density"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>), </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df, mu, Sigma)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y, <span class="at">z =</span> p)) <span class="sc">+</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>, <span class="at">y =</span> <span class="st">"Child's Height (in)"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot52" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot52-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.2: The marginal desnity of childs’ and parents’ height along with their joint density</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>If we want to predict a child’s height using parents’ height <span class="math inline">\(X = x\)</span>, we can inspect the conditional expectation <span class="math inline">\(\text{E}\left[Y\mid X = x\right]\)</span>. This expectation is given as</p>
<p><span class="math display">\[\begin{align*}
\text{E}\left[Y\mid X = x\right] &amp; = \int_{\mathcal Y} y\cdot f_{Y\mid x}(y\mid x)\ dy\\ &amp; = \int_{\mathcal Y} y\cdot \frac{f_{Y,X}(y \mid x, \boldsymbol \mu, \boldsymbol \Sigma)}{f_{X}(x \mid \mu_1, \sigma_{X})}\ dy\\
&amp; = \int_{\mathcal Y}y\cdot \frac{\exp\left(-\frac{1}{2}([x,y]' - \boldsymbol \mu)'\boldsymbol\Sigma^{-1}([x,y]' - \boldsymbol \mu)\right)/\sqrt{(2\pi)^k\det(\boldsymbol\Sigma)}}{\exp[-(x-\mu_X)^2/2\sigma^2]/\sqrt{2\pi\sigma_X^2}}\ dF_Y\\
&amp;\vdots\\
&amp; = \mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X)
\end{align*}\]</span> If we substitute in our parameters, and the calculated correlation coefficient of <span class="math display">\[ \rho = \frac{\text{Cov}\left(X,Y\right)}{\sigma_{X}\sigma_{Y}} = \frac{1}{4\sqrt 3} \approx 0.577,\]</span> we have <span class="math display">\[ \text{E}\left[Y\mid X = x\right] \approx 68 + \frac{\sqrt 6}{\sqrt 8}\cdot\frac{1}{4\sqrt 3}(x - 68) = 34 + \frac{1}{2}x.\]</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/fig-plot53_6fe70c142fd7e566097bdc6be8eed490">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>) )<span class="sc">%&gt;%</span> </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_y =</span> p<span class="sc">/</span><span class="fu">dnorm</span>(x, <span class="dv">68</span>, <span class="fu">sqrt</span>(<span class="dv">8</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(y, p_y, <span class="at">color =</span> <span class="fu">as.factor</span>(x))) <span class="sc">+</span> </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"Parents's Height"</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Childs's Height"</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Conditional Density"</span>) <span class="sc">+</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> (Sigma[<span class="dv">1</span>,<span class="dv">2</span>])<span class="sc">/</span>(<span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>]))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">E_y =</span> mu[<span class="dv">2</span>] <span class="sc">+</span> rho<span class="sc">*</span>(s2<span class="sc">/</span>s1)<span class="sc">*</span>(x <span class="sc">-</span> mu[<span class="dv">1</span>]))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> df2 <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,E_y)) <span class="sc">+</span> </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">"Conditional Expectation of Child's Height"</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot53" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot53-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.3: The density of a child’s height conditioning on their parent’s height</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The key observation is that the function <span class="math inline">\(\text{E}\left[Y \mid X = x\right]\)</span> is linear in <span class="math inline">\(x\)</span>! If we overlay the line associated with <span class="math inline">\(\text{E}\left[Y\mid X =x\right]\)</span> on the joint density of <span class="math inline">\((X,Y)\)</span> we end up with a figure emulating one in <span class="citation" data-cites="galton1886regression">Galton (<a href="references.html#ref-galton1886regression" role="doc-biblioref">1886</a>)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot54_c41a7d4369f173f80d1fdeade64a223f">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>), </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df, mu, Sigma)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y, <span class="at">z =</span> p)) <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">68</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">68</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>, <span class="at">y =</span> <span class="st">"Child's Height (in)"</span>) <span class="sc">+</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">34</span>, <span class="at">slope =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">label =</span> <span class="st">"E[Y|X = x]"</span>,<span class="at">x =</span> <span class="fl">75.5</span>, <span class="at">y =</span> <span class="dv">34</span> <span class="sc">+</span> <span class="dv">76</span><span class="sc">/</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">colour =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">label =</span> <span class="st">"X = x"</span>,<span class="at">x =</span> <span class="dv">75</span>, <span class="at">y =</span> <span class="dv">74</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">"blue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot54" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot54-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.4: The conditional expectation of a child’s height given their parent’s height in red, over the joint density of child and parent height. The red lines position in relation to the blue 45° line illustrates regression to the mean</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>On average, children with tall parents tend to be shorter than their parents. Conversely, children with short parents tend to be taller than their parents. In other words, as shown by the lines superimposed on <span class="math inline">\(f_{X,Y}\)</span>, <span class="math inline">\(\text{E}\left[Y\mid X = x\right] &lt; x\)</span> when <span class="math inline">\(x &gt; \text{E}\left[X\right]\)</span> and <span class="math inline">\(\text{E}\left[Y\mid X = x\right] &gt; x\)</span> when <span class="math inline">\(x &lt; \text{E}\left[X\right]\)</span>.</p>
</div>
<p>This phenomenon is known as <strong><em>regression to the mean</em></strong>, and it lends its name to the practice of relating <span class="math inline">\(Y\)</span> to <span class="math inline">\(X\)</span>. The ideas from <span class="citation" data-cites="galton1886regression">Galton (<a href="references.html#ref-galton1886regression" role="doc-biblioref">1886</a>)</span> were extended by one of Galton’s students in <span class="citation" data-cites="pearson1903laws">Pearson and Lee (<a href="references.html#ref-pearson1903laws" role="doc-biblioref">1903</a>)</span>, who actually uses the term “regression line” in reference to the function <span class="math inline">\(\text{E}\left[Y\mid X\right]\)</span>.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>It behooves one to acknowledge that Francis Galton’s motivation for studying height among parents and children stemmed from his interest in genetics and Darwinism. He was an early proponent of eugenics, and even coined the term “eugenics”. He referred to regression to the mean as “regression to mediocrity”, and believed this should be avoided by selective reproduction. Many of Galton’s beliefs are classic examples of scientific racism.</p>
</div>
<p>Let’s abstract from the example of height. Suppose we have <span class="math inline">\((Y, \mathbf X)\sim F_{Y,\mathbf{X}}\)</span> for some <strong><em>dependent variable</em></strong> <span class="math inline">\(Y\)</span> (with sample space <span class="math inline">\(\mathcal Y\)</span>) and a vector of <strong><em>independent variables/explanatory variables/covariates/regressors</em></strong> <span class="math inline">\(\mathbf{X}\)</span> (with sample sapce <span class="math inline">\(\mathcal X\)</span>). If we want to explore the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X}\)</span>, one measure of interest is the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathbf{X}\)</span>. If we know <span class="math inline">\(\mathbf{X}\)</span> takes on the value <span class="math inline">\(\mathbf{x}\)</span>, then on average, what is the value of <span class="math inline">\(Y\)</span>? <span class="math display">\[ \text{E}\left[Y\mid \mathbf{X}= \mathbf{x}\right] = \int_{\mathcal Y}y \ dF_{Y\mid \mathbf{x}} = \int_{\mathcal Y}y\cdot f_{Y\mid \mathbf{x}}(y\mid \mathbf{x}) \ dy = \int_{\mathcal Y}y\cdot\frac{f_{Y, \mathbf{X}}(y,\mathbf{x})}{f_{\mathbf{X}}(\mathbf{x})}\ dy\]</span> This conditional expectation maps values from the sample space of <span class="math inline">\(\mathbf{X}\)</span> to the sample space for <span class="math inline">\(Y\)</span>. In this sense, <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}= \mathbf{x}\right]\)</span> is a function mapping <span class="math inline">\(\mathcal X \mapsto \mathcal Y\)</span>. <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}= \mathbf{x}\right]\)</span> <em>is not</em> a function of <span class="math inline">\(y\)</span>, as it is calculated via integrating over all values of <span class="math inline">\(y\in\mathcal Y\)</span>. Following <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (<a href="references.html#ref-angrist2008mostly" role="doc-biblioref">2008</a>)</span> and <span class="citation" data-cites="hansen2022econometrics">Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span>, we name this function.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 </strong></span>If <span class="math inline">\((Y, \mathbf X)\sim F_{Y,\mathbf{X}}\)</span>, then the <span style="color:red"><strong><em>conditional expectation function (CEF)</em></strong></span> <span class="math inline">\(\hat Y:\mathcal X \to \mathcal Y\)</span> is defined as <span class="math display">\[\hat Y(X) = \text{E}\left[Y\mid \mathbf{X}\right]. \]</span> The CEF is an expectation, so observations of <span class="math inline">\(Y\)</span> are bound to deviate from it. We will define this deviation as the <span style="color:red"><strong><em>CEF error</em></strong></span> <span class="math inline">\(\varepsilon_{c} = Y - \hat Y(X)\)</span>.</p>
</div>
<p>In the height example, <span class="math inline">\(\varepsilon_{c}\)</span> captured how much a child’s height differed from the trend given by <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}\right]\)</span>.</p>
<div id="prp-ceferr" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.1 (Properties of CEF Error) </strong></span>The CEF error is:</p>
<ol type="1">
<li>Mean independent of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\text{E}\left[\varepsilon_{c}\mid\mathbf{X}\right] = 0\)</span>;</li>
<li>Uncorrelated with <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\text{E}\left[\varepsilon_{c}\mathbf{X}\right] = \mathbf{0}\)</span>;</li>
<li>Uncorrelated with any function <span class="math inline">\(h(\mathbf{X})\)</span>, <span class="math inline">\(\text{E}\left[\varepsilon_{c} h(\mathbf{X})\right] = 0\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\text{E}\left[\varepsilon_{c}\mid\mathbf{X}\right] &amp;= \text{E}\left[Y - \hat Y(X)\mid \mathbf{X}\right]\\
&amp; =  \text{E}\left[Y - \text{E}\left[Y\mid \mathbf{X}\right] \mid \mathbf{X}\right] \\
&amp; = \text{E}\left[Y\mid \mathbf{X}\right] - \text{E}\left[\text{E}\left[Y\mid \mathbf{X}\right] \mid \mathbf{X}\right] &amp; (\text{E}\left[\cdot | \mathbf{X}\right]\text{ linear})\\
&amp; = \text{E}\left[Y\mid \mathbf{X}\right] - \text{E}\left[Y\mid \mathbf{X}\right] &amp; (\text{Law of Iterated Expectations})\\
&amp; = 0.
\end{align*}\]</span> <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\varepsilon_{c}\)</span> being uncorrelated is a consequence of mean independence. For some <span class="math inline">\(h(\mathbf{X})\)</span>, we have <span class="math display">\[\begin{align*}
\text{E}\left[\varepsilon_{c} h(\mathbf{X})\right] &amp; = \text{E}\left[\varepsilon_{c} h(\mathbf{X}) \text{E}\left[\varepsilon_{c}\mid\mathbf{X}\right]\right] &amp; (\text{Law of Iterated Expectations})\\
&amp; = \text{E}\left[\varepsilon_{c} h(\mathbf{X})\cdot 0\right] &amp; (\text{E}\left[\varepsilon_{c}\mid\mathbf{X}\right] = 0)\\
&amp; = \text{E}\left[0\right]\\
&amp; = 0.
\end{align*}\]</span> <span style="color:white">space</span></p>
</div>
<p>We <em>are not</em> assuming that <span class="math inline">\(\text{E}\left[\varepsilon_{c}\mid\mathbf{X}\right] = 0\)</span>. This equality holds by the definition of the CEF.</p>
<p>So why restrict our attention to the CEF? Perhaps there are other functions <span class="math inline">\(g:\mathcal X\to\mathcal Y\)</span> which is better at predicted <span class="math inline">\(Y\)</span> than <span class="math inline">\(\hat Y(\mathbf{X})\)</span>. It turns out that <span class="math inline">\(\hat Y(\mathbf{X})\)</span> is the function which minimizes the MSE which arises from predicted <span class="math inline">\(Y\)</span>.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.2 (CEF Minimizes MSE) </strong></span>For some arbitrary <span class="math inline">\(g:\mathcal X\to\mathcal Y\)</span>, <span class="math display">\[ \hat Y(\mathbf{X}) = \text{E}\left[Y\mid \mathbf{X}\right] = \mathop{\mathrm{argmin}}_{g}\text{E}\left[(Y-g(\mathbf{X}))^2\right].\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\text{E}\left[(Y-g(\mathbf{X}))^2\right] &amp; = \text{E}\left[(Y-g(\mathbf{X}) + 0)^2\right]\\
&amp; = \text{E}\left[(Y-g(\mathbf{X}) + (\text{E}\left[Y\mid \mathbf{X}\right] - \text{E}\left[Y\mid \mathbf{X}\right]))^2\right] &amp; (0 = \text{E}\left[Y\mid \mathbf{X}\right] - \text{E}\left[Y\mid \mathbf{X}\right])\\
&amp; = \text{E}\left[(Y - \text{E}\left[Y\mid \mathbf{X}\right]) + (\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X})))^2\right]\\
&amp; = \text{E}\left[(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2 + 2(Y - \text{E}\left[Y\mid \mathbf{X}\right])(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X})) + (\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right]\\
&amp; = \text{E}\left[(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2\right] + 2\text{E}\left[(Y - \text{E}\left[Y\mid \mathbf{X}\right])(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X})) \right] + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - h(\mathbf{X}))^2\right]\\
&amp; = E{(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2} + 2\text{E}\left[\varepsilon_{c}(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X})) \right] + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right] &amp; (\varepsilon_{c} = Y - \text{E}\left[Y\mid \mathbf{X}\right])
\end{align*}\]</span> If we define <span class="math inline">\(h(\mathbf{X}) = \text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X})\)</span>, we can use the fact that <span class="math inline">\(\varepsilon_{c}\)</span> is uncorrelated with any function of <span class="math inline">\(\mathbf{X}\)</span>. <span class="math display">\[\begin{align*}
\text{E}\left[(Y-g(\mathbf{X}))^2\right] &amp; = E{(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2} + 2\text{E}\left[\varepsilon_{c} h(\mathbf{X})\right] + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right]\\
\text{E}\left[(Y-g(\mathbf{X}))^2\right] &amp; = E{(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2} + 2\cdot 0 + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right] &amp; (\text{E}\left[\varepsilon_{c} h(\mathbf{X})\right] = 0)\\
\text{E}\left[(Y-g(\mathbf{X}))^2\right] &amp; = E{(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2} + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right]
\end{align*}\]</span> Only the second term includes the variable we are minimizing over, so <span class="math display">\[ \mathop{\mathrm{argmin}}_{h}\text{E}\left[(Y-g(\mathbf{X}))^2\right] =\mathop{\mathrm{argmin}}_{h}\left\{E{(Y - \text{E}\left[Y\mid \mathbf{X}\right])^2} + \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right]\right\} = \mathop{\mathrm{argmin}}_h \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right],\]</span> where <span class="math inline">\(\text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right] \ge 0\)</span> because we are squaring a quantity. If we take <span class="math inline">\(g(\mathbf{X}) = \hat Y(X) = \text{E}\left[Y\mid \mathbf{X}\right]\)</span>, we have <span class="math display">\[ \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - g(\mathbf{X}))^2\right] = \text{E}\left[(\text{E}\left[Y\mid \mathbf{X}\right] - \text{E}\left[Y\mid \mathbf{X}\right])^2\right] = \text{E}\left[0\right]=0.\]</span> Therefore the CEF does minimize the MSE in question.</p>
</div>
<p>This results makes the CEF the optimal candidate for predicting <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathbf{X}= \mathbf{x}\)</span> in a decision theoretic sense (taking the loss function to be quadratic), but in practice we don’t actually know the precise form of the CEF. When <span class="math inline">\((X,Y) \sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span>, we saw the CEF is linear, but this needn’t be the case.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 </strong></span>Define the following density on the sample space <span class="math inline">\(\mathcal X\times \mathcal Y = [0,2]\times[2,4]\)</span>:</p>
<p><span class="math display">\[f_{X,Y}(x,y)=\frac{1}{8}(6-x-y)\]</span> The marginal density of <span class="math inline">\(X\)</span> is <span class="math display">\[ f_X(x) = \int_{\mathcal Y}f_{X,Y}(x,y)\ dy = \int_2^4\frac{1}{8}(6-x-y)\ dy = \frac{1}{8}(6-2x),\]</span> and the conditional density of <span class="math inline">\(Y\mid X = x\)</span> is <span class="math display">\[ f_{Y\mid x}(y\mid x) = \frac{f_{X,Y}(x,y)}{f_{X}(x)} = \frac{\frac{1}{8}(6-x-y)}{\frac{1}{8}(6-2x)} = \frac{6-x-y}{6-2x}\]</span> Using this to calculate the expectation <span class="math inline">\(\text{E}\left[Y \mid X = x\right]\)</span> gives <span class="math display">\[\text{E}\left[Y \mid X = x\right] = \int_{\mathcal Y} y\cdot f_{Y\mid x}(y\mid x)\ dy= \int_2^4 y\frac{6-x-y}{6-2x}\ dy = \frac{26-9x}{9-3x}.\]</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot55_8eb8f5a9910560a2117b01ef815bfde0">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">1000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> (<span class="sc">-</span><span class="dv">9</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">26</span>)<span class="sc">/</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">9</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"x"</span>, <span class="at">y =</span> <span class="st">"Conditional Expectation of Y given x"</span>) <span class="sc">+</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot55" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot55-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.5: A nonlinear CEF function</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>In order to give the CEF some form, we will approximate it with a linear function (which may hold for certain <span class="math inline">\(f_{\mathbf{X},Y}\)</span>): <span class="math display">\[\text{E}\left[Y\mid \mathbf{X}\right] =  \mathbf{X}\boldsymbol{\beta}.\]</span> Henceforth, we will assume that <span class="math inline">\(\mathbf{X}\)</span> includes a column of 1’s such that <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> includes an intercept term. We will take the coefficient <span class="math inline">\(\boldsymbol{\beta}\)</span> to be that which gives the best linear prediction of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mathbf{X}\)</span>. <span class="math display">\[ \boldsymbol{\beta}= \mathop{\mathrm{argmin}}_{\mathbf{b}}\text{E}\left[(Y-\mathbf{X}\mathbf b)^2\right]\]</span> The error associated with this projection is the <span style="color:red"><strong><em>linear projection error</em></strong></span>, <span class="math inline">\(\varepsilon_{\ell} = Y-\mathbf{X}\mathbf b\)</span>. The linear projection error is not the same as the conditional expectation error. It is only the case that <span class="math inline">\(\varepsilon_{\ell} = \varepsilon_c\)</span> if the CEF is truly linear.</p>
<p>Taking the definition of <span class="math inline">\(\boldsymbol{\beta}\)</span> to be a parameterization, we can define our first model. We’ll follow the naming convention of <span class="citation" data-cites="hansen2022econometrics">Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span>.</p>
<div id="def-cefdef" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 </strong></span>The <span style="color:red"><strong><em>linear projection (CEF) model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{LP} = \{P_\boldsymbol{\beta}\mid \boldsymbol{\beta}\in \mathbb R^{k+1}\}\)</span>, where <span class="math display">\[\begin{align*}
P_\boldsymbol{\beta}&amp;= \{F_{\mathbf{X},Y} \mid \text{E}\left[Y\mid \mathbf{X}\right]= \mathbf{X}\boldsymbol{\beta}\},\\
\boldsymbol{\beta}&amp;= \mathop{\mathrm{argmin}}_{\mathbf{b}}\text{E}\left[(Y-\mathbf{X}\mathbf b)^2\right],\\
\mathbf{X}&amp; = (1, X_2, \ldots, X_K).
\end{align*}\]</span></p>
</div>
<p>This model is not regular, as each element <span class="math inline">\(P_\boldsymbol{\beta}\)</span> is itself a collection of distributions. As the following example highlights, it won’t be possible to identify the underlying <span class="math inline">\(F_{\mathbf{X},Y}\)</span>, only <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}\right]\)</span>. Consequently, each element of <span class="math inline">\(\mathcal P_\text{LP}\)</span> is a collection of distributions with a common <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}\right]\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (Exercise in Identification) </strong></span>Suppose <span class="math inline">\((X,Y)\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span>. In this case, the CEF is actually linear and given as <span class="math display">\[\text{E}\left[Y\mid X\right] = \mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X).\]</span> We can define <em>many</em> normal distributions with a common CEF. For example, if we have <span class="math inline">\(\rho = \mu_x = \mu_y = \rho' = \mu_x' = \mu_y' = 1\)</span>, <span class="math inline">\(\sigma_X = 1\)</span>, <span class="math inline">\(\sigma_Y = 2\)</span>, <span class="math inline">\(\sigma_X' = 2\)</span>, and <span class="math inline">\(\sigma_Y' = 4\)</span>, then <span class="math display">\[\mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X) = \mu_Y' + \frac{\sigma_{Y}'}{\sigma_{X}'}\rho'(x - \mu_X') = \frac{1}{2} +x.\]</span> This means that <span class="math inline">\(\boldsymbol{\beta}= (1/2,1)\)</span> for both distributions. This problem is reminiscent of systems of equations in that we have so many more variables than equations, that there are infinite possibilities (<strong><em>remember this</em></strong>). This is also just the tip of the iceberg when considering how many elements are included in <span class="math inline">\(P_{(1/2,1)}\)</span>. It isn’t just all the Gaussian distributions where the CEF is <span class="math inline">\(\frac{1}{2} +x\)</span>. It isn’t all the distributions with a linear CEF which is <span class="math inline">\(\frac{1}{2} +x\)</span>. It is all the distributions for which the best linear approximation of the CEF is <span class="math inline">\(\frac{1}{2} +x\)</span> (which happens to include the previous groups). Is this an issue? Well not really. We aren’t concerned with the joint distribution <span class="math inline">\(F_{\mathbf{X},Y}\)</span>, as the only thing with any bearing on prediction here is <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}\right]\)</span> (we made no assumptions about <span class="math inline">\(F_{\mathbf{X},Y}\)</span> when proving that the CEF minimizes MSE). In the event we did want to identify <span class="math inline">\(F_{\mathbf{X},Y}\)</span>, we would need to impose additional assumptions on <span class="math inline">\(\mathcal P_\text{LP}\)</span>. Consider the following assumptions:</p>
<ol type="1">
<li><span class="math inline">\((X,Y)\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span></li>
<li><span class="math inline">\(\mu_X = 0\)</span></li>
<li><span class="math inline">\(\sigma_X = \sigma_Y = 1\)</span></li>
</ol>
<p>In this case, <span class="math display">\[\text{E}\left[Y\mid X\right] = \mu_Y + \rho x = \mu_Y + \frac{\text{Cov}\left(X,Y\right)}{\underbrace{\sigma_X \sigma_Y}_{1\cdot 1}}x = \underbrace{\mu_Y}_{\beta_1} +\underbrace{\text{Cov}\left(X,Y\right)}_{\beta_2}x,\]</span> Assuming <span class="math inline">\((X,Y)\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span>, <span class="math inline">\(\mu_X = 0\)</span>, and <span class="math inline">\(\sigma_X = \sigma_Y = 1\)</span>, it must be the case that</p>
<p><span class="math display">\[\begin{align*}
(X,Y) &amp;\sim N(\boldsymbol \mu, \boldsymbol \Sigma),\\
\boldsymbol \mu &amp; = [0, \beta_1]',\\
\boldsymbol \Sigma &amp; = \begin{bmatrix}1 &amp; \beta_2\\ \beta_2 &amp; 1\end{bmatrix}.
\end{align*}\]</span></p>
</div>
<p>Now we can turn to the question of identifying <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.3 </strong></span>If <span class="math inline">\(\text{E}\left[\mathbf{X}\mathbf{X}'\right]\)</span> is invertible, then the parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified in the linear projection (CEF) model.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We must show that <span class="math inline">\(\boldsymbol{\beta}= \boldsymbol{\beta}'\)</span>, then is it necessarily the case that <span class="math inline">\(P_\boldsymbol{\beta}= P_{\boldsymbol{\beta}'}\)</span>. By the definition of the linear projection (CEF) model, it suffices to show that <span class="math inline">\(\mathop{\mathrm{argmin}}_{\mathbf{b}}\text{E}\left[(Y-\mathbf{X}\mathbf b)^2\right]\)</span> has a solution, and that the solution is unique. <span class="math display">\[\begin{align*}
\boldsymbol{\beta}&amp;= \mathop{\mathrm{argmin}}_{\mathbf{b}}\text{E}\left[(Y-\mathbf{X}\mathbf b)^2\right]\\
&amp; = \mathop{\mathrm{argmin}}_{\mathbf{b}}\text{E}\left[Y^2 + 2\mathbf b(\mathbf{X}'Y) + (\mathbf{X}\mathbf b)'(\mathbf{X}\mathbf b)\right]\\
&amp; = \mathop{\mathrm{argmin}}_{\mathbf{b}} \left\{\text{E}\left[Y^2\right] + 2\mathbf b\text{E}\left[\mathbf{X}'Y\right] + 2\mathbf b'\text{E}\left[\mathbf{X}'\mathbf{X}\right]\mathbf b)\right\}
\end{align*}\]</span> The first order condition associated with this problem is <span class="math display">\[ 2\text{E}\left[\mathbf{X}'Y\right] + 2\text{E}\left[\mathbf{X}'\mathbf{X}\right] \boldsymbol{\beta}= \mathbf{0}.\]</span> This is equivalent to <span class="math display">\[\text{E}\left[\mathbf{X}'Y\right] = \text{E}\left[\mathbf{X}'\mathbf{X}\right] \boldsymbol{\beta}.\]</span> We now use the assumption that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible to solve for a unique solution for <span class="math inline">\(\boldsymbol{\beta}\)</span>: <span class="math display">\[ \boldsymbol{\beta}= \left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right].\]</span> Therefore, <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified.</p>
</div>
<p>To illustrate how the assumption that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible leads to identification, consider what happens when it does not hold. In <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is not invertible, than <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> does not have full rank and has infinite solutions. Suppose we have: <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{X}'Y\right] &amp; = \begin{bmatrix} 1 \\ 1 \end{bmatrix}.\\
\text{E}\left[\mathbf{X}'\mathbf{X}\right] &amp; = \begin{bmatrix} 1 &amp; 1\\ 0 &amp; 0 \end{bmatrix}.
\end{align*}\]</span> In this case <span class="math display">\[\begin{align*}
&amp;\text{E}\left[\mathbf{X}'Y\right] = \text{E}\left[\mathbf{X}'\mathbf{X}\right] \boldsymbol{\beta}\\
\implies &amp;\begin{bmatrix} 1 &amp; 1\\ 0 &amp; 0 \end{bmatrix}\begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\\
\implies &amp; \beta_1 + \beta_2 = 1.
\end{align*}\]</span> This final equation has an infinite number of solutions. If two of those solutions happen to be <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\boldsymbol{\beta}'\)</span>, then <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} = P_{\boldsymbol{\beta}'}\)</span> despite <span class="math inline">\(\boldsymbol{\beta}\neq\boldsymbol{\beta}'\)</span>.</p>
</section>
<section id="structural-models-and-the-linear-model" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="structural-models-and-the-linear-model"><span class="header-section-number">5.3</span> Structural Models and <em>The</em> Linear Model</h2>
<p>The CEF approach to regression aims to describe a characteristic of the joint density <span class="math inline">\(f_{Y,\mathbf{X}}\)</span>. It captures an association between variables, but not a causal link. Econometricians are often concerned with causal links to inform economic policy, something which differentiates econometrics from statistics. This is why the approach to linear regression seen in standard econometrics sources such as <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="hayashi2011econometrics">Hayashi (<a href="references.html#ref-hayashi2011econometrics" role="doc-biblioref">2011</a>)</span>, <span class="citation" data-cites="wooldridge2015introductory">Wooldridge (<a href="references.html#ref-wooldridge2015introductory" role="doc-biblioref">2015</a>)</span>, and <span class="citation" data-cites="stock2003introduction">Stock and Watson (<a href="references.html#ref-stock2003introduction" role="doc-biblioref">2003</a>)</span> take a <strong><em>structural</em></strong> approach to linear regression. Before giving a heuristic definition of “structural”, let’s consider an example due to <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span> and inspired <span class="citation" data-cites="cobb1928theory">Cobb and Douglas (<a href="references.html#ref-cobb1928theory" role="doc-biblioref">1928</a>)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 </strong></span>Assume a firms output <span class="math inline">\(Y\)</span> is related to labor input <span class="math inline">\(L\)</span> and capital input <span class="math inline">\(K\)</span> according to <span class="math display">\[Q = AL^{\beta}K^{\alpha}.\]</span> The total factor of productivity is <span class="math inline">\(A\)</span>, while <span class="math inline">\(L\)</span> and <span class="math inline">\(K\)</span> are the elasticity of output with respect to labor and capital, respectively. The production function can be written as <span class="math display">\[ \log Q = \log A + \beta \log L + \alpha \log K.\]</span> Now consider the linear regression: <span class="math display">\[ \log Q = \log A + \beta \log L + \alpha \log K + \varepsilon\]</span> where <span class="math inline">\(\varepsilon\)</span> is an error addressing the fact that the linear relationship may not hold perfectly. In this case are <span class="math inline">\((\log A, \alpha, \beta)\)</span> associated with the best linear projection of <span class="math inline">\(\log Q\)</span> onto <span class="math inline">\(\log L\)</span> and <span class="math inline">\(\alpha \log K\)</span>, or do they correspond to the factor of productivity, and elasticities of output? If the latter is the case, then what does <span class="math inline">\(\varepsilon\)</span> represent in the context of the deterministic theoretical relationship <span class="math inline">\(Q = AL^{\beta}K^{\alpha}\)</span>? It will turn out that for the coefficients of the best linear projection to coincide with the economic interpretation from <span class="math inline">\(Q = AL^{\beta}K^{\alpha}\)</span>, we will need to impose assumptions about <span class="math inline">\(\varepsilon\)</span>, a step that is one of the defining features of econometrics.</p>
</div>
<p>The key difference between this example and the height example from <span class="citation" data-cites="galton1886regression">Galton (<a href="references.html#ref-galton1886regression" role="doc-biblioref">1886</a>)</span>’s is that we are now trying to root our linear regression in structure provided by economic theory/intuition, as to enable us to make economic conclusions. Philip Haile distinguishes these approaches in an excellent set of <a href="http://www.princeton.edu/~reddings/tradephd/Haile_theorymeas.pdf">slides</a>. He would classify <span class="citation" data-cites="galton1886regression">Galton (<a href="references.html#ref-galton1886regression" role="doc-biblioref">1886</a>)</span>’s work as descriptive as it “estimates relationships between observables”. This is opposed to a structural approach which “estimates features of a data generating process (i.e, a model) that are (assumed to be) invariant to the policy changes or other counterfactuals of interest.” This difference is also linked to the error in the linear regression, <span class="math inline">\(\varepsilon\)</span>. As put by <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span>:</p>
<blockquote class="blockquote">
<p>Where did the error term in the empirical model come from? The answer to this question is critical because it affects whether… the parameters of the Cobb–Douglas production function [are identified], as opposed to the parameters of the best linear predictor of the logarithm of output given the logarithms of the two inputs [being identified]. In other words, it is the combination of an economic assumption (production is truly Cobb–Douglas) and statistical assumptions (<span class="math inline">\(\varepsilon\)</span> satisfies certain moment conditions) that distinguishes a structural model from a descriptive one.</p>
</blockquote>
<p>In an effort to beat a dead horse, a final definition of a structural model is due to <span class="citation" data-cites="goldberger1972structural">Goldberger (<a href="references.html#ref-goldberger1972structural" role="doc-biblioref">1972</a>)</span>, who simply puts “By structural equation models, I refer to stochastic models in which each equation represents a causal link, rather than a mere empirical association.” None of this is to say that descriptive model is not useful. Just like descriptive statistics give insight into data, a descriptive model (such as the linear projection model) is an excellent way to investigate data, and findings may inform the development of a structural model.</p>
<p>Let’s now reintroduce linear regression from a structural perspective. We will do so with no assumptions about our model, and amend our definition as we determine which assumptions are required for identification and desirable properties of estimators. The goal of this approach is to emphasize that the assumptions associated with an econometric model aren’t set in stone from the onset. You begin with little to no assumptions, and then determine which assumptions are necessary as you analyze the model and accompanying estimators.</p>
<p>We have a vector of <span class="math inline">\(K\)</span> regressors <span class="math inline">\(\mathbf{X}= [X_1,\ldots,X_K]\)</span> (assuming <span class="math inline">\(X_1 = 1\)</span> to allow for an intercept), structural parameters <span class="math inline">\(\boldsymbol{\beta}= [\beta_1,\ldots,\beta_n]'\)</span> , and some structural error term <span class="math inline">\(\varepsilon\)</span>. The density underlying the model is the joint density between regressors and the error <span class="math inline">\(f_{\mathbf{X},\varepsilon}\)</span>. The independent variable <span class="math inline">\(Y\)</span> is given as <span class="math display">\[ Y = \mathbf{X}\boldsymbol{\beta}+ \varepsilon.\]</span> The major difference between this and the linear projection model is the underlying density for the latter is <span class="math inline">\(f_{\mathbf{X},Y}\)</span> where <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\varepsilon\)</span> are defined using this density. Now we’re determining <span class="math inline">\(Y\)</span> via some structural parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> and the density <span class="math inline">\(f_{\mathbf{X},\varepsilon}\)</span>. There are many situations in which the realizations of <span class="math inline">\(\varepsilon\)</span> may not be identically, or independently, distributed, so we need to consider the joint density of <span class="math inline">\(\boldsymbol{\varepsilon}= (\varepsilon_1, \ldots, \varepsilon_n)\)</span> where our sample is size <span class="math inline">\(n\)</span>. This joint density is <span class="math inline">\(f_{\boldsymbol{\varepsilon}}\)</span>. The underlying density from which we draw regressors and errors <em>is not</em> <span class="math inline">\(f_{\mathbf{X},\boldsymbol{\varepsilon}}\)</span>, as a realization from this distribution would be comprised of <span class="math inline">\(K\)</span> regressors and <span class="math inline">\(n\)</span> errors. What we need is the joint density of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> and <span class="math inline">\(n\)</span> observations of <span class="math inline">\(\mathbf{X}\)</span>, so we need to consider the following random matrix: <span class="math display">\[\mathbb{X}= \begin{bmatrix}\mathbf{X}_1 \\ \vdots \\ \mathbf{X}_i \\ \vdots\\ \mathbf{X}_n\end{bmatrix}\]</span> A sample of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(K\)</span> regressors <span class="math inline">\(\mathbf{X}\)</span> and errors <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> is a single realization drawn from the density <span class="math inline">\(f_{\mathbb{X},\boldsymbol{\varepsilon}}\)</span>.</p>
<p><span class="math display">\[ \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}= \begin{bmatrix}  \mathbf{X}_1\boldsymbol{\beta}+ \varepsilon_1 \\ \vdots \\ \mathbf{X}_i\boldsymbol{\beta}+ \varepsilon_i \\ \vdots \\  \mathbf{X}_n\boldsymbol{\beta}+ \varepsilon_n  \end{bmatrix} = \begin{bmatrix}  \beta_0 + \beta_1X_{21}  + \cdots +\beta_KX_{K1}+ \varepsilon_1 \\ \vdots \\ \beta_0 + \beta_1X_{2i}  + \cdots +\beta_KX_{Ki}+ \varepsilon_i  \\ \vdots \\  \beta_0 + \beta_1X_{2n}  + \cdots +\beta_KX_{Kn}+ \varepsilon_n  \end{bmatrix}\]</span> We could also write <span class="math inline">\(\mathbb{X}\)</span> as <span class="math inline">\(K\)</span> column vectors of length <span class="math inline">\(n\)</span>, each corresponding to the <span class="math inline">\(n\)</span> observations of each regressor. <span class="math display">\[\mathbb{X}= \begin{bmatrix}\mathbf{X}_1 &amp; \cdots &amp; \mathbf{X}_j &amp; \cdots&amp; \mathbf{X}_K\end{bmatrix}.\]</span> To distinguish between <span class="math inline">\(\mathbf{X}_i\)</span> (one observation of <span class="math inline">\(K\)</span> regressors) and <span class="math inline">\(\mathbf{X}_j\)</span> (<span class="math inline">\(n\)</span> observations of one regressor), we will use the indices <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, respectively.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We will assume that our data is the result of a random sample of observations of regressors <span class="math inline">\(\mathbf{X}_i\)</span>: <span class="math display">\[ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i}.\]</span> The random sample assumption is essential as it will allow us to apply the LLN and CLT. Finally, we introduce a parameter which dictates the variance of the error <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>. This will be the PSD matrix <span class="math inline">\(\boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbb{X}\right)\)</span>. Now we can define the linear model in the absence of assumptions.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 </strong></span>The <span style="color:red"><strong><em>linear model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{LM} = \{P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \mid \boldsymbol{\beta}\in \mathbb R^{K},\ \boldsymbol{\Sigma}\in \mathbb R^{n\times n} \}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} &amp;= \{F_{\mathbb{X},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},\ \boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbb{X}\right),\ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i} \},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n],\\
\boldsymbol{\varepsilon}&amp; = [\varepsilon_1, \ldots, \varepsilon_n]\\
\end{align*}\]</span></p>
</div>
<p>When discussing a model <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}}\in\mathcal P_\text{LM}\)</span>, <strong>we are implicitly assuming that the specification of the model is correct, and regressors are IID</strong>. If the model were not linear than <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}}\notin\mathcal P_\text{LM}\)</span>, which is not our focus at the moment, but is a legitimate concern.</p>
<p>In the context of a structural linear model, <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> is not simply an approximation error. In introduces a stochastic element to a deterministic economic model. <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span> enumerate four ways that this randomness can be introduced. We will explore these in the context of the Cobb-Douglas production model where <span class="math inline">\(\log Q_i = \log A + \beta \log L_i + \alpha \log K_i + \varepsilon\)</span> for an observation from firm <span class="math inline">\(i\)</span>.</p>
<ol type="1">
<li>We may be uncertain about the economic environment at hand.</li>
<li>Agent uncertainty about the economic environment;</li>
<li>Optimization errors on the part of economic agents;</li>
<li>Measurement errors in observed variables.</li>
</ol>
<div id="exm-car" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 </strong></span>Suppose an agent is deciding between purchasing two cars (<span class="math inline">\(j=1,2\)</span>) has a linear utility function <span class="math inline">\(u_{ij} = \mathbf{X}_{ij}\boldsymbol{\beta}\)</span>. The vector <span class="math inline">\(\mathbf{x}_{ij}\)</span> are attributes of car <span class="math inline">\(j\)</span> (size, mpg, make, model, etc.). We do observe their choice of vehicle <span class="math inline">\(y_i\)</span>, but cannot observe their utility from the respective vehicles. Assuming agents maximize their utility, then their choice can be defined as <span class="math display">\[y_i = \begin{cases}\text{car }1&amp; u_{i1} \ge u_{i2}\\ \text{car }2&amp; u_{i2} &gt; u_{i1} \end{cases}.\]</span> How would we incorporate <span class="math inline">\(\varepsilon\)</span> into our model? In the linear model the error directly affects the dependent variable, but in this case the (presumable) dependent variable <span class="math inline">\(y_i\)</span> is an indicator. It doesn’t make sense to add a stochastic element to it, as we observe a customer’s choice with no uncertainty.</p>
<ol type="1">
<li>People are inherently heterogeneous in the utility they receive from any product. One agent may live in a city with access to public transit and would not gain much utility from a car, while another agent may live in a rural area and rely on a car to commute to work and run errands. The error term <span class="math inline">\(\varepsilon_i\)</span> could correct for these differences. It also could be the case that the error is specific to a consumer <em>and</em> a particular vehicle <span class="math inline">\(j\)</span>. Maybe consumer <span class="math inline">\(i\)</span>’s is particularly loyal to the manufacturer of car <span class="math inline">\(j\)</span> and they receive more utility as a result. This could be captured with an error <span class="math inline">\(\varepsilon_{ij}\)</span>.</li>
<li>An agent may be not have the opportunity to test drive each car before purchasing, so their is some uncertainty as to how much utility they would receive from purchasing it. This uncertainty could be incorporated via <span class="math inline">\(\varepsilon_{ij}\)</span>.</li>
<li>An agent may not be perfectly rational and could make a mistake while attempting to maximize their utility. They could purchase car <span class="math inline">\(j=2\)</span> despite the fact that <span class="math inline">\(u_{i1} \ge u_{i2}\)</span>. To correct for this miscalculation, we could include an error <span class="math inline">\(\varepsilon_{ij}\)</span> such that <span class="math inline">\(u_{i1} + \varepsilon_{i1} \le u_{i2} + \varepsilon_{i2}\)</span></li>
<li>We may not be able to perfectly measure all the variables in the model. If one of the attributes in the vector <span class="math inline">\(\mathbf{x}_{ij}\)</span> is price, but we only observe MSRP, then we aren’t accounting for the fact that some customers may have purchased their car for a lower price (it could be used, or on sale). This measurement error can be accounted for with <span class="math inline">\(\varepsilon_{ij}\)</span></li>
</ol>
<p>It’s important to notice how the error term is indexed in each example. Sometimes the error arises because of the agent <span class="math inline">\(i\)</span> (<span class="math inline">\(\varepsilon_i\)</span>), or the agent and the specific car (<span class="math inline">\(\varepsilon_{ij}\)</span>). There could also be ways to incorporate an error that is specific to each car, but not agents (<span class="math inline">\(\varepsilon_j\)</span>). Later on in Section @ref(binary-choice) we will talk about how to estimate models like one.</p>
</div>
<p>The precise interpretation of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> is key if we want to justify the statistical assumptions about <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> which <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span> cite as a key player in identification.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>Another classical assumption of linear regression that we have explicitly violated is that <span class="math inline">\(\mathbb{X}\)</span> is a matrix of constants. In certain settings researchers are able to determine the values before collecting data. For instance, in a laboratory setting you may have enough control over the (observed/sampled) regressors as to be able to record the value of <span class="math inline">\(\mathbf{Y}\)</span> at predetermined realizations of <span class="math inline">\(\mathbb{X}\)</span>. This is rarely the case in social sciences, the realm in which econometrics exists. For this reason, we treat <span class="math inline">\(\mathbb{X}\)</span> as random, and the case of fixed regressors as a special case. In practice, this means we need to condition on <span class="math inline">\(\mathbb{X}\)</span> when considering expectations and variances of quantities related to <span class="math inline">\(\mathbb{X}\)</span>.</p>
</div>
<p>Is it still the case that <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified when <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible? It turns out that we will need an additional assumption that we got “for free” with the CEF model via Proposition @ref(prp:ceferr), that being that <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are uncorrelated.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 </strong></span>The covariates <span class="math inline">\(\mathbf{X}\)</span> are <span style="color:red"><strong><em>weakly exogenous</em></strong></span> if <span class="math inline">\(\text{E}\left[X_i\varepsilon_i\right] = 0\)</span>. In matrix form this is <span class="math display">\[\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}.\]</span> Equivalently,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math inline">\(\mathbf{X}\)</span> is weakly exogenous if: <span class="math display">\[\begin{align*}
\text{E}\left[\boldsymbol{\varepsilon}\right] &amp;= \mathbf{0}\\
\text{Cov}\left(\mathbf{X},\varepsilon_i\right)&amp;=0 &amp; (i=1,\ldots,n).
\end{align*}\]</span> If this assumption fails, we say <span class="math inline">\(\mathbf{X}\)</span> is <span style="color:red"><strong><em>endogenous</em></strong></span>.</p>
</div>
<p>You may hear this assumption associated with the word “orthogonal”, or known as an <strong><em>orthogonality condition</em></strong>. This is the precise type of assumption that <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span> referred to when talking about the role <span class="math inline">\(\varepsilon\)</span> plays in structural models. If <span class="math inline">\(\mathbf{X}_1\)</span> is the column of 1s associated with the intercept <span class="math inline">\(\beta_1\)</span>, then <span class="math inline">\(\text{E}\left[\mathbf{X}_1\boldsymbol{\varepsilon}\right] = 0\)</span>.</p>
<div id="exm-endogex" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6 (Endogeneity) </strong></span>A classic example in econometrics due to labor economists is estimating the impact that education has on earnings. An early paper to consider this was <span class="citation" data-cites="griliches1972education">Griliches and Mason (<a href="references.html#ref-griliches1972education" role="doc-biblioref">1972</a>)</span>, <span class="citation" data-cites="card1995using">Card (<a href="references.html#ref-card1995using" role="doc-biblioref">1995</a>)</span> is perhaps the most famous attempt at estimating this effect (<span class="citation" data-cites="card1999causal">Card (<a href="references.html#ref-card1999causal" role="doc-biblioref">1999</a>)</span> and <span class="citation" data-cites="card2001estimating">Card (<a href="references.html#ref-card2001estimating" role="doc-biblioref">2001</a>)</span> reviews similar studies and survey approaches to this problem). Economic intuition tells us that the more schooling someone receives, the higher their earnings/salary will be. Professions that are associated with high salaries often require (or are associated with) graduates degrees: doctors need an MD, lawyers need a JD, and business executives often have MBAs. On the opposite side of the spectrum, many white collar jobs require a college diploma, so only having a high school diploma limits a prospective employee’s ability to qualify for certain jobs which traditionally have higher pay. This observation leads us to posit the deterministic relationship: <span class="math display">\[\log(income_i) = \beta_1 + \beta_2\cdot educ_i,\]</span> where <span class="math inline">\(income_i\)</span> is an agent <span class="math inline">\(i\)</span>’s annual income and <span class="math inline">\(educ_i\)</span> is years of post-secondary education (we will operate under the assumption that every agent has a high school diploma). There are, of course, other factors impacting earnings (work experience, profession, location of residence, etc.) that are readily observable, but for the purpose of the example we will ignore those. There are of course exceptions to this deterministic relationship. Bill Gates and Mark Zuckerberg both only have high school diplomas,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> but have higher incomes than virtually everyone in the world. To account for this, we introduce the stochastic element <span class="math inline">\(\varepsilon_i\)</span> to our model. <span class="math display">\[\log(income_i) = \beta_1 + \beta_2\cdot educ_i + \varepsilon_i\]</span> In this case, <span class="math inline">\(\varepsilon_i\)</span> corresponds to all the other unobservable determinants of earnings. A major unobservable determinant is innate ability. Bill Gates and Mark Zuckerberg make a great deal of money because of their ambition, business acumen, and ability to innovate despite not having a college degree. It’s not really possible to measure something abstract like someone’s ambition, so the best we can do is incorporate it with <span class="math inline">\(\varepsilon_i\)</span>. Is it the case that <span class="math inline">\(\text{E}\left[\mathbb{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span> in this case? Most likely not. In all likelihood <span class="math inline">\(\text{E}\left[educ_i\cdot\varepsilon_i\right]\neq 0\)</span>, because people who are ambitious and have an innate ability to innovate tend to pursue higher education to further their abilities. If this hypothesis is true, then <span class="math inline">\(educ_i\)</span> is endogenous.</p>
</div>
<p>We also can give a name to the assumption that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 </strong></span>The covariates <span class="math inline">\(\mathbf{X}\)</span> exhibit <span style="color:red"><strong><em>(perfect) multicollinearity</em></strong></span> if <span class="math display">\[\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K,\]</span> which is equivalent to <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> failing to be invertible.</p>
</div>
<p>In the event that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is not invertible, then there exists some linear dependence between the set of covariates <span class="math inline">\((1,X_1,\ldots,X_k)\)</span>, i.e one regressor is a linear function of another. These two assumptions insure that <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified for <span class="math inline">\(\mathcal P_\text{LM}\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (Identification of the Linear Model) </strong></span>If <span class="math inline">\(\mathbf{X}\)</span> is weakly exogenous and does not exhibit multicollinearity, then <span class="math inline">\((\boldsymbol{\beta}, \boldsymbol{\Sigma})\)</span> are identified for the linear model <span class="math inline">\(\mathcal P_\text{LM},\)</span> and <span class="math inline">\(\beta\)</span> given as <span class="math display">\[\boldsymbol{\beta}= \left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right].\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Weak exogeneity gives<br>
<span class="math display">\[\begin{align*}
&amp;\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\\
\implies &amp; \text{E}\left[\mathbf{X}'(Y-\mathbf{X}\boldsymbol{\beta})\right] = \mathbf{0}&amp; (\boldsymbol{\varepsilon}= (Y-\mathbf{X}\boldsymbol{\beta}))\\
\implies &amp; \text{E}\left[\mathbf{X}'Y\right]-\boldsymbol{\beta}\text{E}\left[\mathbf{X}'\mathbf{X}\right]= \mathbf{0}\\
\implies &amp; \text{E}\left[\mathbf{X}'Y\right] = \boldsymbol{\beta}\text{E}\left[\mathbf{X}'\mathbf{X}\right].
\end{align*}\]</span> We have also assumed that <span class="math inline">\(\mathbf{X}\)</span> does not exhibit multicollinearity, so <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible. This means <span class="math inline">\(\text{E}\left[\mathbf{X}'Y\right] = \boldsymbol{\beta}\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> has a unique solution in the form of <span class="math inline">\(\boldsymbol{\beta}= \left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right].\)</span> If <span class="math inline">\(\boldsymbol{\beta}\)</span> is unique, then <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is unique and written as <span class="math display">\[ \boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbb{X}\right) = \text{Var}\left(Y - \mathbf{X}\boldsymbol{\beta}\mid \mathbb{X}\right) = \text{Var}\left(Y - \mathbf{X}\left[\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right]\right]\mid \mathbb{X}\right).\]</span> Therefore, if <span class="math inline">\((\boldsymbol{\beta},\boldsymbol{\Sigma})\neq(\boldsymbol{\beta}',\boldsymbol{\Sigma}')\)</span>, then <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\neq \mathbf{X}\boldsymbol{\beta}' + \boldsymbol{\varepsilon}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\neq \boldsymbol{\Sigma}\)</span>, so <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}}\neq P_{\boldsymbol{\beta}',\boldsymbol{\Sigma}'}\)</span>, meaning our parameters are identified.</p>
</div>
<p>The parameter <span class="math inline">\(\boldsymbol{\beta}\)</span> takes the same analytic form as that of the linear projection (CEF) model, but it’s important to remember that they arise from different approaches. We arrived at this form using the relationship between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> in a structural model, not from defining <span class="math inline">\(\boldsymbol{\beta}\)</span> to be the solution to an optimization problem.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (Multicollinearity) </strong></span>Suppose <span class="math inline">\(Y = 1 + 5 X_1 + 2 X_2 + \varepsilon\)</span> where <span class="math inline">\(X_1 = 3X_2\)</span> (suppressing the indices <span class="math inline">\(i\)</span> which are not relevant at the moment). This model corresponds to the parameters <span class="math inline">\(\boldsymbol{\beta}= (1,5,2)\)</span> We can rewrite our model as <span class="math display">\[ Y = 1 + 5 X_1 + 2 X_2 + \varepsilon = 1 + 5(3X_2) + 2 X_2 + \varepsilon = 1 + 0X_1 + 17X_2 + \varepsilon,\]</span> so the model also corresponds to parameters <span class="math inline">\(\boldsymbol{\beta}'=(1,0,17)\)</span>, and our model is not identified.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 (Non-Zero Mean Errors) </strong></span>Suppose <span class="math inline">\(Y = 1 + 5 X_1 + 2 X_2 + \varepsilon\)</span> where <span class="math inline">\(\text{E}\left[\varepsilon\right] = 3\)</span> and <span class="math inline">\(\text{Var}\left(\varepsilon\mid \mathbf{X}\right) = \sigma^2\)</span>. In this case <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]\neq 0\)</span>, so we shouldn’t expect that <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified. In particular, we won’t be able to identify <span class="math inline">\(\beta_0\)</span>. We can write <span class="math inline">\(\varepsilon = 3 + \nu\)</span> for <span class="math inline">\(\text{Var}\left(\nu\mid \mathbf{X}\right) = \sigma^2\)</span>, giving <span class="math display">\[ Y = 1 + 5 X_1 + 2 X_2 + (3 + \nu) = Y = 4 + 5 X_1 + 2 X_2 + \nu.\]</span> So the model can be written with parameters <span class="math inline">\(([1,5,2]', \sigma^2)\)</span> or with parameters <span class="math inline">\(([4,5,2]', \sigma^2)\)</span>. Therefore the model, in particular <span class="math inline">\(\beta_0\)</span>, is not identified.</p>
</div>
<p>We will consider what happens when <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]=0\)</span>, how this situation arises, and what can be done about it in Section @ref(endogeniety-i-iv-and-2sls). For now, let’s update our model with our identifying assumptions</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6 </strong></span>The <strong><em>(identified) linear model</em></strong> is defined as <span class="math inline">\(\mathcal P_\text{LM} = \{P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \boldsymbol{\Sigma}\in\mathbb R^n\times\mathbb R^n\}\)</span>,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} &amp;= \{F_{\mathbb{X},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}, \ \ \boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbb{X}\right),\ \ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i}, \ \text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K,\ \text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n].
\end{align*}\]</span></p>
</div>
</section>
<section id="ordinary-least-squares" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="ordinary-least-squares"><span class="header-section-number">5.4</span> Ordinary Least Squares</h2>
<p>Now that we have identified our model, we can <em>finally</em> estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> using our favorite estimator – ordinary least squares! There are a handful of ways to derive the ordinary least squares estimator, but for now we will focus on two constructions.</p>
<p>We want to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> using observations of <span class="math inline">\((\mathbf{Y}, \mathbb{X})\)</span>, which is the same as saying <span class="math inline">\(n\)</span> observations of <span class="math inline">\((Y, \mathbf{X})\)</span>. By definition, we do not observe realizations of <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>. We know that <span class="math inline">\(\boldsymbol{\beta}=\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right]\)</span>, so perhaps we can simply estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> using the sample analog of <span class="math inline">\(\left(\text{E}\left[\mathbb{X}'\mathbb{X}\right]\right)^{-1}\text{E}\left[\mathbb{X}'\mathbf{Y}\right]\)</span>. This approach is sometimes referred to as the <strong><em>analogy principle</em></strong> (see <span class="citation" data-cites="goldberger1991course">Goldberger (<a href="references.html#ref-goldberger1991course" role="doc-biblioref">1991</a>)</span>), and will come up again. Denote the sample moments as <span class="math inline">\(\widehat{\text{E}\left[\mathbf{X}'\mathbf{X}\right]}\)</span> and <span class="math inline">\(\widehat{\text{E}\left[\mathbf{X}'Y\right]}\)</span>. If we have a sample of size <span class="math inline">\(n\)</span>, then <span class="math display">\[\begin{align*}
\widehat{\text{E}\left[\mathbf{X}'\mathbf{X}\right]} &amp; = \frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}\\
\widehat{\text{E}\left[\mathbf{X}'Y\right]} &amp; = \frac{1}{n}\sum_{i=1}^n\mathbf{X}_iY_i
\end{align*}\]</span> Therefore, our estimator is <span class="math display">\[\hat {\boldsymbol{\beta}}(\mathbb{X}, \mathbf{Y}) = \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_iY_i\right).\]</span></p>
<p>We can also write this in the form of matrices. First we need to expand <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span>: <span class="math display">\[\begin{align*}
\mathbf{X}'\mathbf{X}&amp;= \begin{bmatrix} \mathbf{X}_1 \\ \vdots \\ \mathbf{X}_K\end{bmatrix}\begin{bmatrix} \mathbf{X}_1 &amp; \cdots &amp; \mathbf{X}_K\end{bmatrix} = \begin{bmatrix}\mathbf{X}_1\cdot\mathbf{X}_1 &amp; \mathbf{X}_1\cdot\mathbf{X}_2 &amp; \cdots &amp; \mathbf{X}_1\mathbf{X}_K\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \mathbf{X}_K\cdot\mathbf{X}_1 &amp; \mathbf{X}_K\cdot\mathbf{X}_2 &amp; \cdots &amp; \mathbf{X}_K\cdot \mathbf{X}_k\end{bmatrix} = \begin{bmatrix}\sum_{i=1}^n X_{1,i}^2 &amp; \sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{1,i}X_{K,i}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \sum_{i=1}^n X_{K,i}X_{1,i} &amp; \sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{K,i}^2\end{bmatrix}\\
\end{align*}\]</span> The expectation is taken element-wise where</p>
<p><span class="math display">\[ \text{E}\left[\sum_{i=1}^n X_{j,i}X_{\ell,i}\right] = \sum_{i=1}^n \text{E}\left[X_{j,i}X_{\ell,i}\right] = n \text{E}\left[X_{j,i}X_{\ell,i}\right],\]</span> so applying this to each entry and factoring out the common scalar <span class="math inline">\(n\)</span> gives:</p>
<p><span class="math display">\[ \text{E}\left[\mathbf{X}'\mathbf{X}\right] = n\begin{bmatrix}\text{E}\left[X_1^2\right] &amp; \text{E}\left[X_1X_2\right] &amp; \cdots &amp; \text{E}\left[X_1X_K\right]\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \text{E}\left[X_KX_1\right] &amp; \text{E}\left[X_KX_2\right] &amp; \cdots &amp; \text{E}\left[X_K^2\right]\end{bmatrix}.\]</span></p>
<p>The sample analog (as a function of random variables, <em>not</em> realizations) is <span class="math display">\[\begin{align*}
\widehat{\text{E}\left[\mathbf{X}'\mathbf{X}\right]} &amp;= n\begin{bmatrix}n^{-1}\sum_{i=1}^n X_{1,i}^2 &amp; n^{-1}\sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; n^{-1}\sum_{i=1}^n X_{1,i}X_{K,i}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ n^{-1}\sum_{i=1}^n X_{K,i}X_{1,i} &amp; n^{-1}\sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; n^{-1}\sum_{i=1}^n X_{K,i}^2\end{bmatrix}\\&amp;=\begin{bmatrix}\sum_{i=1}^n X_{1,i}^2 &amp; \sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{1,i}X_{K,i}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \sum_{i=1}^n X_{K,i}X_{1,i} &amp; \sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{K,i}^2\end{bmatrix}\\
&amp; = \begin{bmatrix} \mathbf{X}_1 \\ \vdots \\ \mathbf{X}_K\end{bmatrix}\begin{bmatrix} \mathbf{X}_1 &amp; \cdots &amp; \mathbf{X}_K\end{bmatrix}\\
&amp; = \mathbb{X}'\mathbb{X}
\end{align*}\]</span> We can perform the analogous inspection on <span class="math inline">\(\text{E}\left[\mathbf{X}'Y\right]\)</span> and conclude that <span class="math inline">\(\widehat{\text{E}\left[\mathbf{X}'Y\right]} = \mathbb{X}\mathbf{Y}\)</span>. Therefore, in matrix form, our estimator is <span class="math display">\[\hat {\boldsymbol{\beta}} =(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\mathbf{Y}\]</span></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9 (The Simple Linear Model) </strong></span>In the event <span class="math inline">\(K = 2\)</span>, we have <span class="math inline">\(Y = \beta_1 + \beta_2 X + \varepsilon\)</span> for a single non trivial regressor <span class="math inline">\(X\)</span>. The random vector of regressors is <span class="math inline">\(\mathbf{X}= [\mathbf 1, X]\)</span>. Let’s calculate the population parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> in this setting. <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{X}'\mathbf{X}\right] &amp; = \begin{bmatrix}1 &amp; \text{E}\left[X\right]\\\text{E}\left[X\right] &amp; \text{E}\left[X^2\right] \end{bmatrix}\\
\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1} &amp; =\frac{1}{\underbrace{\text{E}\left[X^2\right] - \text{E}\left[X\right]^2}_{\text{Var}\left(X\right)}} \begin{bmatrix}\text{E}\left[X^2\right] &amp; -\text{E}\left[X\right]\\-\text{E}\left[X\right] &amp; 1 \end{bmatrix} = \begin{bmatrix}\text{E}\left[X^2\right]/\text{Var}\left(X\right) &amp; -\text{E}\left[X\right]/\text{Var}\left(X\right)\\-\text{E}\left[X\right]/\text{Var}\left(X\right) &amp; 1/\text{Var}\left(X\right) \end{bmatrix}\\
\text{E}\left[\mathbf{X}'Y\right] &amp; = \begin{bmatrix} \text{E}\left[Y\right] \\ \text{E}\left[XY\right] \end{bmatrix}\\
\boldsymbol{\beta}&amp; = \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\text{E}\left[\mathbf{X}'Y\right] = \frac{1}{\text{Var}\left(X\right)} \begin{bmatrix} \text{E}\left[X^2\right]\text{E}\left[Y\right] -\text{E}\left[X\right]\text{E}\left[XY\right] &amp; -\text{E}\left[X\right]\text{E}\left[Y\right] + \text{E}\left[XY\right] \end{bmatrix}\\
\beta_2 &amp;= \frac{\text{E}\left[XY\right] -\text{E}\left[X\right]\text{E}\left[Y\right]}{\text{Var}\left(X\right)} \\ &amp;= \frac{\text{Cov}\left(X,Y\right)}{\text{Var}\left(X\right)}\\
\beta_1 &amp; = \frac{\text{E}\left[X^2\right]\text{E}\left[Y\right] -\text{E}\left[X\right]\text{E}\left[XY\right] }{\text{Var}\left(X\right)} \\
      &amp; = \frac{(\text{E}\left[X^2\right] - \text{E}\left[X\right]^2 + \text{E}\left[X\right]^2)\text{E}\left[Y\right] -\text{E}\left[X\right]\text{E}\left[XY\right] }{\text{Var}\left(X\right)}\\
       &amp; = \frac{(\text{Var}\left(X\right)+ \text{E}\left[X\right]^2)\text{E}\left[Y\right] -\text{E}\left[X\right]\text{E}\left[XY\right] }{\text{Var}\left(X\right)}\\
       &amp; = \text{E}\left[Y\right] - \text{E}\left[X\right]\cdot \frac{\text{Cov}\left(X,Y\right)}{\text{Var}\left(X\right)}\\
       &amp; = \text{E}\left[Y\right] - \beta_2\text{E}\left[X\right]
\end{align*}\]</span> The estimator calculated using the analogous moments is the familiar OLS estimator for the simple linear model: <span class="math display">\[\begin{align*}
\hat\beta_2 &amp; = \frac{\widehat{\text{Cov}}(X,Y) }{\widehat{\text{Var}}(X)} = \frac{(1/n)\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{(1/n)\sum_{i=1}^n(X_i - \bar X)^2} = \frac{\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n(X_i - \bar X)^2}\\
\hat\beta_1 &amp; = \bar Y - \hat\beta_2 \bar X
\end{align*}\]</span></p>
</div>
<p>An alternate way of arriving at this estimator is possible by solving the least squares problem that we encountered with the linear projection model.</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} &amp;= \mathop{\mathrm{argmin}}_{\mathbf b} \sum_{i=1}^n (Y_i - \mathbf{X}_i\mathbf b)^2\\
  &amp; = \mathop{\mathrm{argmin}}_{\mathbf b} \left\{(\mathbf{Y}- \mathbb{X}\mathbf b)'(\mathbf{Y}- \mathbb{X}\mathbf b)\right\} \\
  &amp; = \mathop{\mathrm{argmin}}_{\mathbf b} \left\{ \mathbf{Y}' \mathbf{Y}- 2\mathbf{Y}\mathbb{X}\mathbf b +\mathbf b' \mathbb{X}' \mathbb{X}\mathbf b \right\}
\end{align*}\]</span> The first order condition is <span class="math display">\[\begin{align*}
&amp;-2\mathbb{X}'\mathbf{Y}+ 2\mathbb{X}'\mathbb{X}\hat{\boldsymbol{\beta}} = \mathbf{0}\\
\implies &amp;\hat{\boldsymbol{\beta}} = (\mathbb{X}'\mathbb{X})^{-1}(\mathbb{X}\mathbf{Y})
\end{align*}\]</span> This is the same estimator we derived with the analogy principle. In order to reference estimates given by our estimator, we’ll need to introduce notation for realizations of <span class="math inline">\((\mathbb{X}, \mathbf{Y}, \boldsymbol{\varepsilon})\)</span>, which makes notation even more complicated. The following table presents how we will write realizations of random quantities, along with recapping the notation for <span class="math inline">\(\mathcal P_{LM}\)</span>.</p>
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 33%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Random Quantity</th>
<th>Type</th>
<th>Dimension</th>
<th>Definition</th>
<th>Realization/Observation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbf{X}\)</span></td>
<td>vector</td>
<td><span class="math inline">\(1\times K\)</span></td>
<td>dependent variables</td>
<td><span class="math inline">\(\mathbf{x}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y\)</span></td>
<td>variable</td>
<td><span class="math inline">\(1\times 1\)</span></td>
<td>independent variable</td>
<td><span class="math inline">\(y\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\boldsymbol{\varepsilon}\)</span></td>
<td>vector</td>
<td><span class="math inline">\(n\times 1\)</span></td>
<td>vector of errors</td>
<td><span class="math inline">\(\mathbf{e}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{Y}\)</span></td>
<td>vector</td>
<td><span class="math inline">\(n\times 1\)</span></td>
<td>vector of independent variables</td>
<td><span class="math inline">\(\mathbf{y}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbb{X}\)</span></td>
<td>matrix</td>
<td><span class="math inline">\(n\times K\)</span></td>
<td>matrix of dependent variables</td>
<td><span class="math inline">\(\mathbf X\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{X}_i\)</span></td>
<td>vector</td>
<td><span class="math inline">\(1\times K\)</span></td>
<td><span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbb{X}\)</span></td>
<td><span class="math inline">\(\mathbf{x}_i\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{X}_j\)</span></td>
<td>vector</td>
<td><span class="math inline">\(n \times 1\)</span></td>
<td><span class="math inline">\(j\)</span>th row of <span class="math inline">\(\mathbb{X}\)</span></td>
<td><span class="math inline">\(\mathbf{x}_j\)</span></td>
</tr>
</tbody>
</table>
<p>This notation is by no means standard, an notation unfortunately varies widely across sources. The only piece of notation which conflicts is the random vector of regressors <span class="math inline">\(\mathbf{X}\)</span> and the realization of <span class="math inline">\(\mathbb{X}= \mathbf{X}\)</span>. Whenever it is unclear which is being referenced, I will try to be specific.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.7 </strong></span>The <span style="color:red"><strong><em>ordinary least squares (OLS) estimator</em></strong></span> is defined as <span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_\text{OLS}(\mathbb{X},\mathbf{Y}) &amp;= (\mathbb{X}'\mathbb{X})^{-1}(\mathbb{X}\mathbf{Y})
= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_iY_i\right)
\end{align*}\]</span> An realization of this estimator (an estimate) is <span class="math display">\[\begin{align*}
\hat{\mathbf b}_\text{OLS} = \hat{\boldsymbol{\beta}}_\text{OLS}(\mathbf{X},\mathbf{y}) &amp;= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}\mathbf{y})
= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{x}_i'\mathbf{x}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{x}_iy_i\right)
\end{align*}\]</span> and will exist when the inverse <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\)</span> exists.</p>
</div>
<div id="exm-funref" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10 </strong></span>We can easily write a function which calculates an OLS estimate given a random sample.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-7_65acb862d9e0b38c486d77141cb93f7a">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X){</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">==</span> <span class="dv">0</span>) {<span class="fu">stop</span>(<span class="st">"rank(X'X) &lt; K"</span>)}</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  hat_beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(hat_beta) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"β"</span>, <span class="dv">1</span><span class="sc">:</span>K, <span class="st">" estimate"</span>, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(hat_beta)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s randomly generate a sample to test our function. Suppose we have a sample of size <span class="math inline">\(n=1000\)</span> from a linear model where <span class="math inline">\(X_1 \overset{iid}{\sim}\text{Uni}(0,10)\)</span>, <span class="math inline">\(\varepsilon \overset{iid}{\sim}\text{Uni}(-5,5)\)</span>, <span class="math inline">\(X_1 \perp \varepsilon\)</span>, and <span class="math inline">\(Y = 2 + 4X_1 + \varepsilon\)</span>. Because <span class="math inline">\(\varepsilon\)</span> and <span class="math inline">\(X\)</span> are independent, we’ve specified their respective marginal densities instead of joint density.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-8_7b8e1b948146c535a3008a1ec2c7ba53">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before we estimate our model, we should think about whether our model satisfies the assumptions that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible and <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>. The first assumption holds because we only have one non-trivial independent variable (the trivial one is constant 1 which gives the intercept), and it is not a constant (so it cannot be a linear function of the constant 1). We have <span class="math inline">\(\text{E}\left[\varepsilon\right] = 0\)</span>, so by independence we have</p>
<p><span class="math display">\[\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \text{E}\left[X_1\varepsilon\right] = \text{E}\left[X_1\right]\text{E}\left[\varepsilon\right] =\text{E}\left[X_1\right]\cdot0 = 0\]</span> We can actually use the LLN to consistently estimate <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> and <span class="math inline">\(\text{E}\left[X_1'\varepsilon\right]\)</span>, and see if our estimates satisfy our assumptions. For a sufficiently large <span class="math inline">\(n\)</span>, we should see that <span class="math display">\[\begin{align*}
\text{rank}\left(\frac{1}{n}\sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i\right) &amp;\approx K\\
\left(\frac{1}{n}\sum_{i=1}^n x_{1i}'e_i\right) &amp;\approx 0
\end{align*}\]</span> The sample size <span class="math inline">\(n=25\)</span> may be a bit too modest, so let’s generate a new sample of size <span class="math inline">\(n'=100,000\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-9_0e8c4e8a1ada82e76140fb190dbfd175">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n_prime <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>x1_prime <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_prime, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>e_prime <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_prime, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X_prime <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n_prime), x1_prime)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample rank of X'X</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rankMatrix</span>((<span class="fu">t</span>(X_prime) <span class="sc">%*%</span> X_prime)<span class="sc">/</span>n_prime)[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample mean of X'ε</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x1_prime<span class="sc">*</span>e_prime)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.06257445</code></pre>
</div>
</div>
<p>It appears our assumptions are met, so we can go ahead with estimation. It is important to recognize that in practice we don’t observe the realizations <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>, so calculating the sample analog of <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]\)</span> is not possible with real data, but it is a good gut check when conducting simulations.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-10_65bcec94544f6ac248c9b8e599498039">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>beta_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 [,1]
β1 estimate 0.3890429
β2 estimate 4.1678826</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot56_10cd2718b3b8258795c998aaa7daab55">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> X[,<span class="dv">2</span>], </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> y</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"(Observed) Sample"</span>)) <span class="sc">+</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> beta[<span class="dv">1</span>], <span class="at">slope =</span> beta[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">"(Unobserved) Population Model"</span>)) <span class="sc">+</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> beta_hat[<span class="dv">1</span>], <span class="at">slope =</span> beta_hat[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">"Estimated Model"</span>)) <span class="sc">+</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot56" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot56-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.6: Our observed data (in black) is drawn from the unobserved population model (blue). We use this data to estimate the model, and the line associated with this estimate is shown in red.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.11 (Linear Projection Model, OLS) </strong></span>OLS can be used in the context of <span class="math inline">\(\mathcal P_\text{LM}\)</span> to estimate the best linear projection between two random variables <span class="math inline">\((Y,\mathbf{X})\)</span>. OLS was the method used by <span class="citation" data-cites="pearson1903laws">Pearson and Lee (<a href="references.html#ref-pearson1903laws" role="doc-biblioref">1903</a>)</span> to estimate the relationship between height and genetics. We can replicate this work with an data set based on the original data collected by <span class="citation" data-cites="pearson1903laws">Pearson and Lee (<a href="references.html#ref-pearson1903laws" role="doc-biblioref">1903</a>)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-12_0afc87d4702d02484a34d3cc1bfad37d">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>height_df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/height_data.csv"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(height_df<span class="sc">$</span>Father)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> height_df<span class="sc">$</span>Son</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y,X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                [,1]
β1 estimate 1.013914</code></pre>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.12 (OLS Estimate Does not Exist) </strong></span>It is possible that <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> is invertible while realized value <span class="math inline">\(\mathbb{X}'\mathbb{X}= \mathbf{X}'\mathbf{X}\)</span> is not invertible. For example, if our model is <span class="math inline">\(Y = \beta_1 X_1 + \epsilon\)</span> where <span class="math inline">\(X \sim \text{Binom}(4, 0.5)\)</span>,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> we have <span class="math display">\[\text{E}\left[\mathbf{X}'\mathbf{X}\right] = \text{E}\left[X_1^2\right] = 5.\]</span> If we observe an independent sample of size <span class="math inline">\(n=2\)</span> generated from this model, we may observe something like <span class="math inline">\(\mathbf{x}_1 = [2,2]\)</span>. In this case <span class="math display">\[\mathbf{x}'\mathbf{x}= \begin{bmatrix} 4 &amp;4 \\ 4 &amp; 4\end{bmatrix},\]</span> which is certainly not invertible. Furthermore, the probability we draw this sample is <span class="math display">\[\Pr(\mathbf{x}= [2,2]) = [\Pr(X = 2)]^2 = (0.375)^2 = 0.140625,\]</span> so the chances this happen are not trivial. However, <span class="math inline">\(n\)</span> is usually much greater than <span class="math inline">\(2\)</span>, and as <span class="math inline">\(n\to\infty\)</span> the probability that <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> is not invertible will go to zero. This is a direct consequence of the LLN:</p>
<p><span class="math display">\[\mathbf{X}'\mathbf{X}= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{x}_i'\mathbf{x}_i\right) \overset{p}{\to}\text{E}\left[\mathbf{X}'\mathbf{X}\right] \]</span></p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>Whether it is easier to write our terms related to <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> in terms of matrices or sums of vectors will depend on the result we are building to or trying to prove. This can be a bit confusing, so here is a list of various equalities (many of which imply others), that we will use: <span class="math display">\[\begin{align*}
\mathbb{X}'\mathbb{X}&amp; = \sum_{i=1}^n \mathbf{X}_i'\mathbf{X}_i\\
\mathbb{X}'\mathbf{Y}&amp; = \sum_{i=1}^n \mathbf{X}_i'Y_i\\
\mathbb{X}'\boldsymbol{\varepsilon}&amp; = \sum_{i=1}^n \mathbf{X}_i'\varepsilon_i\\
\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}&amp; = \sum_{i=1}^n \varepsilon_i^2
\end{align*}\]</span> An important result which follows from the first equality is <span class="math inline">\(\text{E}\left[\mathbb{X}'\mathbb{X}\right] = n \text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> in the event that <span class="math inline">\(\mathbf{X}_i\)</span> are independent.</p>
</div>
</section>
<section id="properties-of-ols" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="properties-of-ols"><span class="header-section-number">5.5</span> Properties of OLS</h2>
<p>As likely anticipated, the OLS estimator has a number of desirable properties under certain assumptions, some of which we will make in addition to weak exogeneity and lack of multicollinearity. The first property we establish is consistency.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.4 (Consistency) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span> and <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, then <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have <span class="math inline">\(\boldsymbol{\beta}= \left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right)^{-1}\text{E}\left[\mathbf{X}'Y\right]\)</span>, where <span class="math inline">\(\boldsymbol{\beta}\)</span> is guaranteed to exist and be unique using our assumptions. As <span class="math inline">\(n\to\infty\)</span>, <span class="math inline">\(\mathbf{X}'\mathbf{X}\)</span> will be invertible with probability one, so <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span> will exist (with probability one). We can write our estimator as <span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_\text{OLS} &amp;= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'Y_i\right)\\
&amp;= \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'(\mathbf{X}_i\boldsymbol{\beta}+ \varepsilon_i)\right) &amp; (Y_i = \mathbf{X}_i\boldsymbol{\beta}+ \varepsilon_i)\\
&amp; = \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\boldsymbol{\beta}\right) + \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)\\
&amp; = \boldsymbol{\beta}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)}_{\mathbf I} + \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)\\
&amp; = \boldsymbol{\beta}+ \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)
\end{align*}\]</span> We can apply the LLN along with the continuous mapping theorem (applied to the inverse term) and Slutky’s theorem (applied to the product of convergent sequences) to conclude, <span class="math display">\[ \hat{\boldsymbol{\beta}}_\text{OLS} =  \boldsymbol{\beta}+ \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}}_{\overset{p}{\to}\text{E}\left[\mathbf{X}'\mathbf{X}\right]}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)}_{\overset{p}{\to}\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]} \overset{p}{\to}\boldsymbol{\beta}+ \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\underbrace{\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right]}_\mathbf{0}= \boldsymbol{\beta}.\]</span> Therefore <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span>, where the limit <span class="math inline">\(\boldsymbol{\beta}\)</span> is unique by identification.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.13 </strong></span>Return to the model <span class="math inline">\(X_1 \overset{iid}{\sim}\text{Uni}(0,10)\)</span>, <span class="math inline">\(\varepsilon \overset{iid}{\sim}\text{Uni}(-5,5)\)</span>, <span class="math inline">\(X_1 \perp \varepsilon\)</span> (implying <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>), and <span class="math inline">\(Y = 2 + 4X_1 + \varepsilon\)</span>. If we estimate this model for samples of increasing size, we should see that our estimates converge to the true values.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-13_e33f9b88043fc50637c722d3dfb6994c">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>beta_hat1 <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">9999</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>beta_hat2 <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">9999</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10000</span>){</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  beta_hat1[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> beta_hat[<span class="dv">1</span>]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  beta_hat2[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> beta_hat[<span class="dv">2</span>]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It does appear that as <span class="math inline">\(n\to\infty\)</span> we have <span class="math inline">\(\hat{\beta}_\text{1,OLS} \overset{p}{\to}2\)</span> and <span class="math inline">\(\hat{\beta}_\text{2,OLS} \overset{p}{\to}4\)</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot57_08c6257e8685ac05652eb11a115f3c4f">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">rep</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10000</span>, <span class="dv">2</span>), </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(beta_hat1, beta_hat2), </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Estimated β1"</span>, <span class="dv">9999</span>), <span class="fu">rep</span>(<span class="st">"Estimated β2"</span>, <span class="dv">9999</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span><span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Estimated Value"</span>) <span class="sc">+</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot57" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot57-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.7: As the sample size increases, our estimates approach their true values.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>Now let’s consider whether if (and under what conditions) <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span> unbiased.</p>
<p><span class="math display">\[\begin{align*}
\text{E}\left[\hat{\boldsymbol{\beta}}_\text{OLS}\right] &amp; = \text{E}\left[\text{E}\left[\hat{\boldsymbol{\beta}}_\text{OLS}\right]\mid \mathbb{X}\right] &amp; (\text{iterated expectations})\\
&amp; = \text{E}\left[\text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{Y}\right]\mid \mathbb{X}\right]\\
&amp; = \text{E}\left[\text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'(\mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon})\right]\mid \mathbb{X}\right] &amp; (\mathbf{Y}&amp; = \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon})\\
&amp; = \text{E}\left[\text{E}\left[((\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{X})\boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon})\right]\mid \mathbb{X}\right]\\
&amp; = \text{E}\left[\text{E}\left[\boldsymbol{\beta}\right]\mid \mathbb{X}\right] +\text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right]\\
&amp; = \boldsymbol{\beta}+  \text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right] &amp; (\boldsymbol{\beta}\text{ is a constant})\\
&amp; \neq \boldsymbol{\beta}
\end{align*}\]</span> Under our current assumption, OLS is has a bias of <span class="math inline">\(\text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbf{X}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right]\)</span>. While we are operating under the assumption that <span class="math inline">\(\text{E}\left[ \mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, this does not imply that <span class="math inline">\(\boldsymbol{\varepsilon}\perp\mathbb{X}\)</span> (which would mean <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \text{E}\left[\boldsymbol{\varepsilon}\right]=\mathbf{0}\)</span>) For this to happen, we need to impose our third assumption on the linear model.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.8 </strong></span>The random regressors <span class="math inline">\(\mathbf{X}\)</span> are <span style="color:red"><strong><em>exogenous</em></strong></span> if<br>
<span class="math display">\[\begin{align*}
\text{E}\left[\varepsilon_i\mid \mathbf{X}\right] &amp; = 0 &amp;(i=1,\ldots,n)
\end{align*}\]</span> which is written compactly as <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>.</p>
</div>
<p>By properties of independence and conditional expectation, exogeneity implies weak exogeneity, hence its name alluding to it being a stronger assumption. Technically speaking, we aren’t adding a third assumption, as much as we are strengthening one of our current assumptions.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.5 (OLS is Unbiased) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span> and <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>, then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is an unbiased estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\text{E}\left[\hat{\boldsymbol{\beta}}_\text{OLS}\right] = \boldsymbol{\beta}+  \text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right] =\boldsymbol{\beta}+  \text{E}\left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{0}\right] = \boldsymbol{\beta}\]</span> Therefore, <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span> is an unbiased estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.9 </strong></span>The <span style="color:red"><strong><em>(unbiased) linear model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{LM} = \{P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \boldsymbol{\Sigma}\in\mathbb R^n\times\mathbb R^n\}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta}, \boldsymbol{\Sigma}} &amp;= \{F_{\mathbb{X},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}, \ \ \boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbb{X}\right),\ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i}, \ \text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K,\ \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n].
\end{align*}\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.14 </strong></span>If we go back to our simulated estimates where <span class="math inline">\(X_1 \overset{iid}{\sim}\text{Uni}(0,10)\)</span>, <span class="math inline">\(\varepsilon \overset{iid}{\sim}\text{Uni}(-5,5)\)</span>, <span class="math inline">\(X_1 \perp \varepsilon\)</span>, and <span class="math inline">\(Y = 2 + 4X_1 + \varepsilon\)</span>, we should see that the sample mean of our simulated estimates are approximately equal to the true values <span class="math inline">\(\boldsymbol{\beta}= (2,4)\)</span></p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-15_23af26a7f34c6a9427fda76070d74325">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(<span class="fu">cbind</span>(beta_hat1, beta_hat2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta_hat1 beta_hat2 
 1.999171  4.000034 </code></pre>
</div>
</div>
</div>
<p>The assumption of this stronger form of exogeneity also gives several nice properties beyond unbiasedness.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.6 (Consequences of Exogeneity) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right)\)</span>. Then</p>
<ol type="1">
<li><span class="math inline">\(\text{E}\left[\mathbf{Y}\mid \mathbb{X}\right] = \mathbb{X}\boldsymbol{\beta}\)</span>;</li>
<li><span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span> (even if <span class="math inline">\(\beta_1 = 0\)</span>)</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}= \text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right] = \text{Var}\left(\boldsymbol{\varepsilon}\right)\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<ol type="1">
<li><span class="math inline">\(\text{E}\left[\mathbf{Y}\mid \mathbb{X}\right] = \text{E}\left[\mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \text{E}\left[\mathbb{X}\boldsymbol{\beta}\mid \mathbb{X}\right] + \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]= \mathbb{X}\boldsymbol{\beta}+ \mathbf{0}= \mathbb{X}\boldsymbol{\beta}\)</span></li>
<li><span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\right] = \text{E}\left[\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right] = \text{E}\left[\mathbf{0}\right] = \mathbf{0}\)</span></li>
<li>The first portion is a consequence of the definition of variance. <span class="math display">\[\boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right) = \text{E}\left[[\boldsymbol{\varepsilon}- \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]][\boldsymbol{\varepsilon}- \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]]'\mid \mathbb{X}\right] = \text{E}\left[[\boldsymbol{\varepsilon}- \mathbf{0}][\boldsymbol{\varepsilon}- \mathbf{0}]'\mid \mathbb{X}\right] = \text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]\]</span> The second portion follows from the <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">law of total variance</a>. <span class="math display">\[ \text{Var}\left(\boldsymbol{\varepsilon}\right) = \text{E}\left[\text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right)\right] + \text{Var}\left(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right) = \text{E}\left[\boldsymbol{\Sigma}\right] + \text{Var}\left(0\right) = \boldsymbol{\Sigma}\]</span></li>
</ol>
<p><span style="color:white">space</span></p>
</div>
<p>Finally let’s consider the variance of our OLS estimator. Due to the stochastic nature of <span class="math inline">\(\mathbb{X}\)</span>, our interest is actually in the conditional variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span> given <span class="math inline">\(\mathbb{X}\)</span>. Until now, we haven’t paid much attention to the parameter <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, but it will come into play here.</p>
<p><span class="math display">\[\begin{align*}
\text{Var}\left(\hat{\boldsymbol{\beta}}_\text{OLS}\mid \mathbb{X}\right) &amp; = \text{E}\left[ \left(\hat{\boldsymbol{\beta}}_\text{OLS} - \text{E}\left[ \hat{\boldsymbol{\beta}}_\text{OLS} \right]\right) \left(\hat{\boldsymbol{\beta}}_\text{OLS} - \text{E}\left[\hat{\boldsymbol{\beta}}_\text{OLS}\right]\right)'\mid \mathbb{X}\right]\\
&amp; = \text{E}\left[ \left(\hat{\boldsymbol{\beta}}_\text{OLS} - \boldsymbol{\beta}\right) \left(\hat{\boldsymbol{\beta}}_\text{OLS} - \boldsymbol{\beta}\right)'\mid \mathbb{X}\right] &amp; (\hat{\boldsymbol{\beta}}_\text{OLS} \text{ unbiased})\\
&amp; = \text{E}\left[ \left[(\boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right] \left[(\boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right]'\mid \mathbb{X}\right] &amp; (\hat{\boldsymbol{\beta}}_\text{OLS} = \boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol{\varepsilon})\\
&amp; = \text{E}\left[ \left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}\right] \left[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}\right]'\mid \mathbb{X}\right]\\
&amp; = \text{E}\left[ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mid \mathbb{X}\right]\\
&amp; =  (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\\
&amp; =  (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\Sigma}\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1} &amp; (\boldsymbol{\Sigma}= \text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right])
\end{align*}\]</span></p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.7 (OLS Variance I) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\boldsymbol{\Sigma}} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}= \text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right)\)</span>. Then <span class="math display">\[ \text{Var}\left(\hat{\boldsymbol{\beta}}_\text{OLS}\mid \mathbb{X}\right) = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\Sigma}\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\]</span></p>
</div>
<div class="example">
<p>Suppose the our model is given as <span class="math display">\[\begin{align*}
Y &amp; = 2 + 4X_1 + \varepsilon\\
X_i &amp;\overset{iid}{\sim}\text{Uni}(0,10)\\
\boldsymbol{\varepsilon}&amp;\sim N(\mathbf{0}, \boldsymbol{\Sigma})\\
\boldsymbol{\Sigma}_{ii} &amp; = \begin{cases}1 &amp; i \text{ even}\\ 2 &amp; i \text{ odd}\end{cases}\\
\boldsymbol{\Sigma}_{i\ell} &amp; = \left\lvert i-\ell\right\rvert^{-1}
\end{align*}\]</span> Let’s perform a simulation to verify the formula for <span class="math inline">\(\text{Var}\left(\hat{\boldsymbol{\beta}}_\text{OLS}\mid \mathbb{X}\right)\)</span> The variance of the error is defined such that if our observation has an even index, <span class="math inline">\(\text{Var}\left(\varepsilon_i\right) = 1\)</span>, otherwise <span class="math inline">\(\text{Var}\left(\varepsilon_i\right) = 2\)</span>. Errors are also correlated across observations. We have <span class="math inline">\(\text{Cov}\left(\varepsilon_i,\varepsilon_\ell\right) = \left\lvert i-\ell\right\rvert^{-1}\)</span>. The closer two observations are index-wise, the stronger their correlation. We will simulate estimates for sample sizes of <span class="math inline">\(n=100\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-16_ef8484c0325c46392c6b6702dae4599c">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n, <span class="at">ncol =</span> n)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#if i == l we have a diagonal entry </span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> l){</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>      <span class="co">#assign variance 1 or 2 based on modular arithmetic </span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>      Sigma[i,l] <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">^</span>((i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">==</span> <span class="dv">1</span>))<span class="sc">*</span>(<span class="dv">1</span><span class="sc">^</span>((i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">==</span> <span class="dv">0</span>))</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>      <span class="co">#otherwise asign covariance to be inverse of distance between entries</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>      Sigma[i,l] <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">abs</span>(i<span class="sc">-</span>l)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(Sigma[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
[1,] 2.0000000 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667
[2,] 1.0000000 1.0000000 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000
[3,] 0.5000000 1.0000000 2.0000000 1.0000000 0.5000000 0.3333333 0.2500000
[4,] 0.3333333 0.5000000 1.0000000 1.0000000 1.0000000 0.5000000 0.3333333
[5,] 0.2500000 0.3333333 0.5000000 1.0000000 2.0000000 1.0000000 0.5000000
[6,] 0.2000000 0.2500000 0.3333333 0.5000000 1.0000000 1.0000000 1.0000000
[7,] 0.1666667 0.2000000 0.2500000 0.3333333 0.5000000 1.0000000 2.0000000</code></pre>
</div>
</div>
<p>We are calculating the variance <em>conditional on <span class="math inline">\(\mathbb{X}\)</span></em>, so this means we will use the same realization <span class="math inline">\(\mathbb{X}= \mathbf{X}\)</span> for each simulation. Before we simulate things, let’s draw our fixed realization of <span class="math inline">\(\mathbf{X}\)</span> and calculate the true variance of our OLS estimator using the formula <span class="math inline">\((\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\boldsymbol{\Sigma}\mathbf{X}' (\mathbf{X}'\mathbf{X})^{-1}\)</span> where <span class="math inline">\(\mathbf{X}\)</span> is our fixed regressors.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-17_5969dc51e45762d96f6f65726f29ef99">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual variance</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                          x1
    0.135371873 -0.007006977
x1 -0.007006977  0.001335754</code></pre>
</div>
</div>
<p>Now let’s simulate 1000 estimates, where we only draw new realizations <span class="math inline">\(\mathbf e\)</span> for each simulation and leave <span class="math inline">\(\mathbf{X}\)</span> fixed.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-18_43dbc0fdf66155f8e6bf82d6003236e2">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> N_sim)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, n), Sigma))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  store[k,] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance across simulations</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(store)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]         [,2]
[1,]  0.132725660 -0.006357347
[2,] -0.006357347  0.001291283</code></pre>
</div>
</div>
<p>Our simulated variance is quite close to the true conditional variance!</p>
</div>
<p>We can simplify the formulas for the OLS estimator’s variance greatly if we impose one final assumption on our model.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.10 </strong></span>We the errors of a model are <span style="color:red"><strong><em>homoskedastic</em></strong></span> if <span class="math display">\[\begin{align*}
\text{Var}\left(\varepsilon_i \mid \mathbf{X}\right) &amp;= \sigma^2 &amp; (i=1,\ldots,n)
\end{align*}\]</span> (where <span class="math inline">\(\mathbf{X}\)</span> is the random vector of regressors), otherwise they are <span style="color:red"><strong><em>heteroskedastic</em></strong></span>. If <span class="math display">\[\text{Cov}\left(\varepsilon_i,\varepsilon_\ell\right) = 0\]</span> for all <span class="math inline">\(i,\ell = 1,\ldots,n\)</span> where <span class="math inline">\(i\neq \ell\)</span> we say errors are <span style="color:red"><strong><em>nonautocorrelated</em></strong></span>, otherwise they are <span style="color:red"><strong><em>autocorrelated/serially correlated</em></strong></span>. If errors are both homoskedastic and nonautocorrelated, then we have <span style="color:red"><strong><em>spherical errors</em></strong></span> and can write <span class="math display">\[\text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right) = \sigma^2\mathbf I.\]</span></p>
</div>
<p>With the addition of this assumption, we have the classical linear model that you are likely familiar with.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.11 </strong></span>The <span style="color:red"><strong><em>(Gauss-Markov/classical) linear model</em></strong></span> is defined as <span class="math inline">\(\mathcal P_\text{LM} = \{P_{\boldsymbol{\beta},\sigma^2} \mid \boldsymbol{\beta}\in \mathbb R^{K}, \sigma^2\in\mathbb R\}\)</span>, where <span class="math display">\[\begin{align*}
P_{\boldsymbol{\beta},\sigma^2} &amp;= \{F_{\mathbb{X},\boldsymbol{\varepsilon}} \mid \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}, \ \text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbf{X}\right]=\sigma^2\mathbf I, \ f_{\mathbb{X}}=\textstyle\prod_{i=1}^n f_{\mathbf{X}_i}, \ \text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K,\ \text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\},\\
\mathbb{X}&amp; = [\mathbf{X}_1, \cdots, \mathbf{X}_j, \cdots \mathbf{X}_K] = [\mathbf{X}_1, \cdots, \mathbf{X}_i, \cdots \mathbf{X}_n]',\\
\mathbf{Y}&amp; = [Y_1, \ldots, Y_n].
\end{align*}\]</span></p>
</div>
<p>When people talk about “the linear (regression) model”, this is usually the model they are discussing. The collective assumptions are sometimes known as the “Gauss-Markov assumptions”, as they are the sufficient conditions for the Gauss-Markov theorem (which will be presented shortly) to hold.</p>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 5.1 (OLS Variance II) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>, and <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid\mathbb{X}\right] = \sigma^2\mathbf I\)</span>. Then <span class="math display">\[\begin{align*}
\text{Var}\left(\hat{\boldsymbol{\beta}}_\text{OLS}\mid \mathbf{X}\right) &amp;= \sigma^2(\mathbb{X}'\mathbb{X})^{-1} = \sigma^2 \left(\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}\right)^{-1}
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\((\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1} = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\sigma^2\mathbf I\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1} = \sigma^2\underbrace{[(\mathbb{X}'\mathbb{X})^{-1}(\mathbb{X}'\mathbb{X})]}_{\mathbf I}(\mathbb{X}'\mathbb{X})^{-1} = \sigma^2(\mathbb{X}'\mathbb{X})^{-1}\)</span></p>
</div>
<div id="exm-csvarols" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.15 (Comparative Statics and Variance) </strong></span>Suppose the linear model satisfies the Gauss-Markov assumptions and <span class="math inline">\(K = 2\)</span>. This simple setting allows us to gain a great deal of insight into the variance of the OLS estimator. In this case <span class="math inline">\(\mathbf{X}\)</span> has two columns: a column one 1s, and <span class="math inline">\(\mathbf{x}\)</span>. <span class="math display">\[\begin{align*}
\mathbb{X}&amp; = [\mathbf 1, \mathbf{X}_1]\\
\mathbb{X}'\mathbb{X}&amp; = \begin{bmatrix}n &amp; \sum_{i=1}^nX_i\\ \sum_{i=1}^nX_i &amp;  \sum_{i=1}^nX_i^2 \end{bmatrix}\\
\sigma^2(\mathbb{X}'\mathbb{X})^{-1} &amp;= \frac{\sigma^2}{n\sum_{i=1}^n X_i^2 - \left(\sum_{i=1}^n X_i\right)^2} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i \\  -\sum_{i=1}^n X_i &amp; n \end{bmatrix} \\
\\ &amp; = \frac{\sigma^2}{n[n(\bar{X})^2] - (n\bar{X^2})} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i \\  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}\\
&amp; = \frac{\sigma^2}{n^2(\bar{X}^2 - \bar{X^2})} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i \\  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}\\
&amp; = \frac{\sigma^2}{(n^2 - n)\widehat{\text{Var}}(X)} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i \\  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}\\  &amp; = \begin{bmatrix} \frac{\sigma^2\sum_{i=1}^n X_i^2}{(n^2 - n)\widehat{\text{Var}}(X)}  &amp; -\frac{\sigma^2\sum_{i=1}^n X_i}{(n^2 - n)\widehat{\text{Var}}(X)} \\  -\frac{\sigma^2\sum_{i=1}^n X_i}{(n^2 - n)\widehat{\text{Var}}(X)} &amp; \frac{\sigma^2n}{(n^2 - n)\widehat{\text{Var}}(X)} \end{bmatrix}\\
\text{Var}\left(\hat{\beta}_{1,OLS}\mid \mathbb{X}\right) &amp; = \frac{\sigma^2\sum_{i=1}^n X_i^2}{(n^2 - n)\widehat{\text{Var}}(X)}\\
\text{Var}\left(\hat{\beta}_{2,OLS}\mid \mathbb{X}\right) &amp; =\frac{\sigma^2n}{(n^2 - n)\widehat{\text{Var}}(X)}
\end{align*}\]</span> What happens to these variances as we change the variance of the error <span class="math inline">\(\sigma^2\)</span>, and the values of <span class="math inline">\(x_i\)</span> change? Instead of finding the signs of various taking partial derivatives, let’s graph some examples. First let’s see what happens when we hold <span class="math inline">\(\mathbf{X}\)</span> constant but increase <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot58_211778829a99a1d98e6a5f8510aa01c2">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="sc">-</span><span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, <span class="at">group =</span> <span class="st">"Low σ^2"</span>) </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="sc">-</span><span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">5</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, <span class="at">group =</span> <span class="st">"High σ^2"</span>) </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot58" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot58-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.8: The larger the variance of the error term, the larger the standard errors associated with our OLS estimates.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Each graph contains the estimates linear model, with variance illustrated by the gray envelope around the lines. The width of this envelope at the red line <span class="math inline">\(x=0\)</span> corresponds to the variance <span class="math inline">\(\hat{\beta}_{1,OLS}\)</span>, while the degree to which the width of the envelope varies along the <span class="math inline">\(x\)</span>-axis corresponds to the variance of <span class="math inline">\(\hat{\beta}_{1,OLS}\)</span>. We can see that the variance of both estimators decreases when <span class="math inline">\(\sigma^2\)</span> decreases. As the uncertainty about the stochastic element of the model <span class="math inline">\(\varepsilon_i\)</span> decreases, we become more confident in our estimates. Now consider what happens when we change the variance of <span class="math inline">\(x\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot59_34b89dd0cee657efa8c1febf93f0198f">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="dv">4</span>,<span class="dv">6</span>), </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e,</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"Low Variance of X"</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">5</span>,<span class="dv">15</span>),</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"High Variance of X"</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">fullrange=</span><span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot59" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot59-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.9: The variance of the OLS estimator decreases as the variance of independent variables increases</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The more variance we have in our regressors, the less variance our estimator exhibits. Essentially, the variance in observations provides more information about the relationship between the dependent and independent variables, so we get better estimates. Finally consider how the variance changes as the location of our data change relative to the y-axis</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="ols_cache/html/fig-plot510_1a1a833d1250eb016dbfa93e1ca2ab23">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"X Near Origin"</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="dv">10</span>,<span class="dv">12</span>), </span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"X Far from Origin"</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">fullrange=</span><span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot510" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot510-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.10: The proximity of the regressors to origin affects the variance of the intercept estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The closer our observations are to the y-axis the better out estimates of the intercept are.</p>
</div>
<p>If we forget the intercept term for a moment, then we can think <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span> roughly as the amount of variance in our regressors. The variance in regressors amounts to information about <span class="math inline">\(\boldsymbol{\beta}\)</span>. The more variance/information we have about <span class="math inline">\(\mathbf{X}\)</span>, the better our estimates will be.</p>
</section>
<section id="gauss-markov-theorem" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="gauss-markov-theorem"><span class="header-section-number">5.6</span> Gauss-Markov Theorem</h2>
<p>How do we know that there aren’t any other estimators that may be better than OLS? Recall from Section @ref(finite-sample-properties-of-estimators) we discussed the concept of a MVUE – an unbiased estimator which is more efficient (has lower variance) than all other unbiased estimators. Finding a MVUE is difficult without additional assumptions about the unbiased estimators. With one such assumption, we do have that OLS is a MVUE among all unbiased estimators satisfying this assumption. This result is known as the Gauss-Markov theorem, and tells us that the OLS estimator has the minimum variance among all linear unbiased estimators. For the remainder of our discussion of the Gauss-Markov theorem, we will assume that our linear model satisfies: <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span> , <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid\mathbb{X}\right]=\mathbf{0}\)</span>,and <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid\mathbb{X}\right] = \sigma^2\mathbf I\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>In the context of the linear model, a linear estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> will take the form <span class="math inline">\(\hat{\boldsymbol{\beta}} = \mathbf C\mathbf{Y}+\mathbf{D}\)</span> for some matrix <span class="math inline">\(\mathbf C\)</span> (which may be a function of <span class="math inline">\(\mathbb{X}\)</span>). In the case of <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span>, <span class="math inline">\(\mathbf C = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\)</span>. We will denote a general linear unbiased estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span> as <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span>. For now, let’s condition on the random matrix <span class="math inline">\(\mathbb{X}\)</span>. To restrict our attention to unbiased linear estimators, <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> must satisfy: <span class="math display">\[\begin{align*}
&amp;\text{E}\left[\tilde{\boldsymbol{\beta}} \mid \mathbb{X}\right] = \boldsymbol{\beta}\\
\implies &amp; \text{E}\left[\mathbf C\mathbf{Y}\mid \mathbb{X}\right] = \boldsymbol{\beta}\\
\implies &amp; \text{E}\left[\mathbf C\mathbb{X}\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \boldsymbol{\beta}&amp; (\mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon})\\
\implies &amp; \mathbf C\mathbb{X}\underbrace{\text{E}\left[\boldsymbol{\beta}\mid \mathbb{X}\right]}_{\boldsymbol{\beta}} + \mathbf C\underbrace{\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]}_{\mathbf{0}} = \boldsymbol{\beta}\\
\implies &amp;  \mathbf C\mathbb{X}\boldsymbol{\beta}= \boldsymbol{\beta}\\
\implies &amp; \mathbf C\mathbb{X}= \mathbf I
\end{align*}\]</span> The variance of <span class="math inline">\(\tilde{\boldsymbol{\beta}}\)</span> can be calculated using the same exact steps we took to calculate the variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_\text{OLS}\)</span>: <span class="math display">\[\begin{align*}
\text{Var}\left(\tilde{\boldsymbol{\beta}}\mid \mathbb{X}\right) &amp; = \text{E}\left[ \left(\tilde{\boldsymbol{\beta}} - \text{E}\left[ \tilde{\boldsymbol{\beta}} \right]\right) \left(\tilde{\boldsymbol{\beta}} - \text{E}\left[\tilde{\boldsymbol{\beta}}\right]\right)'\mid \mathbb{X}\right]\\
&amp; = \text{E}\left[ \left(\tilde{\boldsymbol{\beta}} - \boldsymbol{\beta}\right) \left(\tilde{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)'\mid \mathbb{X}\right] &amp; (\tilde{\boldsymbol{\beta}} \text{ unbiased})\\
&amp; = \text{E}\left[ \left[(\mathbf C\mathbb{X}\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right] \left[(\mathbf C\mathbb{X}\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right]'\mid \mathbb{X}\right] &amp; (\tilde{\boldsymbol{\beta}} = \mathbf C\mathbb{X}\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon})\\
&amp; = \text{E}\left[ \left[(\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right] \left[(\boldsymbol{\beta}+ \mathbf C\boldsymbol{\varepsilon}) - \boldsymbol{\beta}\right]'\mid \mathbb{X}\right]  &amp; (\mathbf{C}\mathbf{X}= \mathbf I)\\
&amp; = \text{E}\left[ \left[\mathbf C\boldsymbol{\varepsilon}\right] \left[\mathbf C\boldsymbol{\varepsilon}\right]'\mid \mathbb{X}\right] \\
&amp; = \text{E}\left[\mathbf C\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mathbf C'\mid \mathbb{X}\right]\\
&amp; = \mathbf C\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]\mathbf C'\\
&amp;  = \sigma^2\mathbf C\mathbf C' &amp; (\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\right] = \sigma^2\mathbf I)
\end{align*}\]</span></p>
<p>Our goal is to show that: <span class="math display">\[\hat{\boldsymbol\beta}_\text{OLS} = \mathop{\mathrm{argmin}}_{\tilde{\boldsymbol{\beta}}} \text{Var}\left(\tilde{\boldsymbol{\beta}} \mid \mathbb{X}\right).\]</span> Write <span class="math inline">\(\mathbf{C}= (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}\)</span> for some non-zero matrix <span class="math inline">\(\mathbf{D}\)</span>. The requirement that <span class="math inline">\(\mathbf{C}\mathbb{X}= \mathbf I\)</span> implies that: <span class="math display">\[ [(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}]\mathbb{X}= \mathbf I \implies\underbrace{(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{X}}_{\mathbf I} + \mathbf{D}\mathbf{X}= \mathbf I \implies \mathbf{D}\mathbb{X}= \mathbf{0}\]</span> Note that <span class="math display">\[ \tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{Y}= [(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}]\mathbf{y}= (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\mathbf{Y}+ \mathbf{D}\mathbf{y}= \hat{\boldsymbol\beta}_\text{OLS} + \mathbf{D}\mathbf{Y},\]</span> so <span class="math inline">\(\tilde{\boldsymbol{\beta}} = \hat{\boldsymbol\beta}_\text{OLS} \)</span> when <span class="math inline">\(\mathbf{D}= \mathbf{0}\)</span>. Our optimization problem becomes <span class="math display">\[\begin{align*}
\mathop{\mathrm{argmin}}_{\mathbf{D}} \text{Var}\left(\tilde{\boldsymbol{\beta}} \mid \mathbb{X}\right) &amp; = \mathop{\mathrm{argmin}}_{\mathbf{D}} \sigma^2\mathbf C\mathbf C'\\
&amp; =  \mathop{\mathrm{argmin}}_{\mathbf{D}} \sigma^2[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}][(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}]'\\
&amp; =  \mathop{\mathrm{argmin}}_{\mathbf{D}} \sigma^2[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}+ \mathbf{D}][\mathbb{X}'(\mathbb{X}'\mathbb{X})^{-1} + \mathbf{D}']\\
&amp; = \mathop{\mathrm{argmin}}_{\mathbf{D}} \sigma^2[\underbrace{(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\mathbb{X}'}_{\mathbf I}(\mathbb{X}'\mathbb{X})^{-1} + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbf{D}' + \underbrace{\mathbf{D}\mathbb{X}}_\mathbf{0}(\mathbb{X}'\mathbb{X})^{-1} + \mathbf{D}\mathbf{D}']\\
&amp; = \mathop{\mathrm{argmin}}_{\mathbf{D}} \sigma^2[(\mathbb{X}'\mathbb{X})^{-1} + (\mathbb{X}'\mathbb{X})^{-1}\underbrace{(\mathbb{X}\mathbf{D})}_\mathbf{0}' +  \mathbf{D}\mathbf{D}'] &amp; (\mathbb{X}'\mathbf{D}' = (\mathbf{X}\mathbf{D})')\\
&amp; = \sigma^2(\mathbb{X}'\mathbb{X})^{-1} + \sigma\mathbf{D}\mathbf{D}'.
\end{align*}\]</span> This variance is minimized when <span class="math inline">\(\mathbf{D}= \mathbf{0}\)</span>,<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> so <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is the most efficient unbiased linear estimator for any fixed <span class="math inline">\(\mathbb{X}=\mathbf{X}\)</span>. This holds for all realizations <span class="math inline">\(\mathbb{X}=\mathbf{X}\)</span>, so it will hold unconditionally as well.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.2 (Gauss-Markov Theorem) </strong></span>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2} \in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right] = \mathbf{0}\)</span>, and <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid\mathbb{X}\right] = \sigma^2\mathbf I\)</span>. Then <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is the <span style="color:red"><strong><em>best linear unbiased estimator (BLUE)/minimum variance linear unbiased estimator (MVLUE)</em></strong></span>.</p>
</div>
<p>The Gauss-Markov theorem is one of the major justifications for estimating linear models with <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. With estimation thoroughly treated, we can now consider making inferences about <span class="math inline">\((\boldsymbol{\beta},\sigma^2)\)</span> for <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2}\in\mathcal P_\text{LM}\)</span>.</p>
</section>
<section id="asymptotic-distribution-of-the-ols-estimator" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="asymptotic-distribution-of-the-ols-estimator"><span class="header-section-number">5.7</span> Asymptotic Distribution of the OLS Estimator</h2>
<p>We know that our estimator is consistent and the BLUE, but how does its distribution behave? It turns out that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is root-n CAN under weaker assumptions than those required for the Gauss-Markov theorem. Before showing this in earnest, let’s look at a special case of the linear model.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.16 (Gaussian Linear Model) </strong></span>Suppose <span class="math inline">\(P_\text{LM}\)</span> satisfies the Gauss-Markov assumptions, <em>in addition</em> to the assumption that <span class="math inline">\(\boldsymbol{\varepsilon}\mid\mathbb{X}\sim N(\mathbf{0},\sigma^2\mathbf I)\)</span> (which is equivalent to <span class="math inline">\(\varepsilon_i\overset{iid}{\sim}N(0,\sigma^2)\)</span> because we have assumed spherical errors). This model is sometimes referred to as the <strong><em>Gaussian linear model</em></strong>. A common way of writing this model is <span class="math inline">\(\mathbf{Y}\mid\mathbb{X}\sim N(\mathbb{X}\boldsymbol{\beta},\sigma^2\mathbf I)\)</span>, which emphasizes the fact that <span class="math inline">\(\text{E}\left[\mathbf{Y}\mid\mathbb{X}\right] = \mathbb{X}\boldsymbol{\beta}\)</span>. It’s quite easy to derive the distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> for this model. We won’t even need to approximate the distribution via asymptotics! Using the properties of the multivariate distribution, we have: <span class="math display">\[\begin{align*}
&amp;\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}= [\boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol{\varepsilon}] - \boldsymbol{\beta}\\
\implies &amp; \hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}= (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol{\varepsilon}\\
\implies &amp; \hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}\sim N(\mathbf{0}, (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\sigma^2\mathbf I[(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}]' ) &amp; (\mathbf A \boldsymbol{\varepsilon}\sim N(\mathbf{0}, \mathbf A \sigma^2 \mathbf I \mathbf A'))\\
\implies &amp; \hat{\boldsymbol\beta}_\text{OLS} \sim N(\boldsymbol{\beta}, \sigma^2\underbrace{(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\mathbb{X}'}_{\mathbf I}(\mathbb{X}'\mathbb{X})^{-1})\\
\implies &amp; \hat{\boldsymbol\beta}_\text{OLS} \mid \mathbb{X}\sim N(\boldsymbol{\beta}, \sigma^2(\mathbb{X}'\mathbb{X})^{-1})
\end{align*}\]</span></p>
<p>To verify this, we can simulate 50,000 estimates for the model Gaussian linear model where <span class="math inline">\(\beta = [2,4]'\)</span>, and <span class="math inline">\(\sigma^2 = 1\)</span>. We’ll pick a modest sample size of <span class="math inline">\(n=5\)</span> to emphasize that this is the precise distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>, not just the asymptotic distribution. Because this distribution is conditional on <span class="math inline">\(\mathbb{X}\)</span>, we’ll fix the realization <span class="math inline">\(\mathbb{X}=\mathbf{X}\)</span> over the simulations.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-22_a07ec7e6dcd6fdccdfefa7f39c379a78">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> N_sim)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>  store[k,] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/fig-plot511_0b5780610001c45393f81fa97fb8547f">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(store[,<span class="dv">1</span>], store[,<span class="dv">2</span>]),</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"β1 Estimate"</span>, <span class="dv">50000</span>), <span class="fu">rep</span>(<span class="st">"β2 Estimate"</span>, <span class="dv">50000</span>))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">1</span>]), <span class="fu">max</span>(store[,<span class="dv">1</span>]), <span class="at">length =</span> <span class="dv">1000</span>), <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">2</span>]) ,<span class="fu">max</span>(store[,<span class="dv">2</span>]), <span class="at">length =</span> <span class="dv">1000</span>)),</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"β1 Estimate"</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="st">"β2 Estimate"</span>, <span class="dv">1000</span>))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(group <span class="sc">==</span> <span class="st">"β1 Estimate"</span>, <span class="fu">dnorm</span>(x, beta[<span class="dv">1</span>], <span class="fu">sqrt</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)[<span class="dv">1</span>,<span class="dv">1</span>])), <span class="fu">dnorm</span>(x, beta[<span class="dv">2</span>], <span class="fu">sqrt</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)[<span class="dv">2</span>,<span class="dv">2</span>]))))</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">data =</span> df1, <span class="fu">aes</span>(x, <span class="at">y =</span> ..density..), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> df2, <span class="fu">aes</span>(x,y), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span> </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">1</span>]) ,<span class="fu">max</span>(store[,<span class="dv">1</span>]), <span class="at">length =</span> <span class="dv">1000</span>),</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y =</span> <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">2</span>]) ,<span class="fu">max</span>(store[,<span class="dv">2</span>]), <span class="at">length =</span> <span class="dv">1000</span>),</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>                  )</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>df1<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df1, beta, <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X))</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> store[,<span class="dv">1</span>], </span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> store[,<span class="dv">2</span>]</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> df2, <span class="fu">aes</span>(x,y), <span class="at">size =</span> <span class="fl">0.001</span>)<span class="sc">+</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">data =</span> df1, <span class="fu">aes</span>(x,y, <span class="at">z=</span> p), <span class="at">bins =</span> <span class="dv">14</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"β1 Estimate"</span>) <span class="sc">+</span></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"β2 Estimate"</span>) <span class="sc">+</span> </span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">3.6</span>,<span class="fl">4.4</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot511" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ols_files/figure-html/fig-plot511-1.png" class="img-fluid figure-img" width="1056"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.11: The simulated marginal density of our estimators and their simulated joint density (along with the true underlying distributions shown in red)</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>If we abandon the assumption that <span class="math inline">\(\boldsymbol{\varepsilon}\mid\mathbf{X}\sim N(\mathbf{0},\sigma^2\mathbf I)\)</span> we are back to the standard (Gauss-Markov) linear model. All the assumptions about or model take the form of moment conditions, and not specific distributions, so we will not be able to calculate the exact distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> in general. Fortunately we can use our asymptotic toolkit to find the limiting distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>.</p>
<p>The overwhelming majority of the time, estimators will be root-n consistent, so the best starting point of finding the asymptotic distribution of an estimator is by first calculating <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta})\)</span>. In the case of the OLS estimator: <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}) &amp; = n^{1/2}[\boldsymbol{\beta}+ (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol{\varepsilon}] - \boldsymbol{\beta}\\
&amp; = \sqrt{n}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\boldsymbol{\varepsilon}\\
&amp; = \sqrt{n}\left(\frac{\mathbb{X}'\mathbb{X}}{n}\right)^{-1}\left(\frac{\mathbb{X}'\boldsymbol{\varepsilon}}{n}\right)\\
&amp; = \left(\frac{\mathbb{X}'\mathbb{X}}{n}\right)^{-1}\left(\frac{\mathbb{X}'\boldsymbol{\varepsilon}}{\sqrt{n}}\right)\\
&amp; = \left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}\left(\frac{1}{\sqrt n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)
\end{align*}\]</span> Whether you want to show the result using the matrix form of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> or the form which is sums of vector is a matter of preference. Regardless, the first term will converge to its population counterpart <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\)</span>. The second term is a bit more interesting. We have <span class="math display">\[\left(\frac{1}{\sqrt n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right) = \sqrt n \left(\frac{1}{ n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i - \mathbf{0}\right) =  \sqrt n \left(\frac{1}{ n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i - \text{E}\left[\mathbf{X}_i'\varepsilon_i\right]\right),\]</span> but this is the precise expression which the CLT applies to, so we have: <span class="math display">\[\begin{align*}
\left(\frac{1}{\sqrt n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right) &amp;\overset{d}{\to}N(\text{E}\left[\mathbf{X}_i'\varepsilon_i\right], \text{Var}\left(\textstyle \sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)/n)\\
&amp; \overset{d}{\to}N(\mathbf{0}, \textstyle \sum_{i=1}^n\sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right]/n)\\
&amp; \overset{d}{\to}N(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right])
\end{align*}\]</span> because <span class="math inline">\(\mathbf{X}_i'\varepsilon_i\)</span> is an iid sample. Using this fact along with Slutsky’s theorem and the LLN gives us the distribution of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}) &amp; =\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}}_{\overset{p}{\to}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}}\underbrace{\left(\frac{1}{\sqrt n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)}_{\overset{d}{\to}N(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right])}\\
&amp; \overset{d}{\to}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}N(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right])\\
&amp; = N(\mathbf{0}, \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right][\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}]')\\
&amp; = N(\mathbf{0}, \text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right]\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1})\\
&amp; = N(\mathbf{0}, \sigma^2\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1})
\end{align*}\]</span></p>
<p>If we express the variance in terms of the random matrix <span class="math inline">\(\mathbb{X}\)</span> instead of the random vector of covariates using the equality <span class="math inline">\(\text{E}\left[\mathbf{X}'\mathbf{X}\right] = \text{E}\left[\mathbb{X}'\mathbb{X}\right]/n\)</span>, we have</p>
<p><span class="math display">\[\sqrt{n}(\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}) \overset{d}{\to}N(\mathbf{0}, \sigma^2(\text{E}\left[\mathbb{X}'\mathbb{X}\right]/n)^{-1})= N(\mathbf{0}, \sigma^2n\text{E}\left[\mathbb{X}'\mathbb{X}\right]^{-1}) \]</span></p>
<div id="asymols" class="theorem" name="Asymptotic Distribution of OLS">
<p>Suppose <span class="math inline">\(P_{\boldsymbol{\beta},\sigma^2}\in \mathcal P_\text{LM}\)</span> where <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\right] = \sigma^2\mathbf I\)</span>. Then <span class="math display">\[ \hat{\boldsymbol\beta}_\text{OLS} \overset{a}{\sim}N\left(\boldsymbol{\beta},\sigma^2\text{E}\left[\mathbb{X}'\mathbb{X}\right]^{-1}\right) = N\left(\boldsymbol{\beta}, \frac{\sigma^2}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\right). \]</span></p>
</div>
<p>We have stated the asymptotic distribution without conditioning on <span class="math inline">\(\mathbb{X}\)</span>, so <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\)</span> will be in terms of the expectation of <span class="math inline">\(\mathbb{X}\)</span> opposed to some fixed <span class="math inline">\(\mathbb{X}\)</span>. We only appealed to the assumption <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\right] = \sigma^2\mathbf I\)</span> to simplify the asymptotic variance, so in the event <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\right] \neq \sigma^2\mathbf I\)</span>, our estimator will still be root-n CAN, albeit with a different asymptotic variance (we will show this in Section @ref(generalized-least-squares)). Depending on the level of technical rigor the assumptions which give this result may differ. I followed the derivation provided by <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, but others will delineate regularity conditions on <span class="math inline">\(\mathbb{X}\)</span> so it is “well-behaved”, or impose assumptions about the behavior of errors as a martingale. These assumptions tend to be rather weak and will hold in many practical applications. We also could extend this result to data which are not independent using the Lindeberg-Feller CLT.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.17 </strong></span>Consider the case where <span class="math inline">\(\beta = 2\)</span>, <span class="math inline">\(\varepsilon_i \overset{iid}{\sim}\text{Uni}(-1,1)\)</span>, <span class="math inline">\(X \sim \text{Uni}(-5,5)\)</span>, <span class="math inline">\(\varepsilon\perp X\)</span>, and <span class="math inline">\(Y= 2X + \varepsilon\)</span>. By properties of the uniform distribution, <span class="math display">\[\begin{align*}
\sigma^2 &amp;= \frac{1}{3},\\
\text{E}\left[X^2\right] &amp; = \frac{25}{3}.
\end{align*}\]</span> If simulate realizations of <span class="math inline">\(\hat\beta\)</span> for a sufficiently large <span class="math inline">\(n\)</span>, we should expect it to approximately follow a normal distribution with mean <span class="math inline">\(2\)</span> and variance: <span class="math display">\[\frac{\sigma^2}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1} =  \frac{1/3}{n} \text{E}\left[X^2\right]^{-1} = \frac{1}{3n}(25/3)^{-1} = \frac{1}{25n}.\]</span> Let’s perform 10,000 simulations for sample sizes <span class="math inline">\(n\in\{3,5,8,10\}\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-24_c02d2908de78a1bc6316ea77879a358a">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">estimate =</span> <span class="cn">NA</span>, <span class="at">n =</span> <span class="cn">NA</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">8</span>,<span class="dv">10</span>)) {</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">ncol =</span> <span class="dv">1</span>) </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> X <span class="sc">*</span> beta <span class="sc">+</span> e</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    store <span class="ot">&lt;-</span> <span class="fu">rbind</span>(store, <span class="fu">c</span>(<span class="fu">OLS</span>(y,X),n))</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As <span class="math inline">\(n\)</span> increases we should see the bias of our estimator shrink, and the simulated variance approach <span class="math inline">\(1/25n\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-25_ccb8b6594d7d328f5cc733f8257a272d">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: right;">n</th>
<th style="text-align: right;">Bias</th>
<th style="text-align: right;">Simulated Variance</th>
<th style="text-align: right;">Limiting Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">-0.0015484</td>
<td style="text-align: right;">0.0228532</td>
<td style="text-align: right;">0.0133333</td>
</tr>
<tr class="even">
<td style="text-align: right;">5</td>
<td style="text-align: right;">-0.0000008</td>
<td style="text-align: right;">0.0102583</td>
<td style="text-align: right;">0.0080000</td>
</tr>
<tr class="odd">
<td style="text-align: right;">8</td>
<td style="text-align: right;">0.0002803</td>
<td style="text-align: right;">0.0056143</td>
<td style="text-align: right;">0.0050000</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.0001717</td>
<td style="text-align: right;">0.0045340</td>
<td style="text-align: right;">0.0040000</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>More importantly (because we knew how to calculate the bias and asymptotic variance of <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> prior to deriving its limiting distribution), if we use our estimates to make a Q-Q plot, we see that as <span class="math inline">\(n\)</span> increases our estimates fit a normal distribution increasingly well.</p>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/unnamed-chunk-26_80b11412ed2b89ad905450ea26102fd8">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>store <span class="sc">%&gt;%</span> </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(n)) <span class="sc">%&gt;%</span> </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>n) <span class="sc">+</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="1056"></p>
</figure>
</div>
</div>
</div>
<p>Even for modest sample sizes such as <span class="math inline">\(n=10\)</span>, it’s clear that our estimates are approximately normally distributed.</p>
</div>
</section>
<section id="estimating-textavarlefthatboldsymbolbeta_textols-right" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="estimating-textavarlefthatboldsymbolbeta_textols-right"><span class="header-section-number">5.8</span> Estimating <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol\beta}_\text{OLS} \right)\)</span></h2>
<p>We’ve spent so much time considering the estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span>, and completely ignored the other parameter of our model – <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. In the case of the Gauss-Markov assumptions, <span class="math inline">\(\Sigma = \sigma^2\mathbf I\)</span>, so estimating <span class="math inline">\(\boldsymbol{\Sigma}\)</span> simplifies to estimating <span class="math inline">\(\sigma^2\)</span>.</p>
<p>A natural suggestion for the estimator would be<br>
<span class="math display">\[\frac{1}{n-1}\sum_{i=1}^n(e_i - \underbrace{\text{E}\left[e_i\right]}_0)^2 =   \frac{1}{n-1}\sum_{i=1}^ne_i^2 =\mathbf{e}'\mathbf{e}\]</span> for realizations <span class="math inline">\(\mathbf{e}\)</span> of the random variable <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>. This was our approach to calculating the standard error associated with the mean when we didn’t know the population variance, but it is a nonstarter in this case because we don’t observe <span class="math inline">\(\boldsymbol{\varepsilon}\)</span>. So right from the start, we need to think of a way to estimate <span class="math inline">\(\mathbf{e}\)</span>. The immediate candidate are the observed errors associated with the estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>. This estimator for <span class="math inline">\(\mathbf{e}\)</span> can be defined as</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf{e}} &amp; = \mathbf{Y}- \mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} ,\\
&amp; = \mathbf{Y}- \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbf{X}'\mathbf{Y},\\
&amp; = (\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}')\mathbf{Y}.
\end{align*}\]</span></p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.12 </strong></span>The <span style="color:red"><strong><em>(least squares) residuals</em></strong></span> associated with the estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> are defined as <span class="math display">\[ \hat{\mathbf{e}}(\mathbf{Y},\mathbb{X}) = \mathbf{Y}- \mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} = \mathbb M\mathbf{Y},\]</span> where <span class="math inline">\(\mathbb M = \mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\)</span>. The estimated residuals associated with observations <span class="math inline">\((\mathbf{y}, \mathbf{X})\)</span> is <span class="math display">\[ \hat{\mathbf{e}}(\mathbf{y},\mathbf{X}) =\mathbf{y}- \mathbf{X}\hat{\mathbf{b}}_\text{OLS}= \mathbf M\mathbf{y}.\]</span></p>
</div>
<p>To estimate <span class="math inline">\(\text{Var}\left(\boldsymbol{\varepsilon}\mid \mathbb{X}\right) = \text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]\)</span>, let’s appeal to the analogy principle and inspect its sample counterpart, only do so using the residuals <span class="math inline">\(\hat{\mathbf{e}}\)</span>. To do this, we’ll need a few quick results, the proofs of which are applications of linear algebra and can be found in <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>.</p>
<div id="lem-" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 5.1 (Properties of Residuals) </strong></span>Let the estimator <span class="math inline">\(\hat{\mathbf{e}}\)</span> be the least squared residuals. Then:</p>
<ol type="1">
<li><span class="math inline">\(\mathbb M\)</span> is symmetric (<span class="math inline">\(\mathbb M'=\mathbb M\)</span>) and idempotent (<span class="math inline">\(\mathbb M^2=\mathbb M\)</span>). Together these imply that <span class="math inline">\(\mathbb M'\mathbb M= \mathbb M\)</span>.</li>
<li><span class="math inline">\(\text{tr}(\boldsymbol{\varepsilon}'\mathbb M\boldsymbol{\varepsilon}) =\text{tr}(\mathbb M\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon})\)</span> where <span class="math inline">\(\text{tr}(\mathbf A)= \sum_{i=1}^n \text{diag}(\mathbf A)\)</span></li>
</ol>
</div>
<p>The matrix <span class="math inline">\(\mathbb M\)</span> satisfies <span class="math inline">\(\mathbb M\mathbb{X}=\mathbf{0}\)</span>: <span class="math display">\[\mathbb M\mathbb{X}=(\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}')\mathbb{X}=\mathbb{X}-  \mathbb{X}\underbrace{(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{X}}_{\mathbf I} = \mathbf{0}.\]</span></p>
<p>The sample analog of <span class="math inline">\(\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\)</span>, using residuals as estimates for <span class="math inline">\(\mathbf{e}\)</span>, is:</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf{e}}'\hat{\mathbf{e}} &amp;= \mathbf{Y}\mathbb M'\mathbb M\mathbf{Y}\\
&amp; = [\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} + \boldsymbol{\varepsilon}]\mathbb M[\mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} + \boldsymbol{\varepsilon}] &amp; (\mathbf{Y}= \mathbb{X}\hat{\boldsymbol\beta}_\text{OLS} + \boldsymbol{\varepsilon},\ \mathbb M'\mathbb M =\mathbb M)\\
&amp; = \boldsymbol{\varepsilon}' \mathbb M\boldsymbol{\varepsilon}&amp; (\mathbb M\mathbb{X}=\mathbf{0})\\
\end{align*}\]</span></p>
<p>The expectation of this estimator is <span class="math display">\[\begin{align*}
\text{E}\left[\hat{\mathbf{e}}'\hat{\mathbf{e}}\right] &amp;= \text{E}\left[\text{E}\left[\hat{\mathbf{e}}'\hat{\mathbf{e}} \mid \mathbb{X}\right]\right] \\
&amp; = \text{E}\left[\text{E}\left[\boldsymbol{\varepsilon}' \mathbb M\boldsymbol{\varepsilon}\mid \mathbb{X}\right]\right]\\
&amp; = \text{E}\left[\text{E}\left[\text{tr}(\boldsymbol{\varepsilon}' \mathbb M\boldsymbol{\varepsilon}) \mid \mathbb{X}\right]\right] &amp; (\boldsymbol{\varepsilon}' \mathbb M\boldsymbol{\varepsilon}\text{ is a scalar})\\
&amp; = \text{E}\left[\text{E}\left[\text{tr}( \mathbb M\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}) \mid \mathbb{X}\right]\right] &amp; (\text{tr}(\boldsymbol{\varepsilon}'\mathbb M\boldsymbol{\varepsilon}) =\text{tr}(\mathbb M\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}))\\
&amp; = \text{E}\left[\mathbb M(\text{tr}\text{E}\left[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}\mid \mathbb{X}\right])\right] &amp; (\mathbb M\text{ is a function of }\mathbb{X})\\
&amp; = \text{tr}(\mathbb M \sigma^2 \mathbf I) \\
&amp; = \sigma^2 \text{tr}(\mathbb M) \\
&amp; = \sigma^2 \text{tr}(\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}')\\
&amp; = \sigma^2 \text{tr}(\mathbf I) - \text{tr}((\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}\mathbb{X}')\\
&amp; = \sigma^2(n-K)
\end{align*}\]</span></p>
<p>Much like the estimator <span class="math inline">\(n^{-1}\sum_{i=1}^n(X_i - \bar X)^2\)</span> for some <span class="math inline">\(\text{Var}\left(X\right)\)</span>, our estimator for the variance of our residuals is biased. If we correct for this bias, we have <span class="math display">\[ S^2 = \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n-K}.\]</span> This correction follows from the same intuition behind Bessel’s correction. Bessel’s correction accounted for the estimation of population variance have two steps: first we estimate <span class="math inline">\(\bar X\)</span> because we do not know <span class="math inline">\(\mu = \text{E}\left[X\right]\)</span>, and then we use this intermediate estimate to calculate the sample variance. We’re doing precisely the same thing when estimating the variance of our errors. It requires an intermediate step where we estimate <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>, and then we use our estimated value to calculate <span class="math inline">\(\hat{\mathbf{e}}'\hat{\mathbf{e}}/(n-K)\)</span>. The estimator <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is a <span class="math inline">\(K-\)</span>vector, so we need to correct for each dimension.</p>
<div id="prp-olsvar" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.8 (Estimation of OLS Variance) </strong></span>Define the estimator <span class="math display">\[S^2 =  \frac{\hat{\mathbf{e}}'\hat{\mathbf{e}}}{n-K}\]</span> in the context of the classic linear model. Then:</p>
<ol type="1">
<li><span class="math inline">\(S^2\)</span> is an unbiased for <span class="math inline">\(\text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right) = \sigma^2\)</span>.</li>
<li><span class="math inline">\(S^2\)</span> is a consistent estimator <span class="math inline">\(\text{Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right) = \sigma^2\)</span>.</li>
<li>The estimator <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ) = S^2(\mathbb{X}'\mathbb{X})^{-1}\)</span> is a consistent estimator for <span class="math inline">\({\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ) = \sigma^2\text{E}\left[\mathbb{X}'\mathbb{X}\right]^{-1}\)</span></li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<ol type="1">
<li><p>This follows from our derivation of the estimator: <span class="math display">\[\text{E}\left[S^2\right] = \text{E}\left[\hat{\mathbf{e}}'\hat{\mathbf{e}}\right]/(n-K) = [\sigma^2/(n-K)]/(n-K) = \sigma^2.\]</span></p></li>
<li><p>We have: <span class="math display">\[\begin{align*}
S^2 &amp; = \text{E}\left[\hat{\mathbf{e}}'\hat{\mathbf{e}}\right]/(n-K)\\
&amp; = \frac{1}{n-K}\text{E}\left[\boldsymbol{\varepsilon}'\mathbb M\boldsymbol{\varepsilon}\right]\\
&amp; = \frac{1}{n-k}[\boldsymbol{\varepsilon}'(\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}')\boldsymbol{\varepsilon}]\\
&amp; = \frac{1}{n-k}[\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}- \boldsymbol{\varepsilon}' \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\boldsymbol{\varepsilon}]\\
&amp; = \frac{n}{n-k}\left[\frac{\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}}{n} - \frac{\boldsymbol{\varepsilon}' \mathbb{X}}{n}\frac{(\mathbb{X}'\mathbb{X})^{-1}}{n}\frac{\mathbb{X}'\boldsymbol{\varepsilon}}{n}\right]\\
&amp; = \underbrace{\frac{n}{n-k}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^n \varepsilon_i^2}_{\overset{p}{\to}\sigma^2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\varepsilon_i\mathbf{X}_i\right)}_{\overset{p}{\to}\mathbf{0}}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\mathbf{X}_i\right)^{-1}}_{\overset{p}{\to}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\mathbf{X}_i'\varepsilon_i\right)}_{\overset{p}{\to}\mathbf{0}} \Bigg] &amp; (\text{LLN})\\
&amp; \overset{p}{\to}1(\sigma^2 - \mathbf{0}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}\mathbf{0}) &amp; (\text{Slutsky's theorem})\\
&amp; = \sigma^2
\end{align*}\]</span></p></li>
<li><p>We can now use the fact that <span class="math inline">\(S^2\overset{p}{\to}\sigma^2\)</span> along with Slutsky’s theorem:</p></li>
</ol>
<p><span class="math display">\[ \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ) = \underbrace{S^2}_{\overset{p}{\to}\sigma^2}[\underbrace{(\mathbb{X}'\mathbb{X})}_{\overset{p}{\to}n\text{E}\left[\mathbf{X}'\mathbf{X}\right]}]^{-1} \overset{p}{\to}\sigma^2[n\text{E}\left[\mathbf{X}'\mathbf{X}\right]]^{-1}=\frac{\sigma^2}{n}\text{E}\left[\mathbf{X}'\mathbf{X}\right]^{-1}={\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ). \]</span></p>
</div>
</section>
<section id="basic-model-selection-and-inference" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="basic-model-selection-and-inference"><span class="header-section-number">5.9</span> Basic Model Selection and Inference</h2>
<p>We’ve been operating under the assumption that we know the true model <span class="math inline">\(\mathcal P_\text{LM}\)</span>, but in reality knowing this is impossible. In fact, a common aphorism in statistics is that “all models are wrong”, because the world is too complex to systematically describe any phenomenon. This is especially true of the social sciences. Fortunately, the full aphorism is “all models are wrong…but some are useful.” Even if models are approximations of reality, they offer insights into the world. So how do we pick the right one?</p>
<p>In the context of <span class="math inline">\(\mathcal P_\text{LM}\)</span>, this question amounts to considering the random vector of regressors <span class="math inline">\(\mathbf{X}\)</span>. Even if our model is founded in rigorous economic theory, it still may be unclear which independent variables are pertinent. In Example @ref(exm:car), we considered a model where an agent <span class="math inline">\(i\)</span> got utility <span class="math inline">\(u_{ij}\)</span> from purchasing a car <span class="math inline">\(j\)</span>, assuming that their utility function took the form <span class="math inline">\(u_{ij}=\mathbf{X}_{ij}\boldsymbol{\beta}+ \varepsilon_i\)</span> for a vector of vehicle and consumer attributes <span class="math inline">\(\mathbf{X}_{ij}\)</span> where <span class="math inline">\(\varepsilon_i\)</span> corresponds to heterogeneity. It is up to us to determine which variables to include in <span class="math inline">\(\mathbf{X}_{ij}\)</span>. Attributes likes vehicle price, consumer location, whether a car is new or used, and model year of the car likely affect a consumers utility. But what about things like car color, technical specifications like a vehicles torque? There is no cut and dry answer to this, hence the black hole that is literature regrading model select. For now, we will take a basic approach to model selection rooted in methods introduced in @ref(hypothesis-testing).</p>
<p>Consider two models <span class="math inline">\(\mathcal P_\text{LM}\)</span> and <span class="math inline">\(\mathcal P_\text{LM}'\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathcal P_\text{LM}&amp;: Y = \beta_0 + \beta_1 X_1 + \varepsilon\\
\mathcal P_\text{LM}'&amp;: Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon
\end{align*}\]</span></p>
<p>These models are referred to as <strong><em>nested models</em></strong>, because the parameter space corresponding to <span class="math inline">\(\mathcal P_\text{LM}\)</span> is a subset of the parameter space corresponding to <span class="math inline">\(\mathcal P_\text{LM}'\)</span>. If we are tasked with choosing between these two models, we can estimate <span class="math inline">\(\mathcal P_\text{LM}'\)</span> and test the hypothesis <span class="math inline">\(H_0:\beta_2 = 0\)</span>. If we find sufficient evidence to reject this null hypothesis, than <span class="math inline">\(\beta_2\)</span> is likely nontrivial and should be included in the model, prompting us to favor <span class="math inline">\(\mathcal P_\text{LM}'\)</span>.</p>
<p>In general, suppose you want to test <span class="math inline">\(H_0:\beta_j = \beta_{0}\)</span>. We established that <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> is root-n CAN so we can use the <span class="math inline">\(t-\)</span>test discussed in <span class="quarto-unresolved-ref">?sec-tsting</span> to test this hypothesis. The statistic would be <span class="math display">\[ t = \frac{\hat\beta_{\text{OLS},j} - \beta_0}{\widehat{\text{se}}(\hat\beta_{\text{OLS},j})}.\]</span> This statistic relies on a consistent estimator <span class="math inline">\(\widehat{\text{se}}(\hat\beta_{\text{OLS},j})\)</span>, but this is given immediately by our consistent estimator <span class="math inline">\(\widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} ) = S^2(\mathbb{X}'\mathbb{X})^{-1}\)</span>.</p>
<p><span class="math display">\[\widehat{\text{se}}(\hat{\boldsymbol\beta}_\text{OLS} ) = \left[\text{diag}(S^2(\mathbb{X}'\mathbb{X})^{-1})\right]^{1/2}.\]</span> In the context of model selection, our default hypothesis is <span class="math inline">\(H_0:\beta_j = \beta_0\)</span> – is the addition of <span class="math inline">\(\beta_j\)</span> in our specification nontrivial? This is why if you run a regression using almost any statistical software, it will automatically report the results associated with the hypotheses <span class="math inline">\(H_0:\beta_j = 0\)</span> for each separate <span class="math inline">\(\beta_j\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.18 (Coding Exercise) </strong></span>Now that we know how to estimate and draw inferences about <span class="math inline">\(\boldsymbol{\beta}\)</span>, let’s return to our <code>OLS()</code> function which we first defined in Example @ref(exm:funref). Along with calculating <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span>, let’s calculate <span class="math inline">\(\widehat{\text{se}}(\hat{\boldsymbol\beta}_\text{OLS} )\)</span>, the <span class="math inline">\(t\)</span>-statistic associated with testing <span class="math inline">\(K\)</span> null hypotheses <span class="math inline">\(\beta_j = 0\)</span> (separately) at a significance level of <span class="math inline">\(\alpha = 0.05\)</span>, a 95% confidence interval for <span class="math inline">\(\boldsymbol{\beta}\)</span>, and the associated <span class="math inline">\(p-\)</span>value.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-27_f1cb7ae8e328c8dc60af1b64bf36c9ad">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X){</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">#determine dimensions, confirm estimate exists, perform OLS</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">==</span> <span class="dv">0</span>) {<span class="fu">stop</span>(<span class="st">"rank(X'X) &lt; K"</span>)}</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  hat_beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#use OLS estimates to calculate residuals and estimate SEs</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> hat_beta)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>  S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>  var_hat <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>  se_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(var_hat))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>  <span class="co">#t-stat, confidence intervals, p values</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> hat_beta<span class="sc">/</span>se_hat</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>  lower_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>  upper_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>  p_val <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pt</span>(t, n<span class="sc">-</span>K))</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">#combine everything into one table to return</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">cbind</span>(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(output) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"β"</span>, <span class="dv">1</span><span class="sc">:</span>K, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(output) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Estimate"</span>, <span class="st">"Std.Error"</span>, <span class="st">"t-Stat"</span>, <span class="st">"Lower 95% CI"</span>, <span class="st">"Upper 95% CI"</span>, <span class="st">"p-Value"</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll estimate the model given by <span class="math inline">\(\boldsymbol{\beta}= [2,5,4,3,6]'\)</span>, <span class="math inline">\(\mathbf{X}\sim N(\mathbf{0}, \mathbf I)\)</span>, <span class="math inline">\(n = 15\)</span>, and <span class="math inline">\(\varepsilon\overset{iid}{\sim}\text{Uni}(-1,1)\)</span>. Before we use our <code>OLS()</code> function, let’s see what R’s base function <code>lm()</code> (which stands for “linear model”) give us.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-28_39c9cc5401b4797a8a25dfa8711f2c54">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">6</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="fu">diag</span>(<span class="dv">1</span>,<span class="dv">4</span>))</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">#base R function, -1 to omit intercept which we added a column for</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">$</span>coefficients)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Estimate</th>
<th style="text-align: right;">Std. Error</th>
<th style="text-align: right;">t value</th>
<th style="text-align: right;">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">X1</td>
<td style="text-align: right;">2.068600</td>
<td style="text-align: right;">0.2034276</td>
<td style="text-align: right;">10.16873</td>
<td style="text-align: right;">1.4e-06</td>
</tr>
<tr class="even">
<td style="text-align: left;">X2</td>
<td style="text-align: right;">4.815065</td>
<td style="text-align: right;">0.2177389</td>
<td style="text-align: right;">22.11394</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X3</td>
<td style="text-align: right;">4.092429</td>
<td style="text-align: right;">0.2163911</td>
<td style="text-align: right;">18.91219</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X4</td>
<td style="text-align: right;">2.672951</td>
<td style="text-align: right;">0.2393738</td>
<td style="text-align: right;">11.16643</td>
<td style="text-align: right;">6.0e-07</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X5</td>
<td style="text-align: right;">6.393333</td>
<td style="text-align: right;">0.2220177</td>
<td style="text-align: right;">28.79651</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Now for our function.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-29_a44f01f7f5602a653303bd17b0f3accd">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">OLS</span>(y,X))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 4%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 13%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Estimate</th>
<th style="text-align: right;">Std.Error</th>
<th style="text-align: right;">t-Stat</th>
<th style="text-align: right;">Lower 95% CI</th>
<th style="text-align: right;">Upper 95% CI</th>
<th style="text-align: right;">p-Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">β1</td>
<td style="text-align: right;">2.068600</td>
<td style="text-align: right;">0.2034276</td>
<td style="text-align: right;">10.16873</td>
<td style="text-align: right;">1.669889</td>
<td style="text-align: right;">2.467311</td>
<td style="text-align: right;">1.4e-06</td>
</tr>
<tr class="even">
<td style="text-align: left;">β2</td>
<td style="text-align: right;">4.815065</td>
<td style="text-align: right;">0.2177389</td>
<td style="text-align: right;">22.11394</td>
<td style="text-align: right;">4.388305</td>
<td style="text-align: right;">5.241826</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">β3</td>
<td style="text-align: right;">4.092429</td>
<td style="text-align: right;">0.2163911</td>
<td style="text-align: right;">18.91219</td>
<td style="text-align: right;">3.668311</td>
<td style="text-align: right;">4.516548</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
<tr class="even">
<td style="text-align: left;">β4</td>
<td style="text-align: right;">2.672951</td>
<td style="text-align: right;">0.2393738</td>
<td style="text-align: right;">11.16643</td>
<td style="text-align: right;">2.203787</td>
<td style="text-align: right;">3.142115</td>
<td style="text-align: right;">6.0e-07</td>
</tr>
<tr class="odd">
<td style="text-align: left;">β5</td>
<td style="text-align: right;">6.393333</td>
<td style="text-align: right;">0.2220177</td>
<td style="text-align: right;">28.79651</td>
<td style="text-align: right;">5.958187</td>
<td style="text-align: right;">6.828480</td>
<td style="text-align: right;">0.0e+00</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The outputs are identical, so our function works perfectly!</p>
</div>
<p>If we want to test hypotheses jointly, we need to use the Wald test instead of the <span class="math inline">\(t\)</span>-test. For some hypothesis <span class="math inline">\(H_0:\mathbf h(\boldsymbol{\beta}) = \mathbf{0}\)</span> given by <span class="math inline">\(\mathbf h:\mathbb R^K\to\mathbb R^q\)</span>, our statistic is <span class="math display">\[W = \mathbf h(\hat{\boldsymbol\beta}_\text{OLS} )'  \left[\frac{\partial \mathbf h}{\partial\boldsymbol{\beta}}(\hat{\boldsymbol\beta}_\text{OLS} ) \widehat{\text{Avar}}(\hat{\boldsymbol\beta}_\text{OLS} )\frac{\partial \mathbf h}{\partial\boldsymbol{\beta}}(\hat{\boldsymbol\beta}_\text{OLS} )'\right]^{-1}\mathbf h(\hat{\boldsymbol\beta}_\text{OLS} ),\]</span> where <span class="math inline">\(W \overset{a}{\sim}\chi_q^2\)</span> under <span class="math inline">\(H_0\)</span>.</p>
<div id="exm-ftest" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.19 (F-Test) </strong></span>Consider the Gaussian linear model where all Gauss-Markov assumptions are met and <span class="math inline">\(\boldsymbol{\varepsilon}\sim N(\mathbf{0}, \sigma^2\mathbf I)\)</span>, along with the linear hypothesis that <span class="math inline">\(\mathbf H\boldsymbol{\beta}= \boldsymbol{\beta}_0\)</span> for a <span class="math inline">\(q\times K\)</span> matrix <span class="math inline">\(\mathbf H\)</span>. In this case the Wald statistic is <span class="math display">\[\begin{align*}
W &amp; = [\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'\left[\text{Var}\left(\mathbf H\hat{\boldsymbol\beta}_\text{OLS} \mid \mathbf{X}\right)\right]^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]\\
  &amp; = [\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[\mathbf H\text{Var}\left(\hat{\boldsymbol\beta}_\text{OLS} \mid \mathbb{X}\right)\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]\\
  &amp; = [\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[\sigma^2\mathbf H(\mathbb{X}'\mathbb{X})^{-1}\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0] &amp; \left(\text{Var}\left(\hat{\boldsymbol\beta}_\text{OLS} \mid \mathbb{X}\right) = \sigma^2 (\mathbb{X}'\mathbb{X})^{-1}\right)\\
  &amp; = \frac{[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[\mathbf H(\mathbb{X}'\mathbb{X})^{-1}\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]}{\sigma^2}
\end{align*}\]</span> We don’t know <span class="math inline">\(\sigma^2\)</span> so we cannot use this test statistic. Instead we define a new statistic <span class="math inline">\(F\)</span> which uses the estimator <span class="math inline">\(S^2\)</span>. <span class="math display">\[\begin{align*}
F &amp; = \frac{W}{q}\frac{\sigma^2}{S^2}\\
  &amp; = \frac{[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[\mathbf H(\mathbb{X}'\mathbb{X})^{-1}\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]}{\sigma^2}\cdot \frac{1}{q}\cdot \frac{\sigma^2}{S^2} \cdot \frac{n-K}{n-K}\\
  &amp; = \frac{[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[\sigma^2\mathbf H(\mathbb{X}'\mathbb{X})^{-1}\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]/q}{[(n-K)S^2/\sigma^2]/(n-K)}
\end{align*}\]</span></p>
<p>Under the assumptions that <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> is normally distributed, it can be shown that the denominator is distributed according to <span class="math inline">\(\frac{1}{n-K}\chi_{n-K}^2\)</span>. This along with the numerator being distributed according the <span class="math inline">\(\frac{1}{q}\chi_q^2\)</span> means <span class="math inline">\(F \sim F_{q,n-K}\)</span>. If we simplify the statistic <span class="math inline">\(F\)</span> we have <span class="math display">\[ F = \frac{[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]'[S^2\mathbf H(\mathbb{X}'\mathbb{X})^{-1}\mathbf H']^{-1}[\mathbf H\hat{\boldsymbol\beta}_\text{OLS} - \boldsymbol{\beta}_0]}{q} \sim F_{q,n-K},\]</span> and can test <span class="math inline">\(\mathbf H\boldsymbol{\beta}= \boldsymbol{\beta}_0\)</span> with the <strong><em><span class="math inline">\(F-\)</span>test</em></strong>.</p>
</div>
<p>The <span class="math inline">\(F-\)</span>test is just a special case of the Wald test where we can derive an exact distribution of our test statistic. Most statistical softwares will present the <span class="math inline">\(F-\)</span>stat associated with <span class="math inline">\(H_0:\boldsymbol{\beta}= \mathbf{0}\)</span> when you run a linear regression. This test corresponds to the hypothesis that our specification is completely wrong and none of the independent variables appear (jointly) relevant. Unfortunately, this test is not as robust as the Wald test, as it requires normal errors.</p>
<p>The next example is due to <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span> and shows how a Wald test can be used in the context of model selection.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.20 (Investement Model) </strong></span>Let <span class="math inline">\(I_t\)</span> denote the investment in a fixed economy at time <span class="math inline">\(t\)</span>. One linear model for investment specifies: <span class="math display">\[ \log I_t = \beta_1 + \beta_2 i_t + \beta_3 \Delta p_t + \beta_4\log Y_t + \beta_5t + \varepsilon_t\]</span> where <span class="math inline">\(i_t\)</span> is the nominal interest rate, <span class="math inline">\(\Delta p_t\)</span> is the inflation rate, and <span class="math inline">\(Y_t\)</span> is real output. We’ve also included a time trend <span class="math inline">\(t\)</span> as an independent variable. Instead of the nominal interest rate, agents may care about the real (adjusted for inflation) interest rate <span class="math inline">\(i_t - \Delta p_t\)</span>. So perhaps investment is only affected by inflation insofar that inflation determines the real interest rate.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> If this is the case our model is <span class="math display">\[ \log I_t = \beta_1 + \beta_2 (i_t - \Delta p_t) + \beta_4\log Y_t + \beta_5t + \varepsilon_t.\]</span> We can test if this second specification is favorable by estimating the first model, and then testing the hypothesis <span class="math inline">\(H_0 : \beta_2 + \beta_3 = 0\)</span>. Let’s perform a simulation where the null hypothesis is true, and perform to corresponding Wald test. For the sake of ease, we will assume everything is uniformly distributed instead of simulating values such that they are realistic in the economic context of the model.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-30_9cf9e7302fb39b9f1532a43346b04d2b">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.1</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">5</span> </span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>del_p <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, i, del_p, <span class="fu">log</span>(y), t)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>I <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>hat_beta <span class="ot">&lt;-</span> <span class="fu">OLS</span>(I,X)[,<span class="dv">1</span>]</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> hat_beta)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>var_hat <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">t</span>(H <span class="sc">%*%</span> hat_beta) <span class="sc">%*%</span> <span class="fu">solve</span>(H <span class="sc">%*%</span> var_hat <span class="sc">%*%</span> <span class="fu">t</span>(H)) <span class="sc">%*%</span> (H <span class="sc">%*%</span> hat_beta)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="dv">1</span>)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">c</span>(W, C))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.187603e-05 3.841459e+00</code></pre>
</div>
</div>
<p>The value of the Wald test stat is not even close to exceeding the critical value, so we fail to reject the null hypothesis and conclude that <span class="math inline">\(\beta_2 + \beta_3 = 0\)</span>.</p>
</div>
</section>
<section id="partialmarginal-effects-linear-projection-revisited" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="partialmarginal-effects-linear-projection-revisited"><span class="header-section-number">5.10</span> Partial/Marginal Effects, Linear Projection Revisited</h2>
<p>When interpreting the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> in <span class="math inline">\(\mathcal P_\text{LM}\)</span>, it’s very common to think in terms of derivatives. We will define these derivatives according to <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.13 </strong></span>Suppose <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X}= (X_1,\ldots, X_K)\)</span> are a random variable and vector, respectively. The <span style="color:red"><strong><em>partial/marginal effect</em></strong></span> of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(\text{E}\left[Y\mid\mathbf{X}\right]\)</span> (sometimes called the partial effect of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span>), is <span class="math display">\[ \frac{\partial \text{E}\left[Y\mid\mathbf{X}\right]}{\partial X_j}.\]</span> In the event that <span class="math inline">\(X_j\)</span> is discrete, partial effects are given as the difference between <span class="math inline">\(\text{E}\left[Y\mid\mathbf{X}\right]\)</span> evaluated at two discrete values of <span class="math inline">\(X_j\)</span>.</p>
</div>
<p>If we have a linear model <span class="math inline">\(Y = \mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span>, we’re almost conditioned to conclude the marginal effect of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(\text{E}\left[Y\mid\mathbf{X}\right]\)</span> is <span class="math inline">\(\beta_j\)</span>, but in general this is not true.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.21 </strong></span>This example is due to this <a href="https://stats.stackexchange.com/questions/190703/non-linear-endogeneity/190800#190800">post</a>. Suppose <span class="math inline">\(Y=X\beta + \varepsilon\)</span> where <span class="math inline">\(X\sim N(0,1)\)</span> and <span class="math inline">\(\varepsilon = X^2 - 1\)</span>. To insure that <span class="math inline">\(\beta\)</span> is identified, we need to verify that <span class="math inline">\(\text{E}\left[\varepsilon\right] = 0\)</span> (we do not have an intercept) and <span class="math inline">\(\text{E}\left[X\varepsilon\right] =0\)</span> (the multicollinearity assumption is trivially met). Note that <span class="math inline">\(X^2\sim \chi_1^2\)</span>, so <span class="math inline">\(\text{E}\left[X^2\right] = 1\)</span>. We also have <span class="math inline">\(\text{E}\left[X^3\right] = 0\)</span>, as the normal distribution is not skewed (skewness being defined as the third moment centered about the mean).<br>
<span class="math display">\[\begin{align*}
\text{E}\left[\varepsilon\right]&amp;= \text{E}\left[X^2 - 1\right]= \text{E}\left[X^2\right] - 1 = 1 - 1 = 0\\
\text{E}\left[X\varepsilon\right]&amp;= \text{E}\left[X^3 - X\right] = \text{E}\left[X^3\right] - \text{E}\left[X\right] = 0 - 0 = 0
\end{align*}\]</span> Our model is identified. Furthermore <span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> will present consistent estimates of <span class="math inline">\(\beta\)</span>. For the sake of illustration, let <span class="math inline">\(\boldsymbol{\beta}= 2\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-31_971dc648aa088baeb356aa9cca62c5bd">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span> </span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim<span class="dv">-1</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>N_sim) {</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> beta<span class="sc">*</span>X <span class="sc">+</span> e</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>  estimates[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)[<span class="dv">1</span>]</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/unnamed-chunk-32_42972018914d19e82cee29872896ac9c">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">2</span><span class="sc">:</span>N_sim, </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> estimates</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"sample size"</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"OLS estimate"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid figure-img" width="1056"></p>
</figure>
</div>
</div>
</div>
<p>We have estimated <span class="math inline">\(\beta = 2\)</span> consistently, but this is not the partial effect! We have</p>
<p><span class="math display">\[\frac{\partial \text{E}\left[Y\mid X\right]}{\partial X}= \frac{\partial}{\partial X}\text{E}\left[X\beta + \varepsilon \mid X\right] = \frac{\partial}{\partial X}\text{E}\left[X\beta + X^2 -1\mid X\right] = \frac{\partial}{\partial X}[X\beta + X^2 - 1] = \beta + 2X \neq \beta.\]</span> This follows from the fact that <span class="math inline">\(X\)</span> is only weakly exogenous, so <span class="math inline">\(\text{E}\left[\varepsilon\mid X\right]\neq 0\)</span>.</p>
</div>
<p>In general, we can still have <span class="math inline">\(\text{E}\left[X_i\varepsilon_i\right] = 0\)</span> for all <span class="math inline">\(i\)</span> where <span class="math inline">\(\varepsilon = g(\mathbf{X})\)</span> for some nonlinear function <span class="math inline">\(g\)</span>, because weak exogeneity only insures that our error and regressors are uncorrelated (i.e they have no linear relationship). What we need is exogeneity so we can conclude <span class="math display">\[\text{E}\left[Y \mid \mathbf{X}\right] = \text{E}\left[\mathbf{X}\boldsymbol{\beta}\mid \mathbf{X}\right] + \underbrace{\text{E}\left[\varepsilon \mid \mathbf{X}\right]}_{\mathbf{0}} = \mathbf{X}\boldsymbol{\beta},\]</span> so <span class="math display">\[\frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial X_j} = \beta_j.\]</span> <strong><em>At the heart of this issue is the relationship between the linear projection model and the (structural) linear model</em></strong>. Early on we emphasized that there was a difference between what we called the linear projection model and the linear model. The prior is concerned with the statistical association of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X}\)</span> and describes a feature of their joint density. Proposition @ref(prp:ceferr) established that by definition the error in this model, <span class="math inline">\(\varepsilon_c\)</span>, satisfied <span class="math inline">\(\text{E}\left[\varepsilon_c\mid\mathbf{X}\right] = \mathbf{0}\)</span>. In the case of the linear model, <span class="math inline">\(\varepsilon\)</span> has a structural interpretation and may not satisfy this property, <em>but if it does</em> the linear projection model and the linear model will coincide in the sense that <span class="math inline">\(\boldsymbol{\beta}\)</span> is interpreted as a marginal effect. Some treatments of OLS, such as <span class="citation" data-cites="cameron2005microeconometrics">Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span>, actually restrict their attention to the identification of the linear model such that <span class="math inline">\(\boldsymbol{\beta}\)</span> is associated with a marginal effect, requiring exogeneity instead of weak exogeneity for identification.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 5.9 (Identification of Marginal Effects) </strong></span>Suppose <span class="math inline">\(\mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\)</span> is a linear model. If <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbf{X}\right] = \mathbf{0}\)</span> and <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, then <span class="math display">\[\frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial \mathbf{X}} = \boldsymbol{\beta},\]</span> where <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>In this case, <span class="math inline">\(\boldsymbol{\beta}\)</span> is identified as <span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbf{X}\right] = \mathbf{0}\)</span> implies <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span>, and we are given <span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>.</p>
</div>
<p>More generally, if <span class="math inline">\(\mathbf f(\mathbf{X}) = [f_1(\mathbf{X}), \ldots, f_K(\mathbf{X})]\)</span> are a series of continuous functions of regressors, and <span class="math inline">\(Y = \mathbf f(\mathbf{X})\beta + \varepsilon\)</span>, then <span class="math display">\[ \frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial X_j} = \frac{\partial \mathbf f}{\partial X_j}\boldsymbol{\beta}= \sum_{\ell =1}^K\frac{\partial f_\ell}{\partial X_j} \boldsymbol{\beta}.\]</span> For example, if <span class="math inline">\(Y = \beta_1 + \beta_2\log X_1 + \beta_3 \exp[ X_1X_2] + \varepsilon\)</span> where <span class="math inline">\(\text{E}\left[\varepsilon \mid X_1\right]=0\)</span>, then <span class="math display">\[ \frac{\partial \text{E}\left[Y\mid X_1\right]}{\partial X_1} = \frac{\beta_1}{X_1} + \beta_3X_2\exp[X_1X_2].\]</span></p>
<p><span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span> provide a more nuanced discussion of <span class="math inline">\(\frac{\partial \text{E}\left[Y\mid X\right]}{\partial X}\)</span> in the context of structural models, and how it relates the the linear projection model we discussed at the opening.</p>
</section>
<section id="frischwaughlovell-theorem" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="frischwaughlovell-theorem"><span class="header-section-number">5.11</span> Frisch–Waugh–Lovell Theorem</h2>
<p>Even if our model contains multiple regressors concatenated in the vector <span class="math inline">\(\mathbf{X}\)</span>, we may be especially interested in a subset of regressors. For instance, in Example <a href="#exm-car">Example&nbsp;<span>5.5</span></a> we may be especially interested in the price of cars if we are a manufacturer, as estimating consumers’ sensitivity to price changes could give us valuable insights into maximizing our profit. In situations like this, is it possible to “ignore” the independent variables of secondary importance? The answer, as provided by <span class="citation" data-cites="frisch1933partial">Frisch and Waugh (<a href="references.html#ref-frisch1933partial" role="doc-biblioref">1933</a>)</span> <span class="citation" data-cites="lovell1963seasonal">Lovell (<a href="references.html#ref-lovell1963seasonal" role="doc-biblioref">1963</a>)</span>, is “kind of”, and deals with the algebra of OLS.</p>
<p>Suppose <span class="math inline">\(\mathbb{X}_1\)</span> and <span class="math inline">\(\mathbb{X}_2\)</span> are two random matrices of observations where <span class="math inline">\(\mathbb{X}= [\mathbb{X}_1,\mathbb{X}_2]\)</span>. If <span class="math inline">\(\boldsymbol{\beta}=[\boldsymbol{\beta}_1,\boldsymbol{\beta}_2]'\)</span>, then <span class="math display">\[ \mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}= \mathbb{X}_1\boldsymbol{\beta}_1 + \mathbb{X}_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon}. \]</span> The first order condition associated with the least squares problem is now <span class="math display">\[ \begin{bmatrix}\mathbb{X}_1'\mathbb{X}_1 &amp; \mathbb{X}_1'\mathbb{X}_2\\\mathbb{X}_2'\mathbb{X}_1 &amp; \mathbb{X}_2'\mathbb{X}_2\end{bmatrix} \begin{bmatrix} \hat{\boldsymbol{\beta}}_{\text{OLS},1} \\ \hat{\boldsymbol{\beta}}_{\text{OLS},2} \end{bmatrix} =  \begin{bmatrix} \mathbb{X}_1'\mathbf{Y}\\ \mathbb{X}_2'\mathbf{Y}\end{bmatrix}\]</span> If we expand this, we have <span class="math display">\[\begin{align*}
\mathbb{X}_1'\mathbb{X}_1\hat{\boldsymbol{\beta}}_{\text{OLS},1} + \mathbb{X}_1'\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2} = \mathbb{X}_1'\mathbf{Y},\\
\mathbb{X}_2'\mathbb{X}_1\hat{\boldsymbol{\beta}}_{\text{OLS},1} + \mathbb{X}_2'\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2} = \mathbb{X}_2'\mathbf{Y}.
\end{align*}\]</span> If we solve the first equation for <span class="math inline">\(\hat{\boldsymbol{\beta}}_{\text{OLS},1}\)</span>, we have <span class="math display">\[\hat{\boldsymbol{\beta}}_{\text{OLS},1} = (\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'(\mathbf{Y}-\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2}).\]</span> If we insert this into the second equation in our system, we have <span class="math display">\[\begin{align*}
&amp;\mathbb{X}_2'\mathbb{X}_1[(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'(\mathbf{Y}-\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2})] + \mathbb{X}_2'\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2} = \mathbb{X}_2'\mathbf{Y}\\
\implies &amp;  \mathbb{X}_2'\mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'\mathbf{Y}-\mathbb{X}_2'\mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2} + \mathbb{X}_2'\mathbb{X}_2\hat{\boldsymbol{\beta}}_{\text{OLS},2} = \mathbb{X}_2'\mathbf{Y}\\
\implies &amp;  \hat{\boldsymbol{\beta}}_{\text{OLS},2}[\mathbb{X}_2'\mathbb{X}_2 - \mathbb{X}_2'\mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'\mathbb{X}_2]= \mathbb{X}_2'\mathbf{Y}- \mathbb{X}_2'\mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1'\mathbf{Y}\\
\implies &amp; \hat{\boldsymbol{\beta}}_{\text{OLS},2}[\mathbb{X}_2'(\mathbf I - \mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1')\mathbb{X}_2] = [\mathbb{X}_2'(\mathbf I - \mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1')\mathbf{Y}]\\
\implies &amp; \hat{\boldsymbol{\beta}}_{\text{OLS},2} = [\mathbb{X}_2'(\mathbf I - \mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1')\mathbb{X}_2]^{-1}[\mathbb{X}_2'(\mathbf I - \mathbb{X}_1(\mathbb{X}_1'\mathbb{X}_1)^{-1}\mathbb{X}_1')\mathbf{Y}]
\end{align*}\]</span> Recalling that we defined a matrix <span class="math inline">\(\mathbb M =\mathbf I - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\)</span> such that <span class="math inline">\(\hat{\mathbf{e}} = \mathbb M\mathbf{Y}\)</span> where <span class="math inline">\(\mathbb M = \mathbb M'\)</span> and <span class="math inline">\(\mathbb M^2 = \mathbb M\)</span>, we have <span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_{\text{OLS},2} &amp;= [\mathbb{X}_2'\mathbb M_1\mathbb{X}_2]^{-1}[\mathbb{X}_2'\mathbb M_1\mathbf{Y}]\\
&amp; = [\mathbb{X}_2'\mathbb M_1'\mathbb M_1\mathbb{X}_2]^{-1}[\mathbb{X}_2'\mathbb M_1\mathbf{Y}]\\
&amp; = [\mathbb X_2^{*\prime} \mathbb X_2^{*}]^{-1}[\mathbb{X}_2'\mathbf{Y}^*] &amp; (\mathbb X_2^{*} = \mathbb M_1\mathbb{X}_2,\ \mathbf{Y}^* =\mathbb M_1\mathbf{Y})
\end{align*}\]</span> The estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}_{\text{OLS},2}\)</span> follows from regressing <span class="math inline">\(\mathbb M_1\mathbb{X}_2\)</span> on <span class="math inline">\(\mathbb M_1\mathbf{Y}\)</span>, which correspond to the residuals <span class="math inline">\(\mathbb{X}_2 - \mathbb{X}_1\hat{\boldsymbol\gamma}_\text{OLS}\)</span> and <span class="math inline">\(\mathbf Y - \mathbb{X}\hat{\boldsymbol{\beta}}_{\text{OLS},1}\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.3 (Frisch–Waugh–Lovell Theorem) </strong></span>For the linear model, <span class="math inline">\(\mathbf{Y}= \mathbb{X}\boldsymbol{\beta}+ \boldsymbol{\varepsilon}= \mathbb{X}_1\boldsymbol{\beta}_1 + \mathbb{X}_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon}\)</span>, <span class="math display">\[\hat{\boldsymbol{\beta}}_{\text{OLS},2} = [\mathbb X_2^{*\prime} \mathbb X_2^{*}]^{-1}[\mathbb{X}_2'\mathbf{Y}^*],\]</span> where <span class="math inline">\(\mathbb X_2^{*\prime}\)</span> and <span class="math inline">\(\mathbf{Y}^*\)</span> are the residual vectors from least squares regression of <span class="math inline">\(\mathbb{X}_2\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> on <span class="math inline">\(\mathbb{X}_1\)</span>, respectively.</p>
</div>
<p>One of the useful applications of this theorem deals with visualization.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.22 </strong></span>Suppose <span class="math inline">\(Y = 1 + 4 X_1 + 2 X_2 + 8 X_3 + 3X_4 + \varepsilon\)</span>. Let’s estimate this model for simulated data.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-33_5a81f424bf14b448600ff4cadbe1f5c0">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">8</span>,<span class="dv">3</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="fu">diag</span>(<span class="dv">1</span>,<span class="dv">4</span>))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y,X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Estimate  Std.Error    t-Stat Lower 95% CI Upper 95% CI p-Value
β1 1.001301 0.06590534  15.19302    0.8721294     1.130474       0
β2 4.120027 0.06091637  67.63415    4.0006334     4.239421       0
β3 1.953073 0.05279017  36.99692    1.8496066     2.056540       0
β4 8.018764 0.06786839 118.15167    7.8857443     8.151784       0
β5 2.967293 0.06105884  48.59728    2.8476203     3.086967       0</code></pre>
</div>
</div>
<p>If we’re interested in the <span class="math inline">\(\hat\beta_{\text{OLS},2}\)</span>, we may want to visualize it. Unfortunately, our parameter space is a subset of <span class="math inline">\(\mathbb R^5\)</span>, so it isn’t feasible to plot the hyperplane correspond to our estimated model over our sample. Fortunately, we can use the Frisch–Waugh–Lovell Theorem.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-34_95927876e5e7aaaaf76088682257465e">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> X[,<span class="sc">-</span><span class="dv">2</span>]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> X[,<span class="dv">2</span>]</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>y_res <span class="ot">&lt;-</span> y <span class="sc">-</span> X1 <span class="sc">%*%</span> <span class="fu">OLS</span>(y, X1)[,<span class="dv">1</span>]</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>X2_res <span class="ot">&lt;-</span> X2 <span class="sc">-</span> X1 <span class="sc">%*%</span> <span class="fu">OLS</span>(X2, X1)[,<span class="dv">1</span>]</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y_res, X2_res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Estimate  Std.Error   t-Stat Lower 95% CI Upper 95% CI p-Value
β1 4.120027 0.05967305 69.04335      4.00307     4.236984       0</code></pre>
</div>
</div>
<p>We end up with the same estimate <span class="math inline">\(\hat\beta_{\text{OLS},2}\)</span>, and can visualize it by plotting the dependent and independent variables in this alternate regression.</p>
<div class="cell" data-layout-align="center" data-fig.asp="1" data-hash="ols_cache/html/unnamed-chunk-35_28dcbb99594506f105ff8f0c9bcbd86d">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X2_res, </span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y_res</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"x2 Residuals"</span>, <span class="at">y =</span> <span class="st">"y Residuals"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid figure-img" width="1056"></p>
</figure>
</div>
</div>
</div>
<p>While our estimate is the same, the standard error associated with the estimate is not quite the same. For the first regression, we had <span class="math inline">\(K = 5\)</span> regressors, whereas the second had <span class="math inline">\(K = 1\)</span>. This affects how <span class="math inline">\(\widehat{\text{se}}(\hat{\boldsymbol\beta}_\text{OLS} )\)</span> is calculated, as the numerator of <span class="math inline">\(S^2\)</span> which ensures it is unbiased is <span class="math inline">\(n - K\)</span>. We have <span class="math inline">\(\widehat{\text{se}}(\hat{\boldsymbol\beta}_\text{OLS} )\propto \sqrt{n-K}\)</span>, so if we scale the standard error in the second regression by <span class="math inline">\(\sqrt{(n - 1)/(n - 5)}\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-36_12ded7c839ce660227c3f0c0a896b09f">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>incorrect_se <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y_res, X2_res)[<span class="dv">2</span>]</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>incorrect_se <span class="sc">*</span> <span class="fu">sqrt</span>((n<span class="dv">-1</span>)<span class="sc">/</span>(n<span class="dv">-5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06091637</code></pre>
</div>
</div>
<p>Despite being well known, the relationship between the standard errors of each regression seem to have gone unformalized until <span class="citation" data-cites="ding2021frisch">Ding (<a href="references.html#ref-ding2021frisch" role="doc-biblioref">2021</a>)</span>.</p>
</div>
</section>
<section id="recap" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="recap"><span class="header-section-number">5.12</span> Recap</h2>
<p>The linear model, along with the array of assumptions and what they yield, can be quite a bit to take in. The following table shows the cumulative properties given by the addition of each assumption.</p>
<table class="table">
<colgroup>
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 28%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(\text{rank}\left(\text{E}\left[\mathbf{X}'\mathbf{X}\right]\right) = K\)</span>, <span class="math inline">\(\text{E}\left[\mathbf{X}'\boldsymbol{\varepsilon}\right] = \mathbf{0}\)</span></th>
<th><span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\mid \mathbb{X}\right]= \mathbf{0}\)</span></th>
<th><span class="math inline">\(\text{E}\left[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'\mid \mathbb{X}\right]= \sigma^2\mathbf I\)</span></th>
<th><span class="math inline">\(\boldsymbol{\varepsilon}\sim N(\mathbf{0},\sigma^2\mathbf I)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\((\boldsymbol{\beta},\boldsymbol{\Sigma})\)</span> identified</td>
<td><span class="math inline">\((\boldsymbol{\beta},\boldsymbol{\Sigma})\)</span> identified</td>
<td><span class="math inline">\((\boldsymbol{\beta},\sigma^2)\)</span> identified</td>
<td><span class="math inline">\((\boldsymbol{\beta},\sigma^2)\)</span> identified</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span></td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span></td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span></td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \overset{p}{\to}\boldsymbol{\beta}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Asymptotically Normal</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Asymptotically Normal</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Asymptotically Normal</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Normal</td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Unbiased</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Unbiased</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> Unbiased</td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\(\frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial \mathbf{X}} = \boldsymbol{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial \mathbf{X}} = \boldsymbol{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\partial \text{E}\left[Y\mid \mathbf{X}\right]}{\partial \mathbf{X}} = \boldsymbol{\beta}\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> BLUE</td>
<td><span class="math inline">\(\hat{\boldsymbol\beta}_\text{OLS} \)</span> BLUE</td>
</tr>
</tbody>
</table>
<p>We also have maintained two implicit assumptions throughout: IID regressors, and the model is correctly specified.</p>
</section>
<section id="rep1" class="level2" data-number="5.13">
<h2 data-number="5.13" class="anchored" data-anchor-id="rep1"><span class="header-section-number">5.13</span> Example/Replication</h2>
<p>Chapter 4 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span> provides a useful exercise in the form of replication <span class="citation" data-cites="christensen1976economies">Christensen and Greene (<a href="references.html#ref-christensen1976economies" role="doc-biblioref">1976</a>)</span> who estimate the economies of scale in US electrical power generation.</p>
<p>In response to rising electrical rates in the United States during the mid-20th century, <span class="citation" data-cites="christensen1976economies">Christensen and Greene (<a href="references.html#ref-christensen1976economies" role="doc-biblioref">1976</a>)</span> consider whether vertically integrated electrical providers (who generate, transmit, and distribute electricity) should be disintegrated. If these firms could only generate electricity as a result of regulation, then these firms would have to compete to sell generated electricity to separate intermediary firms which would transmit and distribute it to consumers. This competition has the potential to lower prices. At the same time, perhaps the vertical integration of firms cuts costs, which may actually lead to better prices for consumers. To determine if this is the case, we can consider the economies of scale of firms.</p>
<p>Economies of scale refers to the phenomenon where the more quantity of a good a firm produces, the lower the costs associated with that production. A firms cost, <span class="math inline">\(C\)</span>, will be a function of output <span class="math inline">\(Q\)</span>, and prices of inputs <span class="math inline">\(P_1,\ldots,P_k\)</span> used in production. We will specify our cost function as: <span class="math display">\[\begin{align*}
\log C &amp;= \alpha + \beta\log Q + \gamma[(\log Q)^2/2] + \sum_{i=1}^k\delta_i\log P_i &amp; (\textstyle\sum_{i=1}^k\delta_i = 1)
\end{align*}\]</span></p>
<p>The functional form may look a bit arbitrary, but it’s consistent with a handful of appealing properties we expect a cost function to exhibit, the most important of which being a U-shape in quantity corresponding to decreasing costs as quantity increases up to a point (economies of scale), followed by increasing costs. <span class="citation" data-cites="christensen1973transcendental">Christensen, Jorgenson, and Lau (<a href="references.html#ref-christensen1973transcendental" role="doc-biblioref">1973</a>)</span> provide all the details about this functional form. This point is given as the minimum of <span class="math inline">\(C\)</span>, which will coincide with the minimum of <span class="math inline">\(\log C\)</span> because <span class="math inline">\(\log\)</span> is a monotonic function. The first order condition for this minimization is <span class="math display">\[\begin{align*}
&amp;\frac{\partial \log C}{\partial \log Q}  = 1, \\
\implies &amp; \beta + \gamma \log Q^* = 1,\\
\implies &amp; Q^* = \exp\left(\frac{1-\beta}{\gamma}\right),
\end{align*}\]</span> where the derivative is set to <span class="math inline">\(1\)</span> instead of <span class="math inline">\(0\)</span> because <span class="math inline">\(\log(0) = 1\)</span>. If we are able to estimate <span class="math inline">\((\beta,\gamma)\)</span> we can calculate <span class="math inline">\(Q^*\)</span>. If we observe <span class="math inline">\(Q_i &gt; Q^*\)</span>, for many firms <span class="math inline">\(i,\)</span> then the same output <span class="math inline">\(Q\)</span> could be produced at a lower cost if the market was comprised of a larger number of firms producing a smaller output.</p>
<p>The data from <span class="citation" data-cites="christensen1976economies">Christensen and Greene (<a href="references.html#ref-christensen1976economies" role="doc-biblioref">1976</a>)</span> is available <a href="https://pages.stern.nyu.edu/~wgreene/Text/Edition7/TableF4-4.txt">here</a>.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-37_d48ad9b8c322133942d929906fd89805">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/christensen_greene_1976.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first few rows of the data give us an idea of what type of information we have.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-38_17faf9d3f0178d1d509fe8c63a276bf3">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: right;">cost</th>
<th style="text-align: right;">fuel</th>
<th style="text-align: right;">output</th>
<th style="text-align: right;">capital</th>
<th style="text-align: right;">labor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.2130</td>
<td style="text-align: right;">18.000</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">64.945</td>
<td style="text-align: right;">6869.47</td>
</tr>
<tr class="even">
<td style="text-align: right;">3.0427</td>
<td style="text-align: right;">21.067</td>
<td style="text-align: right;">869</td>
<td style="text-align: right;">68.227</td>
<td style="text-align: right;">8372.96</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9.4059</td>
<td style="text-align: right;">41.530</td>
<td style="text-align: right;">1412</td>
<td style="text-align: right;">40.692</td>
<td style="text-align: right;">7960.90</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.7606</td>
<td style="text-align: right;">28.539</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">41.243</td>
<td style="text-align: right;">8971.89</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2.2587</td>
<td style="text-align: right;">39.200</td>
<td style="text-align: right;">295</td>
<td style="text-align: right;">71.940</td>
<td style="text-align: right;">8218.40</td>
</tr>
<tr class="even">
<td style="text-align: right;">1.3422</td>
<td style="text-align: right;">35.510</td>
<td style="text-align: right;">183</td>
<td style="text-align: right;">74.430</td>
<td style="text-align: right;">5063.49</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We observe the unit prices of three inputs: capital, labor, and fuel. Taking these into account, the cost function becomes <span class="math display">\[\log C = \alpha + \beta\log Q + \frac{1}{2}\gamma[(\log Q)^2/2] + \delta_k \log P_k + \delta_l \log P_l + \delta_f \log P_f,\]</span> where <span class="math inline">\(\delta_k + \delta_l + \delta_f = 1\)</span>. We can rewrite out model to implicitly satisfy the constraint if we write <span class="math inline">\(\delta_f = 1 -\delta_l - \delta_f\)</span>:</p>
<p><span class="math display">\[\begin{align*}
&amp;\log C = \alpha + \beta\log Q + \gamma[(\log Q)^2/2] + \delta_k \log P_k + \delta_l \log P_l + (1 -\delta_l - \delta_f) \log P_f\\
\implies &amp; \log C - \log P_f= \alpha + \beta\log Q + \gamma[(\log Q)^2/2] + \delta_k( \log P_k - \log P_f) + \delta_l (\log P_l-\log P_f) \\
\implies &amp; \log(C/P_f) = \alpha + \beta\log Q +\gamma[(\log Q)^2/2] + \delta_k \log (P_k/P_f) + \delta_l \log (P_l/P_f)
\end{align*}\]</span></p>
<p>This model cannot account for all the possible factors which influence a firm’s costs, so we will introduce the element <span class="math inline">\(\varepsilon\)</span> to account for unobserved factors which influence cost, and assume that all our regressors are exogenous.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> This gives us our estimable model. <span class="math display">\[ \log(C/P_f) = \alpha + \beta\log Q +\gamma[(\log Q)^2/2] + \delta_k \log (P_k/P_f) + \delta_l \log (P_l/P_f) + \varepsilon\]</span></p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-39_3c496d115c0526f7429f1fb3c2537cd0">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="ot">&lt;-</span> CG_1976 <span class="sc">%&gt;%</span> </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">C =</span> <span class="fu">log</span>(cost<span class="sc">/</span>fuel),</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">Q =</span> <span class="fu">log</span>(output),</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">Q2 =</span> Q<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">P_kf =</span> <span class="fu">log</span>(capital<span class="sc">/</span>fuel),</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">P_lf =</span> <span class="fu">log</span>(labor<span class="sc">/</span>fuel)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(C <span class="sc">~</span> Q <span class="sc">+</span> Q2 <span class="sc">+</span> P_kf <span class="sc">+</span> P_lf, <span class="at">data =</span> CG_1976)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = C ~ Q + Q2 + P_kf + P_lf, data = CG_1976)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42576 -0.08891 -0.00223  0.08404  0.37363 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.818163   0.252439 -27.009  &lt; 2e-16 ***
Q            0.402745   0.031483  12.792  &lt; 2e-16 ***
Q2           0.060895   0.004325  14.079  &lt; 2e-16 ***
P_kf         0.162034   0.040406   4.010 9.46e-05 ***
P_lf         0.152445   0.046597   3.272  0.00132 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1378 on 153 degrees of freedom
Multiple R-squared:  0.9922,    Adjusted R-squared:  0.992 
F-statistic:  4880 on 4 and 153 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can use our estimates <span class="math inline">\(\hat \beta\)</span> and <span class="math inline">\(\hat \gamma\)</span> to calculate <span class="math inline">\(\hat Q^*\)</span>.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-40_7587d276032ad78c73ea3a5fb6a9771b">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> model<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>gamma_hat <span class="ot">&lt;-</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>Q_hat <span class="ot">&lt;-</span> <span class="fu">exp</span>((<span class="dv">1</span><span class="sc">-</span>beta_hat)<span class="sc">/</span>gamma_hat)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>Q_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Q 
18177.1 </code></pre>
</div>
</div>
<p>The straightforward way of determining if costs are higher than they need to be as a result of vertical integration is by counting the number of firms for which <span class="math inline">\(Q &gt; \hat Q^*\)</span>. This doesn’t account for the degree to which firms are exceeding the efficient scale. To do this, let’s consider the total output produced by firms which are exceeding the efficient scale, opposed to those which are not.</p>
<div class="cell" data-hash="ols_cache/html/unnamed-chunk-41_5c612d952c29ec4323346216282f0e63">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="sc">%&gt;%</span> </span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">inefficient =</span> (output <span class="sc">&gt;</span> Q_hat)) <span class="sc">%&gt;%</span> </span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(inefficient) <span class="sc">%&gt;%</span> </span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_firms =</span> <span class="fu">n</span>(),</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">total_output =</span> <span class="fu">sum</span>(output)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prop_output =</span> total_output<span class="sc">/</span><span class="fu">sum</span>(total_output)) <span class="sc">%&gt;%</span> </span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">inefficient</th>
<th style="text-align: right;">n_firms</th>
<th style="text-align: right;">total_output</th>
<th style="text-align: right;">prop_output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">FALSE</td>
<td style="text-align: right;">133</td>
<td style="text-align: right;">711548.9</td>
<td style="text-align: right;">0.4301554</td>
</tr>
<tr class="even">
<td style="text-align: left;">TRUE</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">942618.0</td>
<td style="text-align: right;">0.5698446</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>57% of output is attributed to the 25 firms producing over the efficient scale. This seems to indicate that the market is too vertically integrated.</p>
</section>
<section id="further-reading" class="level2" data-number="5.14">
<h2 data-number="5.14" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">5.14</span> Further Reading</h2>
<p><strong>Conditional Expectation and Linear Projection</strong>: Chapter 2 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 2 of <span class="citation" data-cites="hansen2022econometrics">Hansen (<a href="references.html#ref-hansen2022econometrics" role="doc-biblioref">2022</a>)</span>, Chapter 3 of <span class="citation" data-cites="angrist2008mostly">Angrist and Pischke (<a href="references.html#ref-angrist2008mostly" role="doc-biblioref">2008</a>)</span></p>
<p><strong>Structural Modeling</strong>: Chapter 1 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span> , <span class="citation" data-cites="reiss2007structural">Reiss and Wolak (<a href="references.html#ref-reiss2007structural" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="goldberger1972structural">Goldberger (<a href="references.html#ref-goldberger1972structural" role="doc-biblioref">1972</a>)</span>, portions of Chapter 2 of <span class="citation" data-cites="cameron2005microeconometrics">Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span></p>
<p><strong>OLS</strong>: Chapter 4 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapters 3-5 of <span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, portions of Chapter 1-2 of <span class="citation" data-cites="hayashi2011econometrics">Hayashi (<a href="references.html#ref-hayashi2011econometrics" role="doc-biblioref">2011</a>)</span></p>
<p><strong>Model selection</strong>: <span class="citation" data-cites="phillips1996econometric">Phillips (<a href="references.html#ref-phillips1996econometric" role="doc-biblioref">1996</a>)</span> <span class="citation" data-cites="hansen2005challenges">Hansen (<a href="references.html#ref-hansen2005challenges" role="doc-biblioref">2005</a>)</span> <span class="citation" data-cites="leamer1978specification">Leamer (<a href="references.html#ref-leamer1978specification" role="doc-biblioref">1978</a>)</span> <span class="citation" data-cites="hendry2000econometrics">Hendry (<a href="references.html#ref-hendry2000econometrics" role="doc-biblioref">2000</a>)</span> <span class="citation" data-cites="davidson1981several">(<a href="references.html#ref-davidson1981several" role="doc-biblioref"><strong>davidson1981several?</strong></a>)</span> <span class="citation" data-cites="hendry1995dynamic">Hendry et al. (<a href="references.html#ref-hendry1995dynamic" role="doc-biblioref">1995</a>)</span></p>
</section>
<section id="sec-proj" class="level2" data-number="5.15">
<h2 data-number="5.15" class="anchored" data-anchor-id="sec-proj"><span class="header-section-number">5.15</span> Math Appendix: Projection</h2>
<p>Linear projection can be generalized far beyond the setting of a random vector <span class="math inline">\((Y,\mathbf{X})\)</span>. I’ll <em>quickly</em> define projection in the general setting of Hilbert spaces. For details, see <span class="citation" data-cites="rudin">Rudin (<a href="references.html#ref-rudin" role="doc-biblioref">1987</a>)</span>, <span class="citation" data-cites="royden1988real">Royden and Fitzpatrick (<a href="references.html#ref-royden1988real" role="doc-biblioref">1988</a>)</span>, or <span class="citation" data-cites="folland1999real">Folland (<a href="references.html#ref-folland1999real" role="doc-biblioref">1999</a>)</span>.</p>
<p>A normed vector space <span class="math inline">\(V\)</span> defined over a field <span class="math inline">\(F\)</span> is, as the name implies, a vector space equipped with a norm <span class="math inline">\(\left\lVert\cdot\right\rVert:V\mapsto [0,\infty)\)</span> satisfying:</p>
<ol type="1">
<li><span class="math inline">\(\left\lVert v\right\rVert = 0 \iff v = 0\)</span>;</li>
<li><span class="math inline">\(\left\lVert av\right\rVert = \left\lvert a\right\rvert\left\lVert v\right\rVert\)</span>;</li>
<li><span class="math inline">\(\left\lVert w + v\right\rVert \le \left\lVert w\right\rVert + \left\lVert v\right\rVert\)</span>;</li>
</ol>
<p>for all vectors <span class="math inline">\(v,w\in V\)</span> and scalars <span class="math inline">\(a\in F\)</span>. The norm tells us how “far” a vector <span class="math inline">\(v\in V\)</span> is from the zero element (“origin”) <span class="math inline">\(0\in V\)</span>. Any normed vector space is also a metric space if we define a metric as <span class="math inline">\(d(v,w)=\left\lVert v-w\right\rVert\)</span>. With this metric comes the familiar definitions of convergence. If <span class="math inline">\(V\)</span> is a complete metric space (all Cauchy sequences converge in <span class="math inline">\(V\)</span>), then we say <span class="math inline">\(V\)</span> is a Banach space.</p>
<p>A Hilbert space is a complete vector space <span class="math inline">\(H\)</span> (defined over a field of scalars <span class="math inline">\(F\)</span>) equipped with an inner product <span class="math inline">\(\langle\cdot,\cdot\rangle: H\times H\to F\)</span> which satisfies:</p>
<ol type="1">
<li><span class="math inline">\(\langle x,y \rangle = \langle y,x \rangle\)</span>;</li>
<li><span class="math inline">\(\langle cx,y \rangle = c\langle y,x \rangle\)</span></li>
<li><span class="math inline">\(\langle x+z,y \rangle = \langle x,z \rangle + \langle y,z \rangle\)</span>;</li>
<li><span class="math inline">\(\langle x,x \rangle &gt; 0 \iff x\neq 0\)</span>;</li>
</ol>
<p>for all <span class="math inline">\(x,y\in H\)</span> and <span class="math inline">\(c\in F\)</span>. All Hilbert spaces are normed vector spaces, as <span class="math inline">\(\left\lVert x\right\rVert = \sqrt{\langle x,x \rangle}\)</span> satisfies all the properties of a norm. This makes a Hilbert space a Banach space, as we’ve assumed <span class="math inline">\(H\)</span> is complete. The development of Hilbert spaces was motivated by Euclidean space, as the vector space <span class="math inline">\(\mathbb R^k\)</span> (over the field of scalars <span class="math inline">\(\mathbb R\)</span>) is a Hilbert space: <span class="math display">\[\begin{align*}
\langle \mathbf{x},\mathbf{y}\rangle &amp;= \mathbf{x}\cdot\mathbf{y}= \sum_{i=1}^k x_iy_i,\\
\left\lVert\mathbf{x}\right\rVert &amp;= \left(\sum_{i=1}^k x_i^2\right)^{1/2},\\
d(\mathbf{x},\mathbf{y}) &amp;= \left(\sum_{i=1}^k(y_i - x_i)^2\right)^{1/2}.
\end{align*}\]</span></p>
<p>Another useful Hilbert space is a special case of a normed vector space known as an <span class="math inline">\(L^p\)</span> space. For a measure space <span class="math inline">\((X,\mathcal N, \mu)\)</span>, define <span class="math inline">\(L^p\)</span> to be the set of all measurable (real) functions <span class="math inline">\(f:X\to\mathbb R\)</span>. The norm for this space is <span class="math display">\[ \left\lVert f\right\rVert_p = \left(\int\left\lvert f\right\rvert^p\ d\mu\right)^{1/p}.\]</span> In general <span class="math inline">\(L^p\)</span> spaces are not Hilbert spaces, but if <span class="math inline">\(p=2\)</span>, then they are. In this case the inner product is</p>
<p><span class="math display">\[\langle f,g \rangle = \int\left\lvert f\right\rvert\left\lvert g\right\rvert\ d\mu.\]</span> In the event that the measure space is <span class="math inline">\((\mathbb Z^+, \mathbb Z^+, \mu)\)</span> where <span class="math inline">\(\mu\)</span> is the counting measure, then a measurable function <span class="math inline">\(f:\mathbb Z^+\to\mathbb R\)</span> is a sequence of numbers real <span class="math inline">\(\{x_1,x_2,\ldots\}\)</span>. In this case we denote the <span class="math inline">\(L^p\)</span> space as <span class="math inline">\(\ell^p\)</span> and have <span class="math display">\[ \left\lVert f\right\rVert_p = \left(\int\left\lvert f\right\rvert^p\ d\mu\right)^{1/p} = \left(\sum_{i=1}^\infty |x_i|^p\right)^{1/2}.\]</span> If we <span class="math inline">\(\{x_1,x_2,\ldots,x_k, 0,0,0,\ldots \}\in\ell ^k\)</span>, we could also consider this an element of <span class="math inline">\(\mathbb R^k\)</span> because the trailing zeros. In this sense, Euclidean space is an <span class="math inline">\(L^p\)</span> space: <span class="math display">\[\left\lVert\mathbf{x}\right\rVert_2 = \left(\sum_{i=1}^k x_i^2\right)^{1/2}.\]</span></p>
<p>Two elements of a Hilbert space are orthogonal if <span class="math inline">\(\langle x,y \rangle = 0\)</span>. If <span class="math inline">\(S\subset H\)</span> is a subspace of a Hilbert space <span class="math inline">\(H\)</span>, there exists a unique element <span class="math inline">\(\hat y\in S\)</span> such that <span class="math display">\[\begin{align*}
\left\lVert x - \hat y\right\rVert &amp;= \inf_{y\in S}\left\lVert x - y\right\rVert,\\
\langle x-\hat y,z \rangle &amp;= 0 &amp; (\forall z\in S).
\end{align*}\]</span> We refer to <span class="math inline">\(\hat y\)</span> as the projection of <span class="math inline">\(x\)</span> onto <span class="math inline">\(S\)</span>.</p>
<p>For a probability space <span class="math inline">\((\Omega, \mathcal F, P)\)</span>. If we define a <span class="math inline">\(L^2\)</span> space as all the random variables (measurable functions) with squared values that are integrable, then <span class="math display">\[ \langle X,Y \rangle = \int\ xy\ dP = \text{E}\left[XY\right].\]</span> This is why we refer to random variables as orthogonal when <span class="math inline">\(\text{E}\left[XY\right] = 0\)</span>! In the event we have a random vector <span class="math inline">\(\mathbf{Z}= (Y,\mathbf{X})\)</span>, the projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(\mathbf{X}\)</span> happens to be <span class="math inline">\(\text{E}\left[Y\mid \mathbf{X}\right]\)</span>.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-angrist2008mostly" class="csl-entry" role="doc-biblioentry">
Angrist, Joshua D, and Jörn-Steffen Pischke. 2008. <span>“Mostly Harmless Econometrics.”</span> In <em>Mostly Harmless Econometrics</em>. Princeton university press.
</div>
<div id="ref-cameron2005microeconometrics" class="csl-entry" role="doc-biblioentry">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-card1995using" class="csl-entry" role="doc-biblioentry">
Card, David. 1995. <span>“Using Geographic Variation in College Proximity to Estimate the Return to Schooling.”</span> National Bureau of Economic Research Cambridge, Mass., USA.
</div>
<div id="ref-card1999causal" class="csl-entry" role="doc-biblioentry">
———. 1999. <span>“The Causal Effect of Education on Earnings.”</span> <em>Handbook of Labor Economics</em> 3: 1801–63.
</div>
<div id="ref-card2001estimating" class="csl-entry" role="doc-biblioentry">
———. 2001. <span>“Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.”</span> <em>Econometrica</em> 69 (5): 1127–60.
</div>
<div id="ref-christensen1976economies" class="csl-entry" role="doc-biblioentry">
Christensen, Laurits R, and William H Greene. 1976. <span>“Economies of Scale in US Electric Power Generation.”</span> <em>Journal of Political Economy</em> 84 (4, Part 1): 655–76.
</div>
<div id="ref-christensen1973transcendental" class="csl-entry" role="doc-biblioentry">
Christensen, Laurits R, Dale W Jorgenson, and Lawrence J Lau. 1973. <span>“Transcendental Logarithmic Production Frontiers.”</span> <em>The Review of Economics and Statistics</em>, 28–45.
</div>
<div id="ref-cobb1928theory" class="csl-entry" role="doc-biblioentry">
Cobb, Charles W, and Paul H Douglas. 1928. <span>“A Theory of Production.”</span> <em>The American Economic Review</em> 18 (1): 139–65.
</div>
<div id="ref-ding2021frisch" class="csl-entry" role="doc-biblioentry">
Ding, Peng. 2021. <span>“The Frisch–Waugh–Lovell Theorem for Standard Errors.”</span> <em>Statistics &amp; Probability Letters</em> 168: 108945.
</div>
<div id="ref-folland1999real" class="csl-entry" role="doc-biblioentry">
Folland, Gerald B. 1999. <em>Real Analysis: Modern Techniques and Their Applications</em>. Vol. 40. John Wiley &amp; Sons.
</div>
<div id="ref-frisch1933partial" class="csl-entry" role="doc-biblioentry">
Frisch, Ragnar, and Frederick V Waugh. 1933. <span>“Partial Time Regressions as Compared with Individual Trends.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 387–401.
</div>
<div id="ref-galton1886regression" class="csl-entry" role="doc-biblioentry">
Galton, Francis. 1886. <span>“Regression Towards Mediocrity in Hereditary Stature.”</span> <em>The Journal of the Anthropological Institute of Great Britain and Ireland</em> 15: 246–63.
</div>
<div id="ref-goldberger1972structural" class="csl-entry" role="doc-biblioentry">
Goldberger, Arthur S. 1972. <span>“Structural Equation Methods in the Social Sciences.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 979–1001.
</div>
<div id="ref-goldberger1991course" class="csl-entry" role="doc-biblioentry">
———. 1991. <em>A Course in Econometrics</em>. Harvard University Press.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-griliches1972education" class="csl-entry" role="doc-biblioentry">
Griliches, Zvi, and William M Mason. 1972. <span>“Education, Income, and Ability.”</span> <em>Journal of Political Economy</em> 80 (3, Part 2): S74–103.
</div>
<div id="ref-hansen2005challenges" class="csl-entry" role="doc-biblioentry">
Hansen, Bruce. 2005. <span>“Challenges for Econometric Model Selection.”</span> <em>Econometric Theory</em> 21 (1): 60–68.
</div>
<div id="ref-hansen2022econometrics" class="csl-entry" role="doc-biblioentry">
———. 2022. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-hayashi2011econometrics" class="csl-entry" role="doc-biblioentry">
Hayashi, Fumio. 2011. <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-hendry1995dynamic" class="csl-entry" role="doc-biblioentry">
Hendry, David F et al. 1995. <em>Dynamic Econometrics</em>. Oxford University Press on Demand.
</div>
<div id="ref-hendry2000econometrics" class="csl-entry" role="doc-biblioentry">
Hendry, David F. 2000. <em>Econometrics: Alchemy or Science?: Essays in Econometric Methodology</em>. OUP Oxford.
</div>
<div id="ref-leamer1978specification" class="csl-entry" role="doc-biblioentry">
Leamer, Edward E. 1978. <em>Specification Searches: Ad Hoc Inference with Nonexperimental Data</em>. Vol. 53. John Wiley &amp; Sons Incorporated.
</div>
<div id="ref-lewbel2019identification" class="csl-entry" role="doc-biblioentry">
Lewbel, Arthur. 2019. <span>“The Identification Zoo: Meanings of Identification in Econometrics.”</span> <em>Journal of Economic Literature</em> 57 (4): 835–903.
</div>
<div id="ref-lovell1963seasonal" class="csl-entry" role="doc-biblioentry">
Lovell, Michael C. 1963. <span>“Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis.”</span> <em>Journal of the American Statistical Association</em> 58 (304): 993–1010.
</div>
<div id="ref-pearson1903laws" class="csl-entry" role="doc-biblioentry">
Pearson, Karl, and Alice Lee. 1903. <span>“On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.”</span> <em>Biometrika</em> 2 (4): 357–462.
</div>
<div id="ref-phillips1996econometric" class="csl-entry" role="doc-biblioentry">
Phillips, Peter CB. 1996. <span>“Econometric Model Determination.”</span> <em>Econometrica: Journal of the Econometric Society</em>, 763–812.
</div>
<div id="ref-reiss2007structural" class="csl-entry" role="doc-biblioentry">
Reiss, Peter C, and Frank A Wolak. 2007. <span>“Structural Econometric Modeling: Rationales and Examples from Industrial Organization.”</span> <em>Handbook of Econometrics</em> 6: 4277–4415.
</div>
<div id="ref-royden1988real" class="csl-entry" role="doc-biblioentry">
Royden, Halsey Lawrence, and Patrick Fitzpatrick. 1988. <em>Real Analysis</em>. Vol. 32. Macmillan New York.
</div>
<div id="ref-rudin" class="csl-entry" role="doc-biblioentry">
Rudin, Walter. 1987. <em>Real and Complex Analysis</em>. McGraw-hill New York.
</div>
<div id="ref-stock2003introduction" class="csl-entry" role="doc-biblioentry">
Stock, James H, and Mark W Watson. 2003. <em>Introduction to Econometrics</em>. Vol. 104. Addison Wesley Boston.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="doc-biblioentry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
<div id="ref-wooldridge2015introductory" class="csl-entry" role="doc-biblioentry">
———. 2015. <em>Introductory Econometrics: A Modern Approach</em>. Cengage learning.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>For example, identification is sometimes defined in the context of the rank of a particular matrix.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Later on we may index observations by different letters depending on context. For instance if our data is a time series we’ll index observations with <span class="math inline">\(t\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Technically this equivalency only holds if <span class="math inline">\(\beta_0\neq 0\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Okay, this is admittedly a bit misleading because both dropped out of Harvard.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>We are implicitly assuming <span class="math inline">\(\beta_1\neq 0\)</span>, which eliminates the need to assume <span class="math inline">\(\text{E}\left[\varepsilon\right] = \mathbf{0}\)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>If <span class="math inline">\(X\sim \text{Binom}(n, p)\)</span>, then <span class="math inline">\(\text{E}\left[X\right] = np(1-p) + (np)^2\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>A weaker version of these assumptions are provided in chapter 4 of <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Remember that we have implicitly assumed that the true model is linear.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>I’m skirting around proving the fact that the matrix <span class="math inline">\(\mathbf{D}\)</span> cannot be negative definite as it isn’t especially informative.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>It could be the case that investment is a function of real interest rates along with inflation if inflation has affects outside of that on the real interest rate.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>I did do some cleaning of the raw data beforehand.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Is this reasonable? Could it be the case that <span class="math inline">\(P_k\)</span>, <span class="math inline">\(P_l\)</span>, and/or <span class="math inline">\(P_f\)</span> are correlated with other input prices that we don’t observe? How does the context of electrical power production impact this assumption?<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./exp_fam.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./endog.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb53" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb53-61"><a href="#cb53-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb53-62"><a href="#cb53-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb53-63"><a href="#cb53-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb53-64"><a href="#cb53-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb53-65"><a href="#cb53-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb53-66"><a href="#cb53-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb53-67"><a href="#cb53-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb53-68"><a href="#cb53-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-69"><a href="#cb53-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Classical Linear Model</span></span>
<span id="cb53-70"><a href="#cb53-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-73"><a href="#cb53-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-74"><a href="#cb53-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb53-75"><a href="#cb53-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb53-76"><a href="#cb53-76" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb53-77"><a href="#cb53-77" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb53-78"><a href="#cb53-78" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb53-79"><a href="#cb53-79" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggtern)</span>
<span id="cb53-80"><a href="#cb53-80" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb53-81"><a href="#cb53-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-82"><a href="#cb53-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-83"><a href="#cb53-83" aria-hidden="true" tabindex="-1"></a>Now that we're equipped with all the tools necessary to consider our first econometric model -- the classic linear model. </span>
<span id="cb53-84"><a href="#cb53-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-85"><a href="#cb53-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## Identification, Estimation, and Inference</span></span>
<span id="cb53-86"><a href="#cb53-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-87"><a href="#cb53-87" aria-hidden="true" tabindex="-1"></a>Before we cover our first, and arguably most important, model in econometrics, it's worth reiterating how the problems of identification, estimation, and inference are related. It's also important to consider whether these problems occur in the context of a model/population, or a sample drawn from the model/population. <span class="co">[</span><span class="ot">These slides</span><span class="co">](https://scholar.harvard.edu/files/kasy/files/ia-causalityslides-iv.pdf)</span> provide a nice diagram which provides some insight.  </span>
<span id="cb53-88"><a href="#cb53-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-91"><a href="#cb53-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-92"><a href="#cb53-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb53-93"><a href="#cb53-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-94"><a href="#cb53-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot51</span></span>
<span id="cb53-95"><a href="#cb53-95" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.5</span></span>
<span id="cb53-96"><a href="#cb53-96" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 2</span></span>
<span id="cb53-97"><a href="#cb53-97" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The relationship between models, parameters, and observed data."</span></span>
<span id="cb53-98"><a href="#cb53-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-99"><a href="#cb53-99" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/identification.png"</span>)</span>
<span id="cb53-100"><a href="#cb53-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-101"><a href="#cb53-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-102"><a href="#cb53-102" aria-hidden="true" tabindex="-1"></a>When we posit a model $\mathcal P$ (which is just a collection of probability distributions), the first thing we need to do is parameterize our model. @angrist2008mostly refer to this as a "population first" approach in which we "define the objects <span class="co">[</span><span class="ot">parameters</span><span class="co">]</span> of interest before we can use data to study <span class="co">[</span><span class="ot">estimate/make inferences</span><span class="co">]</span> about them." We then must address the question of identification -- for a parameterization $\thet \mapsto P_\thet$, does $\thet$ uniquely determine $P_\thet$? If $\thet = \thet'$, then is it necessarily the case that $P_\thet = P_{\thet'}$? For this to be the case, we will usually need to amend our initial model by adding one or more assumptions. Once this is done, we can tackle the problem of estimation/inference given a sample drawn from the population knowing that once we've made a decision (whether that be an estimate or rejecting a null hypothesis) regarding the parameter space $\boldsymbol\Theta$, that it will be equivalent to a decision about the model $\mathcal P$ via identification. </span>
<span id="cb53-103"><a href="#cb53-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-104"><a href="#cb53-104" aria-hidden="true" tabindex="-1"></a>Another way to think about identification is in terms of some "perfect" estimate of $\thet$. Imagine that you had an infinite amount of data such that it was guaranteed that $\hat{\thet} = \thet$. If $\thet$ is not identified, then our perfect estimate $\hat{\thet} = \thet$ could correspond to multiple $P_\thet \in \mathcal P$, so it is impossible to know the which model value $P_\thet$ generated the infinite amount of data which gave us our estimate. This speaks to how fundamental the problem of identification is. We usually like to focus on all the nice properties an estimator has, but even if that estimator checks all the boxes, it is meaningless if our parameters/model isn't identified. </span>
<span id="cb53-105"><a href="#cb53-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-106"><a href="#cb53-106" aria-hidden="true" tabindex="-1"></a>The term "identification" can sometimes be the cause of confusion because it appears in a wide array of contexts, and definitions of identification sometimes only apply to a specific model.^<span class="co">[</span><span class="ot">For example, identification is sometimes defined in the context of the rank of a particular matrix.</span><span class="co">]</span> For an *excellent* survey of identification in econometrics see @lewbel2019identification.    </span>
<span id="cb53-107"><a href="#cb53-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-108"><a href="#cb53-108" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional Expectation and Linear Projection</span></span>
<span id="cb53-109"><a href="#cb53-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-110"><a href="#cb53-110" aria-hidden="true" tabindex="-1"></a>We will begin with an example owing to @galton1886regression.</span>
<span id="cb53-111"><a href="#cb53-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-112"><a href="#cb53-112" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-113"><a href="#cb53-113" aria-hidden="true" tabindex="-1"></a>Suppose we are interested in how the height of two parents is related to their child's height. Let $X$ be a random variable associated with the average height of two parents, and $Y$ be a random variable associated with the height of a child. Furthermore, assume the joint distribution of $(X,Y)$ is:</span>
<span id="cb53-114"><a href="#cb53-114" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-115"><a href="#cb53-115" aria-hidden="true" tabindex="-1"></a>(X,Y) &amp;\sim N(\boldsymbol \mu, \boldsymbol \Sigma),<span class="sc">\\</span></span>
<span id="cb53-116"><a href="#cb53-116" aria-hidden="true" tabindex="-1"></a>\boldsymbol \mu &amp; = <span class="co">[</span><span class="ot">68, 68</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-117"><a href="#cb53-117" aria-hidden="true" tabindex="-1"></a>\boldsymbol \Sigma &amp; = \begin{bmatrix}8 &amp; 4<span class="sc">\\</span> 4 &amp; 6\end{bmatrix}.</span>
<span id="cb53-118"><a href="#cb53-118" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-119"><a href="#cb53-119" aria-hidden="true" tabindex="-1"></a>As a consequence, $X \sim N(68, 8)$ and $Y ~ N(68,6)$ have the same marginal density of $N(68, 8)$. In other words, the average height of individuals is the same across generations. The variance of $X$ and $Y$ are given as $\sigma_X^2=\Sig_{11}$ and $\sigma_Y^2 = \Sig_{22}$.  </span>
<span id="cb53-120"><a href="#cb53-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-123"><a href="#cb53-123" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-124"><a href="#cb53-124" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-125"><a href="#cb53-125" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot52</span></span>
<span id="cb53-126"><a href="#cb53-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-127"><a href="#cb53-127" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-128"><a href="#cb53-128" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-129"><a href="#cb53-129" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The marginal desnity of childs' and parents' height along with their joint density"</span></span>
<span id="cb53-130"><a href="#cb53-130" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-131"><a href="#cb53-131" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">68</span>, <span class="dv">68</span>)</span>
<span id="cb53-132"><a href="#cb53-132" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb53-133"><a href="#cb53-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-134"><a href="#cb53-134" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)),</span>
<span id="cb53-135"><a href="#cb53-135" aria-hidden="true" tabindex="-1"></a>           <span class="at">key =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Parents"</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="st">"Child"</span>, <span class="dv">1000</span>))</span>
<span id="cb53-136"><a href="#cb53-136" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-137"><a href="#cb53-137" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">68</span>, <span class="fu">sqrt</span>(<span class="dv">8</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb53-138"><a href="#cb53-138" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb53-139"><a href="#cb53-139" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb53-140"><a href="#cb53-140" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>key, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb53-141"><a href="#cb53-141" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-142"><a href="#cb53-142" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Height (in)"</span>,</span>
<span id="cb53-143"><a href="#cb53-143" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"density"</span>)</span>
<span id="cb53-144"><a href="#cb53-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-145"><a href="#cb53-145" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb53-146"><a href="#cb53-146" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>), </span>
<span id="cb53-147"><a href="#cb53-147" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)</span>
<span id="cb53-148"><a href="#cb53-148" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-149"><a href="#cb53-149" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df, mu, Sigma)</span>
<span id="cb53-150"><a href="#cb53-150" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y, <span class="at">z =</span> p)) <span class="sc">+</span></span>
<span id="cb53-151"><a href="#cb53-151" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb53-152"><a href="#cb53-152" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-153"><a href="#cb53-153" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>, <span class="at">y =</span> <span class="st">"Child's Height (in)"</span>)</span>
<span id="cb53-154"><a href="#cb53-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-155"><a href="#cb53-155" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb53-156"><a href="#cb53-156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-157"><a href="#cb53-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-158"><a href="#cb53-158" aria-hidden="true" tabindex="-1"></a>If we want to predict a child's height using parents' height $X = x$, we can inspect the conditional expectation $\E{Y\mid X = x}$. This expectation is given as </span>
<span id="cb53-159"><a href="#cb53-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-160"><a href="#cb53-160" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-161"><a href="#cb53-161" aria-hidden="true" tabindex="-1"></a>\E{Y\mid X = x} &amp; = \int_{\mathcal Y} y\cdot f_{Y\mid x}(y\mid x)\ dy<span class="sc">\\</span> &amp; = \int_{\mathcal Y} y\cdot \frac{f_{Y,X}(y \mid x, \boldsymbol \mu, \boldsymbol \Sigma)}{f_{X}(x \mid \mu_1, \sigma_{X})}\ dy<span class="sc">\\</span></span>
<span id="cb53-162"><a href="#cb53-162" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{\mathcal Y}y\cdot \frac{\exp\left(-\frac{1}{2}(<span class="co">[</span><span class="ot">x,y</span><span class="co">]</span>' - \boldsymbol \mu)'\boldsymbol\Sigma^{-1}(<span class="co">[</span><span class="ot">x,y</span><span class="co">]</span>' - \boldsymbol \mu)\right)/\sqrt{(2\pi)^k\det(\boldsymbol\Sigma)}}{\exp<span class="co">[</span><span class="ot">-(x-\mu_X)^2/2\sigma^2</span><span class="co">]</span>/\sqrt{2\pi\sigma_X^2}}\ dF_Y<span class="sc">\\</span></span>
<span id="cb53-163"><a href="#cb53-163" aria-hidden="true" tabindex="-1"></a>&amp;\vdots<span class="sc">\\</span></span>
<span id="cb53-164"><a href="#cb53-164" aria-hidden="true" tabindex="-1"></a>&amp; = \mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X)</span>
<span id="cb53-165"><a href="#cb53-165" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-166"><a href="#cb53-166" aria-hidden="true" tabindex="-1"></a>If we substitute in our parameters, and the calculated correlation coefficient of </span>
<span id="cb53-167"><a href="#cb53-167" aria-hidden="true" tabindex="-1"></a>$$ \rho = \frac{\cov{X,Y}}{\sigma_{X}\sigma_{Y}} = \frac{1}{4\sqrt 3} \approx 0.577,$$</span>
<span id="cb53-168"><a href="#cb53-168" aria-hidden="true" tabindex="-1"></a>we have $$ \E{Y\mid X = x} \approx 68 + \frac{\sqrt 6}{\sqrt 8}\cdot\frac{1}{4\sqrt 3}(x - 68) = 34 + \frac{1}{2}x.$$</span>
<span id="cb53-169"><a href="#cb53-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-172"><a href="#cb53-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-173"><a href="#cb53-173" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-174"><a href="#cb53-174" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-175"><a href="#cb53-175" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot53</span></span>
<span id="cb53-176"><a href="#cb53-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-177"><a href="#cb53-177" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-178"><a href="#cb53-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The density of a child's height conditioning on their parent's height"</span></span>
<span id="cb53-179"><a href="#cb53-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-180"><a href="#cb53-180" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb53-181"><a href="#cb53-181" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>) )<span class="sc">%&gt;%</span> </span>
<span id="cb53-182"><a href="#cb53-182" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_y =</span> p<span class="sc">/</span><span class="fu">dnorm</span>(x, <span class="dv">68</span>, <span class="fu">sqrt</span>(<span class="dv">8</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb53-183"><a href="#cb53-183" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(y, p_y, <span class="at">color =</span> <span class="fu">as.factor</span>(x))) <span class="sc">+</span> </span>
<span id="cb53-184"><a href="#cb53-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb53-185"><a href="#cb53-185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-186"><a href="#cb53-186" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"Parents's Height"</span>,</span>
<span id="cb53-187"><a href="#cb53-187" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Childs's Height"</span>,</span>
<span id="cb53-188"><a href="#cb53-188" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Conditional Density"</span>) <span class="sc">+</span></span>
<span id="cb53-189"><a href="#cb53-189" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb53-190"><a href="#cb53-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-191"><a href="#cb53-191" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> (Sigma[<span class="dv">1</span>,<span class="dv">2</span>])<span class="sc">/</span>(<span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>]))</span>
<span id="cb53-192"><a href="#cb53-192" aria-hidden="true" tabindex="-1"></a>s1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb53-193"><a href="#cb53-193" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb53-194"><a href="#cb53-194" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-195"><a href="#cb53-195" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">E_y =</span> mu[<span class="dv">2</span>] <span class="sc">+</span> rho<span class="sc">*</span>(s2<span class="sc">/</span>s1)<span class="sc">*</span>(x <span class="sc">-</span> mu[<span class="dv">1</span>]))</span>
<span id="cb53-196"><a href="#cb53-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-197"><a href="#cb53-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-198"><a href="#cb53-198" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> df2 <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,E_y)) <span class="sc">+</span> </span>
<span id="cb53-199"><a href="#cb53-199" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb53-200"><a href="#cb53-200" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-201"><a href="#cb53-201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>,</span>
<span id="cb53-202"><a href="#cb53-202" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">"Conditional Expectation of Child's Height"</span>)</span>
<span id="cb53-203"><a href="#cb53-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-204"><a href="#cb53-204" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb53-205"><a href="#cb53-205" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-206"><a href="#cb53-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-207"><a href="#cb53-207" aria-hidden="true" tabindex="-1"></a>The key observation is that the function $\E{Y \mid X = x}$ is linear in $x$! If we overlay the line associated with $\E{Y\mid X =x}$ on the joint density of $(X,Y)$ we end up with a figure emulating one in @galton1886regression.</span>
<span id="cb53-210"><a href="#cb53-210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-211"><a href="#cb53-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-212"><a href="#cb53-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot54</span></span>
<span id="cb53-213"><a href="#cb53-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-214"><a href="#cb53-214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-215"><a href="#cb53-215" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-216"><a href="#cb53-216" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The conditional expectation of a child's height given their parent's height in red, over the joint density of child and parent height. The red lines position in relation to the blue 45° line illustrates regression to the mean"</span></span>
<span id="cb53-217"><a href="#cb53-217" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-218"><a href="#cb53-218" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(</span>
<span id="cb53-219"><a href="#cb53-219" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>), <span class="dv">60</span>, <span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">75</span>), </span>
<span id="cb53-220"><a href="#cb53-220" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">seq</span>(<span class="dv">57</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">1000</span>)</span>
<span id="cb53-221"><a href="#cb53-221" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-222"><a href="#cb53-222" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df, mu, Sigma)</span>
<span id="cb53-223"><a href="#cb53-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-224"><a href="#cb53-224" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(x, y, <span class="at">z =</span> p)) <span class="sc">+</span></span>
<span id="cb53-225"><a href="#cb53-225" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">68</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb53-226"><a href="#cb53-226" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">68</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb53-227"><a href="#cb53-227" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb53-228"><a href="#cb53-228" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-229"><a href="#cb53-229" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Parents' (Average) Height (in)"</span>, <span class="at">y =</span> <span class="st">"Child's Height (in)"</span>) <span class="sc">+</span></span>
<span id="cb53-230"><a href="#cb53-230" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">34</span>, <span class="at">slope =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb53-231"><a href="#cb53-231" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb53-232"><a href="#cb53-232" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">label =</span> <span class="st">"E[Y|X = x]"</span>,<span class="at">x =</span> <span class="fl">75.5</span>, <span class="at">y =</span> <span class="dv">34</span> <span class="sc">+</span> <span class="dv">76</span><span class="sc">/</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">colour =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb53-233"><a href="#cb53-233" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">label =</span> <span class="st">"X = x"</span>,<span class="at">x =</span> <span class="dv">75</span>, <span class="at">y =</span> <span class="dv">74</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">colour =</span> <span class="st">"blue"</span>)</span>
<span id="cb53-234"><a href="#cb53-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-235"><a href="#cb53-235" aria-hidden="true" tabindex="-1"></a>On average, children with tall parents tend to be shorter than their parents. Conversely, children with short parents tend to be taller than their parents. In other words, as shown by the lines superimposed on $f_{X,Y}$, $\E{Y\mid X = x} &lt; x$ when $x &gt; \E{X}$ and $\E{Y\mid X = x} &gt; x$ when $x &lt; \E{X}$.</span>
<span id="cb53-236"><a href="#cb53-236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-237"><a href="#cb53-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-238"><a href="#cb53-238" aria-hidden="true" tabindex="-1"></a>This phenomenon is known as **_regression to the mean_**, and it lends its name to the practice of relating $Y$ to $X$. The ideas from @galton1886regression were extended by one of Galton's students in @pearson1903laws, who actually uses the term "regression line" in reference to the function $\E{Y\mid X}$.</span>
<span id="cb53-239"><a href="#cb53-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-240"><a href="#cb53-240" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb53-241"><a href="#cb53-241" aria-hidden="true" tabindex="-1"></a>It behooves one to acknowledge that Francis Galton's motivation for studying height among parents and children stemmed from his interest in genetics and Darwinism. He was an early proponent of eugenics, and even coined the term "eugenics". He referred to regression to the mean as "regression to mediocrity", and believed this should be avoided by selective reproduction. Many of Galton's beliefs are classic examples of scientific racism.</span>
<span id="cb53-242"><a href="#cb53-242" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-243"><a href="#cb53-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-244"><a href="#cb53-244" aria-hidden="true" tabindex="-1"></a>Let's abstract from the example of height. Suppose we have $(Y, \mathbf X)\sim F_{Y,\X}$ for some **_dependent variable_** $Y$ (with sample space $\mathcal Y$) and a vector of **_independent variables/explanatory variables/covariates/regressors_** $\X$ (with sample sapce $\mathcal X$). If we want to explore the relationship between $Y$ and $\X$, one measure of interest is the conditional expectation of $Y$ given $\X$. If we know $\X$ takes on the value $\x$, then on average, what is the value of $Y$?</span>
<span id="cb53-245"><a href="#cb53-245" aria-hidden="true" tabindex="-1"></a>$$ \E{Y\mid \X = \x} = \int_{\mathcal Y}y \ dF_{Y\mid \x} = \int_{\mathcal Y}y\cdot f_{Y\mid \x}(y\mid \x) \ dy = \int_{\mathcal Y}y\cdot\frac{f_{Y, \X}(y,\x)}{f_{\X}(\x)}\ dy$$ This conditional expectation maps values from the sample space of $\X$ to the sample space for $Y$. In this sense, $\E{Y\mid \X = \x}$ is a function mapping $\mathcal X \mapsto \mathcal Y$. $\E{Y\mid \X = \x}$ *is not* a function of $y$, as it is calculated via integrating over all values of $y\in\mathcal Y$. Following @angrist2008mostly and @hansen2022econometrics, we name this function.</span>
<span id="cb53-246"><a href="#cb53-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-247"><a href="#cb53-247" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-248"><a href="#cb53-248" aria-hidden="true" tabindex="-1"></a>If $(Y, \mathbf X)\sim F_{Y,\X}$, then the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_conditional expectation function (CEF)_**<span class="kw">&lt;/span&gt;</span> $\hat Y:\mathcal X \to \mathcal Y$ is defined as </span>
<span id="cb53-249"><a href="#cb53-249" aria-hidden="true" tabindex="-1"></a>$$\hat Y(X) = \E{Y\mid \X}. $$</span>
<span id="cb53-250"><a href="#cb53-250" aria-hidden="true" tabindex="-1"></a>The CEF is an expectation, so observations of $Y$ are bound to deviate from it. We will define this deviation as the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_CEF error_**<span class="kw">&lt;/span&gt;</span> $\varepsilon_{c} = Y - \hat Y(X)$.</span>
<span id="cb53-251"><a href="#cb53-251" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-252"><a href="#cb53-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-253"><a href="#cb53-253" aria-hidden="true" tabindex="-1"></a>In the height example, $\varepsilon_{c}$ captured how much a child's height differed from the trend given by $\E{Y\mid \X}$.</span>
<span id="cb53-254"><a href="#cb53-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-255"><a href="#cb53-255" aria-hidden="true" tabindex="-1"></a>:::{#prp-ceferr}</span>
<span id="cb53-256"><a href="#cb53-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-257"><a href="#cb53-257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of CEF Error</span></span>
<span id="cb53-258"><a href="#cb53-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-259"><a href="#cb53-259" aria-hidden="true" tabindex="-1"></a>The CEF error is:</span>
<span id="cb53-260"><a href="#cb53-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-261"><a href="#cb53-261" aria-hidden="true" tabindex="-1"></a>1) Mean independent of $\X$, $\E{\varepsilon_{c}\mid\X} = 0$;</span>
<span id="cb53-262"><a href="#cb53-262" aria-hidden="true" tabindex="-1"></a>2) Uncorrelated with $\X$, $\E{\varepsilon_{c}\X} = \zer$;</span>
<span id="cb53-263"><a href="#cb53-263" aria-hidden="true" tabindex="-1"></a>3) Uncorrelated with any function $h(\X)$, $\E{\varepsilon_{c} h(\X)} = 0$ </span>
<span id="cb53-264"><a href="#cb53-264" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-265"><a href="#cb53-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-266"><a href="#cb53-266" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-267"><a href="#cb53-267" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-268"><a href="#cb53-268" aria-hidden="true" tabindex="-1"></a>\E{\varepsilon_{c}\mid\X} &amp;= \E{Y - \hat Y(X)\mid \X}<span class="sc">\\</span></span>
<span id="cb53-269"><a href="#cb53-269" aria-hidden="true" tabindex="-1"></a>&amp; =  \E{Y - \E{Y\mid \X} \mid \X} <span class="sc">\\</span></span>
<span id="cb53-270"><a href="#cb53-270" aria-hidden="true" tabindex="-1"></a>&amp; = \E{Y\mid \X} - \E{\E{Y\mid \X} \mid \X} &amp; (\E{\cdot | \X}\text{ linear})<span class="sc">\\</span></span>
<span id="cb53-271"><a href="#cb53-271" aria-hidden="true" tabindex="-1"></a>&amp; = \E{Y\mid \X} - \E{Y\mid \X} &amp; (\text{Law of Iterated Expectations})<span class="sc">\\</span></span>
<span id="cb53-272"><a href="#cb53-272" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb53-273"><a href="#cb53-273" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-274"><a href="#cb53-274" aria-hidden="true" tabindex="-1"></a>$\X$ and $\varepsilon_{c}$ being uncorrelated is a consequence of mean independence. For some $h(\X)$, we have </span>
<span id="cb53-275"><a href="#cb53-275" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-276"><a href="#cb53-276" aria-hidden="true" tabindex="-1"></a>\E{\varepsilon_{c} h(\X)} &amp; = \E{\varepsilon_{c} h(\X) \E{\varepsilon_{c}\mid\X}} &amp; (\text{Law of Iterated Expectations})<span class="sc">\\</span></span>
<span id="cb53-277"><a href="#cb53-277" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\varepsilon_{c} h(\X)\cdot 0} &amp; (\E{\varepsilon_{c}\mid\X} = 0)<span class="sc">\\</span></span>
<span id="cb53-278"><a href="#cb53-278" aria-hidden="true" tabindex="-1"></a>&amp; = \E{0}<span class="sc">\\</span></span>
<span id="cb53-279"><a href="#cb53-279" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb53-280"><a href="#cb53-280" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-281"><a href="#cb53-281" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span></span>
<span id="cb53-282"><a href="#cb53-282" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-283"><a href="#cb53-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-284"><a href="#cb53-284" aria-hidden="true" tabindex="-1"></a>We *are not* assuming that $\E{\varepsilon_{c}\mid\X} = 0$. This equality holds by the definition of the CEF. </span>
<span id="cb53-285"><a href="#cb53-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-286"><a href="#cb53-286" aria-hidden="true" tabindex="-1"></a>So why restrict our attention to the CEF? Perhaps there are other functions $g:\mathcal X\to\mathcal Y$ which is better at predicted $Y$ than $\hat Y(\X)$. It turns out that $\hat Y(\X)$ is the function which minimizes the MSE which arises from predicted $Y$.</span>
<span id="cb53-287"><a href="#cb53-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-288"><a href="#cb53-288" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-289"><a href="#cb53-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-290"><a href="#cb53-290" aria-hidden="true" tabindex="-1"></a><span class="fu">## CEF Minimizes MSE</span></span>
<span id="cb53-291"><a href="#cb53-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-292"><a href="#cb53-292" aria-hidden="true" tabindex="-1"></a>For some arbitrary $g:\mathcal X\to\mathcal Y$,</span>
<span id="cb53-293"><a href="#cb53-293" aria-hidden="true" tabindex="-1"></a>$$ \hat Y(\X) = \E{Y\mid \X} = \argmin_{g}\E{(Y-g(\X))^2}.$$</span>
<span id="cb53-294"><a href="#cb53-294" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-295"><a href="#cb53-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-296"><a href="#cb53-296" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-297"><a href="#cb53-297" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-298"><a href="#cb53-298" aria-hidden="true" tabindex="-1"></a>\E{(Y-g(\X))^2} &amp; = \E{(Y-g(\X) + 0)^2}<span class="sc">\\</span></span>
<span id="cb53-299"><a href="#cb53-299" aria-hidden="true" tabindex="-1"></a>&amp; = \E{(Y-g(\X) + (\E{Y\mid \X} - \E{Y\mid \X}))^2} &amp; (0 = \E{Y\mid \X} - \E{Y\mid \X})<span class="sc">\\</span></span>
<span id="cb53-300"><a href="#cb53-300" aria-hidden="true" tabindex="-1"></a>&amp; = \E{(Y - \E{Y\mid \X}) + (\E{Y\mid \X} - g(\X)))^2}<span class="sc">\\</span></span>
<span id="cb53-301"><a href="#cb53-301" aria-hidden="true" tabindex="-1"></a>&amp; = \E{(Y - \E{Y\mid \X})^2 + 2(Y - \E{Y\mid \X})(\E{Y\mid \X} - g(\X)) + (\E{Y\mid \X} - g(\X))^2}<span class="sc">\\</span></span>
<span id="cb53-302"><a href="#cb53-302" aria-hidden="true" tabindex="-1"></a>&amp; = \E{(Y - \E{Y\mid \X})^2} + 2\E{(Y - \E{Y\mid \X})(\E{Y\mid \X} - g(\X)) } + \E{(\E{Y\mid \X} - h(\X))^2}<span class="sc">\\</span></span>
<span id="cb53-303"><a href="#cb53-303" aria-hidden="true" tabindex="-1"></a>&amp; = E{(Y - \E{Y\mid \X})^2} + 2\E{\varepsilon_{c}(\E{Y\mid \X} - g(\X)) } + \E{(\E{Y\mid \X} - g(\X))^2} &amp; (\varepsilon_{c} = Y - \E{Y\mid \X})</span>
<span id="cb53-304"><a href="#cb53-304" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb53-305"><a href="#cb53-305" aria-hidden="true" tabindex="-1"></a>If we define $h(\X) = \E{Y\mid \X} - g(\X)$, we can use the fact that $\varepsilon_{c}$ is uncorrelated with any function of $\X$. </span>
<span id="cb53-306"><a href="#cb53-306" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-307"><a href="#cb53-307" aria-hidden="true" tabindex="-1"></a>\E{(Y-g(\X))^2} &amp; = E{(Y - \E{Y\mid \X})^2} + 2\E{\varepsilon_{c} h(\X)} + \E{(\E{Y\mid \X} - g(\X))^2}<span class="sc">\\</span></span>
<span id="cb53-308"><a href="#cb53-308" aria-hidden="true" tabindex="-1"></a>\E{(Y-g(\X))^2} &amp; = E{(Y - \E{Y\mid \X})^2} + 2\cdot 0 + \E{(\E{Y\mid \X} - g(\X))^2} &amp; (\E{\varepsilon_{c} h(\X)} = 0)<span class="sc">\\</span> </span>
<span id="cb53-309"><a href="#cb53-309" aria-hidden="true" tabindex="-1"></a>\E{(Y-g(\X))^2} &amp; = E{(Y - \E{Y\mid \X})^2} + \E{(\E{Y\mid \X} - g(\X))^2}</span>
<span id="cb53-310"><a href="#cb53-310" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb53-311"><a href="#cb53-311" aria-hidden="true" tabindex="-1"></a>Only the second term includes the variable we are minimizing over, so </span>
<span id="cb53-312"><a href="#cb53-312" aria-hidden="true" tabindex="-1"></a>$$ \argmin_{h}\E{(Y-g(\X))^2} =\argmin_{h}\left<span class="sc">\{</span>E{(Y - \E{Y\mid \X})^2} + \E{(\E{Y\mid \X} - g(\X))^2}\right<span class="sc">\}</span> = \argmin_h \E{(\E{Y\mid \X} - g(\X))^2},$$ where $\E{(\E{Y\mid \X} - g(\X))^2} \ge 0$ because we are squaring a quantity. If we take $g(\X) = \hat Y(X) = \E{Y\mid \X}$, we have </span>
<span id="cb53-313"><a href="#cb53-313" aria-hidden="true" tabindex="-1"></a>$$ \E{(\E{Y\mid \X} - g(\X))^2} = \E{(\E{Y\mid \X} - \E{Y\mid \X})^2} = \E{0}=0.$$ Therefore the CEF does minimize the MSE in question. </span>
<span id="cb53-314"><a href="#cb53-314" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-315"><a href="#cb53-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-316"><a href="#cb53-316" aria-hidden="true" tabindex="-1"></a>This results makes the CEF the optimal candidate for predicting $Y$ given $\X = \x$ in a decision theoretic sense (taking the loss function to be quadratic), but in practice we don't actually know the precise form of the CEF. When $(X,Y) \sim N(\boldsymbol \mu, \Sig)$, we saw the CEF is linear, but this needn't be the case.  </span>
<span id="cb53-317"><a href="#cb53-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-318"><a href="#cb53-318" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-319"><a href="#cb53-319" aria-hidden="true" tabindex="-1"></a>Define the following density on the sample space $\mathcal X\times \mathcal Y = <span class="co">[</span><span class="ot">0,2</span><span class="co">]</span>\times<span class="co">[</span><span class="ot">2,4</span><span class="co">]</span>$: </span>
<span id="cb53-320"><a href="#cb53-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-321"><a href="#cb53-321" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y)=\frac{1}{8}(6-x-y)$$</span>
<span id="cb53-322"><a href="#cb53-322" aria-hidden="true" tabindex="-1"></a>The marginal density of $X$ is </span>
<span id="cb53-323"><a href="#cb53-323" aria-hidden="true" tabindex="-1"></a>$$ f_X(x) = \int_{\mathcal Y}f_{X,Y}(x,y)\ dy = \int_2^4\frac{1}{8}(6-x-y)\ dy = \frac{1}{8}(6-2x),$$ and the conditional density of $Y\mid X = x$ is </span>
<span id="cb53-324"><a href="#cb53-324" aria-hidden="true" tabindex="-1"></a>$$ f_{Y\mid x}(y\mid x) = \frac{f_{X,Y}(x,y)}{f_{X}(x)} = \frac{\frac{1}{8}(6-x-y)}{\frac{1}{8}(6-2x)} = \frac{6-x-y}{6-2x}$$ Using this to calculate the expectation $\E{Y \mid X = x}$ gives</span>
<span id="cb53-325"><a href="#cb53-325" aria-hidden="true" tabindex="-1"></a>$$\E{Y \mid X = x} = \int_{\mathcal Y} y\cdot f_{Y\mid x}(y\mid x)\ dy= \int_2^4 y\frac{6-x-y}{6-2x}\ dy = \frac{26-9x}{9-3x}.$$</span>
<span id="cb53-326"><a href="#cb53-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-329"><a href="#cb53-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-330"><a href="#cb53-330" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-331"><a href="#cb53-331" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-332"><a href="#cb53-332" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot55</span></span>
<span id="cb53-333"><a href="#cb53-333" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-334"><a href="#cb53-334" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-335"><a href="#cb53-335" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A nonlinear CEF function"</span></span>
<span id="cb53-336"><a href="#cb53-336" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-337"><a href="#cb53-337" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">1000</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-338"><a href="#cb53-338" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> (<span class="sc">-</span><span class="dv">9</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">26</span>)<span class="sc">/</span>(<span class="sc">-</span><span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="dv">9</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-339"><a href="#cb53-339" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span> </span>
<span id="cb53-340"><a href="#cb53-340" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb53-341"><a href="#cb53-341" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"x"</span>, <span class="at">y =</span> <span class="st">"Conditional Expectation of Y given x"</span>) <span class="sc">+</span></span>
<span id="cb53-342"><a href="#cb53-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-343"><a href="#cb53-343" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-344"><a href="#cb53-344" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-345"><a href="#cb53-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-346"><a href="#cb53-346" aria-hidden="true" tabindex="-1"></a>In order to give the CEF some form, we will approximate it with a linear function (which may hold for certain $f_{\X,Y}$):</span>
<span id="cb53-347"><a href="#cb53-347" aria-hidden="true" tabindex="-1"></a>$$\E{Y\mid \X } =  \X\bet.$$ Henceforth, we will assume that $\X$ includes a column of 1's such that $\X\bet$ includes an intercept term. We will take the coefficient $\bet$ to be that which gives the best linear prediction of $Y$ given $\X$.</span>
<span id="cb53-348"><a href="#cb53-348" aria-hidden="true" tabindex="-1"></a>$$ \bet = \argmin_{\mathbf{b}}\E{(Y-\X\mathbf b)^2}$$</span>
<span id="cb53-349"><a href="#cb53-349" aria-hidden="true" tabindex="-1"></a>The error associated with this projection is the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_linear projection error_**<span class="kw">&lt;/span&gt;</span>,</span>
<span id="cb53-350"><a href="#cb53-350" aria-hidden="true" tabindex="-1"></a>$\varepsilon_{\ell} =  Y-\X\mathbf b$. The linear projection error is not the same as the conditional expectation error. It is only the case that $\varepsilon_{\ell} = \varepsilon_c$ if the CEF is truly linear. </span>
<span id="cb53-351"><a href="#cb53-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-352"><a href="#cb53-352" aria-hidden="true" tabindex="-1"></a>Taking the definition of $\bet$ to be a parameterization, we can define our first model. We'll follow the naming convention of @hansen2022econometrics.</span>
<span id="cb53-353"><a href="#cb53-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-354"><a href="#cb53-354" aria-hidden="true" tabindex="-1"></a>:::{#def-cefdef}</span>
<span id="cb53-355"><a href="#cb53-355" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_linear projection (CEF) model_**<span class="kw">&lt;/span&gt;</span> is defined as $\mathcal P_\text{LP} = <span class="sc">\{</span>P_\bet \mid \bet \in \mathbb R^{k+1}<span class="sc">\}</span>$, where </span>
<span id="cb53-356"><a href="#cb53-356" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-357"><a href="#cb53-357" aria-hidden="true" tabindex="-1"></a>P_\bet &amp;= <span class="sc">\{</span>F_{\X,Y} \mid \E{Y\mid \X}= \X\bet <span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb53-358"><a href="#cb53-358" aria-hidden="true" tabindex="-1"></a>\bet &amp;= \argmin_{\mathbf{b}}\E{(Y-\X\mathbf b)^2},<span class="sc">\\</span></span>
<span id="cb53-359"><a href="#cb53-359" aria-hidden="true" tabindex="-1"></a>\X &amp; = (1, X_2, \ldots, X_K).</span>
<span id="cb53-360"><a href="#cb53-360" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-361"><a href="#cb53-361" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-362"><a href="#cb53-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-363"><a href="#cb53-363" aria-hidden="true" tabindex="-1"></a>This model is not regular, as each element $P_\bet$ is itself a collection of distributions. As the following example highlights, it won't be possible to identify the underlying $F_{\X,Y}$, only $\E{Y\mid \X}$. Consequently, each element of $\mathcal P_\text{LP}$ is a collection of distributions with a common $\E{Y\mid \X}$.</span>
<span id="cb53-364"><a href="#cb53-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-365"><a href="#cb53-365" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-366"><a href="#cb53-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-367"><a href="#cb53-367" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercise in Identification</span></span>
<span id="cb53-368"><a href="#cb53-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-369"><a href="#cb53-369" aria-hidden="true" tabindex="-1"></a>Suppose $(X,Y)\sim N(\boldsymbol \mu, \Sig)$. In this case, the CEF is actually linear and given as </span>
<span id="cb53-370"><a href="#cb53-370" aria-hidden="true" tabindex="-1"></a>$$\E{Y\mid X} = \mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X).$$ We can define *many* normal distributions with a common CEF. For example, if we have $\rho = \mu_x = \mu_y = \rho' = \mu_x' = \mu_y' = 1$, $\sigma_X = 1$, $\sigma_Y = 2$, $\sigma_X' = 2$, and $\sigma_Y' = 4$, then </span>
<span id="cb53-371"><a href="#cb53-371" aria-hidden="true" tabindex="-1"></a>$$\mu_Y + \frac{\sigma_{Y}}{\sigma_{X}}\rho(x - \mu_X) = \mu_Y' + \frac{\sigma_{Y}'}{\sigma_{X}'}\rho'(x - \mu_X') = \frac{1}{2} +x.$$ This means that $\bet = (1/2,1)$ for both distributions.</span>
<span id="cb53-372"><a href="#cb53-372" aria-hidden="true" tabindex="-1"></a>This problem is reminiscent of systems of equations in that we have so many more variables than equations, that there are infinite possibilities (**_remember this_**). This is also just the tip of the iceberg when considering how many elements are included in $P_{(1/2,1)}$. It isn't just all the Gaussian distributions where the CEF is $\frac{1}{2} +x$. It isn't all the distributions with a linear CEF which is $\frac{1}{2} +x$. It is all the distributions for which the best linear approximation of the CEF is $\frac{1}{2} +x$ (which happens to include the previous groups). Is this an issue? Well not really. We aren't concerned with the joint distribution $F_{\X,Y}$, as the only thing with any bearing on prediction here is $\E{Y\mid \X}$ (we made no assumptions about $F_{\X,Y}$ when proving that the CEF minimizes MSE). In the event we did want to identify $F_{\X,Y}$, we would need to impose additional assumptions on $\mathcal P_\text{LP}$. Consider the following assumptions:</span>
<span id="cb53-373"><a href="#cb53-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-374"><a href="#cb53-374" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$(X,Y)\sim N(\boldsymbol \mu, \Sig)$</span>
<span id="cb53-375"><a href="#cb53-375" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mu_X = 0$</span>
<span id="cb53-376"><a href="#cb53-376" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\sigma_X = \sigma_Y = 1$</span>
<span id="cb53-377"><a href="#cb53-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-378"><a href="#cb53-378" aria-hidden="true" tabindex="-1"></a>In this case, $$\E{Y\mid X} = \mu_Y + \rho x = \mu_Y + \frac{\cov{X,Y}}{\underbrace{\sigma_X \sigma_Y}_{1\cdot 1}}x = \underbrace{\mu_Y}_{\beta_1} +\underbrace{\cov{X,Y}}_{\beta_2}x,$$</span>
<span id="cb53-379"><a href="#cb53-379" aria-hidden="true" tabindex="-1"></a> Assuming $(X,Y)\sim N(\boldsymbol \mu, \Sig)$, $\mu_X = 0$, and $\sigma_X = \sigma_Y = 1$, it must be the case that</span>
<span id="cb53-380"><a href="#cb53-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-381"><a href="#cb53-381" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-382"><a href="#cb53-382" aria-hidden="true" tabindex="-1"></a>(X,Y) &amp;\sim N(\boldsymbol \mu, \boldsymbol \Sigma),<span class="sc">\\</span></span>
<span id="cb53-383"><a href="#cb53-383" aria-hidden="true" tabindex="-1"></a>\boldsymbol \mu &amp; = <span class="co">[</span><span class="ot">0, \beta_1</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-384"><a href="#cb53-384" aria-hidden="true" tabindex="-1"></a>\boldsymbol \Sigma &amp; = \begin{bmatrix}1 &amp; \beta_2<span class="sc">\\</span> \beta_2 &amp; 1\end{bmatrix}.</span>
<span id="cb53-385"><a href="#cb53-385" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-386"><a href="#cb53-386" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-387"><a href="#cb53-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-388"><a href="#cb53-388" aria-hidden="true" tabindex="-1"></a>Now we can turn to the question of identifying $\bet$.</span>
<span id="cb53-389"><a href="#cb53-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-390"><a href="#cb53-390" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-391"><a href="#cb53-391" aria-hidden="true" tabindex="-1"></a>If $\E{\X\X'}$ is invertible, then the parameter $\bet$ is identified in the linear projection (CEF) model. </span>
<span id="cb53-392"><a href="#cb53-392" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-393"><a href="#cb53-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-394"><a href="#cb53-394" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-395"><a href="#cb53-395" aria-hidden="true" tabindex="-1"></a>We must show that $\bet = \bet'$, then is it necessarily the case that $P_\bet = P_{\bet'}$. By the definition of the linear projection (CEF) model, it suffices to show that $\argmin_{\mathbf{b}}\E{(Y-\X\mathbf b)^2}$ has a solution, and that the solution is unique. </span>
<span id="cb53-396"><a href="#cb53-396" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-397"><a href="#cb53-397" aria-hidden="true" tabindex="-1"></a>\bet &amp;= \argmin_{\mathbf{b}}\E{(Y-\X\mathbf b)^2}<span class="sc">\\</span></span>
<span id="cb53-398"><a href="#cb53-398" aria-hidden="true" tabindex="-1"></a>&amp; = \argmin_{\mathbf{b}}\E{Y^2 + 2\mathbf b(\X'Y) + (\X\mathbf b)'(\X\mathbf b)}<span class="sc">\\</span></span>
<span id="cb53-399"><a href="#cb53-399" aria-hidden="true" tabindex="-1"></a>&amp; = \argmin_{\mathbf{b}} \left<span class="sc">\{</span>\E{Y^2} + 2\mathbf b\E{\X'Y} + 2\mathbf b'\E{\X'\X}\mathbf b)\right<span class="sc">\}</span></span>
<span id="cb53-400"><a href="#cb53-400" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-401"><a href="#cb53-401" aria-hidden="true" tabindex="-1"></a>The first order condition associated with this problem is </span>
<span id="cb53-402"><a href="#cb53-402" aria-hidden="true" tabindex="-1"></a>$$ 2\E{\X'Y} + 2\E{\X'\X} \bet = \zer.$$ This is equivalent to </span>
<span id="cb53-403"><a href="#cb53-403" aria-hidden="true" tabindex="-1"></a>$$\E{\X'Y} = \E{\X'\X} \bet.$$ We now use the assumption that $\E{\X'\X}$ is invertible to solve for a unique solution for $\bet$:</span>
<span id="cb53-404"><a href="#cb53-404" aria-hidden="true" tabindex="-1"></a>$$ \bet = \left(\E{\X'\X}\right)^{-1}\E{\X'Y}.$$</span>
<span id="cb53-405"><a href="#cb53-405" aria-hidden="true" tabindex="-1"></a>Therefore, $\bet$ is identified.</span>
<span id="cb53-406"><a href="#cb53-406" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-407"><a href="#cb53-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-408"><a href="#cb53-408" aria-hidden="true" tabindex="-1"></a>To illustrate how the assumption that $\E{\X'\X}$ is invertible leads to identification, consider what happens when it does not hold. In $\E{\X'\X}$ is not invertible, than $\X'\X$ does not have full rank</span>
<span id="cb53-409"><a href="#cb53-409" aria-hidden="true" tabindex="-1"></a>and has infinite solutions. Suppose we have:</span>
<span id="cb53-410"><a href="#cb53-410" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-411"><a href="#cb53-411" aria-hidden="true" tabindex="-1"></a>\E{\X'Y} &amp; = \begin{bmatrix} 1 <span class="sc">\\</span> 1 \end{bmatrix}.<span class="sc">\\</span></span>
<span id="cb53-412"><a href="#cb53-412" aria-hidden="true" tabindex="-1"></a>\E{\X'\X} &amp; = \begin{bmatrix} 1 &amp; 1<span class="sc">\\</span> 0 &amp; 0 \end{bmatrix}.</span>
<span id="cb53-413"><a href="#cb53-413" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-414"><a href="#cb53-414" aria-hidden="true" tabindex="-1"></a>In this case</span>
<span id="cb53-415"><a href="#cb53-415" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-416"><a href="#cb53-416" aria-hidden="true" tabindex="-1"></a>&amp;\E{\X'Y} = \E{\X'\X} \bet<span class="sc">\\</span></span>
<span id="cb53-417"><a href="#cb53-417" aria-hidden="true" tabindex="-1"></a>\implies &amp;\begin{bmatrix} 1 &amp; 1<span class="sc">\\</span> 0 &amp; 0 \end{bmatrix}\begin{bmatrix} \beta_1 <span class="sc">\\</span> \beta_2 \end{bmatrix} = \begin{bmatrix} 1 <span class="sc">\\</span> 1 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-418"><a href="#cb53-418" aria-hidden="true" tabindex="-1"></a>\implies &amp; \beta_1 + \beta_2 = 1.</span>
<span id="cb53-419"><a href="#cb53-419" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-420"><a href="#cb53-420" aria-hidden="true" tabindex="-1"></a>This final equation has an infinite number of solutions. If two of those solutions happen to be $\bet$ and $\bet'$, then $P_{\bet,\Sig} = P_{\bet'}$ despite  $\bet\neq\bet'$.</span>
<span id="cb53-421"><a href="#cb53-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-422"><a href="#cb53-422" aria-hidden="true" tabindex="-1"></a><span class="fu">## Structural Models and *The* Linear Model</span></span>
<span id="cb53-423"><a href="#cb53-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-424"><a href="#cb53-424" aria-hidden="true" tabindex="-1"></a>The CEF approach to regression aims to describe a characteristic of the joint density $f_{Y,\X}$. It captures an association between variables, but not a causal link. Econometricians are often concerned with causal links to inform economic policy, something which differentiates econometrics from statistics. This is why the approach to linear regression seen in standard econometrics sources such as @greene2003econometric, @wooldridge2010econometric, @hayashi2011econometrics, @wooldridge2015introductory, and @stock2003introduction take a **_structural_** approach to linear regression. Before giving a heuristic definition of "structural", let's consider an example due to  @reiss2007structural and inspired @cobb1928theory.  </span>
<span id="cb53-425"><a href="#cb53-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-426"><a href="#cb53-426" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-427"><a href="#cb53-427" aria-hidden="true" tabindex="-1"></a>Assume a firms output $Y$ is related to labor input $L$ and capital input $K$ according to $$Q = AL^{\beta}K^{\alpha}.$$ The total factor of productivity is $A$, while $L$ and $K$ are the elasticity of output with respect to labor and capital, respectively. The production function can be written as </span>
<span id="cb53-428"><a href="#cb53-428" aria-hidden="true" tabindex="-1"></a>$$ \log Q = \log A + \beta \log L + \alpha \log K.$$ Now consider the linear regression:</span>
<span id="cb53-429"><a href="#cb53-429" aria-hidden="true" tabindex="-1"></a>$$ \log Q = \log A + \beta \log L + \alpha \log K + \varepsilon$$ where $\varepsilon$ is an error addressing the fact that the linear relationship may not hold perfectly. In this case are $(\log A, \alpha, \beta)$ associated with the best linear projection of $\log Q$ onto $\log L$ and $\alpha \log K$, or do they correspond to the factor of productivity, and elasticities of output? If the latter is the case, then what does $\varepsilon$ represent in the context of the deterministic theoretical relationship $Q = AL^{\beta}K^{\alpha}$? It will turn out that for the coefficients of the best linear projection to coincide with the economic interpretation from $Q = AL^{\beta}K^{\alpha}$, we will need to impose assumptions about $\varepsilon$, a step that is one of the defining features of econometrics. </span>
<span id="cb53-430"><a href="#cb53-430" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-431"><a href="#cb53-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-432"><a href="#cb53-432" aria-hidden="true" tabindex="-1"></a>The key difference between this example and the height example from @galton1886regression's is that we are now trying to root our linear regression in structure provided by economic theory/intuition, as to enable us to make economic conclusions. Philip Haile distinguishes these approaches in an excellent set of <span class="co">[</span><span class="ot">slides</span><span class="co">](http://www.princeton.edu/~reddings/tradephd/Haile_theorymeas.pdf)</span>. He would classify @galton1886regression's work as descriptive as it "estimates relationships between observables". This is opposed to a structural approach which "estimates features of a data generating process (i.e, a model) that are (assumed to be) invariant to the policy changes or other counterfactuals of interest." This difference is also linked to the error in the linear regression, $\varepsilon$. As put by @reiss2007structural:</span>
<span id="cb53-433"><a href="#cb53-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-434"><a href="#cb53-434" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;Where did the error term in the empirical model come from? The</span></span>
<span id="cb53-435"><a href="#cb53-435" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;answer to this question is critical because it affects whether...</span></span>
<span id="cb53-436"><a href="#cb53-436" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;the parameters of the Cobb–Douglas production function </span><span class="co">[</span><span class="ot">are identified</span><span class="co">]</span><span class="at">, as opposed to</span></span>
<span id="cb53-437"><a href="#cb53-437" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;the parameters of the best linear predictor of the logarithm of</span></span>
<span id="cb53-438"><a href="#cb53-438" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;output given the logarithms of the two inputs </span><span class="co">[</span><span class="ot">being identified</span><span class="co">]</span><span class="at">. In other words, it is the combination of</span></span>
<span id="cb53-439"><a href="#cb53-439" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;an economic assumption (production is truly Cobb–Douglas) and statistical assumptions ($\varepsilon$ </span></span>
<span id="cb53-440"><a href="#cb53-440" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;satisfies certain moment conditions) that distinguishes a structural model from</span></span>
<span id="cb53-441"><a href="#cb53-441" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;a descriptive one.</span></span>
<span id="cb53-442"><a href="#cb53-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-443"><a href="#cb53-443" aria-hidden="true" tabindex="-1"></a>In an effort to beat a dead horse, a final definition of a structural model is due to @goldberger1972structural, who simply puts "By structural equation models, I refer to stochastic models in which each equation represents a causal link, rather than a mere empirical association." None of this is to say that descriptive model is not useful. Just like descriptive statistics give insight into data, a descriptive model (such as the linear projection model) is an excellent way to investigate data, and findings may inform the development of a structural model.     </span>
<span id="cb53-444"><a href="#cb53-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-445"><a href="#cb53-445" aria-hidden="true" tabindex="-1"></a>Let's now reintroduce linear regression from a structural perspective. We will do so with no assumptions about our model, and amend our definition as we determine which assumptions are required for identification and desirable properties of estimators. The goal of this approach is to emphasize that the assumptions associated with an econometric model aren't set in stone from the onset. You begin with little to no assumptions, and then determine which assumptions are necessary as you analyze the model and accompanying estimators. </span>
<span id="cb53-446"><a href="#cb53-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-447"><a href="#cb53-447" aria-hidden="true" tabindex="-1"></a>We have a vector of $K$ regressors $\X = <span class="co">[</span><span class="ot">X_1,\ldots,X_K</span><span class="co">]</span>$ (assuming $X_1 = 1$ to allow for an intercept), structural parameters $\bet = <span class="co">[</span><span class="ot">\beta_1,\ldots,\beta_n</span><span class="co">]</span>'$ , and some structural error term $\varepsilon$. The density underlying the model is the joint density between regressors and the error $f_{\X,\varepsilon}$. The independent variable $Y$ is given as</span>
<span id="cb53-448"><a href="#cb53-448" aria-hidden="true" tabindex="-1"></a>$$ Y = \X\bet + \varepsilon.$$ The major difference between this and the linear projection model is the underlying density for the latter is $f_{\X,Y}$ where $\bet$ and $\varepsilon$ are defined using this density. Now we're determining $Y$ via some structural parameter $\bet$ and the density $f_{\X,\varepsilon}$. There are many situations in which the realizations of $\varepsilon$ may not be identically, or independently, distributed, so we need to consider the joint density of $\ep = (\varepsilon_1, \ldots, \varepsilon_n)$ where our sample is size $n$. This joint density is $f_{\ep}$. The underlying density from which we draw regressors and errors *is not* $f_{\X,\ep}$, as a realization from this distribution would be comprised of $K$ regressors and $n$ errors. What we need is the joint density of $\ep$ and $n$ observations of $\X$, so we need to consider the following random matrix:</span>
<span id="cb53-449"><a href="#cb53-449" aria-hidden="true" tabindex="-1"></a>$$\Xm = \begin{bmatrix}\X_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_i <span class="sc">\\</span> \vdots<span class="sc">\\</span> \X_n\end{bmatrix}$$ A sample of $n$ observations of $K$ regressors $\X$ and errors $\ep$ is a single realization drawn from the density $f_{\Xm,\ep}$.</span>
<span id="cb53-450"><a href="#cb53-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-451"><a href="#cb53-451" aria-hidden="true" tabindex="-1"></a>$$ \Y = \Xm\bet + \ep = \begin{bmatrix}  \X_1\bet + \varepsilon_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_i\bet + \varepsilon_i <span class="sc">\\</span> \vdots <span class="sc">\\</span>  \X_n\bet + \varepsilon_n  \end{bmatrix} = \begin{bmatrix}  \beta_0 + \beta_1X_{21}  + \cdots +\beta_KX_{K1}+ \varepsilon_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \beta_0 + \beta_1X_{2i}  + \cdots +\beta_KX_{Ki}+ \varepsilon_i  <span class="sc">\\</span> \vdots <span class="sc">\\</span>  \beta_0 + \beta_1X_{2n}  + \cdots +\beta_KX_{Kn}+ \varepsilon_n  \end{bmatrix}$$</span>
<span id="cb53-452"><a href="#cb53-452" aria-hidden="true" tabindex="-1"></a>We could also write $\Xm$ as $K$ column vectors of length $n$, each corresponding to the $n$ observations of each regressor.</span>
<span id="cb53-453"><a href="#cb53-453" aria-hidden="true" tabindex="-1"></a>$$\Xm = \begin{bmatrix}\X_1 &amp; \cdots &amp; \X_j &amp; \cdots&amp; \X_K\end{bmatrix}.$$ To distinguish between $\X_i$ (one observation of $K$ regressors) and $\X_j$ ($n$ observations of one regressor), we will use the indices $i$ and $j$, respectively.^<span class="co">[</span><span class="ot">Later on we may index observations by different letters depending on context. For instance if our data is a time series we'll index observations with $t$.</span><span class="co">]</span> We will assume that our data is the result of a random sample of observations of regressors $\X_i$:</span>
<span id="cb53-454"><a href="#cb53-454" aria-hidden="true" tabindex="-1"></a>$$ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i}.$$ The random sample assumption is essential as it will allow us to apply the LLN and CLT.</span>
<span id="cb53-455"><a href="#cb53-455" aria-hidden="true" tabindex="-1"></a>Finally, we introduce a parameter which dictates the variance of the error $\ep$. This will be the PSD matrix $\Sig = \var{\ep\mid\Xm}$. Now we can define the linear model in the absence of assumptions.</span>
<span id="cb53-456"><a href="#cb53-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-457"><a href="#cb53-457" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-458"><a href="#cb53-458" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_linear model_**<span class="kw">&lt;/span&gt;</span> is defined as $\mathcal P_\text{LM} = <span class="sc">\{</span>P_{\bet,\Sig} \mid \bet \in \mathbb R^{K},\ \Sig \in \mathbb R^{n\times n} <span class="sc">\}</span>$, where </span>
<span id="cb53-459"><a href="#cb53-459" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-460"><a href="#cb53-460" aria-hidden="true" tabindex="-1"></a>P_{\bet,\Sig} &amp;= <span class="sc">\{</span>F_{\Xm,\ep} \mid \Y= \Xm\bet +\ep,\ \Sig  = \var{\ep\mid\Xm},\ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i} <span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb53-461"><a href="#cb53-461" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_j, \cdots \X_K</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_i, \cdots \X_n</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-462"><a href="#cb53-462" aria-hidden="true" tabindex="-1"></a>\Y &amp; = <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>,<span class="sc">\\</span></span>
<span id="cb53-463"><a href="#cb53-463" aria-hidden="true" tabindex="-1"></a>\ep &amp; = <span class="co">[</span><span class="ot">\varepsilon_1, \ldots, \varepsilon_n</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-464"><a href="#cb53-464" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-465"><a href="#cb53-465" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-466"><a href="#cb53-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-467"><a href="#cb53-467" aria-hidden="true" tabindex="-1"></a>When discussing a model  $P_{\bet,\Sig}\in\mathcal P_\text{LM}$, **we are implicitly assuming that the specification of the model is correct, and regressors are IID**. If the model were not linear than $P_{\bet,\Sig}\notin\mathcal P_\text{LM}$, which is not our focus at the moment, but is a legitimate concern. </span>
<span id="cb53-468"><a href="#cb53-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-469"><a href="#cb53-469" aria-hidden="true" tabindex="-1"></a>In the context of a structural linear model, $\ep$ is not simply an approximation error. In introduces a stochastic element to a deterministic economic model. @reiss2007structural enumerate four ways that this randomness can be introduced. We will explore these in the context of the Cobb-Douglas production model where $\log Q_i = \log A + \beta \log L_i + \alpha \log K_i + \varepsilon$ for an observation from firm $i$.</span>
<span id="cb53-470"><a href="#cb53-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-471"><a href="#cb53-471" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We may be uncertain about the economic environment at hand.</span>
<span id="cb53-472"><a href="#cb53-472" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Agent uncertainty about the economic environment;</span>
<span id="cb53-473"><a href="#cb53-473" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Optimization errors on the part of economic agents;</span>
<span id="cb53-474"><a href="#cb53-474" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Measurement errors in observed variables.</span>
<span id="cb53-475"><a href="#cb53-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-476"><a href="#cb53-476" aria-hidden="true" tabindex="-1"></a>:::{#exm-car}</span>
<span id="cb53-477"><a href="#cb53-477" aria-hidden="true" tabindex="-1"></a>Suppose an agent is deciding between purchasing two cars ($j=1,2$) has a linear utility function $u_{ij} = \X_{ij}\bet$. The vector $\x_{ij}$ are attributes of car $j$ (size, mpg, make, model, etc.). We do observe their choice of vehicle $y_i$, but cannot observe their utility from the respective vehicles. Assuming agents maximize their utility, then their choice can be defined as </span>
<span id="cb53-478"><a href="#cb53-478" aria-hidden="true" tabindex="-1"></a>$$y_i = \begin{cases}\text{car }1&amp; u_{i1} \ge u_{i2}<span class="sc">\\</span> \text{car }2&amp; u_{i2} &gt; u_{i1} \end{cases}.$$ </span>
<span id="cb53-479"><a href="#cb53-479" aria-hidden="true" tabindex="-1"></a>How would we incorporate $\varepsilon$ into our model? In the linear model the error directly affects the dependent variable, but in this case the (presumable) dependent variable $y_i$ is an indicator. It doesn't make sense to add a stochastic element to it, as we observe a customer's choice with no uncertainty.   </span>
<span id="cb53-480"><a href="#cb53-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-481"><a href="#cb53-481" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>People are inherently heterogeneous in the utility they receive from any product. One agent may live in a city with access to public transit and would not gain much utility from a car, while another agent may live in a rural area and rely on a car to commute to work and run errands. The error term $\varepsilon_i$ could correct for these differences. It also could be the case that the error is specific to a consumer *and* a particular vehicle $j$. Maybe consumer $i$'s is particularly loyal to the manufacturer of car $j$ and they receive more utility as a result. This could be captured with an error $\varepsilon_{ij}$.</span>
<span id="cb53-482"><a href="#cb53-482" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>An agent may be not have the opportunity to test drive each car before purchasing, so their is some uncertainty as to how much utility they would receive from purchasing it. This uncertainty could be incorporated via $\varepsilon_{ij}$.</span>
<span id="cb53-483"><a href="#cb53-483" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>An agent may not be perfectly rational and could make a mistake while attempting to maximize their utility. They could purchase car $j=2$ despite the fact that $u_{i1} \ge u_{i2}$. To correct for this miscalculation, we could include an error $\varepsilon_{ij}$ such that $u_{i1} + \varepsilon_{i1} \le u_{i2} + \varepsilon_{i2}$</span>
<span id="cb53-484"><a href="#cb53-484" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>We may not be able to perfectly measure all the variables in the model. If one of the attributes in the vector $\x_{ij}$ is price, but we only observe MSRP, then we aren't accounting for the fact that some customers may have purchased their car for a lower price (it could be used, or on sale). This measurement error can be accounted for with $\varepsilon_{ij}$</span>
<span id="cb53-485"><a href="#cb53-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-486"><a href="#cb53-486" aria-hidden="true" tabindex="-1"></a>It's important to notice how the error term is indexed in each example. Sometimes the error arises because of the agent $i$ ($\varepsilon_i$), or the agent and the specific car ($\varepsilon_{ij}$). There could also be ways to incorporate an error that is specific to each car, but not agents ($\varepsilon_j$). Later on in Section \@ref(binary-choice) we will talk about how to estimate models like one.</span>
<span id="cb53-487"><a href="#cb53-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-488"><a href="#cb53-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-489"><a href="#cb53-489" aria-hidden="true" tabindex="-1"></a>The precise interpretation of $\ep$ is key if we want to justify the statistical assumptions about $\ep$ which @reiss2007structural cite as a key player in identification. </span>
<span id="cb53-490"><a href="#cb53-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-491"><a href="#cb53-491" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb53-492"><a href="#cb53-492" aria-hidden="true" tabindex="-1"></a>Another classical assumption of linear regression that we have explicitly violated is that $\Xm$ is a matrix of constants. In certain settings researchers are able to determine the values before collecting data. For instance, in a laboratory setting you may have enough control over the (observed/sampled) regressors  as to be able to record the value of $\Y$ at predetermined realizations of $\Xm$. This is rarely the case in social sciences, the realm in which econometrics exists. For this reason, we treat $\Xm$ as random, and the case of fixed regressors as a special case. In practice, this means we need to condition on $\Xm$ when considering expectations and variances of quantities related to $\Xm$.  </span>
<span id="cb53-493"><a href="#cb53-493" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-494"><a href="#cb53-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-495"><a href="#cb53-495" aria-hidden="true" tabindex="-1"></a>Is it still the case that $\bet$ is identified when $\E{\X'\X}$ is invertible? It turns out that we will need an additional assumption that we got "for free" with the CEF model via Proposition \@ref(prp:ceferr), that being that $\ep$ and $\X$ are uncorrelated. </span>
<span id="cb53-496"><a href="#cb53-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-497"><a href="#cb53-497" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-498"><a href="#cb53-498" aria-hidden="true" tabindex="-1"></a>The covariates $\X$ are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_weakly exogenous_**<span class="kw">&lt;/span&gt;</span> if $\E{X_i\varepsilon_i} = 0$. In matrix form this is </span>
<span id="cb53-499"><a href="#cb53-499" aria-hidden="true" tabindex="-1"></a>$$\E{\X'\ep} = \zer.$$ Equivalently,^<span class="co">[</span><span class="ot">Technically this equivalency only holds if $\beta_0\neq 0$.</span><span class="co">]</span> $\X$ is weakly exogenous if:</span>
<span id="cb53-500"><a href="#cb53-500" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-501"><a href="#cb53-501" aria-hidden="true" tabindex="-1"></a>\E{\ep} &amp;= \zer <span class="sc">\\</span></span>
<span id="cb53-502"><a href="#cb53-502" aria-hidden="true" tabindex="-1"></a>\cov{\X,\varepsilon_i}&amp;=0 &amp; (i=1,\ldots,n).</span>
<span id="cb53-503"><a href="#cb53-503" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-504"><a href="#cb53-504" aria-hidden="true" tabindex="-1"></a>If this assumption fails, we say $\X$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_endogenous_**<span class="kw">&lt;/span&gt;</span>. </span>
<span id="cb53-505"><a href="#cb53-505" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-506"><a href="#cb53-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-507"><a href="#cb53-507" aria-hidden="true" tabindex="-1"></a>You may hear this assumption associated with the word "orthogonal", or known as an **_orthogonality condition_**. This is the precise type of assumption that @reiss2007structural referred to when talking about the role $\varepsilon$ plays in structural models. If $\X_1$ is the column of 1s associated with the intercept $\beta_1$, then $\E{\X_1\ep} = 0$.</span>
<span id="cb53-508"><a href="#cb53-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-509"><a href="#cb53-509" aria-hidden="true" tabindex="-1"></a>:::{#exm-endogex}</span>
<span id="cb53-510"><a href="#cb53-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-511"><a href="#cb53-511" aria-hidden="true" tabindex="-1"></a><span class="fu">## Endogeneity</span></span>
<span id="cb53-512"><a href="#cb53-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-513"><a href="#cb53-513" aria-hidden="true" tabindex="-1"></a>A classic example in econometrics due to labor economists is estimating the impact that education has on earnings. An early paper to consider this was @griliches1972education, @card1995using is perhaps the most famous attempt at estimating this effect (@card1999causal and @card2001estimating  reviews similar studies and survey approaches to this problem). Economic intuition tells us that the more schooling someone receives, the higher their earnings/salary will be. Professions that are associated with high salaries often require (or are associated with) graduates degrees: doctors need an MD, lawyers need a JD, and business executives often have MBAs. On the opposite side of the spectrum, many white collar jobs require a college diploma, so only having a high school diploma limits a prospective employee's ability to qualify for certain jobs which traditionally have higher pay. This observation leads us to posit the deterministic relationship:</span>
<span id="cb53-514"><a href="#cb53-514" aria-hidden="true" tabindex="-1"></a>$$\log(income_i) = \beta_1 + \beta_2\cdot educ_i,$$ where $income_i$ is an agent $i$'s annual income and $educ_i$ is years of post-secondary education (we will operate under the assumption that every agent has a high school diploma). There are, of course, other factors impacting earnings (work experience, profession, location of residence, etc.) that are readily observable, but for the purpose of the example we will ignore those. There are of course exceptions to this deterministic relationship. Bill Gates and Mark Zuckerberg both only have high school diplomas,^<span class="co">[</span><span class="ot">Okay, this is admittedly a bit misleading because both dropped out of Harvard.</span><span class="co">]</span> but have higher incomes than virtually everyone in the world. To account for this, we introduce the stochastic element $\varepsilon_i$ to our model.</span>
<span id="cb53-515"><a href="#cb53-515" aria-hidden="true" tabindex="-1"></a>$$\log(income_i) = \beta_1 + \beta_2\cdot educ_i + \varepsilon_i$$</span>
<span id="cb53-516"><a href="#cb53-516" aria-hidden="true" tabindex="-1"></a>In this case, $\varepsilon_i$ corresponds to all the other unobservable determinants of earnings. A major unobservable determinant is innate ability. Bill Gates and Mark Zuckerberg make a great deal of money because of their ambition, business acumen, and ability to innovate despite not having a college degree. It's not really possible to measure something abstract like someone's ambition, so the best we can do is incorporate it with $\varepsilon_i$. Is it the case that $\E{\Xm'\ep} = \zer$ in this case? Most likely not. In all likelihood $\E{educ_i\cdot\varepsilon_i}\neq 0$, because people who are ambitious and have an innate ability to innovate tend to pursue higher education to further their abilities. If this hypothesis is true, then $educ_i$ is endogenous. </span>
<span id="cb53-517"><a href="#cb53-517" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-518"><a href="#cb53-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-519"><a href="#cb53-519" aria-hidden="true" tabindex="-1"></a>We also can give a name to the assumption that $\E{\X'\X}$ is invertible. </span>
<span id="cb53-520"><a href="#cb53-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-521"><a href="#cb53-521" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-522"><a href="#cb53-522" aria-hidden="true" tabindex="-1"></a>The covariates $\X$ exhibit  <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(perfect) multicollinearity_**<span class="kw">&lt;/span&gt;</span> if $$\text{rank}\left(\E{\X'\X}\right) = K,$$ which is equivalent to $\E{\X'\X}$ failing to be invertible.</span>
<span id="cb53-523"><a href="#cb53-523" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-524"><a href="#cb53-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-525"><a href="#cb53-525" aria-hidden="true" tabindex="-1"></a>In the event that $\E{\X'\X}$ is not invertible, then there exists some linear dependence between the set of covariates $(1,X_1,\ldots,X_k)$, i.e one regressor is a linear function of another. These two assumptions insure that $\bet$ is identified for $\mathcal P_\text{LM}$.</span>
<span id="cb53-526"><a href="#cb53-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-527"><a href="#cb53-527" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb53-528"><a href="#cb53-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-529"><a href="#cb53-529" aria-hidden="true" tabindex="-1"></a><span class="fu">## Identification of the Linear Model</span></span>
<span id="cb53-530"><a href="#cb53-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-531"><a href="#cb53-531" aria-hidden="true" tabindex="-1"></a>If $\X$ is weakly exogenous and does not exhibit multicollinearity, then $(\bet, \Sig)$ are identified for the linear model $\mathcal P_\text{LM},$ and $\beta$ given as </span>
<span id="cb53-532"><a href="#cb53-532" aria-hidden="true" tabindex="-1"></a>$$\bet = \left(\E{\X'\X}\right)^{-1}\E{\X'Y}.$$</span>
<span id="cb53-533"><a href="#cb53-533" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-534"><a href="#cb53-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-535"><a href="#cb53-535" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-536"><a href="#cb53-536" aria-hidden="true" tabindex="-1"></a>Weak exogeneity gives  </span>
<span id="cb53-537"><a href="#cb53-537" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-538"><a href="#cb53-538" aria-hidden="true" tabindex="-1"></a>&amp;\E{\X'\ep} = \zer<span class="sc">\\</span></span>
<span id="cb53-539"><a href="#cb53-539" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\X'(Y-\X\bet)} = \zer &amp; (\ep = (Y-\X\bet))<span class="sc">\\</span></span>
<span id="cb53-540"><a href="#cb53-540" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\X'Y}-\bet\E{\X'\X}= \zer<span class="sc">\\</span></span>
<span id="cb53-541"><a href="#cb53-541" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\X'Y} = \bet\E{\X'\X}.</span>
<span id="cb53-542"><a href="#cb53-542" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-543"><a href="#cb53-543" aria-hidden="true" tabindex="-1"></a>We have also assumed that $\X$ does not exhibit multicollinearity, so $\E{\X'\X}$ is invertible. This means $\E{\X'Y} = \bet\E{\X'\X}$ has a unique solution in the form of </span>
<span id="cb53-544"><a href="#cb53-544" aria-hidden="true" tabindex="-1"></a>$\bet = \left(\E{\X'\X}\right)^{-1}\E{\X'Y}.$ If $\bet$ is unique, then $\Sig$ is unique and written as </span>
<span id="cb53-545"><a href="#cb53-545" aria-hidden="true" tabindex="-1"></a>$$ \Sig = \var{\ep\mid\Xm } = \var{Y - \X\bet\mid \Xm} = \var{Y - \X\left<span class="co">[</span><span class="ot">\left(\E{\X'\X}\right)^{-1}\E{\X'Y}\right</span><span class="co">]</span>\mid \Xm }.$$ Therefore, if $(\bet,\Sig)\neq(\bet',\Sig')$, then $\X\bet + \ep \neq \X\bet' + \ep$ and $\Sig \neq \Sig$, so $P_{\bet,\Sig}\neq P_{\bet',\Sig'}$, meaning our parameters are identified.</span>
<span id="cb53-546"><a href="#cb53-546" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-547"><a href="#cb53-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-548"><a href="#cb53-548" aria-hidden="true" tabindex="-1"></a>The parameter $\bet$ takes the same analytic form as that of the linear projection (CEF) model, but it's important to remember that they arise from different approaches. We arrived at this form using the relationship between $\X$ and $\ep$ in a structural model, not from defining $\bet$ to be the solution to an optimization problem.</span>
<span id="cb53-549"><a href="#cb53-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-550"><a href="#cb53-550" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-551"><a href="#cb53-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-552"><a href="#cb53-552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multicollinearity</span></span>
<span id="cb53-553"><a href="#cb53-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-554"><a href="#cb53-554" aria-hidden="true" tabindex="-1"></a>Suppose $Y = 1 + 5 X_1 + 2 X_2 + \varepsilon$ where $X_1 = 3X_2$ (suppressing the indices $i$ which are not relevant at the moment). This model corresponds to the parameters $\bet = (1,5,2)$ We can rewrite our model as </span>
<span id="cb53-555"><a href="#cb53-555" aria-hidden="true" tabindex="-1"></a>$$ Y = 1 + 5 X_1 + 2 X_2 + \varepsilon = 1 + 5(3X_2) + 2 X_2 + \varepsilon = 1 + 0X_1 + 17X_2 + \varepsilon,$$ so the model also corresponds to parameters $\bet'=(1,0,17)$, and our model is not identified. </span>
<span id="cb53-556"><a href="#cb53-556" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-557"><a href="#cb53-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-558"><a href="#cb53-558" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-559"><a href="#cb53-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-560"><a href="#cb53-560" aria-hidden="true" tabindex="-1"></a><span class="fu">## Non-Zero Mean Errors</span></span>
<span id="cb53-561"><a href="#cb53-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-562"><a href="#cb53-562" aria-hidden="true" tabindex="-1"></a>Suppose $Y = 1 + 5 X_1 + 2 X_2 + \varepsilon$ where $\E{\varepsilon} = 3$ and $\var{\varepsilon\mid \X} = \sigma^2$. In this case $\E{\X'\ep}\neq 0$, so we shouldn't expect that $\bet$ is identified. In particular, we won't be able to identify $\beta_0$. We can write $\varepsilon = 3 + \nu$ for $\var{\nu\mid \X} = \sigma^2$, giving </span>
<span id="cb53-563"><a href="#cb53-563" aria-hidden="true" tabindex="-1"></a>$$ Y = 1 + 5 X_1 + 2 X_2 + (3 + \nu) = Y = 4 + 5 X_1 + 2 X_2 + \nu.$$</span>
<span id="cb53-564"><a href="#cb53-564" aria-hidden="true" tabindex="-1"></a>So the model can be written with parameters $(<span class="co">[</span><span class="ot">1,5,2</span><span class="co">]</span>', \sigma^2)$ or with parameters $(<span class="co">[</span><span class="ot">4,5,2</span><span class="co">]</span>', \sigma^2)$. Therefore the model, in particular $\beta_0$, is not identified. </span>
<span id="cb53-565"><a href="#cb53-565" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-566"><a href="#cb53-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-567"><a href="#cb53-567" aria-hidden="true" tabindex="-1"></a>We will consider what happens when $\E{\X'\ep}=0$, how this situation arises, and what can be done about it in Section \@ref(endogeniety-i-iv-and-2sls). For now, let's update our model with our identifying assumptions </span>
<span id="cb53-568"><a href="#cb53-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-569"><a href="#cb53-569" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-570"><a href="#cb53-570" aria-hidden="true" tabindex="-1"></a>The **_(identified) linear model_** is defined as $\mathcal P_\text{LM} = <span class="sc">\{</span>P_{\bet,\Sig} \mid \bet \in \mathbb R^{K}, \Sig\in\mathbb R^n\times\mathbb R^n<span class="sc">\}</span>$,^<span class="co">[</span><span class="ot">We are implicitly assuming $\beta_1\neq 0$, which eliminates the need to assume $\E{\varepsilon} = \zer$</span><span class="co">]</span> where </span>
<span id="cb53-571"><a href="#cb53-571" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-572"><a href="#cb53-572" aria-hidden="true" tabindex="-1"></a>P_{\bet,\Sig} &amp;= <span class="sc">\{</span>F_{\Xm,\ep} \mid \Y= \Xm\bet +\ep, \ \ \Sig  = \var{\ep\mid\Xm},\ \ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i}, \ \text{rank}\left(\E{\X'\X}\right) = K,\ \E{\X'\ep} = \zer<span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb53-573"><a href="#cb53-573" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_j, \cdots \X_K</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_i, \cdots \X_n</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-574"><a href="#cb53-574" aria-hidden="true" tabindex="-1"></a>\Y &amp; = <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>.</span>
<span id="cb53-575"><a href="#cb53-575" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-576"><a href="#cb53-576" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-577"><a href="#cb53-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-578"><a href="#cb53-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-579"><a href="#cb53-579" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ordinary Least Squares </span></span>
<span id="cb53-580"><a href="#cb53-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-581"><a href="#cb53-581" aria-hidden="true" tabindex="-1"></a>Now that we have identified our model, we can *finally* estimate $\bet$ using our favorite estimator -- ordinary least squares! There are a handful of ways to derive the ordinary least squares estimator, but for now we will focus on two constructions. </span>
<span id="cb53-582"><a href="#cb53-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-583"><a href="#cb53-583" aria-hidden="true" tabindex="-1"></a>We want to estimate $\bet$ using observations of $(\Y, \Xm)$, which is the same as saying $n$ observations of $(Y, \X)$. By definition, we do not observe realizations of $\ep$. We know that $\bet =\left(\E{\X'\X}\right)^{-1}\E{\X'Y}$, so perhaps we can simply estimate $\bet$ using the sample analog of $\left(\E{\Xm'\Xm}\right)^{-1}\E{\Xm'\Y}$. This approach is sometimes referred to as the **_analogy principle_** (see @goldberger1991course), and will come up again. Denote the sample moments as $\widehat{\E{\X'\X}}$ and $\widehat{\E{\X'Y}}$. If we have a sample of size $n$, then </span>
<span id="cb53-584"><a href="#cb53-584" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-585"><a href="#cb53-585" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\X'\X}} &amp; = \frac{1}{n}\sum_{i=1}^n\X_i'\X<span class="sc">\\</span></span>
<span id="cb53-586"><a href="#cb53-586" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\X'Y}} &amp; = \frac{1}{n}\sum_{i=1}^n\X_iY_i</span>
<span id="cb53-587"><a href="#cb53-587" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-588"><a href="#cb53-588" aria-hidden="true" tabindex="-1"></a>Therefore, our estimator is </span>
<span id="cb53-589"><a href="#cb53-589" aria-hidden="true" tabindex="-1"></a>$$\hat {\bet}(\Xm, \Y) = \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_iY_i\right).$$ </span>
<span id="cb53-590"><a href="#cb53-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-591"><a href="#cb53-591" aria-hidden="true" tabindex="-1"></a>We can also write this in the form of matrices. First we need to expand $\X'\X$:</span>
<span id="cb53-592"><a href="#cb53-592" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-593"><a href="#cb53-593" aria-hidden="true" tabindex="-1"></a>\X'\X  &amp;= \begin{bmatrix} \X_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_K\end{bmatrix}\begin{bmatrix} \X_1 &amp; \cdots &amp; \X_K\end{bmatrix} = \begin{bmatrix}\X_1\cdot\X_1 &amp; \X_1\cdot\X_2 &amp; \cdots &amp; \X_1\X_K<span class="sc">\\</span> \vdots &amp; \vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span> \X_K\cdot\X_1 &amp; \X_K\cdot\X_2 &amp; \cdots &amp; \X_K\cdot \X_k\end{bmatrix} = \begin{bmatrix}\sum_{i=1}^n X_{1,i}^2 &amp; \sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{1,i}X_{K,i}<span class="sc">\\</span> \vdots &amp; \vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span> \sum_{i=1}^n X_{K,i}X_{1,i} &amp; \sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{K,i}^2\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-594"><a href="#cb53-594" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-595"><a href="#cb53-595" aria-hidden="true" tabindex="-1"></a>The expectation is taken element-wise where </span>
<span id="cb53-596"><a href="#cb53-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-597"><a href="#cb53-597" aria-hidden="true" tabindex="-1"></a>$$ \E{\sum_{i=1}^n X_{j,i}X_{\ell,i}} = \sum_{i=1}^n \E{X_{j,i}X_{\ell,i}} = n \E{X_{j,i}X_{\ell,i}},$$ so applying this to each entry and factoring out the common scalar $n$ gives: </span>
<span id="cb53-598"><a href="#cb53-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-599"><a href="#cb53-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-600"><a href="#cb53-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-601"><a href="#cb53-601" aria-hidden="true" tabindex="-1"></a>$$ \E{\X'\X} = n\begin{bmatrix}\E{X_1^2} &amp; \E{X_1X_2} &amp; \cdots &amp; \E{X_1X_K}<span class="sc">\\</span> \vdots &amp; \vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span> \E{X_KX_1} &amp; \E{X_KX_2} &amp; \cdots &amp; \E{X_K^2}\end{bmatrix}.$$</span>
<span id="cb53-602"><a href="#cb53-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-603"><a href="#cb53-603" aria-hidden="true" tabindex="-1"></a>The sample analog (as a function of random variables, *not* realizations) is </span>
<span id="cb53-604"><a href="#cb53-604" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-605"><a href="#cb53-605" aria-hidden="true" tabindex="-1"></a>\widehat{\E{\X'\X}} &amp;= n\begin{bmatrix}n^{-1}\sum_{i=1}^n X_{1,i}^2 &amp; n^{-1}\sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; n^{-1}\sum_{i=1}^n X_{1,i}X_{K,i}<span class="sc">\\</span> \vdots &amp; \vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span> n^{-1}\sum_{i=1}^n X_{K,i}X_{1,i} &amp; n^{-1}\sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; n^{-1}\sum_{i=1}^n X_{K,i}^2\end{bmatrix}<span class="sc">\\</span>&amp;=\begin{bmatrix}\sum_{i=1}^n X_{1,i}^2 &amp; \sum_{i=1}^n X_{1,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{1,i}X_{K,i}<span class="sc">\\</span> \vdots &amp; \vdots &amp; \ddots &amp; \vdots<span class="sc">\\</span> \sum_{i=1}^n X_{K,i}X_{1,i} &amp; \sum_{i=1}^n X_{K,i}X_{2,i} &amp; \cdots &amp; \sum_{i=1}^n X_{K,i}^2\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-606"><a href="#cb53-606" aria-hidden="true" tabindex="-1"></a>&amp; = \begin{bmatrix} \X_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_K\end{bmatrix}\begin{bmatrix} \X_1 &amp; \cdots &amp; \X_K\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-607"><a href="#cb53-607" aria-hidden="true" tabindex="-1"></a> &amp; = \Xm'\Xm</span>
<span id="cb53-608"><a href="#cb53-608" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-609"><a href="#cb53-609" aria-hidden="true" tabindex="-1"></a>We can perform the analogous inspection on $\E{\X'Y}$ and conclude that $\widehat{\E{\X'Y}} = \Xm\Y$. Therefore, in matrix form, our estimator is </span>
<span id="cb53-610"><a href="#cb53-610" aria-hidden="true" tabindex="-1"></a>$$\hat {\bet} =(\Xm'\Xm)^{-1}\Xm\Y$$ </span>
<span id="cb53-611"><a href="#cb53-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-612"><a href="#cb53-612" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-613"><a href="#cb53-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-614"><a href="#cb53-614" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Simple Linear Model</span></span>
<span id="cb53-615"><a href="#cb53-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-616"><a href="#cb53-616" aria-hidden="true" tabindex="-1"></a>In the event $K = 2$, we have $Y = \beta_1 + \beta_2 X + \varepsilon$ for a single non trivial regressor $X$. The random vector of regressors is $\X = <span class="co">[</span><span class="ot">\mathbf 1, X</span><span class="co">]</span>$. Let's calculate the population parameters $\bet$ in this setting.</span>
<span id="cb53-617"><a href="#cb53-617" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-618"><a href="#cb53-618" aria-hidden="true" tabindex="-1"></a>\E{\X'\X} &amp; = \begin{bmatrix}1 &amp; \E{X}<span class="sc">\\</span>\E{X} &amp; \E{X^2} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-619"><a href="#cb53-619" aria-hidden="true" tabindex="-1"></a>\E{\X'\X}^{-1} &amp; =\frac{1}{\underbrace{\E{X^2} - \E{X}^2}_{\var{X}}} \begin{bmatrix}\E{X^2} &amp; -\E{X}<span class="sc">\\</span>-\E{X} &amp; 1 \end{bmatrix} = \begin{bmatrix}\E{X^2}/\var{X} &amp; -\E{X}/\var{X}<span class="sc">\\</span>-\E{X}/\var{X} &amp; 1/\var{X} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-620"><a href="#cb53-620" aria-hidden="true" tabindex="-1"></a>\E{\X'Y} &amp; = \begin{bmatrix} \E{Y} <span class="sc">\\</span> \E{XY} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-621"><a href="#cb53-621" aria-hidden="true" tabindex="-1"></a>\bet &amp; = \E{\X'\X}^{-1}\E{\X'Y} = \frac{1}{\var{X}} \begin{bmatrix} \E{X^2}\E{Y} -\E{X}\E{XY} &amp; -\E{X}\E{Y} + \E{XY} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-622"><a href="#cb53-622" aria-hidden="true" tabindex="-1"></a>\beta_2 &amp;= \frac{\E{XY} -\E{X}\E{Y}}{\var{X}} <span class="sc">\\</span> &amp;= \frac{\cov{X,Y}}{\var{X}}<span class="sc">\\</span></span>
<span id="cb53-623"><a href="#cb53-623" aria-hidden="true" tabindex="-1"></a>\beta_1 &amp; = \frac{\E{X^2}\E{Y} -\E{X}\E{XY} }{\var{X}} <span class="sc">\\</span></span>
<span id="cb53-624"><a href="#cb53-624" aria-hidden="true" tabindex="-1"></a>      &amp; = \frac{(\E{X^2} - \E{X}^2 + \E{X}^2)\E{Y} -\E{X}\E{XY} }{\var{X}}<span class="sc">\\</span></span>
<span id="cb53-625"><a href="#cb53-625" aria-hidden="true" tabindex="-1"></a>       &amp; = \frac{(\var{X}+ \E{X}^2)\E{Y} -\E{X}\E{XY} }{\var{X}}<span class="sc">\\</span></span>
<span id="cb53-626"><a href="#cb53-626" aria-hidden="true" tabindex="-1"></a>       &amp; = \E{Y} - \E{X}\cdot \frac{\cov{X,Y}}{\var{X}}<span class="sc">\\</span></span>
<span id="cb53-627"><a href="#cb53-627" aria-hidden="true" tabindex="-1"></a>       &amp; = \E{Y} - \beta_2\E{X}</span>
<span id="cb53-628"><a href="#cb53-628" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-629"><a href="#cb53-629" aria-hidden="true" tabindex="-1"></a>The estimator calculated using the analogous moments is the familiar OLS estimator for the simple linear model:</span>
<span id="cb53-630"><a href="#cb53-630" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-631"><a href="#cb53-631" aria-hidden="true" tabindex="-1"></a>\hat\beta_2 &amp; = \frac{\widehat{\text{Cov}}(X,Y) }{\widehat{\text{Var}}(X)} = \frac{(1/n)\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{(1/n)\sum_{i=1}^n(X_i - \bar X)^2} = \frac{\sum_{i=1}^n(X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n(X_i - \bar X)^2}<span class="sc">\\</span></span>
<span id="cb53-632"><a href="#cb53-632" aria-hidden="true" tabindex="-1"></a>\hat\beta_1 &amp; = \bar Y - \hat\beta_2 \bar X</span>
<span id="cb53-633"><a href="#cb53-633" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-634"><a href="#cb53-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-635"><a href="#cb53-635" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-636"><a href="#cb53-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-637"><a href="#cb53-637" aria-hidden="true" tabindex="-1"></a>An alternate way of arriving at this estimator is possible by solving the least squares problem that we encountered with the linear projection model.</span>
<span id="cb53-638"><a href="#cb53-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-639"><a href="#cb53-639" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-640"><a href="#cb53-640" aria-hidden="true" tabindex="-1"></a>\hat{\bet} &amp;= \argmin_{\mathbf b} \sum_{i=1}^n (Y_i - \X_i\mathbf b)^2<span class="sc">\\</span></span>
<span id="cb53-641"><a href="#cb53-641" aria-hidden="true" tabindex="-1"></a>  &amp; = \argmin_{\mathbf b} \left<span class="sc">\{</span>(\Y - \Xm\mathbf b)'(\Y - \Xm\mathbf b)\right<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb53-642"><a href="#cb53-642" aria-hidden="true" tabindex="-1"></a>  &amp; = \argmin_{\mathbf b} \left<span class="sc">\{</span> \Y' \Y - 2\Y\Xm \mathbf b +\mathbf b' \Xm' \Xm \mathbf b \right<span class="sc">\}</span></span>
<span id="cb53-643"><a href="#cb53-643" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-644"><a href="#cb53-644" aria-hidden="true" tabindex="-1"></a>The first order condition is </span>
<span id="cb53-645"><a href="#cb53-645" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-646"><a href="#cb53-646" aria-hidden="true" tabindex="-1"></a>&amp;-2\Xm'\Y + 2\Xm'\Xm\hat{\bet} = \zer <span class="sc">\\</span></span>
<span id="cb53-647"><a href="#cb53-647" aria-hidden="true" tabindex="-1"></a>\implies &amp;\hat{\bet} = (\Xm'\Xm)^{-1}(\Xm\Y)</span>
<span id="cb53-648"><a href="#cb53-648" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-649"><a href="#cb53-649" aria-hidden="true" tabindex="-1"></a>This is the same estimator we derived with the analogy principle. In order to reference estimates given by our estimator, we'll need to introduce notation for realizations of $(\Xm, \Y, \ep)$, which makes notation even more complicated. The following table presents how we will write realizations of random quantities, along with recapping the notation for $\mathcal P_{LM}$.</span>
<span id="cb53-650"><a href="#cb53-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-651"><a href="#cb53-651" aria-hidden="true" tabindex="-1"></a>| Random Quantity | Type     | Dimension    | Definition                      | Realization/Observation |</span>
<span id="cb53-652"><a href="#cb53-652" aria-hidden="true" tabindex="-1"></a>|-----------------|----------|--------------|---------------------------------|-------------------------|</span>
<span id="cb53-653"><a href="#cb53-653" aria-hidden="true" tabindex="-1"></a>| $\X$            | vector   | $1\times K$  | dependent variables             | $\x$                    |</span>
<span id="cb53-654"><a href="#cb53-654" aria-hidden="true" tabindex="-1"></a>| $Y$             | variable | $1\times 1$  | independent variable            | $y$                     |</span>
<span id="cb53-655"><a href="#cb53-655" aria-hidden="true" tabindex="-1"></a>| $\ep$           | vector   | $n\times 1$  | vector of errors                          | $\e$                    |</span>
<span id="cb53-656"><a href="#cb53-656" aria-hidden="true" tabindex="-1"></a>| $\Y$            | vector   | $n\times 1$  | vector of independent variables | $\y$                    |</span>
<span id="cb53-657"><a href="#cb53-657" aria-hidden="true" tabindex="-1"></a>| $\Xm$           | matrix   | $n\times K$  | matrix of dependent variables   | $\mathbf X$             |</span>
<span id="cb53-658"><a href="#cb53-658" aria-hidden="true" tabindex="-1"></a>| $\X_i$          | vector   | $1\times K$  | $i$th row of $\Xm$              | $\x_i$                  |</span>
<span id="cb53-659"><a href="#cb53-659" aria-hidden="true" tabindex="-1"></a>| $\X_j$          | vector   | $n \times 1$ | $j$th row of $\Xm$              | $\x_j$                  |</span>
<span id="cb53-660"><a href="#cb53-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-661"><a href="#cb53-661" aria-hidden="true" tabindex="-1"></a>This notation is by no means standard, an notation unfortunately varies widely across sources. The only piece of notation which conflicts is the random vector of regressors $\X$ and the realization of $\Xm = \X$. Whenever it is unclear which is being referenced, I will try to be specific.</span>
<span id="cb53-662"><a href="#cb53-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-663"><a href="#cb53-663" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-664"><a href="#cb53-664" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_ordinary least squares (OLS) estimator_**<span class="kw">&lt;/span&gt;</span> is defined as </span>
<span id="cb53-665"><a href="#cb53-665" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-666"><a href="#cb53-666" aria-hidden="true" tabindex="-1"></a>\hat{\bet}_\text{OLS}(\Xm,\Y) &amp;= (\Xm'\Xm)^{-1}(\Xm\Y)</span>
<span id="cb53-667"><a href="#cb53-667" aria-hidden="true" tabindex="-1"></a>= \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_iY_i\right)</span>
<span id="cb53-668"><a href="#cb53-668" aria-hidden="true" tabindex="-1"></a>\end{align*} An realization of this estimator (an estimate) is </span>
<span id="cb53-669"><a href="#cb53-669" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-670"><a href="#cb53-670" aria-hidden="true" tabindex="-1"></a>\hat{\mathbf b}_\text{OLS} = \hat{\bet}_\text{OLS}(\X,\y) &amp;= (\X'\X)^{-1}(\X\y)</span>
<span id="cb53-671"><a href="#cb53-671" aria-hidden="true" tabindex="-1"></a>= \left(\frac{1}{n}\sum_{i=1}^n\x_i'\x_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\x_iy_i\right)</span>
<span id="cb53-672"><a href="#cb53-672" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb53-673"><a href="#cb53-673" aria-hidden="true" tabindex="-1"></a>and will exist when the inverse $(\X'\X)^{-1}$ exists.</span>
<span id="cb53-674"><a href="#cb53-674" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-675"><a href="#cb53-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-676"><a href="#cb53-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-677"><a href="#cb53-677" aria-hidden="true" tabindex="-1"></a>:::{#exm-funref}</span>
<span id="cb53-678"><a href="#cb53-678" aria-hidden="true" tabindex="-1"></a>We can easily write a function which calculates an OLS estimate given a random sample. </span>
<span id="cb53-679"><a href="#cb53-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-682"><a href="#cb53-682" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-683"><a href="#cb53-683" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X){</span>
<span id="cb53-684"><a href="#cb53-684" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb53-685"><a href="#cb53-685" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">==</span> <span class="dv">0</span>) {<span class="fu">stop</span>(<span class="st">"rank(X'X) &lt; K"</span>)}</span>
<span id="cb53-686"><a href="#cb53-686" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-687"><a href="#cb53-687" aria-hidden="true" tabindex="-1"></a>  hat_beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb53-688"><a href="#cb53-688" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(hat_beta) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"β"</span>, <span class="dv">1</span><span class="sc">:</span>K, <span class="st">" estimate"</span>, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb53-689"><a href="#cb53-689" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(hat_beta)</span>
<span id="cb53-690"><a href="#cb53-690" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-691"><a href="#cb53-691" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-692"><a href="#cb53-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-693"><a href="#cb53-693" aria-hidden="true" tabindex="-1"></a>Let's randomly generate a sample to test our function. Suppose we have a sample of size $n=1000$ from a linear model where $X_1 \iid \text{Uni}(0,10)$, $\varepsilon \iid\text{Uni}(-5,5)$, $X_1 \perp \varepsilon$, and $Y = 2 + 4X_1 + \varepsilon$. Because $\varepsilon$ and $X$ are independent, we've specified their respective marginal densities instead of joint density. </span>
<span id="cb53-694"><a href="#cb53-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-697"><a href="#cb53-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-698"><a href="#cb53-698" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb53-699"><a href="#cb53-699" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb53-700"><a href="#cb53-700" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-701"><a href="#cb53-701" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb53-702"><a href="#cb53-702" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb53-703"><a href="#cb53-703" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-704"><a href="#cb53-704" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-705"><a href="#cb53-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-706"><a href="#cb53-706" aria-hidden="true" tabindex="-1"></a>Before we estimate our model, we should think about whether our model satisfies the assumptions that $\E{\X'\X}$ is invertible and $\E{\X'\ep} = \zer$. The first assumption holds because we only have one non-trivial independent variable (the trivial one is constant 1 which gives the intercept), and it is not a constant (so it cannot be a linear function of the constant 1). We have $\E{\varepsilon} = 0$, so by independence we have </span>
<span id="cb53-707"><a href="#cb53-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-708"><a href="#cb53-708" aria-hidden="true" tabindex="-1"></a>$$\E{\X'\ep} = \E{X_1\varepsilon} = \E{X_1}\E{\varepsilon} =\E{X_1}\cdot0 = 0$$</span>
<span id="cb53-709"><a href="#cb53-709" aria-hidden="true" tabindex="-1"></a>We can actually use the LLN to consistently estimate $\E{\X'\X}$ and $\E{X_1'\varepsilon}$, and see if our estimates satisfy our assumptions. For a sufficiently large $n$, we should see that </span>
<span id="cb53-710"><a href="#cb53-710" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-711"><a href="#cb53-711" aria-hidden="true" tabindex="-1"></a>\text{rank}\left(\frac{1}{n}\sum_{i=1}^n \x_i'\x_i\right) &amp;\approx K<span class="sc">\\</span></span>
<span id="cb53-712"><a href="#cb53-712" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{n}\sum_{i=1}^n x_{1i}'e_i\right) &amp;\approx 0</span>
<span id="cb53-713"><a href="#cb53-713" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-714"><a href="#cb53-714" aria-hidden="true" tabindex="-1"></a>The sample size $n=25$ may be a bit too modest, so let's generate a new sample of size $n'=100,000$.</span>
<span id="cb53-715"><a href="#cb53-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-718"><a href="#cb53-718" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-719"><a href="#cb53-719" aria-hidden="true" tabindex="-1"></a>n_prime <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb53-720"><a href="#cb53-720" aria-hidden="true" tabindex="-1"></a>x1_prime <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_prime, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-721"><a href="#cb53-721" aria-hidden="true" tabindex="-1"></a>e_prime <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_prime, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb53-722"><a href="#cb53-722" aria-hidden="true" tabindex="-1"></a>X_prime <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n_prime), x1_prime)</span>
<span id="cb53-723"><a href="#cb53-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-724"><a href="#cb53-724" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample rank of X'X</span></span>
<span id="cb53-725"><a href="#cb53-725" aria-hidden="true" tabindex="-1"></a><span class="fu">rankMatrix</span>((<span class="fu">t</span>(X_prime) <span class="sc">%*%</span> X_prime)<span class="sc">/</span>n_prime)[<span class="dv">1</span>]</span>
<span id="cb53-726"><a href="#cb53-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-727"><a href="#cb53-727" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample mean of X'ε</span></span>
<span id="cb53-728"><a href="#cb53-728" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x1_prime<span class="sc">*</span>e_prime)</span>
<span id="cb53-729"><a href="#cb53-729" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-730"><a href="#cb53-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-731"><a href="#cb53-731" aria-hidden="true" tabindex="-1"></a>It appears our assumptions are met, so we can go ahead with estimation. It is important to recognize that in practice we don't observe the realizations $\ep$, so calculating the sample analog of $\E{\X'\ep}$ is not possible with real data, but it is a good gut check when conducting simulations.</span>
<span id="cb53-732"><a href="#cb53-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-735"><a href="#cb53-735" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-736"><a href="#cb53-736" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb53-737"><a href="#cb53-737" aria-hidden="true" tabindex="-1"></a>beta_hat</span>
<span id="cb53-738"><a href="#cb53-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-739"><a href="#cb53-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-742"><a href="#cb53-742" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-743"><a href="#cb53-743" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-744"><a href="#cb53-744" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-745"><a href="#cb53-745" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot56</span></span>
<span id="cb53-746"><a href="#cb53-746" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-747"><a href="#cb53-747" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-748"><a href="#cb53-748" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Our observed data (in black) is drawn from the unobserved population model (blue). We use this data to estimate the model, and the line associated with this estimate is shown in red."</span></span>
<span id="cb53-749"><a href="#cb53-749" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-750"><a href="#cb53-750" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> X[,<span class="dv">2</span>], </span>
<span id="cb53-751"><a href="#cb53-751" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> y</span>
<span id="cb53-752"><a href="#cb53-752" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb53-753"><a href="#cb53-753" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-754"><a href="#cb53-754" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"(Observed) Sample"</span>)) <span class="sc">+</span></span>
<span id="cb53-755"><a href="#cb53-755" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> beta[<span class="dv">1</span>], <span class="at">slope =</span> beta[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">"(Unobserved) Population Model"</span>)) <span class="sc">+</span></span>
<span id="cb53-756"><a href="#cb53-756" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="fu">aes</span>(<span class="at">intercept =</span> beta_hat[<span class="dv">1</span>], <span class="at">slope =</span> beta_hat[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">"Estimated Model"</span>)) <span class="sc">+</span></span>
<span id="cb53-757"><a href="#cb53-757" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-758"><a href="#cb53-758" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour=</span><span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb53-759"><a href="#cb53-759" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb53-760"><a href="#cb53-760" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span>
<span id="cb53-761"><a href="#cb53-761" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-762"><a href="#cb53-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-763"><a href="#cb53-763" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-764"><a href="#cb53-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-765"><a href="#cb53-765" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-766"><a href="#cb53-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-767"><a href="#cb53-767" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear Projection Model, OLS</span></span>
<span id="cb53-768"><a href="#cb53-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-769"><a href="#cb53-769" aria-hidden="true" tabindex="-1"></a>OLS can be used in the context of $\mathcal P_\text{LM}$ to estimate the best linear projection between two random variables $(Y,\X)$. OLS was the method used by @pearson1903laws to estimate the relationship between height and genetics. We can replicate this work with an data set based on the original data collected by @pearson1903laws.</span>
<span id="cb53-770"><a href="#cb53-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-773"><a href="#cb53-773" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-774"><a href="#cb53-774" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-775"><a href="#cb53-775" aria-hidden="true" tabindex="-1"></a>height_df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/height_data.csv"</span>)</span>
<span id="cb53-776"><a href="#cb53-776" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(height_df<span class="sc">$</span>Father)</span>
<span id="cb53-777"><a href="#cb53-777" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> height_df<span class="sc">$</span>Son</span>
<span id="cb53-778"><a href="#cb53-778" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y,X)</span>
<span id="cb53-779"><a href="#cb53-779" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-780"><a href="#cb53-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-781"><a href="#cb53-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-782"><a href="#cb53-782" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-783"><a href="#cb53-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-784"><a href="#cb53-784" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-785"><a href="#cb53-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-786"><a href="#cb53-786" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS Estimate Does not Exist</span></span>
<span id="cb53-787"><a href="#cb53-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-788"><a href="#cb53-788" aria-hidden="true" tabindex="-1"></a>It is possible that $\E{\X'\X}$ is invertible while realized value $\Xm'\Xm = \X'\X$ is not invertible. For example, if our model is $Y = \beta_1 X_1 + \epsilon$ where $X \sim \text{Binom}(4, 0.5)$,^<span class="co">[</span><span class="ot">If $X\sim \text{Binom}(n, p)$, then $\E{X} = np(1-p) + (np)^2$</span><span class="co">]</span> we have $$\E{\X'\X} = \E{X_1^2} = 5.$$ If we observe an independent sample of size $n=2$ generated from this model, we may observe something like $\x_1 = <span class="co">[</span><span class="ot">2,2</span><span class="co">]</span>$. In this case </span>
<span id="cb53-789"><a href="#cb53-789" aria-hidden="true" tabindex="-1"></a>$$\x'\x = \begin{bmatrix} 4 &amp;4 <span class="sc">\\</span> 4 &amp; 4\end{bmatrix},$$ which is certainly not invertible. Furthermore, the probability we draw this sample is </span>
<span id="cb53-790"><a href="#cb53-790" aria-hidden="true" tabindex="-1"></a>$$\Pr(\x = <span class="co">[</span><span class="ot">2,2</span><span class="co">]</span>) = <span class="co">[</span><span class="ot">\Pr(X = 2)</span><span class="co">]</span>^2 = (0.375)^2 = 0.140625,$$ so the chances this happen are not trivial. However, $n$ is usually much greater than $2$, and as $n\to\infty$ the probability that $\X'\X$ is not invertible will go to zero. This is a direct consequence of the LLN: </span>
<span id="cb53-791"><a href="#cb53-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-792"><a href="#cb53-792" aria-hidden="true" tabindex="-1"></a>$$\X'\X = \left(\frac{1}{n}\sum_{i=1}^n\x_i'\x_i\right) \pto \E{\X'\X} $$</span>
<span id="cb53-793"><a href="#cb53-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-794"><a href="#cb53-794" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-795"><a href="#cb53-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-796"><a href="#cb53-796" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb53-797"><a href="#cb53-797" aria-hidden="true" tabindex="-1"></a>Whether it is easier to write our terms related to $\OLS$ in terms of matrices or sums of vectors will depend on the result we are building to or trying to prove. This can be a bit confusing, so here is a list of various equalities (many of which imply others), that we will use:</span>
<span id="cb53-798"><a href="#cb53-798" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-799"><a href="#cb53-799" aria-hidden="true" tabindex="-1"></a>\Xm'\Xm &amp; = \sum_{i=1}^n \X_i'\X_i<span class="sc">\\</span></span>
<span id="cb53-800"><a href="#cb53-800" aria-hidden="true" tabindex="-1"></a>\Xm'\Y &amp; = \sum_{i=1}^n \X_i'Y_i<span class="sc">\\</span></span>
<span id="cb53-801"><a href="#cb53-801" aria-hidden="true" tabindex="-1"></a>\Xm'\ep &amp; = \sum_{i=1}^n \X_i'\varepsilon_i<span class="sc">\\</span></span>
<span id="cb53-802"><a href="#cb53-802" aria-hidden="true" tabindex="-1"></a>\ep'\ep &amp; = \sum_{i=1}^n \varepsilon_i^2</span>
<span id="cb53-803"><a href="#cb53-803" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-804"><a href="#cb53-804" aria-hidden="true" tabindex="-1"></a>An important result which follows from the first equality is $\E{\Xm'\Xm} = n \E{\X'\X}$ in the event that $\X_i$ are independent. </span>
<span id="cb53-805"><a href="#cb53-805" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-806"><a href="#cb53-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-807"><a href="#cb53-807" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of OLS</span></span>
<span id="cb53-808"><a href="#cb53-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-809"><a href="#cb53-809" aria-hidden="true" tabindex="-1"></a>As likely anticipated, the OLS estimator has a number of desirable properties under certain assumptions, some of which we will make in addition to weak exogeneity and lack of multicollinearity. The first property we establish is consistency.</span>
<span id="cb53-810"><a href="#cb53-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-811"><a href="#cb53-811" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-812"><a href="#cb53-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-813"><a href="#cb53-813" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency</span></span>
<span id="cb53-814"><a href="#cb53-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-815"><a href="#cb53-815" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\Sig} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$ and $\E{\X'\ep} = \zer$, then $\hat{\bet}_\text{OLS} \pto \bet$.</span>
<span id="cb53-816"><a href="#cb53-816" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-817"><a href="#cb53-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-818"><a href="#cb53-818" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-819"><a href="#cb53-819" aria-hidden="true" tabindex="-1"></a>We have $\bet = \left(\E{\X'\X}\right)^{-1}\E{\X'Y}$, where $\bet$ is guaranteed to exist and be unique using our assumptions. As $n\to\infty$, $\X'\X$ will be invertible with probability one, so $\hat{\bet}_\text{OLS}$ will exist (with probability one). We can write our estimator as </span>
<span id="cb53-820"><a href="#cb53-820" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-821"><a href="#cb53-821" aria-hidden="true" tabindex="-1"></a>\hat{\bet}_\text{OLS} &amp;= \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'Y_i\right)<span class="sc">\\</span></span>
<span id="cb53-822"><a href="#cb53-822" aria-hidden="true" tabindex="-1"></a>&amp;= \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'(\X_i\bet + \varepsilon_i)\right) &amp; (Y_i = \X_i\bet + \varepsilon_i)<span class="sc">\\</span></span>
<span id="cb53-823"><a href="#cb53-823" aria-hidden="true" tabindex="-1"></a>&amp; = \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\bet\right) + \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)<span class="sc">\\</span></span>
<span id="cb53-824"><a href="#cb53-824" aria-hidden="true" tabindex="-1"></a>&amp; = \bet\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)}_{\mathbf I} + \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)<span class="sc">\\</span></span>
<span id="cb53-825"><a href="#cb53-825" aria-hidden="true" tabindex="-1"></a>&amp; = \bet + \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)</span>
<span id="cb53-826"><a href="#cb53-826" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-827"><a href="#cb53-827" aria-hidden="true" tabindex="-1"></a>We can apply the LLN along with the continuous mapping theorem (applied to the inverse term) and Slutky's theorem (applied to the product of convergent sequences) to conclude,</span>
<span id="cb53-828"><a href="#cb53-828" aria-hidden="true" tabindex="-1"></a>$$ \hat{\bet}_\text{OLS} =  \bet + \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}}_{\pto \E{\X'\X}}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)}_{\pto \E{\X'\ep}} \pto \bet + \E{\X'\X}^{-1}\underbrace{\E{\X'\ep}}_\zer = \bet.$$</span>
<span id="cb53-829"><a href="#cb53-829" aria-hidden="true" tabindex="-1"></a>Therefore $\hat{\bet}_\text{OLS} \pto \bet$, where the limit $\bet$ is unique by identification.</span>
<span id="cb53-830"><a href="#cb53-830" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-831"><a href="#cb53-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-832"><a href="#cb53-832" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-833"><a href="#cb53-833" aria-hidden="true" tabindex="-1"></a>Return to the model $X_1 \iid \text{Uni}(0,10)$, $\varepsilon \iid\text{Uni}(-5,5)$, $X_1 \perp \varepsilon$ (implying $\E{\X'\ep} = \zer$), and $Y = 2 + 4X_1 + \varepsilon$. If we estimate this model for samples of increasing size, we should see that our estimates converge to the true values. </span>
<span id="cb53-834"><a href="#cb53-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-837"><a href="#cb53-837" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-838"><a href="#cb53-838" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb53-839"><a href="#cb53-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-840"><a href="#cb53-840" aria-hidden="true" tabindex="-1"></a>beta_hat1 <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">9999</span>)</span>
<span id="cb53-841"><a href="#cb53-841" aria-hidden="true" tabindex="-1"></a>beta_hat2 <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, <span class="dv">9999</span>)</span>
<span id="cb53-842"><a href="#cb53-842" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10000</span>){</span>
<span id="cb53-843"><a href="#cb53-843" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-844"><a href="#cb53-844" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb53-845"><a href="#cb53-845" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb53-846"><a href="#cb53-846" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-847"><a href="#cb53-847" aria-hidden="true" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb53-848"><a href="#cb53-848" aria-hidden="true" tabindex="-1"></a>  beta_hat1[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> beta_hat[<span class="dv">1</span>]</span>
<span id="cb53-849"><a href="#cb53-849" aria-hidden="true" tabindex="-1"></a>  beta_hat2[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> beta_hat[<span class="dv">2</span>]</span>
<span id="cb53-850"><a href="#cb53-850" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-851"><a href="#cb53-851" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-852"><a href="#cb53-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-853"><a href="#cb53-853" aria-hidden="true" tabindex="-1"></a>It does appear that as $n\to\infty$ we have $\hat{\beta}_\text{1,OLS} \pto 2$ and $\hat{\beta}_\text{2,OLS} \pto 4$</span>
<span id="cb53-854"><a href="#cb53-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-857"><a href="#cb53-857" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-858"><a href="#cb53-858" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-859"><a href="#cb53-859" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-860"><a href="#cb53-860" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot57</span></span>
<span id="cb53-861"><a href="#cb53-861" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-862"><a href="#cb53-862" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-863"><a href="#cb53-863" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-864"><a href="#cb53-864" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As the sample size increases, our estimates approach their true values."</span></span>
<span id="cb53-865"><a href="#cb53-865" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-866"><a href="#cb53-866" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb53-867"><a href="#cb53-867" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">rep</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10000</span>, <span class="dv">2</span>), </span>
<span id="cb53-868"><a href="#cb53-868" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(beta_hat1, beta_hat2), </span>
<span id="cb53-869"><a href="#cb53-869" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Estimated β1"</span>, <span class="dv">9999</span>), <span class="fu">rep</span>(<span class="st">"Estimated β2"</span>, <span class="dv">9999</span>))</span>
<span id="cb53-870"><a href="#cb53-870" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb53-871"><a href="#cb53-871" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-872"><a href="#cb53-872" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb53-873"><a href="#cb53-873" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span><span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Estimated Value"</span>) <span class="sc">+</span></span>
<span id="cb53-874"><a href="#cb53-874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb53-875"><a href="#cb53-875" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb53-876"><a href="#cb53-876" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-877"><a href="#cb53-877" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-878"><a href="#cb53-878" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-879"><a href="#cb53-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-880"><a href="#cb53-880" aria-hidden="true" tabindex="-1"></a>Now let's consider whether if (and under what conditions) $\hat{\bet}_\text{OLS}$ unbiased. </span>
<span id="cb53-881"><a href="#cb53-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-882"><a href="#cb53-882" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-883"><a href="#cb53-883" aria-hidden="true" tabindex="-1"></a>\E{\hat{\bet}_\text{OLS}} &amp; = \E{\E{\hat{\bet}_\text{OLS}}\mid \Xm } &amp; (\text{iterated expectations})<span class="sc">\\</span></span>
<span id="cb53-884"><a href="#cb53-884" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{(\Xm'\Xm)^{-1}\Xm'\Y}\mid \Xm }<span class="sc">\\</span></span>
<span id="cb53-885"><a href="#cb53-885" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{(\Xm'\Xm)^{-1}\Xm'(\Xm\bet + \ep)}\mid \Xm} &amp; (\Y &amp; = \Xm\bet + \ep)<span class="sc">\\</span></span>
<span id="cb53-886"><a href="#cb53-886" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{((\Xm'\Xm)^{-1}\Xm'\Xm)\bet + (\Xm'\Xm)^{-1}\Xm'\ep)}\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-887"><a href="#cb53-887" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{\bet}\mid \Xm} +\E{(\Xm'\Xm)^{-1}\Xm'\E{\ep\mid \Xm}}<span class="sc">\\</span></span>
<span id="cb53-888"><a href="#cb53-888" aria-hidden="true" tabindex="-1"></a>&amp; = \bet +  \E{(\Xm'\Xm)^{-1}\Xm'\E{\ep\mid \Xm}} &amp; (\bet\text{ is a constant})<span class="sc">\\</span></span>
<span id="cb53-889"><a href="#cb53-889" aria-hidden="true" tabindex="-1"></a>&amp; \neq \bet</span>
<span id="cb53-890"><a href="#cb53-890" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-891"><a href="#cb53-891" aria-hidden="true" tabindex="-1"></a>Under our current assumption, OLS is has a bias of $\E{(\Xm'\Xm)^{-1}\X'\E{\ep\mid \Xm}}$. While we are operating under the assumption that $\E{ \X'\ep} = \zer$, this does not imply that $\ep\perp\Xm$ (which would mean $\E{\ep\mid \Xm} = \E{\ep}=\zer$) For this to happen, we need to impose our third assumption on the linear model. </span>
<span id="cb53-892"><a href="#cb53-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-893"><a href="#cb53-893" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-894"><a href="#cb53-894" aria-hidden="true" tabindex="-1"></a>The random regressors $\X$ are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_exogenous_**<span class="kw">&lt;/span&gt;</span> if  </span>
<span id="cb53-895"><a href="#cb53-895" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-896"><a href="#cb53-896" aria-hidden="true" tabindex="-1"></a>\E{\varepsilon_i\mid \X} &amp; = 0 &amp;(i=1,\ldots,n)</span>
<span id="cb53-897"><a href="#cb53-897" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-898"><a href="#cb53-898" aria-hidden="true" tabindex="-1"></a>which is written compactly as $\E{\ep\mid \Xm} = \zer$.</span>
<span id="cb53-899"><a href="#cb53-899" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-900"><a href="#cb53-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-901"><a href="#cb53-901" aria-hidden="true" tabindex="-1"></a>By properties of independence and conditional expectation, exogeneity implies weak exogeneity, hence its name alluding to it being a stronger assumption. Technically speaking, we aren't adding a third assumption, as much as we are strengthening one of our current assumptions.</span>
<span id="cb53-902"><a href="#cb53-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-903"><a href="#cb53-903" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-904"><a href="#cb53-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-905"><a href="#cb53-905" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS is Unbiased</span></span>
<span id="cb53-906"><a href="#cb53-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-907"><a href="#cb53-907" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\Sig} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$ and $\E{\ep\mid \Xm} = \zer$, then $\OLS$ is an unbiased estimator for $\bet$.</span>
<span id="cb53-908"><a href="#cb53-908" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-909"><a href="#cb53-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-910"><a href="#cb53-910" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-911"><a href="#cb53-911" aria-hidden="true" tabindex="-1"></a>$$\E{\hat{\bet}_\text{OLS}} = \bet +  \E{(\Xm'\Xm)^{-1}\Xm'\E{\ep\mid \Xm}} =\bet +  \E{(\Xm'\Xm)^{-1}\Xm'\zer} = \bet $$</span>
<span id="cb53-912"><a href="#cb53-912" aria-hidden="true" tabindex="-1"></a>Therefore, $\hat{\bet}_\text{OLS}$ is an unbiased estimator for $\bet$.</span>
<span id="cb53-913"><a href="#cb53-913" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-914"><a href="#cb53-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-915"><a href="#cb53-915" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-916"><a href="#cb53-916" aria-hidden="true" tabindex="-1"></a>The  <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(unbiased) linear model_**<span class="kw">&lt;/span&gt;</span> is defined as $\mathcal P_\text{LM} = <span class="sc">\{</span>P_{\bet,\Sig} \mid \bet \in \mathbb R^{K}, \Sig\in\mathbb R^n\times\mathbb R^n<span class="sc">\}</span>$, where </span>
<span id="cb53-917"><a href="#cb53-917" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-918"><a href="#cb53-918" aria-hidden="true" tabindex="-1"></a>P_{\bet, \Sig} &amp;= <span class="sc">\{</span>F_{\Xm,\ep} \mid \Y= \Xm\bet +\ep, \ \ \Sig  = \var{\ep\mid\Xm},\ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i}, \ \text{rank}\left(\E{\X'\X}\right) = K,\ \E{\ep \mid \Xm} = \zer<span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb53-919"><a href="#cb53-919" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_j, \cdots \X_K</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_i, \cdots \X_n</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-920"><a href="#cb53-920" aria-hidden="true" tabindex="-1"></a>\Y &amp; = <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>.</span>
<span id="cb53-921"><a href="#cb53-921" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-922"><a href="#cb53-922" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-923"><a href="#cb53-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-924"><a href="#cb53-924" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-925"><a href="#cb53-925" aria-hidden="true" tabindex="-1"></a>If we go back to our simulated estimates where $X_1 \iid \text{Uni}(0,10)$, $\varepsilon \iid\text{Uni}(-5,5)$, $X_1 \perp \varepsilon$, and $Y = 2 + 4X_1 + \varepsilon$, we should see that the sample mean of our simulated estimates are approximately equal to the true values $\bet = (2,4)$</span>
<span id="cb53-928"><a href="#cb53-928" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-929"><a href="#cb53-929" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(<span class="fu">cbind</span>(beta_hat1, beta_hat2))</span>
<span id="cb53-930"><a href="#cb53-930" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-931"><a href="#cb53-931" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-932"><a href="#cb53-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-933"><a href="#cb53-933" aria-hidden="true" tabindex="-1"></a>The assumption of this stronger form of exogeneity also gives several nice properties beyond unbiasedness. </span>
<span id="cb53-934"><a href="#cb53-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-935"><a href="#cb53-935" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-936"><a href="#cb53-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-937"><a href="#cb53-937" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consequences of Exogeneity</span></span>
<span id="cb53-938"><a href="#cb53-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-939"><a href="#cb53-939" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\Sig} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\ep\mid \Xm} = \zer$, and $\Sig = \var{\ep\mid \Xm}$. Then </span>
<span id="cb53-940"><a href="#cb53-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-941"><a href="#cb53-941" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\E{\Y\mid \Xm} = \Xm\bet$;</span>
<span id="cb53-942"><a href="#cb53-942" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\E{\ep} = \zer$ (even if $\beta_1 = 0$)</span>
<span id="cb53-943"><a href="#cb53-943" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\Sig = \E{\ep\ep'\mid \Xm} = \var{\ep}$</span>
<span id="cb53-944"><a href="#cb53-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-945"><a href="#cb53-945" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-946"><a href="#cb53-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-947"><a href="#cb53-947" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-948"><a href="#cb53-948" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span> </span>
<span id="cb53-949"><a href="#cb53-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-950"><a href="#cb53-950" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\E{\Y\mid \Xm} = \E{\Xm\bet + \ep \mid \Xm} = \E{\Xm\bet \mid \Xm} + \E{\ep\mid \Xm}= \Xm\bet + \zer = \Xm\bet$</span>
<span id="cb53-951"><a href="#cb53-951" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\E{\ep} = \E{\E{\ep\mid \Xm}} = \E{\zer} = \zer$</span>
<span id="cb53-952"><a href="#cb53-952" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The first portion is a consequence of the definition of variance. $$\Sig = \var{\ep\mid \Xm} = \E{<span class="co">[</span><span class="ot">\ep - \E{\ep \mid \Xm}</span><span class="co">][\ep - \E{\ep \mid \Xm}]</span>'\mid \Xm} = \E{<span class="co">[</span><span class="ot">\ep - \zer</span><span class="co">][\ep - \zer]</span>'\mid \Xm} = \E{\ep\ep'\mid \Xm}$$ The second portion follows from the <span class="co">[</span><span class="ot">law of total variance</span><span class="co">](https://en.wikipedia.org/wiki/Law_of_total_variance)</span>.</span>
<span id="cb53-953"><a href="#cb53-953" aria-hidden="true" tabindex="-1"></a>$$ \var{\ep} = \E{\var{\ep \mid \Xm}} + \var{\E{\ep\mid \Xm}} = \E{\Sig} + \var{0} = \Sig$$</span>
<span id="cb53-954"><a href="#cb53-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-955"><a href="#cb53-955" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span> </span>
<span id="cb53-956"><a href="#cb53-956" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-957"><a href="#cb53-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-958"><a href="#cb53-958" aria-hidden="true" tabindex="-1"></a>Finally let's consider the variance of our OLS estimator. Due to the stochastic nature of $\Xm$, our interest is actually in the conditional variance of $\hat{\bet}_\text{OLS}$ given $\Xm$. Until now, we haven't paid much attention to the parameter $\Sig$, but it will come into play here.</span>
<span id="cb53-959"><a href="#cb53-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-960"><a href="#cb53-960" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-961"><a href="#cb53-961" aria-hidden="true" tabindex="-1"></a>\var{\hat{\bet}_\text{OLS}\mid \Xm} &amp; = \E{ \left(\hat{\bet}_\text{OLS} - \E{ \hat{\bet}_\text{OLS} }\right) \left(\hat{\bet}_\text{OLS} - \E{\hat{\bet}_\text{OLS}}\right)'\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-962"><a href="#cb53-962" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left(\hat{\bet}_\text{OLS} - \bet\right) \left(\hat{\bet}_\text{OLS} - \bet\right)'\mid \Xm} &amp; (\hat{\bet}_\text{OLS} \text{ unbiased})<span class="sc">\\</span></span>
<span id="cb53-963"><a href="#cb53-963" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left<span class="co">[</span><span class="ot">(\bet + (\Xm'\Xm)^{-1}\Xm'\ep) - \bet\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">(\bet + (\Xm'\Xm)^{-1}\Xm'\ep) - \bet\right</span><span class="co">]</span>'\mid \Xm} &amp; (\hat{\bet}_\text{OLS} = \bet + (\Xm'\Xm)^{-1}\Xm\ep)<span class="sc">\\</span></span>
<span id="cb53-964"><a href="#cb53-964" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm'\ep\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm'\ep\right</span><span class="co">]</span>'\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-965"><a href="#cb53-965" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ (\Xm'\Xm)^{-1}\Xm'\ep\ep'\Xm (\Xm'\Xm)^{-1}\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-966"><a href="#cb53-966" aria-hidden="true" tabindex="-1"></a>&amp; =  (\Xm'\Xm)^{-1}\Xm'\E{\ep\ep'\mid \Xm}\Xm (\Xm'\Xm)^{-1}<span class="sc">\\</span></span>
<span id="cb53-967"><a href="#cb53-967" aria-hidden="true" tabindex="-1"></a>&amp; =  (\Xm'\Xm)^{-1}\Xm'\Sig\Xm (\Xm'\Xm)^{-1} &amp; (\Sig = \E{\ep\ep'\mid \Xm})</span>
<span id="cb53-968"><a href="#cb53-968" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-969"><a href="#cb53-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-970"><a href="#cb53-970" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-971"><a href="#cb53-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-972"><a href="#cb53-972" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS Variance I</span></span>
<span id="cb53-973"><a href="#cb53-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-974"><a href="#cb53-974" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\Sig} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\ep\mid \Xm} = \zer$, and $\Sig = \var{\ep\mid \Xm}$. Then </span>
<span id="cb53-975"><a href="#cb53-975" aria-hidden="true" tabindex="-1"></a>$$ \var{\hat{\bet}_\text{OLS}\mid \Xm} = (\Xm'\Xm)^{-1}\Xm'\Sig\Xm (\Xm'\Xm)^{-1}$$</span>
<span id="cb53-976"><a href="#cb53-976" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-977"><a href="#cb53-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-978"><a href="#cb53-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-979"><a href="#cb53-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-980"><a href="#cb53-980" aria-hidden="true" tabindex="-1"></a>:::{.example}</span>
<span id="cb53-981"><a href="#cb53-981" aria-hidden="true" tabindex="-1"></a>Suppose the our model is given as </span>
<span id="cb53-982"><a href="#cb53-982" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-983"><a href="#cb53-983" aria-hidden="true" tabindex="-1"></a>Y &amp; = 2 + 4X_1 + \varepsilon<span class="sc">\\</span></span>
<span id="cb53-984"><a href="#cb53-984" aria-hidden="true" tabindex="-1"></a>X_i &amp;\iid \text{Uni}(0,10)<span class="sc">\\</span></span>
<span id="cb53-985"><a href="#cb53-985" aria-hidden="true" tabindex="-1"></a>\ep &amp;\sim N(\zer, \Sig)<span class="sc">\\</span></span>
<span id="cb53-986"><a href="#cb53-986" aria-hidden="true" tabindex="-1"></a>\Sig_{ii} &amp; = \begin{cases}1 &amp; i \text{ even}<span class="sc">\\</span> 2 &amp; i \text{ odd}\end{cases}<span class="sc">\\</span></span>
<span id="cb53-987"><a href="#cb53-987" aria-hidden="true" tabindex="-1"></a>\Sig_{i\ell} &amp; = \abs{i-\ell}^{-1}</span>
<span id="cb53-988"><a href="#cb53-988" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-989"><a href="#cb53-989" aria-hidden="true" tabindex="-1"></a>Let's perform a simulation to verify the formula for $\var{\hat{\bet}_\text{OLS}\mid \Xm}$ The variance of the error is defined such that if our observation has an even index, $\var{\varepsilon_i} = 1$, otherwise $\var{\varepsilon_i} = 2$. Errors are also correlated across observations. We have $\cov{\varepsilon_i,\varepsilon_\ell} = \abs{i-\ell}^{-1}$. The closer two observations are index-wise, the stronger their correlation. We will simulate estimates for sample sizes of $n=100$.</span>
<span id="cb53-990"><a href="#cb53-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-993"><a href="#cb53-993" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-994"><a href="#cb53-994" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb53-995"><a href="#cb53-995" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n, <span class="at">ncol =</span> n)</span>
<span id="cb53-996"><a href="#cb53-996" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb53-997"><a href="#cb53-997" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb53-998"><a href="#cb53-998" aria-hidden="true" tabindex="-1"></a>    <span class="co">#if i == l we have a diagonal entry </span></span>
<span id="cb53-999"><a href="#cb53-999" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> l){</span>
<span id="cb53-1000"><a href="#cb53-1000" aria-hidden="true" tabindex="-1"></a>      <span class="co">#assign variance 1 or 2 based on modular arithmetic </span></span>
<span id="cb53-1001"><a href="#cb53-1001" aria-hidden="true" tabindex="-1"></a>      Sigma[i,l] <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">^</span>((i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">==</span> <span class="dv">1</span>))<span class="sc">*</span>(<span class="dv">1</span><span class="sc">^</span>((i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">==</span> <span class="dv">0</span>))</span>
<span id="cb53-1002"><a href="#cb53-1002" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb53-1003"><a href="#cb53-1003" aria-hidden="true" tabindex="-1"></a>      <span class="co">#otherwise asign covariance to be inverse of distance between entries</span></span>
<span id="cb53-1004"><a href="#cb53-1004" aria-hidden="true" tabindex="-1"></a>      Sigma[i,l] <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">abs</span>(i<span class="sc">-</span>l)</span>
<span id="cb53-1005"><a href="#cb53-1005" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb53-1006"><a href="#cb53-1006" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb53-1007"><a href="#cb53-1007" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1008"><a href="#cb53-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(Sigma[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>])</span>
<span id="cb53-1009"><a href="#cb53-1009" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1010"><a href="#cb53-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1011"><a href="#cb53-1011" aria-hidden="true" tabindex="-1"></a>We are calculating the variance *conditional on $\Xm$*, so this means we will use the same realization $\Xm = \X$ for each simulation. Before we simulate things, let's draw our fixed realization of $\X$ and calculate the true variance of our OLS estimator using the formula $(\X'\X)^{-1}\X\Sig\X' (\X'\X)^{-1}$ where $\X$ is our fixed regressors.</span>
<span id="cb53-1012"><a href="#cb53-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1015"><a href="#cb53-1015" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1016"><a href="#cb53-1016" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-1017"><a href="#cb53-1017" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb53-1018"><a href="#cb53-1018" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb53-1019"><a href="#cb53-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1020"><a href="#cb53-1020" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual variance</span></span>
<span id="cb53-1021"><a href="#cb53-1021" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb53-1022"><a href="#cb53-1022" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1023"><a href="#cb53-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1024"><a href="#cb53-1024" aria-hidden="true" tabindex="-1"></a>Now let's simulate 1000 estimates, where we only draw new realizations $\mathbf e$ for each simulation and leave $\X$ fixed.</span>
<span id="cb53-1025"><a href="#cb53-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1028"><a href="#cb53-1028" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1029"><a href="#cb53-1029" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb53-1030"><a href="#cb53-1030" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> N_sim)</span>
<span id="cb53-1031"><a href="#cb53-1031" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb53-1032"><a href="#cb53-1032" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, n), Sigma))</span>
<span id="cb53-1033"><a href="#cb53-1033" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1034"><a href="#cb53-1034" aria-hidden="true" tabindex="-1"></a>  store[k,] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb53-1035"><a href="#cb53-1035" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1036"><a href="#cb53-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1037"><a href="#cb53-1037" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance across simulations</span></span>
<span id="cb53-1038"><a href="#cb53-1038" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(store)</span>
<span id="cb53-1039"><a href="#cb53-1039" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1040"><a href="#cb53-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1041"><a href="#cb53-1041" aria-hidden="true" tabindex="-1"></a>Our simulated variance is quite close to the true conditional variance! </span>
<span id="cb53-1042"><a href="#cb53-1042" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1043"><a href="#cb53-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1044"><a href="#cb53-1044" aria-hidden="true" tabindex="-1"></a>We can simplify the formulas for the OLS estimator's variance greatly if we impose one final assumption on our model.^<span class="co">[</span><span class="ot">A weaker version of these assumptions are provided in chapter 4 of @wooldridge2010econometric.</span><span class="co">]</span> </span>
<span id="cb53-1045"><a href="#cb53-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1046"><a href="#cb53-1046" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-1047"><a href="#cb53-1047" aria-hidden="true" tabindex="-1"></a>We the errors of a model are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_homoskedastic_**<span class="kw">&lt;/span&gt;</span> if</span>
<span id="cb53-1048"><a href="#cb53-1048" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1049"><a href="#cb53-1049" aria-hidden="true" tabindex="-1"></a>\var{\varepsilon_i \mid \X} &amp;= \sigma^2 &amp; (i=1,\ldots,n)</span>
<span id="cb53-1050"><a href="#cb53-1050" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1051"><a href="#cb53-1051" aria-hidden="true" tabindex="-1"></a>(where $\X$ is the random vector of regressors), otherwise they are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_heteroskedastic_**&lt;/span&gt;. If $$\cov{\varepsilon_i,\varepsilon_\ell} = 0$$ for all $i,\ell = 1,\ldots,n$ where $i\neq \ell$ we say errors are &lt;span style="color:red"&gt;**_nonautocorrelated_**&lt;/span&gt;, otherwise they are &lt;span style="color:red"&gt;**_autocorrelated/serially correlated_**&lt;/span&gt;. If errors are both homoskedastic and nonautocorrelated, then we have &lt;span style="color:red"&gt;**_spherical errors_**<span class="kw">&lt;/span&gt;</span> and can write</span>
<span id="cb53-1052"><a href="#cb53-1052" aria-hidden="true" tabindex="-1"></a>$$\E{\ep'\ep\mid \Xm} = \var{\ep\mid \Xm} = \sigma^2\mathbf I.$$</span>
<span id="cb53-1053"><a href="#cb53-1053" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1054"><a href="#cb53-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1055"><a href="#cb53-1055" aria-hidden="true" tabindex="-1"></a>With the addition of this assumption, we have the classical linear model that you are likely familiar with. </span>
<span id="cb53-1056"><a href="#cb53-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1057"><a href="#cb53-1057" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-1058"><a href="#cb53-1058" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(Gauss-Markov/classical) linear model_**<span class="kw">&lt;/span&gt;</span> is defined as $\mathcal P_\text{LM} = <span class="sc">\{</span>P_{\bet,\sigma^2} \mid \bet \in \mathbb R^{K}, \sigma^2\in\mathbb R<span class="sc">\}</span>$, where </span>
<span id="cb53-1059"><a href="#cb53-1059" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1060"><a href="#cb53-1060" aria-hidden="true" tabindex="-1"></a>P_{\bet,\sigma^2} &amp;= <span class="sc">\{</span>F_{\Xm,\ep} \mid \Y= \Xm\bet +\ep, \ \E{\ep'\ep\mid \X}=\sigma^2\mathbf I, \ f_{\Xm}=\textstyle\prod_{i=1}^n f_{\X_i}, \ \text{rank}\left(\E{\X'\X}\right) = K,\ \E{\ep \mid \Xm} = \zer<span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb53-1061"><a href="#cb53-1061" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_j, \cdots \X_K</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\X_1, \cdots, \X_i, \cdots \X_n</span><span class="co">]</span>',<span class="sc">\\</span></span>
<span id="cb53-1062"><a href="#cb53-1062" aria-hidden="true" tabindex="-1"></a>\Y &amp; = <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>.</span>
<span id="cb53-1063"><a href="#cb53-1063" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1064"><a href="#cb53-1064" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1065"><a href="#cb53-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1066"><a href="#cb53-1066" aria-hidden="true" tabindex="-1"></a>When people talk about "the linear (regression) model", this is usually the model they are discussing. The collective assumptions are sometimes known as the "Gauss-Markov assumptions", as they are the sufficient conditions for the Gauss-Markov theorem (which will be presented shortly) to hold.  </span>
<span id="cb53-1067"><a href="#cb53-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1068"><a href="#cb53-1068" aria-hidden="true" tabindex="-1"></a>:::{#cor-}</span>
<span id="cb53-1069"><a href="#cb53-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1070"><a href="#cb53-1070" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS Variance II</span></span>
<span id="cb53-1071"><a href="#cb53-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1072"><a href="#cb53-1072" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\sigma^2} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\ep\mid \Xm} = \zer$, and $\E{\ep\ep'\mid\Xm} = \sigma^2\mathbf I$. Then </span>
<span id="cb53-1073"><a href="#cb53-1073" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1074"><a href="#cb53-1074" aria-hidden="true" tabindex="-1"></a>\var{\hat{\bet}_\text{OLS}\mid \X} &amp;= \sigma^2(\Xm'\Xm)^{-1} = \sigma^2 \left(\sum_{i=1}^n\X_i'\X\right)^{-1}</span>
<span id="cb53-1075"><a href="#cb53-1075" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1076"><a href="#cb53-1076" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1077"><a href="#cb53-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1078"><a href="#cb53-1078" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-1079"><a href="#cb53-1079" aria-hidden="true" tabindex="-1"></a>$(\Xm'\Xm)^{-1}\Xm'\E{\ep\ep'\mid \Xm}\Xm (\Xm'\Xm)^{-1} = (\Xm'\Xm)^{-1}\Xm\sigma^2\mathbf I\Xm (\Xm'\Xm)^{-1} = \sigma^2\underbrace{<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}(\Xm'\Xm)</span><span class="co">]</span>}_{\mathbf I}(\Xm'\Xm)^{-1} = \sigma^2(\Xm'\Xm)^{-1}$</span>
<span id="cb53-1080"><a href="#cb53-1080" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1081"><a href="#cb53-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1082"><a href="#cb53-1082" aria-hidden="true" tabindex="-1"></a>:::{#exm-csvarols}</span>
<span id="cb53-1083"><a href="#cb53-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1084"><a href="#cb53-1084" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparative Statics and Variance</span></span>
<span id="cb53-1085"><a href="#cb53-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1086"><a href="#cb53-1086" aria-hidden="true" tabindex="-1"></a>Suppose the linear model satisfies the Gauss-Markov assumptions and $K = 2$. This simple setting allows us to gain a great deal of insight into the variance of the OLS estimator. In this case $\X$ has two columns: a column one 1s, and $\x$. </span>
<span id="cb53-1087"><a href="#cb53-1087" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1088"><a href="#cb53-1088" aria-hidden="true" tabindex="-1"></a>\Xm &amp; = <span class="co">[</span><span class="ot">\mathbf 1, \X_1</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1089"><a href="#cb53-1089" aria-hidden="true" tabindex="-1"></a>\Xm'\Xm &amp; = \begin{bmatrix}n &amp; \sum_{i=1}^nX_i<span class="sc">\\</span> \sum_{i=1}^nX_i &amp;  \sum_{i=1}^nX_i^2 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-1090"><a href="#cb53-1090" aria-hidden="true" tabindex="-1"></a>\sigma^2(\Xm'\Xm)^{-1} &amp;= \frac{\sigma^2}{n\sum_{i=1}^n X_i^2 - \left(\sum_{i=1}^n X_i\right)^2} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i <span class="sc">\\</span>  -\sum_{i=1}^n X_i &amp; n \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb53-1091"><a href="#cb53-1091" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp; = \frac{\sigma^2}{n<span class="co">[</span><span class="ot">n(\bar{X})^2</span><span class="co">]</span> - (n\bar{X^2})} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i <span class="sc">\\</span>  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-1092"><a href="#cb53-1092" aria-hidden="true" tabindex="-1"></a> &amp; = \frac{\sigma^2}{n^2(\bar{X}^2 - \bar{X^2})} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i <span class="sc">\\</span>  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-1093"><a href="#cb53-1093" aria-hidden="true" tabindex="-1"></a> &amp; = \frac{\sigma^2}{(n^2 - n)\widehat{\text{Var}}(X)} \begin{bmatrix} \sum_{i=1}^n X_i^2  &amp; -\sum_{i=1}^n X_i <span class="sc">\\</span>  -\sum_{i=1}^n X_i &amp; n \end{bmatrix}<span class="sc">\\</span>  &amp; = \begin{bmatrix} \frac{\sigma^2\sum_{i=1}^n X_i^2}{(n^2 - n)\widehat{\text{Var}}(X)}  &amp; -\frac{\sigma^2\sum_{i=1}^n X_i}{(n^2 - n)\widehat{\text{Var}}(X)} <span class="sc">\\</span>  -\frac{\sigma^2\sum_{i=1}^n X_i}{(n^2 - n)\widehat{\text{Var}}(X)} &amp; \frac{\sigma^2n}{(n^2 - n)\widehat{\text{Var}}(X)} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb53-1094"><a href="#cb53-1094" aria-hidden="true" tabindex="-1"></a>\var{\hat{\beta}_{1,OLS}\mid \Xm} &amp; = \frac{\sigma^2\sum_{i=1}^n X_i^2}{(n^2 - n)\widehat{\text{Var}}(X)}<span class="sc">\\</span></span>
<span id="cb53-1095"><a href="#cb53-1095" aria-hidden="true" tabindex="-1"></a>\var{\hat{\beta}_{2,OLS}\mid \Xm} &amp; =\frac{\sigma^2n}{(n^2 - n)\widehat{\text{Var}}(X)}</span>
<span id="cb53-1096"><a href="#cb53-1096" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1097"><a href="#cb53-1097" aria-hidden="true" tabindex="-1"></a>What happens to these variances as we change the variance of the error $\sigma^2$, and the values of $x_i$ change? Instead of finding the signs of various taking partial derivatives, let's graph some examples. First let's see what happens when we hold $\X$ constant but increase $\sigma^2$.</span>
<span id="cb53-1098"><a href="#cb53-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1101"><a href="#cb53-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1102"><a href="#cb53-1102" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1103"><a href="#cb53-1103" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1104"><a href="#cb53-1104" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot58</span></span>
<span id="cb53-1105"><a href="#cb53-1105" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1106"><a href="#cb53-1106" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1107"><a href="#cb53-1107" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-1108"><a href="#cb53-1108" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-1109"><a href="#cb53-1109" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The larger the variance of the error term, the larger the standard errors associated with our OLS estimates."</span></span>
<span id="cb53-1110"><a href="#cb53-1110" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1111"><a href="#cb53-1111" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="sc">-</span><span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1112"><a href="#cb53-1112" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, <span class="at">group =</span> <span class="st">"Low σ^2"</span>) </span>
<span id="cb53-1113"><a href="#cb53-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1114"><a href="#cb53-1114" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="sc">-</span><span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">5</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1115"><a href="#cb53-1115" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, <span class="at">group =</span> <span class="st">"High σ^2"</span>) </span>
<span id="cb53-1116"><a href="#cb53-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1117"><a href="#cb53-1117" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1118"><a href="#cb53-1118" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-1119"><a href="#cb53-1119" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb53-1120"><a href="#cb53-1120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-1121"><a href="#cb53-1121" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb53-1122"><a href="#cb53-1122" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb53-1123"><a href="#cb53-1123" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-1124"><a href="#cb53-1124" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1125"><a href="#cb53-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1126"><a href="#cb53-1126" aria-hidden="true" tabindex="-1"></a>Each graph contains the estimates linear model, with variance illustrated by the gray envelope around the lines. The width of this envelope at the red line $x=0$ corresponds to the variance $\hat{\beta}_{1,OLS}$, while the degree to which the width of the envelope varies along the $x$-axis corresponds to the variance of $\hat{\beta}_{1,OLS}$. We can see that the variance of both estimators decreases when $\sigma^2$ decreases. As the uncertainty about the stochastic element of the model $\varepsilon_i$ decreases, we become more confident in our estimates. Now consider what happens when we change the variance of $x$. </span>
<span id="cb53-1127"><a href="#cb53-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1130"><a href="#cb53-1130" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1131"><a href="#cb53-1131" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1132"><a href="#cb53-1132" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1133"><a href="#cb53-1133" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot59</span></span>
<span id="cb53-1134"><a href="#cb53-1134" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1135"><a href="#cb53-1135" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1136"><a href="#cb53-1136" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-1137"><a href="#cb53-1137" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-1138"><a href="#cb53-1138" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The variance of the OLS estimator decreases as the variance of independent variables increases"</span></span>
<span id="cb53-1139"><a href="#cb53-1139" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1140"><a href="#cb53-1140" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1141"><a href="#cb53-1141" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="dv">4</span>,<span class="dv">6</span>), </span>
<span id="cb53-1142"><a href="#cb53-1142" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb53-1143"><a href="#cb53-1143" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1144"><a href="#cb53-1144" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-1145"><a href="#cb53-1145" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e,</span>
<span id="cb53-1146"><a href="#cb53-1146" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"Low Variance of X"</span></span>
<span id="cb53-1147"><a href="#cb53-1147" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb53-1148"><a href="#cb53-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1149"><a href="#cb53-1149" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1150"><a href="#cb53-1150" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">5</span>,<span class="dv">15</span>),</span>
<span id="cb53-1151"><a href="#cb53-1151" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb53-1152"><a href="#cb53-1152" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1153"><a href="#cb53-1153" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-1154"><a href="#cb53-1154" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb53-1155"><a href="#cb53-1155" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"High Variance of X"</span></span>
<span id="cb53-1156"><a href="#cb53-1156" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb53-1157"><a href="#cb53-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1158"><a href="#cb53-1158" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1159"><a href="#cb53-1159" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-1160"><a href="#cb53-1160" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">fullrange=</span><span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb53-1161"><a href="#cb53-1161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-1162"><a href="#cb53-1162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb53-1163"><a href="#cb53-1163" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb53-1164"><a href="#cb53-1164" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-1165"><a href="#cb53-1165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1166"><a href="#cb53-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1167"><a href="#cb53-1167" aria-hidden="true" tabindex="-1"></a>The more variance we have in our regressors, the less variance our estimator exhibits. Essentially, the variance in observations provides more information about the relationship between the dependent and independent variables, so we get better estimates. Finally consider how the variance changes as the location of our data change relative to the y-axis</span>
<span id="cb53-1168"><a href="#cb53-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1171"><a href="#cb53-1171" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1172"><a href="#cb53-1172" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1173"><a href="#cb53-1173" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1174"><a href="#cb53-1174" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot510</span></span>
<span id="cb53-1175"><a href="#cb53-1175" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1176"><a href="#cb53-1176" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1177"><a href="#cb53-1177" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb53-1178"><a href="#cb53-1178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb53-1179"><a href="#cb53-1179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The proximity of the regressors to origin affects the variance of the intercept estimator"</span></span>
<span id="cb53-1180"><a href="#cb53-1180" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1181"><a href="#cb53-1181" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1182"><a href="#cb53-1182" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb53-1183"><a href="#cb53-1183" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb53-1184"><a href="#cb53-1184" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1185"><a href="#cb53-1185" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-1186"><a href="#cb53-1186" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb53-1187"><a href="#cb53-1187" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"X Near Origin"</span></span>
<span id="cb53-1188"><a href="#cb53-1188" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb53-1189"><a href="#cb53-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1190"><a href="#cb53-1190" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb53-1191"><a href="#cb53-1191" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">10</span>, <span class="dv">10</span>,<span class="dv">12</span>), </span>
<span id="cb53-1192"><a href="#cb53-1192" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">rnorm</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb53-1193"><a href="#cb53-1193" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1194"><a href="#cb53-1194" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-1195"><a href="#cb53-1195" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="dv">2</span><span class="sc">*</span>x <span class="sc">+</span> e, </span>
<span id="cb53-1196"><a href="#cb53-1196" aria-hidden="true" tabindex="-1"></a>    <span class="at">group =</span> <span class="st">"X Far from Origin"</span></span>
<span id="cb53-1197"><a href="#cb53-1197" aria-hidden="true" tabindex="-1"></a>  ) </span>
<span id="cb53-1198"><a href="#cb53-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1199"><a href="#cb53-1199" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(df1, df2) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1200"><a href="#cb53-1200" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-1201"><a href="#cb53-1201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">fullrange=</span><span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb53-1202"><a href="#cb53-1202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-1203"><a href="#cb53-1203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb53-1204"><a href="#cb53-1204" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group) <span class="sc">+</span></span>
<span id="cb53-1205"><a href="#cb53-1205" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-1206"><a href="#cb53-1206" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1207"><a href="#cb53-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1208"><a href="#cb53-1208" aria-hidden="true" tabindex="-1"></a>The closer our observations are to the y-axis the better out estimates of the intercept are.</span>
<span id="cb53-1209"><a href="#cb53-1209" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1210"><a href="#cb53-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1211"><a href="#cb53-1211" aria-hidden="true" tabindex="-1"></a>If we forget the intercept term for a moment, then we can think $\E{\X'\X}$ roughly as the amount of variance in our </span>
<span id="cb53-1212"><a href="#cb53-1212" aria-hidden="true" tabindex="-1"></a>regressors. The variance in regressors amounts to information about $\bet$. The more variance/information we have about $\X$, the better our estimates will be. </span>
<span id="cb53-1213"><a href="#cb53-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1214"><a href="#cb53-1214" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gauss-Markov Theorem </span></span>
<span id="cb53-1215"><a href="#cb53-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1216"><a href="#cb53-1216" aria-hidden="true" tabindex="-1"></a>How do we know that there aren't any other estimators that may be better than OLS? Recall from Section \@ref(finite-sample-properties-of-estimators) we discussed the concept of a MVUE -- an unbiased estimator which is more efficient (has lower variance) than all other unbiased estimators. Finding a MVUE is difficult without additional assumptions about the unbiased estimators. With one such assumption, we do have that OLS is a MVUE among all unbiased estimators satisfying this assumption. This result is known as the Gauss-Markov theorem, and tells us that the OLS estimator has the minimum variance among all linear unbiased estimators. For the remainder of our discussion of the Gauss-Markov theorem, we will assume that our linear model satisfies: $\text{rank}\left(\E{\X'\X}\right) = K$ , $\E{\ep\mid\Xm}=\zer$,and $\E{\ep\ep'\mid\Xm} = \sigma^2\mathbf I$.^<span class="co">[</span><span class="ot">Remember that we have implicitly assumed that the true model is linear.</span><span class="co">]</span> </span>
<span id="cb53-1217"><a href="#cb53-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1218"><a href="#cb53-1218" aria-hidden="true" tabindex="-1"></a>In the context of the linear model, a linear estimator $\hat{\bet}$ will take the form $\hat{\bet} = \mathbf C\Y +\D$ for some matrix $\mathbf C$ (which may be a function of $\Xm$). In the case of $\hat{\bet}_\text{OLS}$, $\mathbf C = (\Xm'\Xm)^{-1}\Xm'$. We will denote a general linear unbiased estimator for $\bet$ as $\tilde{\bet}$. For now, let's condition on the random matrix $\Xm$. To restrict our attention to unbiased linear estimators, $\tilde{\bet}$ must satisfy:</span>
<span id="cb53-1219"><a href="#cb53-1219" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1220"><a href="#cb53-1220" aria-hidden="true" tabindex="-1"></a>&amp;\E{\tilde{\bet} \mid \Xm} = \bet<span class="sc">\\</span></span>
<span id="cb53-1221"><a href="#cb53-1221" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\mathbf C\Y \mid \Xm} = \bet <span class="sc">\\</span></span>
<span id="cb53-1222"><a href="#cb53-1222" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\mathbf C\Xm\bet + \mathbf C\ep\mid \Xm} = \bet &amp; (\Y= \Xm\bet + \ep)<span class="sc">\\</span></span>
<span id="cb53-1223"><a href="#cb53-1223" aria-hidden="true" tabindex="-1"></a>\implies &amp; \mathbf C\Xm\underbrace{\E{\bet \mid \Xm}}_{\bet} + \mathbf C\underbrace{\E{\ep\mid \Xm}}_{\zer} = \bet <span class="sc">\\</span></span>
<span id="cb53-1224"><a href="#cb53-1224" aria-hidden="true" tabindex="-1"></a>\implies &amp;  \mathbf C\Xm\bet = \bet<span class="sc">\\</span></span>
<span id="cb53-1225"><a href="#cb53-1225" aria-hidden="true" tabindex="-1"></a>\implies &amp; \mathbf C\Xm = \mathbf I</span>
<span id="cb53-1226"><a href="#cb53-1226" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1227"><a href="#cb53-1227" aria-hidden="true" tabindex="-1"></a>The variance of $\tilde{\bet}$ can be calculated using the same exact steps we took to calculate the variance of $\hat{\bet}_\text{OLS}$: </span>
<span id="cb53-1228"><a href="#cb53-1228" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1229"><a href="#cb53-1229" aria-hidden="true" tabindex="-1"></a>\var{\tilde{\bet}\mid \Xm} &amp; = \E{ \left(\tilde{\bet} - \E{ \tilde{\bet} }\right) \left(\tilde{\bet} - \E{\tilde{\bet}}\right)'\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-1230"><a href="#cb53-1230" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left(\tilde{\bet} - \bet\right) \left(\tilde{\bet} - \bet\right)'\mid \Xm} &amp; (\tilde{\bet} \text{ unbiased})<span class="sc">\\</span></span>
<span id="cb53-1231"><a href="#cb53-1231" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left<span class="co">[</span><span class="ot">(\mathbf C\Xm\bet + \mathbf C\ep) - \bet\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">(\mathbf C\Xm\bet + \mathbf C\ep) - \bet\right</span><span class="co">]</span>'\mid \Xm} &amp; (\tilde{\bet} = \mathbf C\Xm\bet + \mathbf C\ep)<span class="sc">\\</span></span>
<span id="cb53-1232"><a href="#cb53-1232" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left<span class="co">[</span><span class="ot">(\bet + \mathbf C\ep) - \bet\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">(\bet + \mathbf C\ep) - \bet\right</span><span class="co">]</span>'\mid \Xm}  &amp; (\C\X = \mathbf I)<span class="sc">\\</span></span>
<span id="cb53-1233"><a href="#cb53-1233" aria-hidden="true" tabindex="-1"></a>&amp; = \E{ \left<span class="co">[</span><span class="ot">\mathbf C\ep\right</span><span class="co">]</span> \left<span class="co">[</span><span class="ot">\mathbf C\ep\right</span><span class="co">]</span>'\mid \Xm} <span class="sc">\\</span></span>
<span id="cb53-1234"><a href="#cb53-1234" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\mathbf C\ep\ep'\mathbf C'\mid \Xm}<span class="sc">\\</span></span>
<span id="cb53-1235"><a href="#cb53-1235" aria-hidden="true" tabindex="-1"></a>&amp; = \mathbf C\E{\ep\ep'\mid \Xm}\mathbf C'<span class="sc">\\</span></span>
<span id="cb53-1236"><a href="#cb53-1236" aria-hidden="true" tabindex="-1"></a>&amp;  = \sigma^2\mathbf C\mathbf C' &amp; (\E{\ep\ep'} = \sigma^2\mathbf I)</span>
<span id="cb53-1237"><a href="#cb53-1237" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1238"><a href="#cb53-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1239"><a href="#cb53-1239" aria-hidden="true" tabindex="-1"></a>Our goal is to show that:</span>
<span id="cb53-1240"><a href="#cb53-1240" aria-hidden="true" tabindex="-1"></a>$$\OLS = \argmin_{\tilde{\bet}} \var{\tilde{\bet} \mid \Xm}.$$</span>
<span id="cb53-1241"><a href="#cb53-1241" aria-hidden="true" tabindex="-1"></a>Write $\C = (\Xm'\Xm)^{-1}\Xm + \D$ for some non-zero matrix $\D$. The requirement that $\C\Xm = \mathbf I$ implies that:</span>
<span id="cb53-1242"><a href="#cb53-1242" aria-hidden="true" tabindex="-1"></a>$$ <span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm + \D</span><span class="co">]</span>\Xm = \mathbf I \implies\underbrace{(\Xm'\Xm)^{-1}\Xm'\Xm}_{\mathbf I} + \D\X = \mathbf I \implies \D\Xm = \zer $$</span>
<span id="cb53-1243"><a href="#cb53-1243" aria-hidden="true" tabindex="-1"></a>Note that </span>
<span id="cb53-1244"><a href="#cb53-1244" aria-hidden="true" tabindex="-1"></a>$$ \tilde{\bet} = \C\Y = <span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm + \D</span><span class="co">]</span>\y = (\Xm'\Xm)^{-1}\Xm\Y + \D\y = \OLS + \D\Y,$$ so $\tilde{\bet} = \OLS$ when $\D = \zer$.  Our optimization problem becomes </span>
<span id="cb53-1245"><a href="#cb53-1245" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1246"><a href="#cb53-1246" aria-hidden="true" tabindex="-1"></a>\argmin_{\D} \var{\tilde{\bet} \mid \Xm} &amp; = \argmin_{\D} \sigma^2\mathbf C\mathbf C'<span class="sc">\\</span></span>
<span id="cb53-1247"><a href="#cb53-1247" aria-hidden="true" tabindex="-1"></a>&amp; =  \argmin_{\D} \sigma^2<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm + \D</span><span class="co">][(\Xm'\Xm)^{-1}\Xm + \D]</span>'<span class="sc">\\</span></span>
<span id="cb53-1248"><a href="#cb53-1248" aria-hidden="true" tabindex="-1"></a>&amp; =  \argmin_{\D} \sigma^2<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm + \D</span><span class="co">][\Xm'(\Xm'\Xm)^{-1} + \D']</span><span class="sc">\\</span></span>
<span id="cb53-1249"><a href="#cb53-1249" aria-hidden="true" tabindex="-1"></a>&amp; = \argmin_{\D} \sigma^2<span class="co">[</span><span class="ot">\underbrace{(\Xm'\Xm)^{-1}\Xm\Xm'}_{\mathbf I}(\Xm'\Xm)^{-1} + (\Xm'\Xm)^{-1}\Xm'\D' + \underbrace{\D\Xm}_\zer(\Xm'\Xm)^{-1} + \D\D'</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1250"><a href="#cb53-1250" aria-hidden="true" tabindex="-1"></a>&amp; = \argmin_{\D} \sigma^2<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1} + (\Xm'\Xm)^{-1}\underbrace{(\Xm\D)}_\zer' +  \D\D'</span><span class="co">]</span> &amp; (\Xm'\D' = (\X\D)')<span class="sc">\\</span></span>
<span id="cb53-1251"><a href="#cb53-1251" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2(\Xm'\Xm)^{-1} + \sigma\D\D'.</span>
<span id="cb53-1252"><a href="#cb53-1252" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1253"><a href="#cb53-1253" aria-hidden="true" tabindex="-1"></a>This variance is minimized when $\D = \zer$,^<span class="co">[</span><span class="ot">I'm skirting around proving the fact that the matrix $\D$ cannot be negative definite as it isn't especially informative.</span><span class="co">]</span> so $\OLS$ is the most efficient unbiased linear estimator for any fixed $\Xm =\X$. This holds for all realizations $\Xm=\X$, so it will hold unconditionally as well. </span>
<span id="cb53-1254"><a href="#cb53-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1255"><a href="#cb53-1255" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb53-1256"><a href="#cb53-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1257"><a href="#cb53-1257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gauss-Markov Theorem</span></span>
<span id="cb53-1258"><a href="#cb53-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1259"><a href="#cb53-1259" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\sigma^2} \in \mathcal P_\text{LM}$ where $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\ep\mid \Xm} = \zer$, and $\E{\ep\ep'\mid\Xm} = \sigma^2\mathbf I$. Then $\OLS$ is the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_best linear unbiased estimator (BLUE)/minimum variance linear unbiased estimator (MVLUE)_**<span class="kw">&lt;/span&gt;</span>.</span>
<span id="cb53-1260"><a href="#cb53-1260" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1261"><a href="#cb53-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1262"><a href="#cb53-1262" aria-hidden="true" tabindex="-1"></a>The Gauss-Markov theorem is one of the major justifications for estimating linear models with $\OLS$. With estimation thoroughly treated, we can now consider making inferences about $(\bet,\sigma^2)$ for $P_{\bet,\sigma^2}\in\mathcal P_\text{LM}$.</span>
<span id="cb53-1263"><a href="#cb53-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1264"><a href="#cb53-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1265"><a href="#cb53-1265" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotic Distribution of the OLS Estimator</span></span>
<span id="cb53-1266"><a href="#cb53-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1267"><a href="#cb53-1267" aria-hidden="true" tabindex="-1"></a>We know that our estimator is consistent and the BLUE, but how does its distribution behave? It turns out that $\OLS$ is root-n CAN under weaker assumptions than those required for the Gauss-Markov theorem. Before showing this in earnest, let's look at a special case of the linear model.</span>
<span id="cb53-1268"><a href="#cb53-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1269"><a href="#cb53-1269" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1270"><a href="#cb53-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1271"><a href="#cb53-1271" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gaussian Linear Model</span></span>
<span id="cb53-1272"><a href="#cb53-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1273"><a href="#cb53-1273" aria-hidden="true" tabindex="-1"></a>Suppose $P_\text{LM}$ satisfies the Gauss-Markov assumptions, *in addition* to the assumption that $\ep\mid\Xm \sim N(\zer,\sigma^2\mathbf I)$ (which is equivalent to $\varepsilon_i\iid N(0,\sigma^2)$ because we have assumed spherical errors). This model is sometimes referred to as the **_Gaussian linear model_**. A common way of writing this model is $\Y\mid\Xm \sim N(\Xm\bet,\sigma^2\mathbf I)$, which emphasizes the fact that $\E{\Y\mid\Xm} = \Xm\bet$.  It's quite easy to derive the distribution of $\OLS$ for this model. We won't even need to approximate the distribution via asymptotics! Using the properties of the multivariate distribution, we have:</span>
<span id="cb53-1274"><a href="#cb53-1274" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1275"><a href="#cb53-1275" aria-hidden="true" tabindex="-1"></a>&amp;\OLS - \bet = <span class="co">[</span><span class="ot">\bet + (\Xm'\Xm)^{-1}\Xm\ep</span><span class="co">]</span> - \bet<span class="sc">\\</span></span>
<span id="cb53-1276"><a href="#cb53-1276" aria-hidden="true" tabindex="-1"></a>\implies &amp; \OLS - \bet = (\Xm'\Xm)^{-1}\Xm\ep<span class="sc">\\</span></span>
<span id="cb53-1277"><a href="#cb53-1277" aria-hidden="true" tabindex="-1"></a>\implies &amp; \OLS - \bet \sim N(\zer, (\Xm'\Xm)^{-1}\Xm\sigma^2\mathbf I<span class="co">[</span><span class="ot">(\Xm'\Xm)^{-1}\Xm</span><span class="co">]</span>' ) &amp; (\mathbf A \ep \sim N(\zer, \mathbf A \sigma^2 \mathbf I \mathbf A'))<span class="sc">\\</span></span>
<span id="cb53-1278"><a href="#cb53-1278" aria-hidden="true" tabindex="-1"></a>\implies &amp; \OLS \sim N(\bet , \sigma^2\underbrace{(\Xm'\Xm)^{-1}\Xm\Xm'}_{\mathbf I}(\Xm'\Xm)^{-1})<span class="sc">\\</span></span>
<span id="cb53-1279"><a href="#cb53-1279" aria-hidden="true" tabindex="-1"></a>\implies &amp; \OLS \mid \Xm\sim N(\bet , \sigma^2(\Xm'\Xm)^{-1})</span>
<span id="cb53-1280"><a href="#cb53-1280" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1281"><a href="#cb53-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1282"><a href="#cb53-1282" aria-hidden="true" tabindex="-1"></a>To verify this, we can simulate 50,000 estimates for the model Gaussian linear model where $\beta = <span class="co">[</span><span class="ot">2,4</span><span class="co">]</span>'$, and $\sigma^2 = 1$. We'll pick a modest sample size of $n=5$ to emphasize that this is the precise distribution of $\OLS$, not just the asymptotic distribution. Because this distribution is conditional on $\Xm$, we'll fix the realization $\Xm=\X$ over the simulations.</span>
<span id="cb53-1283"><a href="#cb53-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1286"><a href="#cb53-1286" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1287"><a href="#cb53-1287" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb53-1288"><a href="#cb53-1288" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb53-1289"><a href="#cb53-1289" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-1290"><a href="#cb53-1290" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1)</span>
<span id="cb53-1291"><a href="#cb53-1291" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>)</span>
<span id="cb53-1292"><a href="#cb53-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1293"><a href="#cb53-1293" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb53-1294"><a href="#cb53-1294" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> N_sim)</span>
<span id="cb53-1295"><a href="#cb53-1295" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb53-1296"><a href="#cb53-1296" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb53-1297"><a href="#cb53-1297" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1298"><a href="#cb53-1298" aria-hidden="true" tabindex="-1"></a>  store[k,] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)</span>
<span id="cb53-1299"><a href="#cb53-1299" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1300"><a href="#cb53-1300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1301"><a href="#cb53-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1302"><a href="#cb53-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1305"><a href="#cb53-1305" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1306"><a href="#cb53-1306" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1307"><a href="#cb53-1307" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1308"><a href="#cb53-1308" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1309"><a href="#cb53-1309" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot511</span></span>
<span id="cb53-1310"><a href="#cb53-1310" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1311"><a href="#cb53-1311" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-1312"><a href="#cb53-1312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 11</span></span>
<span id="cb53-1313"><a href="#cb53-1313" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The simulated marginal density of our estimators and their simulated joint density (along with the true underlying distributions shown in red)"</span></span>
<span id="cb53-1314"><a href="#cb53-1314" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1315"><a href="#cb53-1315" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1316"><a href="#cb53-1316" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(store[,<span class="dv">1</span>], store[,<span class="dv">2</span>]),</span>
<span id="cb53-1317"><a href="#cb53-1317" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"β1 Estimate"</span>, <span class="dv">50000</span>), <span class="fu">rep</span>(<span class="st">"β2 Estimate"</span>, <span class="dv">50000</span>))</span>
<span id="cb53-1318"><a href="#cb53-1318" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb53-1319"><a href="#cb53-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1320"><a href="#cb53-1320" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1321"><a href="#cb53-1321" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">1</span>]), <span class="fu">max</span>(store[,<span class="dv">1</span>]), <span class="at">length =</span> <span class="dv">1000</span>), <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">2</span>]) ,<span class="fu">max</span>(store[,<span class="dv">2</span>]), <span class="at">length =</span> <span class="dv">1000</span>)),</span>
<span id="cb53-1322"><a href="#cb53-1322" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"β1 Estimate"</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="st">"β2 Estimate"</span>, <span class="dv">1000</span>))</span>
<span id="cb53-1323"><a href="#cb53-1323" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1324"><a href="#cb53-1324" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(group <span class="sc">==</span> <span class="st">"β1 Estimate"</span>, <span class="fu">dnorm</span>(x, beta[<span class="dv">1</span>], <span class="fu">sqrt</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)[<span class="dv">1</span>,<span class="dv">1</span>])), <span class="fu">dnorm</span>(x, beta[<span class="dv">2</span>], <span class="fu">sqrt</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)[<span class="dv">2</span>,<span class="dv">2</span>]))))</span>
<span id="cb53-1325"><a href="#cb53-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1326"><a href="#cb53-1326" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb53-1327"><a href="#cb53-1327" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">data =</span> df1, <span class="fu">aes</span>(x, <span class="at">y =</span> ..density..), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb53-1328"><a href="#cb53-1328" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> df2, <span class="fu">aes</span>(x,y), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span> </span>
<span id="cb53-1329"><a href="#cb53-1329" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb53-1330"><a href="#cb53-1330" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb53-1331"><a href="#cb53-1331" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1332"><a href="#cb53-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1333"><a href="#cb53-1333" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">1</span>]) ,<span class="fu">max</span>(store[,<span class="dv">1</span>]), <span class="at">length =</span> <span class="dv">1000</span>),</span>
<span id="cb53-1334"><a href="#cb53-1334" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y =</span> <span class="fu">seq</span>(<span class="fu">min</span>(store[,<span class="dv">2</span>]) ,<span class="fu">max</span>(store[,<span class="dv">2</span>]), <span class="at">length =</span> <span class="dv">1000</span>),</span>
<span id="cb53-1335"><a href="#cb53-1335" aria-hidden="true" tabindex="-1"></a>                  )</span>
<span id="cb53-1336"><a href="#cb53-1336" aria-hidden="true" tabindex="-1"></a>df1<span class="sc">$</span>p <span class="ot">&lt;-</span> <span class="fu">dmvnorm</span>(df1, beta, <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X))</span>
<span id="cb53-1337"><a href="#cb53-1337" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb53-1338"><a href="#cb53-1338" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> store[,<span class="dv">1</span>], </span>
<span id="cb53-1339"><a href="#cb53-1339" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> store[,<span class="dv">2</span>]</span>
<span id="cb53-1340"><a href="#cb53-1340" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb53-1341"><a href="#cb53-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1342"><a href="#cb53-1342" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb53-1343"><a href="#cb53-1343" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> df2, <span class="fu">aes</span>(x,y), <span class="at">size =</span> <span class="fl">0.001</span>)<span class="sc">+</span></span>
<span id="cb53-1344"><a href="#cb53-1344" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="at">data =</span> df1, <span class="fu">aes</span>(x,y, <span class="at">z=</span> p), <span class="at">bins =</span> <span class="dv">14</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb53-1345"><a href="#cb53-1345" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-1346"><a href="#cb53-1346" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"β1 Estimate"</span>) <span class="sc">+</span></span>
<span id="cb53-1347"><a href="#cb53-1347" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"β2 Estimate"</span>) <span class="sc">+</span> </span>
<span id="cb53-1348"><a href="#cb53-1348" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb53-1349"><a href="#cb53-1349" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">3.6</span>,<span class="fl">4.4</span>)</span>
<span id="cb53-1350"><a href="#cb53-1350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1351"><a href="#cb53-1351" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb53-1352"><a href="#cb53-1352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1353"><a href="#cb53-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1354"><a href="#cb53-1354" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1355"><a href="#cb53-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1356"><a href="#cb53-1356" aria-hidden="true" tabindex="-1"></a>If we abandon the assumption that $\ep\mid\X\sim N(\zer,\sigma^2\mathbf I)$ we are back to the standard (Gauss-Markov) linear model. All the assumptions about or model take the form of moment conditions, and not specific distributions, so we will not be able to calculate the exact distribution of $\OLS$ in general. Fortunately we can use our asymptotic toolkit to find the limiting distribution of $\OLS$.</span>
<span id="cb53-1357"><a href="#cb53-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1358"><a href="#cb53-1358" aria-hidden="true" tabindex="-1"></a>The overwhelming majority of the time, estimators will be root-n consistent, so the best starting point of finding the asymptotic distribution of an estimator is by first calculating $\sqrt{n}(\hat{\thet} - \thet)$. In the case of the OLS estimator:</span>
<span id="cb53-1359"><a href="#cb53-1359" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1360"><a href="#cb53-1360" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\OLS - \bet) &amp; = n^{1/2}<span class="co">[</span><span class="ot">\bet + (\Xm'\Xm)^{-1}\Xm\ep</span><span class="co">]</span> - \bet<span class="sc">\\</span></span>
<span id="cb53-1361"><a href="#cb53-1361" aria-hidden="true" tabindex="-1"></a>&amp; = \sqrt{n}(\Xm'\Xm)^{-1}\Xm\ep <span class="sc">\\</span> </span>
<span id="cb53-1362"><a href="#cb53-1362" aria-hidden="true" tabindex="-1"></a>&amp; = \sqrt{n}\left(\frac{\Xm'\Xm}{n}\right)^{-1}\left(\frac{\Xm'\ep}{n}\right)<span class="sc">\\</span></span>
<span id="cb53-1363"><a href="#cb53-1363" aria-hidden="true" tabindex="-1"></a>&amp; = \left(\frac{\Xm'\Xm}{n}\right)^{-1}\left(\frac{\Xm'\ep}{\sqrt{n}}\right)<span class="sc">\\</span></span>
<span id="cb53-1364"><a href="#cb53-1364" aria-hidden="true" tabindex="-1"></a>&amp; = \left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}\left(\frac{1}{\sqrt n}\sum_{i=1}^n\X_i'\varepsilon_i\right)</span>
<span id="cb53-1365"><a href="#cb53-1365" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1366"><a href="#cb53-1366" aria-hidden="true" tabindex="-1"></a>Whether you want to show the result using the matrix form of $\OLS$ or the form which is sums of vector is a matter of preference. Regardless, the first term will converge to its population counterpart $\E{\X'\X}$. The second term is a bit more interesting. We have </span>
<span id="cb53-1367"><a href="#cb53-1367" aria-hidden="true" tabindex="-1"></a>$$\left(\frac{1}{\sqrt n}\sum_{i=1}^n\X_i'\varepsilon_i\right) = \sqrt n \left(\frac{1}{ n}\sum_{i=1}^n\X_i'\varepsilon_i - \zer\right) =  \sqrt n \left(\frac{1}{ n}\sum_{i=1}^n\X_i'\varepsilon_i - \E{\X_i'\varepsilon_i}\right),$$ but this is the precise expression which the CLT applies to, so we have:</span>
<span id="cb53-1368"><a href="#cb53-1368" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1369"><a href="#cb53-1369" aria-hidden="true" tabindex="-1"></a>\left(\frac{1}{\sqrt n}\sum_{i=1}^n\X_i'\varepsilon_i\right) &amp;\dto N(\E{\X_i'\varepsilon_i}, \var{\textstyle \sum_{i=1}^n\X_i'\varepsilon_i}/n)<span class="sc">\\</span></span>
<span id="cb53-1370"><a href="#cb53-1370" aria-hidden="true" tabindex="-1"></a>&amp; \dto N(\zer, \textstyle \sum_{i=1}^n\sigma^2\E{\X'\X}/n)<span class="sc">\\</span></span>
<span id="cb53-1371"><a href="#cb53-1371" aria-hidden="true" tabindex="-1"></a>&amp; \dto N(\zer, \sigma^2\E{\X'\X})</span>
<span id="cb53-1372"><a href="#cb53-1372" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1373"><a href="#cb53-1373" aria-hidden="true" tabindex="-1"></a>because $\X_i'\varepsilon_i$ is an iid sample. Using this fact along with Slutsky's theorem and the LLN gives us the distribution of $\OLS$.</span>
<span id="cb53-1374"><a href="#cb53-1374" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1375"><a href="#cb53-1375" aria-hidden="true" tabindex="-1"></a>\sqrt{n}(\OLS - \bet) &amp; =\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}}_{\pto\E{\X'\X}^{-1}}\underbrace{\left(\frac{1}{\sqrt n}\sum_{i=1}^n\X_i'\varepsilon_i\right)}_{\dto N(\zer, \sigma^2\E{\X'\X})}<span class="sc">\\</span></span>
<span id="cb53-1376"><a href="#cb53-1376" aria-hidden="true" tabindex="-1"></a>&amp; \dto \E{\X'\X}^{-1}N(\zer, \sigma^2\E{\X'\X})<span class="sc">\\</span></span>
<span id="cb53-1377"><a href="#cb53-1377" aria-hidden="true" tabindex="-1"></a>&amp; = N(\zer, \E{\X'\X}^{-1}\sigma^2\E{\X'\X}<span class="co">[</span><span class="ot">\E{\X'\X}^{-1}</span><span class="co">]</span>')<span class="sc">\\</span></span>
<span id="cb53-1378"><a href="#cb53-1378" aria-hidden="true" tabindex="-1"></a>&amp; = N(\zer, \E{\X'\X}^{-1}\sigma^2\E{\X'\X}\E{\X'\X}^{-1})<span class="sc">\\</span></span>
<span id="cb53-1379"><a href="#cb53-1379" aria-hidden="true" tabindex="-1"></a>&amp; = N(\zer, \sigma^2\E{\X'\X}^{-1})</span>
<span id="cb53-1380"><a href="#cb53-1380" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1381"><a href="#cb53-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1382"><a href="#cb53-1382" aria-hidden="true" tabindex="-1"></a>If we express the variance in terms of the random matrix $\Xm$ instead of the random vector of covariates using the equality $\E{\X'\X} = \E{\Xm'\Xm}/n$, we have </span>
<span id="cb53-1383"><a href="#cb53-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1384"><a href="#cb53-1384" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}(\OLS - \bet) \dto N(\zer, \sigma^2(\E{\Xm'\Xm}/n)^{-1})= N(\zer, \sigma^2n\E{\Xm'\Xm}^{-1}) $$</span>
<span id="cb53-1385"><a href="#cb53-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1386"><a href="#cb53-1386" aria-hidden="true" tabindex="-1"></a>:::{.theorem name="Asymptotic Distribution of OLS" #asymols}</span>
<span id="cb53-1387"><a href="#cb53-1387" aria-hidden="true" tabindex="-1"></a>Suppose $P_{\bet,\sigma^2}\in \mathcal P_\text{LM}$ where $\E{\X'\ep} = \zer$, $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\ep\ep'} = \sigma^2\mathbf I$. Then</span>
<span id="cb53-1388"><a href="#cb53-1388" aria-hidden="true" tabindex="-1"></a>$$ \OLS \asim N\left(\bet,\sigma^2\E{\Xm'\Xm}^{-1}\right) = N\left(\bet, \frac{\sigma^2}{n}\E{\X'\X}^{-1}\right). $$</span>
<span id="cb53-1389"><a href="#cb53-1389" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1390"><a href="#cb53-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1391"><a href="#cb53-1391" aria-hidden="true" tabindex="-1"></a>We have stated the asymptotic distribution without conditioning on $\Xm$, so $\avar{\OLS}$ will be in terms of the expectation of $\Xm$ opposed to some fixed $\Xm$. We only appealed to the assumption $\E{\ep\ep'} = \sigma^2\mathbf I$ to simplify the asymptotic variance, so in the event $\E{\ep\ep'} \neq \sigma^2\mathbf I$, our estimator will still be root-n CAN, albeit with a different asymptotic variance (we will show this in Section \@ref(generalized-least-squares)). Depending on the level of technical rigor the assumptions which give this result may differ. I followed the derivation provided by  @wooldridge2010econometric, but others will delineate regularity conditions on $\Xm$ so it is "well-behaved", or impose assumptions about the behavior of errors as a martingale. These assumptions tend to be rather weak and will hold in many practical applications. We also could extend this result to data which are not independent using the Lindeberg-Feller CLT.     </span>
<span id="cb53-1392"><a href="#cb53-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1393"><a href="#cb53-1393" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1394"><a href="#cb53-1394" aria-hidden="true" tabindex="-1"></a>Consider the case where $\beta = 2$, $\varepsilon_i \iid \text{Uni}(-1,1)$, $X \sim \text{Uni}(-5,5)$, $\varepsilon\perp X$, and $Y= 2X + \varepsilon$. By properties of the uniform distribution, </span>
<span id="cb53-1395"><a href="#cb53-1395" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1396"><a href="#cb53-1396" aria-hidden="true" tabindex="-1"></a>\sigma^2 &amp;= \frac{1}{3},<span class="sc">\\</span></span>
<span id="cb53-1397"><a href="#cb53-1397" aria-hidden="true" tabindex="-1"></a>\E{X^2} &amp; = \frac{25}{3}.</span>
<span id="cb53-1398"><a href="#cb53-1398" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1399"><a href="#cb53-1399" aria-hidden="true" tabindex="-1"></a>If simulate realizations of $\hat\beta$ for a sufficiently large $n$, we should expect it to approximately follow a normal distribution with mean $2$ and variance:</span>
<span id="cb53-1400"><a href="#cb53-1400" aria-hidden="true" tabindex="-1"></a>$$\frac{\sigma^2}{n}\E{\X'\X}^{-1} =  \frac{1/3}{n} \E{X^2}^{-1} = \frac{1}{3n}(25/3)^{-1} = \frac{1}{25n}.$$</span>
<span id="cb53-1401"><a href="#cb53-1401" aria-hidden="true" tabindex="-1"></a>Let's perform 10,000 simulations for sample sizes $n\in<span class="sc">\{</span>3,5,8,10<span class="sc">\}</span>$.</span>
<span id="cb53-1402"><a href="#cb53-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1405"><a href="#cb53-1405" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1406"><a href="#cb53-1406" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb53-1407"><a href="#cb53-1407" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb53-1408"><a href="#cb53-1408" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">estimate =</span> <span class="cn">NA</span>, <span class="at">n =</span> <span class="cn">NA</span>)</span>
<span id="cb53-1409"><a href="#cb53-1409" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">8</span>,<span class="dv">10</span>)) {</span>
<span id="cb53-1410"><a href="#cb53-1410" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb53-1411"><a href="#cb53-1411" aria-hidden="true" tabindex="-1"></a>    e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb53-1412"><a href="#cb53-1412" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>), <span class="at">ncol =</span> <span class="dv">1</span>) </span>
<span id="cb53-1413"><a href="#cb53-1413" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> X <span class="sc">*</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1414"><a href="#cb53-1414" aria-hidden="true" tabindex="-1"></a>    store <span class="ot">&lt;-</span> <span class="fu">rbind</span>(store, <span class="fu">c</span>(<span class="fu">OLS</span>(y,X),n))</span>
<span id="cb53-1415"><a href="#cb53-1415" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb53-1416"><a href="#cb53-1416" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1417"><a href="#cb53-1417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1418"><a href="#cb53-1418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1419"><a href="#cb53-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1420"><a href="#cb53-1420" aria-hidden="true" tabindex="-1"></a>As $n$ increases we should see the bias of our estimator shrink, and the simulated variance approach $1/25n$. </span>
<span id="cb53-1421"><a href="#cb53-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1424"><a href="#cb53-1424" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1425"><a href="#cb53-1425" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb53-1426"><a href="#cb53-1426" aria-hidden="true" tabindex="-1"></a>store <span class="sc">%&gt;%</span> </span>
<span id="cb53-1427"><a href="#cb53-1427" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(n)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1428"><a href="#cb53-1428" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1429"><a href="#cb53-1429" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb53-1430"><a href="#cb53-1430" aria-hidden="true" tabindex="-1"></a>    <span class="at">Bias =</span> <span class="fu">mean</span>(estimate) <span class="sc">-</span> <span class="dv">2</span>,</span>
<span id="cb53-1431"><a href="#cb53-1431" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">Simulated Variance</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">var</span>(estimate)</span>
<span id="cb53-1432"><a href="#cb53-1432" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1433"><a href="#cb53-1433" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">`</span><span class="at">Limiting Variance</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">25</span><span class="sc">*</span>n)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1434"><a href="#cb53-1434" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>()</span>
<span id="cb53-1435"><a href="#cb53-1435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1436"><a href="#cb53-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1437"><a href="#cb53-1437" aria-hidden="true" tabindex="-1"></a>More importantly (because we knew how to calculate the bias and asymptotic variance of $\OLS$ prior to deriving its limiting distribution), if we use our estimates to make a Q-Q plot, we see that as $n$ increases our estimates fit a normal distribution increasingly well. </span>
<span id="cb53-1438"><a href="#cb53-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1441"><a href="#cb53-1441" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1442"><a href="#cb53-1442" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1443"><a href="#cb53-1443" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1444"><a href="#cb53-1444" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1445"><a href="#cb53-1445" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1446"><a href="#cb53-1446" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-1447"><a href="#cb53-1447" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 11</span></span>
<span id="cb53-1448"><a href="#cb53-1448" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: ""</span></span>
<span id="cb53-1449"><a href="#cb53-1449" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1450"><a href="#cb53-1450" aria-hidden="true" tabindex="-1"></a>store <span class="sc">%&gt;%</span> </span>
<span id="cb53-1451"><a href="#cb53-1451" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(n)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1452"><a href="#cb53-1452" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb53-1453"><a href="#cb53-1453" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>n) <span class="sc">+</span></span>
<span id="cb53-1454"><a href="#cb53-1454" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb53-1455"><a href="#cb53-1455" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb53-1456"><a href="#cb53-1456" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-1457"><a href="#cb53-1457" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">3</span>)</span>
<span id="cb53-1458"><a href="#cb53-1458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1459"><a href="#cb53-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1460"><a href="#cb53-1460" aria-hidden="true" tabindex="-1"></a>Even for modest sample sizes such as $n=10$, it's clear that our estimates are approximately normally distributed. </span>
<span id="cb53-1461"><a href="#cb53-1461" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1462"><a href="#cb53-1462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1463"><a href="#cb53-1463" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimating $\avar{\OLS}$</span></span>
<span id="cb53-1464"><a href="#cb53-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1465"><a href="#cb53-1465" aria-hidden="true" tabindex="-1"></a>We've spent so much time considering the estimation of $\bet$, and completely ignored the other parameter of our model -- $\Sig$. In the case of the Gauss-Markov assumptions, $\Sigma = \sigma^2\mathbf I$, so estimating $\Sig$ simplifies to estimating $\sigma^2$. </span>
<span id="cb53-1466"><a href="#cb53-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1467"><a href="#cb53-1467" aria-hidden="true" tabindex="-1"></a>A natural suggestion for the estimator would be  </span>
<span id="cb53-1468"><a href="#cb53-1468" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n-1}\sum_{i=1}^n(e_i - \underbrace{\E{e_i}}_0)^2 =   \frac{1}{n-1}\sum_{i=1}^ne_i^2 =\e'\e$$ for realizations $\e$ of the random variable $\ep$. This was our approach to calculating the standard error associated with the mean when we didn't know the population variance, but it is a nonstarter in this case because we don't observe $\ep$. So right from the start, we need to think of a way to estimate $\e$. The immediate candidate are the observed errors associated with the estimator $\OLS$. This estimator for $\e$ can be defined as </span>
<span id="cb53-1469"><a href="#cb53-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1470"><a href="#cb53-1470" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1471"><a href="#cb53-1471" aria-hidden="true" tabindex="-1"></a>\hat{\e} &amp; = \Y - \Xm\OLS,<span class="sc">\\</span></span>
<span id="cb53-1472"><a href="#cb53-1472" aria-hidden="true" tabindex="-1"></a>&amp; = \Y - \Xm(\Xm'\Xm)^{-1}\X'\Y,<span class="sc">\\</span></span>
<span id="cb53-1473"><a href="#cb53-1473" aria-hidden="true" tabindex="-1"></a>&amp; = (\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm')\Y.</span>
<span id="cb53-1474"><a href="#cb53-1474" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1475"><a href="#cb53-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1476"><a href="#cb53-1476" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-1477"><a href="#cb53-1477" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(least squares) residuals_**<span class="kw">&lt;/span&gt;</span> associated with the estimator $\OLS$ are defined as </span>
<span id="cb53-1478"><a href="#cb53-1478" aria-hidden="true" tabindex="-1"></a>$$ \hat{\e}(\Y,\Xm) = \Y - \Xm\OLS = \mathbb M\Y,$$ where $\mathbb M = \mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm'$. </span>
<span id="cb53-1479"><a href="#cb53-1479" aria-hidden="true" tabindex="-1"></a>The estimated residuals associated with observations $(\y, \X)$ is </span>
<span id="cb53-1480"><a href="#cb53-1480" aria-hidden="true" tabindex="-1"></a>$$ \hat{\e}(\y,\X) =\y - \X\hat{\mathbf{b}}_\text{OLS}= \mathbf M\y.$$</span>
<span id="cb53-1481"><a href="#cb53-1481" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1482"><a href="#cb53-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1483"><a href="#cb53-1483" aria-hidden="true" tabindex="-1"></a>To estimate $\var{\ep\mid \Xm} = \E{\ep\ep'\mid \Xm}$, let's appeal to the analogy principle and inspect its sample counterpart, only do so using the residuals $\hat{\e}$. To do this, we'll need a few quick results, the proofs of which are applications of linear algebra and can be found in @greene2003econometric.</span>
<span id="cb53-1484"><a href="#cb53-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1485"><a href="#cb53-1485" aria-hidden="true" tabindex="-1"></a>:::{#lem-}</span>
<span id="cb53-1486"><a href="#cb53-1486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1487"><a href="#cb53-1487" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Residuals</span></span>
<span id="cb53-1488"><a href="#cb53-1488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1489"><a href="#cb53-1489" aria-hidden="true" tabindex="-1"></a>Let the estimator $\hat{\e}$ be the least squared residuals. Then:</span>
<span id="cb53-1490"><a href="#cb53-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1491"><a href="#cb53-1491" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbb M$ is symmetric ($\mathbb M'=\mathbb M$) and idempotent ($\mathbb M^2=\mathbb M$). Together these imply that $\mathbb M'\mathbb M= \mathbb M$.</span>
<span id="cb53-1492"><a href="#cb53-1492" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\text{tr}(\ep'\mathbb M\ep) =\text{tr}(\mathbb M\ep'\ep)$ where $\text{tr}(\mathbf A)= \sum_{i=1}^n \text{diag}(\mathbf A)$</span>
<span id="cb53-1493"><a href="#cb53-1493" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1494"><a href="#cb53-1494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1495"><a href="#cb53-1495" aria-hidden="true" tabindex="-1"></a>The matrix $\mathbb M$ satisfies $\mathbb M\Xm =\zer$: </span>
<span id="cb53-1496"><a href="#cb53-1496" aria-hidden="true" tabindex="-1"></a>$$\mathbb M\Xm =(\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm')\Xm =\Xm -  \Xm\underbrace{(\Xm'\Xm)^{-1}\Xm'\Xm}_{\mathbf I} = \zer .$$</span>
<span id="cb53-1497"><a href="#cb53-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1498"><a href="#cb53-1498" aria-hidden="true" tabindex="-1"></a>The sample analog of $\ep'\ep$, using residuals as estimates for $\e$, is: </span>
<span id="cb53-1499"><a href="#cb53-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1500"><a href="#cb53-1500" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1501"><a href="#cb53-1501" aria-hidden="true" tabindex="-1"></a>\hat{\e}'\hat{\e} &amp;= \Y\mathbb M'\mathbb M\Y <span class="sc">\\</span> </span>
<span id="cb53-1502"><a href="#cb53-1502" aria-hidden="true" tabindex="-1"></a>&amp; = <span class="co">[</span><span class="ot">\Xm\OLS + \ep</span><span class="co">]</span>\mathbb M<span class="co">[</span><span class="ot">\Xm\OLS + \ep</span><span class="co">]</span> &amp; (\Y = \Xm\OLS + \ep,\ \mathbb M'\mathbb M =\mathbb M)<span class="sc">\\</span></span>
<span id="cb53-1503"><a href="#cb53-1503" aria-hidden="true" tabindex="-1"></a>&amp; = \ep' \mathbb M\ep &amp; (\mathbb M\Xm =\zer)<span class="sc">\\</span></span>
<span id="cb53-1504"><a href="#cb53-1504" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1505"><a href="#cb53-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1506"><a href="#cb53-1506" aria-hidden="true" tabindex="-1"></a>The expectation of this estimator is</span>
<span id="cb53-1507"><a href="#cb53-1507" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1508"><a href="#cb53-1508" aria-hidden="true" tabindex="-1"></a>\E{\hat{\e}'\hat{\e}} &amp;= \E{\E{\hat{\e}'\hat{\e} \mid \Xm}} <span class="sc">\\</span> </span>
<span id="cb53-1509"><a href="#cb53-1509" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{\ep' \mathbb M\ep \mid \Xm}}<span class="sc">\\</span></span>
<span id="cb53-1510"><a href="#cb53-1510" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{\text{tr}(\ep' \mathbb M\ep) \mid \Xm}} &amp; (\ep' \mathbb M\ep\text{ is a scalar})<span class="sc">\\</span></span>
<span id="cb53-1511"><a href="#cb53-1511" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\E{\text{tr}( \mathbb M\ep'\ep) \mid \Xm}} &amp; (\text{tr}(\ep'\mathbb M\ep) =\text{tr}(\mathbb M\ep'\ep))<span class="sc">\\</span></span>
<span id="cb53-1512"><a href="#cb53-1512" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\mathbb M(\text{tr}\E{\ep'\ep \mid \Xm})} &amp; (\mathbb M\text{ is a function of }\Xm)<span class="sc">\\</span></span>
<span id="cb53-1513"><a href="#cb53-1513" aria-hidden="true" tabindex="-1"></a>&amp; = \text{tr}(\mathbb M \sigma^2 \mathbf I) <span class="sc">\\</span></span>
<span id="cb53-1514"><a href="#cb53-1514" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2 \text{tr}(\mathbb M) <span class="sc">\\</span> </span>
<span id="cb53-1515"><a href="#cb53-1515" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2 \text{tr}(\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm')<span class="sc">\\</span></span>
<span id="cb53-1516"><a href="#cb53-1516" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2 \text{tr}(\mathbf I) - \text{tr}((\Xm'\Xm)^{-1}\Xm\Xm')<span class="sc">\\</span></span>
<span id="cb53-1517"><a href="#cb53-1517" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2(n-K)</span>
<span id="cb53-1518"><a href="#cb53-1518" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1519"><a href="#cb53-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1520"><a href="#cb53-1520" aria-hidden="true" tabindex="-1"></a>Much like the estimator $n^{-1}\sum_{i=1}^n(X_i - \bar X)^2$ for some $\var{X}$, our estimator for the variance of our residuals is biased. If we correct for this bias, we have </span>
<span id="cb53-1521"><a href="#cb53-1521" aria-hidden="true" tabindex="-1"></a>$$ S^2 = \frac{\hat{\e}'\hat{\e}}{n-K}.$$ This correction follows from the same intuition behind Bessel's correction. Bessel's correction accounted for the estimation of population variance have two steps: first we estimate $\bar X$ because we do not know $\mu = \E{X}$, and then we use this intermediate estimate to calculate the sample variance. We're doing precisely the same thing when estimating the variance of our errors. It requires an intermediate step where we estimate $\OLS$, and then we use our estimated value to calculate $\hat{\e}'\hat{\e}/(n-K)$. The estimator $\OLS$ is a $K-$vector, so we need to correct for each dimension. </span>
<span id="cb53-1522"><a href="#cb53-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1523"><a href="#cb53-1523" aria-hidden="true" tabindex="-1"></a>:::{#prp-olsvar}</span>
<span id="cb53-1524"><a href="#cb53-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1525"><a href="#cb53-1525" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimation of OLS Variance</span></span>
<span id="cb53-1526"><a href="#cb53-1526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1527"><a href="#cb53-1527" aria-hidden="true" tabindex="-1"></a>Define the estimator $$S^2 =  \frac{\hat{\e}'\hat{\e}}{n-K}$$ in the context of the classic linear model. Then:</span>
<span id="cb53-1528"><a href="#cb53-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1529"><a href="#cb53-1529" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$S^2$ is an unbiased for $\var{\ep\mid\X} = \sigma^2$.</span>
<span id="cb53-1530"><a href="#cb53-1530" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$S^2$ is a consistent estimator $\var{\ep\mid\X} = \sigma^2$.</span>
<span id="cb53-1531"><a href="#cb53-1531" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The estimator $\widehat{\text{Avar}}(\OLS) = S^2(\Xm'\Xm)^{-1}$ is a consistent estimator for </span>
<span id="cb53-1532"><a href="#cb53-1532" aria-hidden="true" tabindex="-1"></a>${\text{Avar}}(\OLS) = \sigma^2\E{\Xm'\Xm}^{-1}$</span>
<span id="cb53-1533"><a href="#cb53-1533" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1534"><a href="#cb53-1534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1535"><a href="#cb53-1535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1536"><a href="#cb53-1536" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-1537"><a href="#cb53-1537" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:white"</span><span class="kw">&gt;</span>space<span class="kw">&lt;/span&gt;</span></span>
<span id="cb53-1538"><a href="#cb53-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1539"><a href="#cb53-1539" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>This follows from our derivation of the estimator: $$\E{S^2} = \E{\hat{\e}'\hat{\e}}/(n-K) = <span class="co">[</span><span class="ot">\sigma^2/(n-K)</span><span class="co">]</span>/(n-K) = \sigma^2.$$</span>
<span id="cb53-1540"><a href="#cb53-1540" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We have:</span>
<span id="cb53-1541"><a href="#cb53-1541" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1542"><a href="#cb53-1542" aria-hidden="true" tabindex="-1"></a>S^2 &amp; = \E{\hat{\e}'\hat{\e}}/(n-K)<span class="sc">\\</span></span>
<span id="cb53-1543"><a href="#cb53-1543" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{n-K}\E{\ep'\mathbb M\ep}<span class="sc">\\</span></span>
<span id="cb53-1544"><a href="#cb53-1544" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{n-k}<span class="co">[</span><span class="ot">\ep'(\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm')\ep</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1545"><a href="#cb53-1545" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{n-k}<span class="co">[</span><span class="ot">\ep'\ep - \ep' \Xm(\Xm'\Xm)^{-1}\Xm'\ep</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1546"><a href="#cb53-1546" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{n}{n-k}\left<span class="co">[</span><span class="ot">\frac{\ep'\ep}{n} - \frac{\ep' \Xm}{n}\frac{(\Xm'\Xm)^{-1}}{n}\frac{\Xm'\ep}{n}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1547"><a href="#cb53-1547" aria-hidden="true" tabindex="-1"></a>&amp; = \underbrace{\frac{n}{n-k}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^n \varepsilon_i^2}_{\pto \sigma^2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^n\varepsilon_i\X_i\right)}_{\pto \zer}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\X_i\right)^{-1}}_{\pto \E{\X'\X}^{-1}}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n\X_i'\varepsilon_i\right)}_{\pto \zer} \Bigg] &amp; (\text{LLN})<span class="sc">\\</span></span>
<span id="cb53-1548"><a href="#cb53-1548" aria-hidden="true" tabindex="-1"></a>&amp; \pto 1(\sigma^2 - \zer\E{\X'\X}^{-1}\zer) &amp; (\text{Slutsky's theorem})<span class="sc">\\</span></span>
<span id="cb53-1549"><a href="#cb53-1549" aria-hidden="true" tabindex="-1"></a>&amp; = \sigma^2</span>
<span id="cb53-1550"><a href="#cb53-1550" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1551"><a href="#cb53-1551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1552"><a href="#cb53-1552" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We can now use the fact that $S^2\pto \sigma^2$ along with Slutsky's theorem:</span>
<span id="cb53-1553"><a href="#cb53-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1554"><a href="#cb53-1554" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\text{Avar}}(\OLS) = \underbrace{S^2}_{\pto \sigma^2}[\underbrace{(\Xm'\Xm)}_{\pto n\E{\X'\X}}]^{-1} \pto \sigma^2<span class="co">[</span><span class="ot">n\E{\X'\X}</span><span class="co">]</span>^{-1}=\frac{\sigma^2}{n}\E{\X'\X}^{-1}={\text{Avar}}(\OLS). $$</span>
<span id="cb53-1555"><a href="#cb53-1555" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1556"><a href="#cb53-1556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1557"><a href="#cb53-1557" aria-hidden="true" tabindex="-1"></a><span class="fu">## Basic Model Selection and Inference</span></span>
<span id="cb53-1558"><a href="#cb53-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1559"><a href="#cb53-1559" aria-hidden="true" tabindex="-1"></a>We've been operating under the assumption that we know the true model $\mathcal P_\text{LM}$, but in reality knowing this is impossible. In fact, a common aphorism in statistics is that "all models are wrong", because the world is too complex to systematically describe any phenomenon. This is especially true of the social sciences. Fortunately, the full aphorism is "all models are wrong...but some are useful." Even if models are approximations of reality, they offer insights into the world. So how do we pick the right one?</span>
<span id="cb53-1560"><a href="#cb53-1560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1561"><a href="#cb53-1561" aria-hidden="true" tabindex="-1"></a>In the context of $\mathcal P_\text{LM}$, this question amounts to considering the random vector of regressors $\X$. Even if our model is founded in rigorous economic theory, it still may be unclear which independent variables are pertinent. In Example \@ref(exm:car), we considered a model where an agent $i$ got utility $u_{ij}$ from purchasing a car $j$, assuming that their utility function took the form $u_{ij}=\X_{ij}\bet + \varepsilon_i$ for a vector of vehicle and consumer attributes $\X_{ij}$ where $\varepsilon_i$ corresponds to heterogeneity. It is up to us to determine which variables to include in $\X_{ij}$. Attributes likes vehicle price, consumer location, whether a car is new or used, and model year of the car likely affect a consumers utility. But what about things like car color, technical specifications like a vehicles torque? There is no cut and dry answer to this, hence the black hole that is literature regrading model select. For now, we will take a basic approach to model selection rooted in methods introduced in \@ref(hypothesis-testing).</span>
<span id="cb53-1562"><a href="#cb53-1562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1563"><a href="#cb53-1563" aria-hidden="true" tabindex="-1"></a>Consider two models $\mathcal P_\text{LM}$ and $\mathcal P_\text{LM}'$:</span>
<span id="cb53-1564"><a href="#cb53-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1565"><a href="#cb53-1565" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1566"><a href="#cb53-1566" aria-hidden="true" tabindex="-1"></a>\mathcal P_\text{LM}&amp;: Y = \beta_0 + \beta_1 X_1 + \varepsilon<span class="sc">\\</span></span>
<span id="cb53-1567"><a href="#cb53-1567" aria-hidden="true" tabindex="-1"></a>\mathcal P_\text{LM}'&amp;: Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon</span>
<span id="cb53-1568"><a href="#cb53-1568" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb53-1569"><a href="#cb53-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1570"><a href="#cb53-1570" aria-hidden="true" tabindex="-1"></a>These models are referred to as **_nested models_**, because the parameter space corresponding to $\mathcal P_\text{LM}$ is a subset of the parameter space corresponding to $\mathcal P_\text{LM}'$. If we are tasked with choosing between these two models, we can estimate $\mathcal P_\text{LM}'$ and test the hypothesis $H_0:\beta_2 = 0$. If we find sufficient evidence to reject this null hypothesis, than $\beta_2$ is likely nontrivial and should be included in the model, prompting us to favor $\mathcal P_\text{LM}'$. </span>
<span id="cb53-1571"><a href="#cb53-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1572"><a href="#cb53-1572" aria-hidden="true" tabindex="-1"></a>In general, suppose you want to test $H_0:\beta_j = \beta_{0}$. We established that $\OLS$ is root-n CAN so we can use the $t-$test discussed in @sec-tsting to test this hypothesis. The statistic would be </span>
<span id="cb53-1573"><a href="#cb53-1573" aria-hidden="true" tabindex="-1"></a>$$ t = \frac{\hat\beta_{\text{OLS},j} - \beta_0}{\widehat{\text{se}}(\hat\beta_{\text{OLS},j})}.$$ This statistic relies on a consistent estimator $\widehat{\text{se}}(\hat\beta_{\text{OLS},j})$, but this is given immediately by our consistent estimator $\widehat{\text{Avar}}(\OLS) = S^2(\Xm'\Xm)^{-1}$. </span>
<span id="cb53-1574"><a href="#cb53-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1575"><a href="#cb53-1575" aria-hidden="true" tabindex="-1"></a>$$\widehat{\text{se}}(\OLS) = \left<span class="co">[</span><span class="ot">\text{diag}(S^2(\Xm'\Xm)^{-1})\right</span><span class="co">]</span>^{1/2}.$$ In the context of model selection, our default hypothesis is $H_0:\beta_j = \beta_0$ -- is the addition of $\beta_j$ in our specification nontrivial? This is why if you run a regression using almost any statistical software, it will automatically report the results associated with the hypotheses $H_0:\beta_j = 0$ for each separate $\beta_j$.   </span>
<span id="cb53-1576"><a href="#cb53-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1577"><a href="#cb53-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1578"><a href="#cb53-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1579"><a href="#cb53-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1580"><a href="#cb53-1580" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1581"><a href="#cb53-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1582"><a href="#cb53-1582" aria-hidden="true" tabindex="-1"></a><span class="fu">## Coding Exercise</span></span>
<span id="cb53-1583"><a href="#cb53-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1584"><a href="#cb53-1584" aria-hidden="true" tabindex="-1"></a>Now that we know how to estimate and draw inferences about $\bet$, let's return to our ```OLS()``` function which we first defined in Example \@ref(exm:funref). Along with calculating $\OLS$, let's calculate $\widehat{\text{se}}(\OLS)$, the $t$-statistic associated with testing $K$ null hypotheses $\beta_j = 0$ (separately) at a significance level of $\alpha = 0.05$, a 95% confidence interval for $\bet$, and the associated $p-$value.</span>
<span id="cb53-1585"><a href="#cb53-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1588"><a href="#cb53-1588" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1589"><a href="#cb53-1589" aria-hidden="true" tabindex="-1"></a>OLS <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X){</span>
<span id="cb53-1590"><a href="#cb53-1590" aria-hidden="true" tabindex="-1"></a>  <span class="co">#determine dimensions, confirm estimate exists, perform OLS</span></span>
<span id="cb53-1591"><a href="#cb53-1591" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb53-1592"><a href="#cb53-1592" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb53-1593"><a href="#cb53-1593" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">det</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">==</span> <span class="dv">0</span>) {<span class="fu">stop</span>(<span class="st">"rank(X'X) &lt; K"</span>)}</span>
<span id="cb53-1594"><a href="#cb53-1594" aria-hidden="true" tabindex="-1"></a>  hat_beta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb53-1595"><a href="#cb53-1595" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1596"><a href="#cb53-1596" aria-hidden="true" tabindex="-1"></a>  <span class="co">#use OLS estimates to calculate residuals and estimate SEs</span></span>
<span id="cb53-1597"><a href="#cb53-1597" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> hat_beta)</span>
<span id="cb53-1598"><a href="#cb53-1598" aria-hidden="true" tabindex="-1"></a>  S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb53-1599"><a href="#cb53-1599" aria-hidden="true" tabindex="-1"></a>  var_hat <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb53-1600"><a href="#cb53-1600" aria-hidden="true" tabindex="-1"></a>  se_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(var_hat))</span>
<span id="cb53-1601"><a href="#cb53-1601" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1602"><a href="#cb53-1602" aria-hidden="true" tabindex="-1"></a>  <span class="co">#t-stat, confidence intervals, p values</span></span>
<span id="cb53-1603"><a href="#cb53-1603" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> hat_beta<span class="sc">/</span>se_hat</span>
<span id="cb53-1604"><a href="#cb53-1604" aria-hidden="true" tabindex="-1"></a>  lower_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb53-1605"><a href="#cb53-1605" aria-hidden="true" tabindex="-1"></a>  upper_CI <span class="ot">&lt;-</span> hat_beta <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se_hat</span>
<span id="cb53-1606"><a href="#cb53-1606" aria-hidden="true" tabindex="-1"></a>  p_val <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">pt</span>(t, n<span class="sc">-</span>K))</span>
<span id="cb53-1607"><a href="#cb53-1607" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1608"><a href="#cb53-1608" aria-hidden="true" tabindex="-1"></a>  <span class="co">#combine everything into one table to return</span></span>
<span id="cb53-1609"><a href="#cb53-1609" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">cbind</span>(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)</span>
<span id="cb53-1610"><a href="#cb53-1610" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(output) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"β"</span>, <span class="dv">1</span><span class="sc">:</span>K, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb53-1611"><a href="#cb53-1611" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(output) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Estimate"</span>, <span class="st">"Std.Error"</span>, <span class="st">"t-Stat"</span>, <span class="st">"Lower 95% CI"</span>, <span class="st">"Upper 95% CI"</span>, <span class="st">"p-Value"</span>)</span>
<span id="cb53-1612"><a href="#cb53-1612" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb53-1613"><a href="#cb53-1613" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1614"><a href="#cb53-1614" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1615"><a href="#cb53-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1616"><a href="#cb53-1616" aria-hidden="true" tabindex="-1"></a>We'll estimate the model given by $\bet = <span class="co">[</span><span class="ot">2,5,4,3,6</span><span class="co">]</span>'$, $\X \sim N(\zer, \mathbf I)$, $n = 15$, and $\varepsilon\iid \text{Uni}(-1,1)$. Before we use our ```OLS()``` function, let's see what R's base function ```lm()``` (which stands for "linear model") give us.</span>
<span id="cb53-1617"><a href="#cb53-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1620"><a href="#cb53-1620" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1621"><a href="#cb53-1621" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">6</span>)</span>
<span id="cb53-1622"><a href="#cb53-1622" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb53-1623"><a href="#cb53-1623" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="fu">diag</span>(<span class="dv">1</span>,<span class="dv">4</span>))</span>
<span id="cb53-1624"><a href="#cb53-1624" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X)</span>
<span id="cb53-1625"><a href="#cb53-1625" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb53-1626"><a href="#cb53-1626" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1627"><a href="#cb53-1627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1628"><a href="#cb53-1628" aria-hidden="true" tabindex="-1"></a><span class="co">#base R function, -1 to omit intercept which we added a column for</span></span>
<span id="cb53-1629"><a href="#cb53-1629" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>))<span class="sc">$</span>coefficients)</span>
<span id="cb53-1630"><a href="#cb53-1630" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1631"><a href="#cb53-1631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1632"><a href="#cb53-1632" aria-hidden="true" tabindex="-1"></a>Now for our function.</span>
<span id="cb53-1633"><a href="#cb53-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1636"><a href="#cb53-1636" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1637"><a href="#cb53-1637" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">OLS</span>(y,X))</span>
<span id="cb53-1638"><a href="#cb53-1638" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1639"><a href="#cb53-1639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1640"><a href="#cb53-1640" aria-hidden="true" tabindex="-1"></a>The outputs are identical, so our function works perfectly!</span>
<span id="cb53-1641"><a href="#cb53-1641" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1642"><a href="#cb53-1642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1643"><a href="#cb53-1643" aria-hidden="true" tabindex="-1"></a>If we want to test hypotheses jointly, we need to use the Wald test instead of the $t$-test. For some hypothesis $H_0:\mathbf h(\bet) = \zer$ given by $\mathbf h:\mathbb R^K\to\mathbb R^q$, our statistic is </span>
<span id="cb53-1644"><a href="#cb53-1644" aria-hidden="true" tabindex="-1"></a>$$W = \mathbf h(\OLS)'  \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf h}{\partial\bet}(\OLS) \widehat{\text{Avar}}(\OLS)\frac{\partial \mathbf h}{\partial\bet}(\OLS)'\right</span><span class="co">]</span>^{-1}\mathbf h(\OLS),$$ where $W \asim \chi_q^2$ under $H_0$. </span>
<span id="cb53-1645"><a href="#cb53-1645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1646"><a href="#cb53-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1647"><a href="#cb53-1647" aria-hidden="true" tabindex="-1"></a>:::{#exm-ftest}</span>
<span id="cb53-1648"><a href="#cb53-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1649"><a href="#cb53-1649" aria-hidden="true" tabindex="-1"></a><span class="fu">## F-Test</span></span>
<span id="cb53-1650"><a href="#cb53-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1651"><a href="#cb53-1651" aria-hidden="true" tabindex="-1"></a>Consider the Gaussian linear model where all Gauss-Markov assumptions are met and $\ep \sim N(\zer, \sigma^2\mathbf I)$, along with the linear hypothesis that $\mathbf H\bet = \bet_0$ for a $q\times K$ matrix $\mathbf H$. In this case the Wald statistic is \begin{align*}</span>
<span id="cb53-1652"><a href="#cb53-1652" aria-hidden="true" tabindex="-1"></a>W &amp; = <span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'\left<span class="co">[</span><span class="ot">\var{\mathbf H\OLS\mid \X}\right</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1653"><a href="#cb53-1653" aria-hidden="true" tabindex="-1"></a>  &amp; = <span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">\mathbf H\var{\OLS\mid \Xm}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1654"><a href="#cb53-1654" aria-hidden="true" tabindex="-1"></a>  &amp; = <span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">\sigma^2\mathbf H(\Xm'\Xm)^{-1}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span> &amp; \left(\var{\OLS\mid \Xm} = \sigma^2 (\Xm'\Xm)^{-1}\right)<span class="sc">\\</span></span>
<span id="cb53-1655"><a href="#cb53-1655" aria-hidden="true" tabindex="-1"></a>  &amp; = \frac{<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">\mathbf H(\Xm'\Xm)^{-1}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>}{\sigma^2}</span>
<span id="cb53-1656"><a href="#cb53-1656" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1657"><a href="#cb53-1657" aria-hidden="true" tabindex="-1"></a>We don't know $\sigma^2$ so we cannot use this test statistic. Instead we define a new statistic $F$ which uses the estimator $S^2$. </span>
<span id="cb53-1658"><a href="#cb53-1658" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1659"><a href="#cb53-1659" aria-hidden="true" tabindex="-1"></a>F &amp; = \frac{W}{q}\frac{\sigma^2}{S^2}<span class="sc">\\</span></span>
<span id="cb53-1660"><a href="#cb53-1660" aria-hidden="true" tabindex="-1"></a>  &amp; = \frac{<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">\mathbf H(\Xm'\Xm)^{-1}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>}{\sigma^2}\cdot \frac{1}{q}\cdot \frac{\sigma^2}{S^2} \cdot \frac{n-K}{n-K}<span class="sc">\\</span></span>
<span id="cb53-1661"><a href="#cb53-1661" aria-hidden="true" tabindex="-1"></a>  &amp; = \frac{<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">\sigma^2\mathbf H(\Xm'\Xm)^{-1}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>/q}{<span class="co">[</span><span class="ot">(n-K)S^2/\sigma^2</span><span class="co">]</span>/(n-K)}</span>
<span id="cb53-1662"><a href="#cb53-1662" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1663"><a href="#cb53-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1664"><a href="#cb53-1664" aria-hidden="true" tabindex="-1"></a>Under the assumptions that $\ep$ is normally distributed, it can be shown that the denominator is distributed according to $\frac{1}{n-K}\chi_{n-K}^2$. This along with the numerator being distributed according the $\frac{1}{q}\chi_q^2$ means $F \sim F_{q,n-K}$. If we simplify the statistic $F$ we have </span>
<span id="cb53-1665"><a href="#cb53-1665" aria-hidden="true" tabindex="-1"></a>$$ F = \frac{<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>'<span class="co">[</span><span class="ot">S^2\mathbf H(\Xm'\Xm)^{-1}\mathbf H'</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\mathbf H\OLS - \bet_0</span><span class="co">]</span>}{q} \sim F_{q,n-K},$$ and can test $\mathbf H\bet = \bet_0$ with the **_$F-$test_**.</span>
<span id="cb53-1666"><a href="#cb53-1666" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1667"><a href="#cb53-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1668"><a href="#cb53-1668" aria-hidden="true" tabindex="-1"></a>The $F-$test is just a special case of the Wald test where we can derive an exact distribution of our test statistic. Most statistical softwares will present the $F-$stat associated with $H_0:\bet = \zer$ when you run a linear regression. This test corresponds to the hypothesis that our specification is completely wrong and none of the independent variables appear (jointly) relevant. Unfortunately, this test is not as robust as the Wald test, as it requires normal errors. </span>
<span id="cb53-1669"><a href="#cb53-1669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1670"><a href="#cb53-1670" aria-hidden="true" tabindex="-1"></a>The next example is due to @greene2003econometric and shows how a Wald test can be used in the context of model selection.</span>
<span id="cb53-1671"><a href="#cb53-1671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1672"><a href="#cb53-1672" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1673"><a href="#cb53-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1674"><a href="#cb53-1674" aria-hidden="true" tabindex="-1"></a><span class="fu">## Investement Model</span></span>
<span id="cb53-1675"><a href="#cb53-1675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1676"><a href="#cb53-1676" aria-hidden="true" tabindex="-1"></a>Let $I_t$ denote the investment in a fixed economy at time $t$. One linear model for investment specifies:</span>
<span id="cb53-1677"><a href="#cb53-1677" aria-hidden="true" tabindex="-1"></a>$$ \log I_t = \beta_1 + \beta_2 i_t + \beta_3 \Delta p_t + \beta_4\log Y_t + \beta_5t + \varepsilon_t$$ where $i_t$ is the nominal interest rate, $\Delta p_t$ is the inflation rate, and $Y_t$ is real output. We've also included a time trend $t$ as an independent variable. Instead of the nominal interest rate, agents may care about the real (adjusted for inflation) interest rate $i_t - \Delta p_t$. So perhaps investment is only affected by inflation insofar that inflation determines the real interest rate.^<span class="co">[</span><span class="ot">It could be the case that investment is a function of real interest rates along with inflation if inflation has affects outside of that on the real interest rate.</span><span class="co">]</span> If this is the case our model is </span>
<span id="cb53-1678"><a href="#cb53-1678" aria-hidden="true" tabindex="-1"></a>$$ \log I_t = \beta_1 + \beta_2 (i_t - \Delta p_t) + \beta_4\log Y_t + \beta_5t + \varepsilon_t.$$ We can test if this second specification is favorable by estimating the first model, and then testing the hypothesis $H_0 : \beta_2 + \beta_3 = 0$. Let's perform a simulation where the null hypothesis is true, and perform to corresponding Wald test. For the sake of ease, we will assume everything is uniformly distributed instead of simulating values such that they are realistic in the economic context of the model. </span>
<span id="cb53-1679"><a href="#cb53-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1682"><a href="#cb53-1682" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1683"><a href="#cb53-1683" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb53-1684"><a href="#cb53-1684" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.1</span>)</span>
<span id="cb53-1685"><a href="#cb53-1685" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">5</span> </span>
<span id="cb53-1686"><a href="#cb53-1686" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-1687"><a href="#cb53-1687" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-1688"><a href="#cb53-1688" aria-hidden="true" tabindex="-1"></a>del_p <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-1689"><a href="#cb53-1689" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb53-1690"><a href="#cb53-1690" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb53-1691"><a href="#cb53-1691" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb53-1692"><a href="#cb53-1692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1693"><a href="#cb53-1693" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, i, del_p, <span class="fu">log</span>(y), t)</span>
<span id="cb53-1694"><a href="#cb53-1694" aria-hidden="true" tabindex="-1"></a>I <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1695"><a href="#cb53-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1696"><a href="#cb53-1696" aria-hidden="true" tabindex="-1"></a>hat_beta <span class="ot">&lt;-</span> <span class="fu">OLS</span>(I,X)[,<span class="dv">1</span>]</span>
<span id="cb53-1697"><a href="#cb53-1697" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> (y<span class="sc">-</span>X <span class="sc">%*%</span> hat_beta)</span>
<span id="cb53-1698"><a href="#cb53-1698" aria-hidden="true" tabindex="-1"></a>S2 <span class="ot">&lt;-</span> ((<span class="fu">t</span>(res) <span class="sc">%*%</span> res)<span class="sc">/</span>(n <span class="sc">-</span> K)) <span class="sc">%&gt;%</span> <span class="fu">as.numeric</span>() </span>
<span id="cb53-1699"><a href="#cb53-1699" aria-hidden="true" tabindex="-1"></a>var_hat <span class="ot">&lt;-</span> (S2) <span class="sc">*</span> <span class="fu">solve</span>( <span class="fu">t</span>(X) <span class="sc">%*%</span> X )</span>
<span id="cb53-1700"><a href="#cb53-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1701"><a href="#cb53-1701" aria-hidden="true" tabindex="-1"></a>H <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb53-1702"><a href="#cb53-1702" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb53-1703"><a href="#cb53-1703" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">t</span>(H <span class="sc">%*%</span> hat_beta) <span class="sc">%*%</span> <span class="fu">solve</span>(H <span class="sc">%*%</span> var_hat <span class="sc">%*%</span> <span class="fu">t</span>(H)) <span class="sc">%*%</span> (H <span class="sc">%*%</span> hat_beta)</span>
<span id="cb53-1704"><a href="#cb53-1704" aria-hidden="true" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="dv">1</span>)</span>
<span id="cb53-1705"><a href="#cb53-1705" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">c</span>(W, C))</span>
<span id="cb53-1706"><a href="#cb53-1706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1707"><a href="#cb53-1707" aria-hidden="true" tabindex="-1"></a>The value of the Wald test stat is not even close to exceeding the critical value, so we fail to reject the null hypothesis and conclude that $\beta_2 + \beta_3 = 0$.</span>
<span id="cb53-1708"><a href="#cb53-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1709"><a href="#cb53-1709" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1710"><a href="#cb53-1710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1711"><a href="#cb53-1711" aria-hidden="true" tabindex="-1"></a><span class="fu">## Partial/Marginal Effects, Linear Projection Revisited</span></span>
<span id="cb53-1712"><a href="#cb53-1712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1713"><a href="#cb53-1713" aria-hidden="true" tabindex="-1"></a>When interpreting the parameters $\bet$ in $\mathcal P_\text{LM}$, it's very common to think in terms of derivatives. We will define these derivatives according to @wooldridge2010econometric.</span>
<span id="cb53-1714"><a href="#cb53-1714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1715"><a href="#cb53-1715" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb53-1716"><a href="#cb53-1716" aria-hidden="true" tabindex="-1"></a>Suppose $Y$ and $\X = (X_1,\ldots, X_K)$ are a random variable and vector, respectively. The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_partial/marginal effect_**<span class="kw">&lt;/span&gt;</span> of $X_j$ on $\E{Y\mid\X}$ (sometimes called the partial effect of $X_j$ on $Y$), is $$ \frac{\partial \E{Y\mid\X}}{\partial X_j}.$$ In the event that $X_j$ is discrete, partial effects are given as the difference between $\E{Y\mid\X}$ evaluated at two discrete values of $X_j$.</span>
<span id="cb53-1717"><a href="#cb53-1717" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1718"><a href="#cb53-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1719"><a href="#cb53-1719" aria-hidden="true" tabindex="-1"></a>If we have a linear model $Y = \X\bet + \ep$, we're almost conditioned to conclude the marginal effect of $X_j$ on $\E{Y\mid\X}$ is $\beta_j$, but in general this is not true. </span>
<span id="cb53-1720"><a href="#cb53-1720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1721"><a href="#cb53-1721" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1722"><a href="#cb53-1722" aria-hidden="true" tabindex="-1"></a>This example is due to this <span class="co">[</span><span class="ot">post</span><span class="co">](https://stats.stackexchange.com/questions/190703/non-linear-endogeneity/190800#190800)</span>. Suppose $Y=X\beta + \varepsilon$ where $X\sim N(0,1)$ and $\varepsilon = X^2 - 1$. To insure that $\beta$ is identified, we need to verify that $\E{\varepsilon} = 0$ (we do not have an intercept) and $\E{X\varepsilon} =0$ (the multicollinearity assumption is trivially met). Note that $X^2\sim \chi_1^2$, so $\E{X^2} = 1$. We also have $\E{X^3} = 0$, as the normal distribution is not skewed (skewness being defined as the third moment centered about the mean).  </span>
<span id="cb53-1723"><a href="#cb53-1723" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1724"><a href="#cb53-1724" aria-hidden="true" tabindex="-1"></a>\E{\varepsilon}&amp;= \E{X^2 - 1}= \E{X^2} - 1 = 1 - 1 = 0<span class="sc">\\</span></span>
<span id="cb53-1725"><a href="#cb53-1725" aria-hidden="true" tabindex="-1"></a>\E{X\varepsilon}&amp;= \E{X^3 - X} = \E{X^3} - \E{X} = 0 - 0 = 0</span>
<span id="cb53-1726"><a href="#cb53-1726" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1727"><a href="#cb53-1727" aria-hidden="true" tabindex="-1"></a>Our model is identified. Furthermore $\OLS$ will present consistent estimates of $\beta$. For the sake of illustration, let $\bet = 2$.</span>
<span id="cb53-1728"><a href="#cb53-1728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1731"><a href="#cb53-1731" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1732"><a href="#cb53-1732" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span> </span>
<span id="cb53-1733"><a href="#cb53-1733" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb53-1734"><a href="#cb53-1734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1735"><a href="#cb53-1735" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim<span class="dv">-1</span>)</span>
<span id="cb53-1736"><a href="#cb53-1736" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>N_sim) {</span>
<span id="cb53-1737"><a href="#cb53-1737" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb53-1738"><a href="#cb53-1738" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb53-1739"><a href="#cb53-1739" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> beta<span class="sc">*</span>X <span class="sc">+</span> e</span>
<span id="cb53-1740"><a href="#cb53-1740" aria-hidden="true" tabindex="-1"></a>  estimates[n<span class="dv">-1</span>] <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y,X)[<span class="dv">1</span>]</span>
<span id="cb53-1741"><a href="#cb53-1741" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-1742"><a href="#cb53-1742" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1743"><a href="#cb53-1743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1746"><a href="#cb53-1746" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1747"><a href="#cb53-1747" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1748"><a href="#cb53-1748" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1749"><a href="#cb53-1749" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1750"><a href="#cb53-1750" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1751"><a href="#cb53-1751" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-1752"><a href="#cb53-1752" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 11</span></span>
<span id="cb53-1753"><a href="#cb53-1753" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: ""</span></span>
<span id="cb53-1754"><a href="#cb53-1754" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1755"><a href="#cb53-1755" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb53-1756"><a href="#cb53-1756" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="dv">2</span><span class="sc">:</span>N_sim, </span>
<span id="cb53-1757"><a href="#cb53-1757" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> estimates</span>
<span id="cb53-1758"><a href="#cb53-1758" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1759"><a href="#cb53-1759" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-1760"><a href="#cb53-1760" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb53-1761"><a href="#cb53-1761" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-1762"><a href="#cb53-1762" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"sample size"</span>,</span>
<span id="cb53-1763"><a href="#cb53-1763" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"OLS estimate"</span>)</span>
<span id="cb53-1764"><a href="#cb53-1764" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1765"><a href="#cb53-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1766"><a href="#cb53-1766" aria-hidden="true" tabindex="-1"></a>We have estimated $\beta = 2$ consistently, but this is not the partial effect! We have </span>
<span id="cb53-1767"><a href="#cb53-1767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1768"><a href="#cb53-1768" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \E{Y\mid X}}{\partial X}= \frac{\partial}{\partial X}\E{X\beta + \varepsilon \mid X} = \frac{\partial}{\partial X}\E{X\beta + X^2 -1\mid X} = \frac{\partial}{\partial X}<span class="co">[</span><span class="ot">X\beta + X^2 - 1</span><span class="co">]</span> = \beta + 2X \neq \beta.$$ This follows from the fact that $X$ is only weakly exogenous, so $\E{\varepsilon\mid X}\neq 0$. </span>
<span id="cb53-1769"><a href="#cb53-1769" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1770"><a href="#cb53-1770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1771"><a href="#cb53-1771" aria-hidden="true" tabindex="-1"></a> In general, we can still have $\E{X_i\varepsilon_i} = 0$ for all $i$ where $\varepsilon = g(\X)$ for some nonlinear function $g$, because  weak exogeneity only insures that our error and regressors are uncorrelated (i.e they have no linear relationship). What we need is exogeneity so we can conclude $$\E{Y \mid \X} = \E{\X\bet \mid \X} + \underbrace{\E{\varepsilon \mid \X}}_{\zer} = \X\bet,$$ so $$\frac{\partial \E{Y\mid \X}}{\partial X_j} = \beta_j.$$ **_At the heart of this issue is the relationship between the linear projection model and the (structural) linear model_**.  Early on we emphasized that there was a difference between what we called the linear projection model and the linear model. The prior is concerned with the statistical association of $Y$ and $\X$ and describes a feature of their joint density. Proposition \@ref(prp:ceferr) established that by definition the error in this model, $\varepsilon_c$, satisfied $\E{\varepsilon_c\mid\X} = \zer$. In the case of the linear model, $\varepsilon$ has a structural interpretation and may not satisfy this property, *but if it does* the linear projection model and the linear model will coincide in the sense that $\bet$ is interpreted as a marginal effect. Some treatments of OLS, such as @cameron2005microeconometrics, actually restrict their attention to the identification of the linear model such that $\bet$ is associated with a marginal effect, requiring  exogeneity instead of weak exogeneity for identification. </span>
<span id="cb53-1772"><a href="#cb53-1772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1773"><a href="#cb53-1773" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb53-1774"><a href="#cb53-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1775"><a href="#cb53-1775" aria-hidden="true" tabindex="-1"></a><span class="fu">## Identification of Marginal Effects</span></span>
<span id="cb53-1776"><a href="#cb53-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1777"><a href="#cb53-1777" aria-hidden="true" tabindex="-1"></a>Suppose $\Y = \Xm\bet + \ep$ is a linear model. If $\E{\ep \mid \X} = \zer$ and $\text{rank}\left(\E{\X'\X}\right) = K$, then </span>
<span id="cb53-1778"><a href="#cb53-1778" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial \E{Y\mid \X}}{\partial \X} = \bet,$$ where $\bet$ is identified.</span>
<span id="cb53-1779"><a href="#cb53-1779" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1780"><a href="#cb53-1780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1781"><a href="#cb53-1781" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb53-1782"><a href="#cb53-1782" aria-hidden="true" tabindex="-1"></a>In this case, $\bet$ is identified as $\E{\ep \mid \X} = \zer$ implies $\E{\X'\ep} = \zer$, and we are given $\text{rank}\left(\E{\X'\X}\right) = K$.</span>
<span id="cb53-1783"><a href="#cb53-1783" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1784"><a href="#cb53-1784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1785"><a href="#cb53-1785" aria-hidden="true" tabindex="-1"></a>More generally, if $\mathbf f(\X) = <span class="co">[</span><span class="ot">f_1(\X), \ldots, f_K(\X)</span><span class="co">]</span>$ are a series of continuous functions of regressors, and $Y = \mathbf f(\X)\beta + \varepsilon$, then</span>
<span id="cb53-1786"><a href="#cb53-1786" aria-hidden="true" tabindex="-1"></a>$$ \frac{\partial \E{Y\mid \X}}{\partial X_j} = \frac{\partial \mathbf f}{\partial X_j}\bet = \sum_{\ell =1}^K\frac{\partial f_\ell}{\partial X_j} \bet.$$ For example, if $Y = \beta_1 + \beta_2\log X_1 + \beta_3 \exp<span class="co">[</span><span class="ot"> X_1X_2</span><span class="co">]</span> + \varepsilon$ where $\E{\varepsilon \mid X_1}=0$, then </span>
<span id="cb53-1787"><a href="#cb53-1787" aria-hidden="true" tabindex="-1"></a>$$ \frac{\partial \E{Y\mid X_1}}{\partial X_1} = \frac{\beta_1}{X_1} + \beta_3X_2\exp<span class="co">[</span><span class="ot">X_1X_2</span><span class="co">]</span>.$$</span>
<span id="cb53-1788"><a href="#cb53-1788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1789"><a href="#cb53-1789" aria-hidden="true" tabindex="-1"></a>@reiss2007structural provide a more nuanced discussion of $\frac{\partial \E{Y\mid X}}{\partial X}$ in the context of structural models, and how it relates the the linear projection model we discussed at the opening. </span>
<span id="cb53-1790"><a href="#cb53-1790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1791"><a href="#cb53-1791" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frisch–Waugh–Lovell Theorem</span></span>
<span id="cb53-1792"><a href="#cb53-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1793"><a href="#cb53-1793" aria-hidden="true" tabindex="-1"></a>Even if our model contains multiple regressors concatenated in the vector $\X$, we may be especially interested in a subset of regressors. For instance, in Example @exm-car we may be especially interested in the price of cars if we are a manufacturer, as estimating consumers' sensitivity to price changes could give us valuable insights into maximizing our profit. In situations like this, is it possible to "ignore" the independent variables of secondary importance? The answer, as provided by @frisch1933partial @lovell1963seasonal, is "kind of", and deals with the algebra of OLS. </span>
<span id="cb53-1794"><a href="#cb53-1794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1795"><a href="#cb53-1795" aria-hidden="true" tabindex="-1"></a>Suppose $\Xm_1$ and $\Xm_2$ are two random matrices of observations where $\Xm= <span class="co">[</span><span class="ot">\Xm_1,\Xm_2</span><span class="co">]</span>$. If $\bet =<span class="co">[</span><span class="ot">\bet_1,\bet_2</span><span class="co">]</span>'$, then </span>
<span id="cb53-1796"><a href="#cb53-1796" aria-hidden="true" tabindex="-1"></a>$$ \Y = \Xm\bet + \ep = \Xm_1\bet_1 + \Xm_2\bet_2 + \ep. $$ The first order condition associated with the least squares problem is now </span>
<span id="cb53-1797"><a href="#cb53-1797" aria-hidden="true" tabindex="-1"></a>$$ \begin{bmatrix}\Xm_1'\Xm_1 &amp; \Xm_1'\Xm_2<span class="sc">\\</span>\Xm_2'\Xm_1 &amp; \Xm_2'\Xm_2\end{bmatrix} \begin{bmatrix} \hat{\bet}_{\text{OLS},1} \\ \hat{\bet}_{\text{OLS},2} \end{bmatrix} =  \begin{bmatrix} \Xm_1'\Y <span class="sc">\\</span> \Xm_2'\Y \end{bmatrix}$$ </span>
<span id="cb53-1798"><a href="#cb53-1798" aria-hidden="true" tabindex="-1"></a>If we expand this, we have </span>
<span id="cb53-1799"><a href="#cb53-1799" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1800"><a href="#cb53-1800" aria-hidden="true" tabindex="-1"></a>\Xm_1'\Xm_1\hat{\bet}_{\text{OLS},1} + \Xm_1'\Xm_2\hat{\bet}_{\text{OLS},2} = \Xm_1'\Y,<span class="sc">\\</span></span>
<span id="cb53-1801"><a href="#cb53-1801" aria-hidden="true" tabindex="-1"></a>\Xm_2'\Xm_1\hat{\bet}_{\text{OLS},1} + \Xm_2'\Xm_2\hat{\bet}_{\text{OLS},2} = \Xm_2'\Y.</span>
<span id="cb53-1802"><a href="#cb53-1802" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1803"><a href="#cb53-1803" aria-hidden="true" tabindex="-1"></a>If we solve the first equation for $\hat{\bet}_{\text{OLS},1}$, we have </span>
<span id="cb53-1804"><a href="#cb53-1804" aria-hidden="true" tabindex="-1"></a>$$\hat{\bet}_{\text{OLS},1} = (\Xm_1'\Xm_1)^{-1}\Xm_1'(\Y -\Xm_2\hat{\bet}_{\text{OLS},2}).$$ If we insert this into the second equation in our system, we have </span>
<span id="cb53-1805"><a href="#cb53-1805" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1806"><a href="#cb53-1806" aria-hidden="true" tabindex="-1"></a>&amp;\Xm_2'\Xm_1<span class="co">[</span><span class="ot">(\Xm_1'\Xm_1)^{-1}\Xm_1'(\Y -\Xm_2\hat{\bet}_{\text{OLS},2})</span><span class="co">]</span> + \Xm_2'\Xm_2\hat{\bet}_{\text{OLS},2} = \Xm_2'\Y<span class="sc">\\</span></span>
<span id="cb53-1807"><a href="#cb53-1807" aria-hidden="true" tabindex="-1"></a>\implies &amp;  \Xm_2'\Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1'\Y -\Xm_2'\Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1'\Xm_2\hat{\bet}_{\text{OLS},2} + \Xm_2'\Xm_2\hat{\bet}_{\text{OLS},2} = \Xm_2'\Y<span class="sc">\\</span></span>
<span id="cb53-1808"><a href="#cb53-1808" aria-hidden="true" tabindex="-1"></a>\implies &amp;  \hat{\bet}_{\text{OLS},2}<span class="co">[</span><span class="ot">\Xm_2'\Xm_2 - \Xm_2'\Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1'\Xm_2</span><span class="co">]</span>= \Xm_2'\Y - \Xm_2'\Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1'\Y<span class="sc">\\</span></span>
<span id="cb53-1809"><a href="#cb53-1809" aria-hidden="true" tabindex="-1"></a>\implies &amp; \hat{\bet}_{\text{OLS},2}<span class="co">[</span><span class="ot">\Xm_2'(\mathbf I - \Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1')\Xm_2</span><span class="co">]</span> = <span class="co">[</span><span class="ot">\Xm_2'(\mathbf I - \Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1')\Y</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1810"><a href="#cb53-1810" aria-hidden="true" tabindex="-1"></a>\implies &amp; \hat{\bet}_{\text{OLS},2} = <span class="co">[</span><span class="ot">\Xm_2'(\mathbf I - \Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1')\Xm_2</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\Xm_2'(\mathbf I - \Xm_1(\Xm_1'\Xm_1)^{-1}\Xm_1')\Y</span><span class="co">]</span></span>
<span id="cb53-1811"><a href="#cb53-1811" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1812"><a href="#cb53-1812" aria-hidden="true" tabindex="-1"></a>Recalling that we defined a matrix $\mathbb M =\mathbf I - \Xm(\Xm'\Xm)^{-1}\Xm'$ such that $\hat{\e} = \mathbb M\Y$ where $\mathbb M = \mathbb M'$ and $\mathbb M^2 = \mathbb M$, we have \begin{align*}</span>
<span id="cb53-1813"><a href="#cb53-1813" aria-hidden="true" tabindex="-1"></a>\hat{\bet}_{\text{OLS},2} &amp;= <span class="co">[</span><span class="ot">\Xm_2'\mathbb M_1\Xm_2</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\Xm_2'\mathbb M_1\Y</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1814"><a href="#cb53-1814" aria-hidden="true" tabindex="-1"></a>&amp; = <span class="co">[</span><span class="ot">\Xm_2'\mathbb M_1'\mathbb M_1\Xm_2</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\Xm_2'\mathbb M_1\Y</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb53-1815"><a href="#cb53-1815" aria-hidden="true" tabindex="-1"></a>&amp; = <span class="co">[</span><span class="ot">\mathbb X_2^{*\prime} \mathbb X_2^{*}</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\Xm_2'\Y^*</span><span class="co">]</span> &amp; (\mathbb X_2^{*} = \mathbb M_1\Xm_2,\ \Y^* =\mathbb M_1\Y)</span>
<span id="cb53-1816"><a href="#cb53-1816" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1817"><a href="#cb53-1817" aria-hidden="true" tabindex="-1"></a>The estimator $\hat{\bet}_{\text{OLS},2}$ follows from regressing $\mathbb M_1\Xm_2$  on $\mathbb M_1\Y$, which correspond to the residuals $\Xm_2 - \Xm_1\hat{\boldsymbol\gamma}_\text{OLS}$ and $\mathbf Y - \Xm\hat{\bet}_{\text{OLS},1}$.</span>
<span id="cb53-1818"><a href="#cb53-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1819"><a href="#cb53-1819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1820"><a href="#cb53-1820" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb53-1821"><a href="#cb53-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1822"><a href="#cb53-1822" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frisch–Waugh–Lovell Theorem</span></span>
<span id="cb53-1823"><a href="#cb53-1823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1824"><a href="#cb53-1824" aria-hidden="true" tabindex="-1"></a>For the linear model, $\Y = \Xm\bet + \ep = \Xm_1\bet_1 + \Xm_2\bet_2 + \ep$, </span>
<span id="cb53-1825"><a href="#cb53-1825" aria-hidden="true" tabindex="-1"></a>$$\hat{\bet}_{\text{OLS},2} = <span class="co">[</span><span class="ot">\mathbb X_2^{*\prime} \mathbb X_2^{*}</span><span class="co">]</span>^{-1}<span class="co">[</span><span class="ot">\Xm_2'\Y^*</span><span class="co">]</span>,$$ where $\mathbb X_2^{*\prime}$ and $\Y^*$ are the residual vectors from least squares regression of $\Xm_2$ and $\Y$ on $\Xm_1$, respectively. </span>
<span id="cb53-1826"><a href="#cb53-1826" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1827"><a href="#cb53-1827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1828"><a href="#cb53-1828" aria-hidden="true" tabindex="-1"></a>One of the useful applications of this theorem deals with visualization. </span>
<span id="cb53-1829"><a href="#cb53-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1830"><a href="#cb53-1830" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb53-1831"><a href="#cb53-1831" aria-hidden="true" tabindex="-1"></a>Suppose $Y = 1 + 4 X_1 + 2 X_2 + 8 X_3 + 3X_4 + \varepsilon$. Let's estimate this model for simulated data. </span>
<span id="cb53-1832"><a href="#cb53-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1835"><a href="#cb53-1835" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1836"><a href="#cb53-1836" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">8</span>,<span class="dv">3</span>)</span>
<span id="cb53-1837"><a href="#cb53-1837" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb53-1838"><a href="#cb53-1838" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">4</span>), <span class="fu">diag</span>(<span class="dv">1</span>,<span class="dv">4</span>))</span>
<span id="cb53-1839"><a href="#cb53-1839" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X)</span>
<span id="cb53-1840"><a href="#cb53-1840" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb53-1841"><a href="#cb53-1841" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> e</span>
<span id="cb53-1842"><a href="#cb53-1842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1843"><a href="#cb53-1843" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y,X)</span>
<span id="cb53-1844"><a href="#cb53-1844" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1845"><a href="#cb53-1845" aria-hidden="true" tabindex="-1"></a>If we're interested in the $\hat\beta_{\text{OLS},2}$, we may want to visualize it. Unfortunately, our parameter space is a subset of $\mathbb R^5$, so it isn't feasible to plot the hyperplane correspond to our estimated model over our sample. Fortunately, we can use the Frisch–Waugh–Lovell Theorem.</span>
<span id="cb53-1846"><a href="#cb53-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1849"><a href="#cb53-1849" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1850"><a href="#cb53-1850" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> X[,<span class="sc">-</span><span class="dv">2</span>]</span>
<span id="cb53-1851"><a href="#cb53-1851" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> X[,<span class="dv">2</span>]</span>
<span id="cb53-1852"><a href="#cb53-1852" aria-hidden="true" tabindex="-1"></a>y_res <span class="ot">&lt;-</span> y <span class="sc">-</span> X1 <span class="sc">%*%</span> <span class="fu">OLS</span>(y, X1)[,<span class="dv">1</span>]</span>
<span id="cb53-1853"><a href="#cb53-1853" aria-hidden="true" tabindex="-1"></a>X2_res <span class="ot">&lt;-</span> X2 <span class="sc">-</span> X1 <span class="sc">%*%</span> <span class="fu">OLS</span>(X2, X1)[,<span class="dv">1</span>]</span>
<span id="cb53-1854"><a href="#cb53-1854" aria-hidden="true" tabindex="-1"></a><span class="fu">OLS</span>(y_res, X2_res)</span>
<span id="cb53-1855"><a href="#cb53-1855" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1856"><a href="#cb53-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1857"><a href="#cb53-1857" aria-hidden="true" tabindex="-1"></a>We end up with the same estimate $\hat\beta_{\text{OLS},2}$, and can visualize it by plotting the dependent and independent variables in this alternate regression.</span>
<span id="cb53-1858"><a href="#cb53-1858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1861"><a href="#cb53-1861" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1862"><a href="#cb53-1862" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb53-1863"><a href="#cb53-1863" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb53-1864"><a href="#cb53-1864" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb53-1865"><a href="#cb53-1865" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1866"><a href="#cb53-1866" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 1</span></span>
<span id="cb53-1867"><a href="#cb53-1867" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 11</span></span>
<span id="cb53-1868"><a href="#cb53-1868" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: ""</span></span>
<span id="cb53-1869"><a href="#cb53-1869" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb53-1870"><a href="#cb53-1870" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb53-1871"><a href="#cb53-1871" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> X2_res, </span>
<span id="cb53-1872"><a href="#cb53-1872" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> y_res</span>
<span id="cb53-1873"><a href="#cb53-1873" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1874"><a href="#cb53-1874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb53-1875"><a href="#cb53-1875" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-1876"><a href="#cb53-1876" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb53-1877"><a href="#cb53-1877" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb53-1878"><a href="#cb53-1878" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"x2 Residuals"</span>, <span class="at">y =</span> <span class="st">"y Residuals"</span>) </span>
<span id="cb53-1879"><a href="#cb53-1879" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1880"><a href="#cb53-1880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1881"><a href="#cb53-1881" aria-hidden="true" tabindex="-1"></a>While our estimate is the same, the standard error associated with the estimate is not quite the same. For the first regression, we had $K = 5$ regressors, whereas the second had $K = 1$. This affects how $\widehat{\text{se}}(\OLS)$ is calculated, as the numerator of $S^2$ which ensures it is unbiased is $n - K$. We have $\widehat{\text{se}}(\OLS)\propto \sqrt{n-K}$, so if we scale the standard error in the second regression by $\sqrt{(n - 1)/(n - 5)}$.</span>
<span id="cb53-1882"><a href="#cb53-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1885"><a href="#cb53-1885" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1886"><a href="#cb53-1886" aria-hidden="true" tabindex="-1"></a>incorrect_se <span class="ot">&lt;-</span> <span class="fu">OLS</span>(y_res, X2_res)[<span class="dv">2</span>]</span>
<span id="cb53-1887"><a href="#cb53-1887" aria-hidden="true" tabindex="-1"></a>incorrect_se <span class="sc">*</span> <span class="fu">sqrt</span>((n<span class="dv">-1</span>)<span class="sc">/</span>(n<span class="dv">-5</span>))</span>
<span id="cb53-1888"><a href="#cb53-1888" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1889"><a href="#cb53-1889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1890"><a href="#cb53-1890" aria-hidden="true" tabindex="-1"></a>Despite being well known, the relationship between the standard errors of each regression seem to have gone unformalized until @ding2021frisch.</span>
<span id="cb53-1891"><a href="#cb53-1891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1892"><a href="#cb53-1892" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb53-1893"><a href="#cb53-1893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1894"><a href="#cb53-1894" aria-hidden="true" tabindex="-1"></a><span class="fu">## Recap </span></span>
<span id="cb53-1895"><a href="#cb53-1895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1896"><a href="#cb53-1896" aria-hidden="true" tabindex="-1"></a>The linear model, along with the array of assumptions and what they yield, can be quite a bit to take in. The following table shows the cumulative properties given by the addition of each assumption.</span>
<span id="cb53-1897"><a href="#cb53-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1898"><a href="#cb53-1898" aria-hidden="true" tabindex="-1"></a>|  $\text{rank}\left(\E{\X'\X}\right) = K$, $\E{\X'\ep} = \zer$|$\E{\ep\mid \Xm}= \zer$|$\E{\ep\ep'\mid \Xm}= \sigma^2\mathbf I$|$\ep\sim N(\zer,\sigma^2\mathbf I)$|</span>
<span id="cb53-1899"><a href="#cb53-1899" aria-hidden="true" tabindex="-1"></a>|-----------------------|-----------------------|-----------------------|------------|</span>
<span id="cb53-1900"><a href="#cb53-1900" aria-hidden="true" tabindex="-1"></a>| $(\bet,\Sig)$ identified |$(\bet,\Sig)$ identified  |$(\bet,\sigma^2)$ identified | $(\bet,\sigma^2)$ identified |</span>
<span id="cb53-1901"><a href="#cb53-1901" aria-hidden="true" tabindex="-1"></a>| $\OLS\pto\bet$            | $\OLS\pto\bet$              | $\OLS\pto\bet$              | $\OLS\pto\bet$   |</span>
<span id="cb53-1902"><a href="#cb53-1902" aria-hidden="true" tabindex="-1"></a>| $\OLS$ Asymptotically Normal | $\OLS$ Asymptotically Normal | $\OLS$ Asymptotically Normal | $\OLS$ Normal     |</span>
<span id="cb53-1903"><a href="#cb53-1903" aria-hidden="true" tabindex="-1"></a>|                       |  $\OLS$ Unbiased              |  $\OLS$ Unbiased              |  $\OLS$ Unbiased   |</span>
<span id="cb53-1904"><a href="#cb53-1904" aria-hidden="true" tabindex="-1"></a>| | $\frac{\partial \E{Y\mid \X}}{\partial \X} = \bet$ |  $\frac{\partial \E{Y\mid \X}}{\partial \X} = \bet$ |  $\frac{\partial \E{Y\mid \X}}{\partial \X} = \bet$      |</span>
<span id="cb53-1905"><a href="#cb53-1905" aria-hidden="true" tabindex="-1"></a>|                       |                       |  $\OLS$ BLUE                  |  $\OLS$ BLUE       |</span>
<span id="cb53-1906"><a href="#cb53-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1907"><a href="#cb53-1907" aria-hidden="true" tabindex="-1"></a>We also have maintained two implicit assumptions throughout: IID regressors, and the model is correctly specified.</span>
<span id="cb53-1908"><a href="#cb53-1908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1909"><a href="#cb53-1909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1910"><a href="#cb53-1910" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example/Replication {#rep1}</span></span>
<span id="cb53-1911"><a href="#cb53-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1912"><a href="#cb53-1912" aria-hidden="true" tabindex="-1"></a>Chapter 4 of @greene2003econometric provides a useful exercise in the form of replication @christensen1976economies who estimate the economies of scale in US electrical power generation.</span>
<span id="cb53-1913"><a href="#cb53-1913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1914"><a href="#cb53-1914" aria-hidden="true" tabindex="-1"></a>In response to rising electrical rates in the United States during the mid-20th century, @christensen1976economies consider whether vertically integrated electrical providers (who generate, transmit, and distribute electricity) should be disintegrated. If these firms could only generate electricity as a result of regulation, then these firms would have to compete to sell generated electricity to separate intermediary firms which would transmit and distribute it to consumers. This competition has the potential to lower prices. At the same time, perhaps the vertical integration of firms cuts costs, which may actually lead to better prices for consumers. To determine if this is the case, we can consider the economies of scale of firms. </span>
<span id="cb53-1915"><a href="#cb53-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1916"><a href="#cb53-1916" aria-hidden="true" tabindex="-1"></a>Economies of scale refers to the phenomenon where the more quantity of a good a firm produces, the lower the costs associated with that production. A firms cost, $C$, will be a function of output $Q$, and prices of inputs $P_1,\ldots,P_k$ used in production. We will specify our cost function as:</span>
<span id="cb53-1917"><a href="#cb53-1917" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1918"><a href="#cb53-1918" aria-hidden="true" tabindex="-1"></a>\log C &amp;= \alpha + \beta\log Q + \gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \sum_{i=1}^k\delta_i\log P_i &amp; (\textstyle\sum_{i=1}^k\delta_i = 1)</span>
<span id="cb53-1919"><a href="#cb53-1919" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1920"><a href="#cb53-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1921"><a href="#cb53-1921" aria-hidden="true" tabindex="-1"></a>The functional form may look a bit arbitrary, but it's consistent with a handful of appealing properties we expect a cost function to exhibit, the most important of which being a U-shape in quantity corresponding to decreasing costs as quantity increases up to a point (economies of scale), followed by increasing costs. @christensen1973transcendental provide all the details about this functional form. This point is given as the minimum of $C$, which will coincide with the minimum of $\log C$ because $\log$ is a monotonic function. The first order condition for this minimization is </span>
<span id="cb53-1922"><a href="#cb53-1922" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1923"><a href="#cb53-1923" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\partial \log C}{\partial \log Q}  = 1, <span class="sc">\\</span></span>
<span id="cb53-1924"><a href="#cb53-1924" aria-hidden="true" tabindex="-1"></a> \implies &amp; \beta + \gamma \log Q^* = 1,<span class="sc">\\</span></span>
<span id="cb53-1925"><a href="#cb53-1925" aria-hidden="true" tabindex="-1"></a> \implies &amp; Q^* = \exp\left(\frac{1-\beta}{\gamma}\right),</span>
<span id="cb53-1926"><a href="#cb53-1926" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1927"><a href="#cb53-1927" aria-hidden="true" tabindex="-1"></a>where the derivative is set to $1$ instead of $0$ because $\log(0) = 1$. If we are able to estimate $(\beta,\gamma)$ we can calculate $Q^*$. If we observe $Q_i &gt; Q^*$, for many firms $i,$ then the same output $Q$ could be produced at a lower cost if the market was comprised of a larger number of firms producing a smaller output. </span>
<span id="cb53-1928"><a href="#cb53-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1929"><a href="#cb53-1929" aria-hidden="true" tabindex="-1"></a>The data from @christensen1976economies is available <span class="co">[</span><span class="ot">here</span><span class="co">](https://pages.stern.nyu.edu/~wgreene/Text/Edition7/TableF4-4.txt)</span>.^<span class="co">[</span><span class="ot">I did do some cleaning of the raw data beforehand.</span><span class="co">]</span></span>
<span id="cb53-1930"><a href="#cb53-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1933"><a href="#cb53-1933" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1934"><a href="#cb53-1934" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb53-1935"><a href="#cb53-1935" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/christensen_greene_1976.csv"</span>)</span>
<span id="cb53-1936"><a href="#cb53-1936" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1937"><a href="#cb53-1937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1938"><a href="#cb53-1938" aria-hidden="true" tabindex="-1"></a>The first few rows of the data give us an idea of what type of information we have.</span>
<span id="cb53-1939"><a href="#cb53-1939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1942"><a href="#cb53-1942" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1943"><a href="#cb53-1943" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb53-1944"><a href="#cb53-1944" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(CG_1976))</span>
<span id="cb53-1945"><a href="#cb53-1945" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1946"><a href="#cb53-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1947"><a href="#cb53-1947" aria-hidden="true" tabindex="-1"></a>We observe the unit prices of three inputs: capital, labor, and fuel. Taking these into account, the cost function becomes </span>
<span id="cb53-1948"><a href="#cb53-1948" aria-hidden="true" tabindex="-1"></a>$$\log C = \alpha + \beta\log Q + \frac{1}{2}\gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \delta_k \log P_k + \delta_l \log P_l + \delta_f \log P_f,$$ where $\delta_k + \delta_l + \delta_f = 1$. We can rewrite out model to implicitly satisfy the constraint if we write $\delta_f = 1 -\delta_l - \delta_f$: </span>
<span id="cb53-1949"><a href="#cb53-1949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1950"><a href="#cb53-1950" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-1951"><a href="#cb53-1951" aria-hidden="true" tabindex="-1"></a>&amp;\log C = \alpha + \beta\log Q + \gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \delta_k \log P_k + \delta_l \log P_l + (1 -\delta_l - \delta_f) \log P_f<span class="sc">\\</span></span>
<span id="cb53-1952"><a href="#cb53-1952" aria-hidden="true" tabindex="-1"></a>\implies &amp; \log C - \log P_f= \alpha + \beta\log Q + \gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \delta_k( \log P_k - \log P_f) + \delta_l (\log P_l-\log P_f) <span class="sc">\\</span></span>
<span id="cb53-1953"><a href="#cb53-1953" aria-hidden="true" tabindex="-1"></a>\implies &amp; \log(C/P_f) = \alpha + \beta\log Q +\gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \delta_k \log (P_k/P_f) + \delta_l \log (P_l/P_f)</span>
<span id="cb53-1954"><a href="#cb53-1954" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-1955"><a href="#cb53-1955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1956"><a href="#cb53-1956" aria-hidden="true" tabindex="-1"></a>This model cannot account for all the possible factors which influence a firm's costs, so we will introduce the element $\varepsilon$ to account for unobserved factors which influence cost, and assume that all our regressors are exogenous.^<span class="co">[</span><span class="ot">Is this reasonable? Could it be the case that $P_k$, $P_l$, and/or $P_f$ are correlated with other input prices that we don't observe? How does the context of electrical power production impact this assumption?</span><span class="co">]</span> This gives us our estimable model.</span>
<span id="cb53-1957"><a href="#cb53-1957" aria-hidden="true" tabindex="-1"></a>$$ \log(C/P_f) = \alpha + \beta\log Q +\gamma<span class="co">[</span><span class="ot">(\log Q)^2/2</span><span class="co">]</span> + \delta_k \log (P_k/P_f) + \delta_l \log (P_l/P_f) + \varepsilon$$</span>
<span id="cb53-1960"><a href="#cb53-1960" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1961"><a href="#cb53-1961" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="ot">&lt;-</span> CG_1976 <span class="sc">%&gt;%</span> </span>
<span id="cb53-1962"><a href="#cb53-1962" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb53-1963"><a href="#cb53-1963" aria-hidden="true" tabindex="-1"></a>    <span class="at">C =</span> <span class="fu">log</span>(cost<span class="sc">/</span>fuel),</span>
<span id="cb53-1964"><a href="#cb53-1964" aria-hidden="true" tabindex="-1"></a>    <span class="at">Q =</span> <span class="fu">log</span>(output),</span>
<span id="cb53-1965"><a href="#cb53-1965" aria-hidden="true" tabindex="-1"></a>    <span class="at">Q2 =</span> Q<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>,</span>
<span id="cb53-1966"><a href="#cb53-1966" aria-hidden="true" tabindex="-1"></a>    <span class="at">P_kf =</span> <span class="fu">log</span>(capital<span class="sc">/</span>fuel),</span>
<span id="cb53-1967"><a href="#cb53-1967" aria-hidden="true" tabindex="-1"></a>    <span class="at">P_lf =</span> <span class="fu">log</span>(labor<span class="sc">/</span>fuel)</span>
<span id="cb53-1968"><a href="#cb53-1968" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb53-1969"><a href="#cb53-1969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1970"><a href="#cb53-1970" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(C <span class="sc">~</span> Q <span class="sc">+</span> Q2 <span class="sc">+</span> P_kf <span class="sc">+</span> P_lf, <span class="at">data =</span> CG_1976)</span>
<span id="cb53-1971"><a href="#cb53-1971" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span>
<span id="cb53-1972"><a href="#cb53-1972" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1973"><a href="#cb53-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1974"><a href="#cb53-1974" aria-hidden="true" tabindex="-1"></a>We can use our estimates $\hat \beta$ and $\hat \gamma$ to calculate $\hat Q^*$.</span>
<span id="cb53-1975"><a href="#cb53-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1978"><a href="#cb53-1978" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1979"><a href="#cb53-1979" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> model<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb53-1980"><a href="#cb53-1980" aria-hidden="true" tabindex="-1"></a>gamma_hat <span class="ot">&lt;-</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb53-1981"><a href="#cb53-1981" aria-hidden="true" tabindex="-1"></a>Q_hat <span class="ot">&lt;-</span> <span class="fu">exp</span>((<span class="dv">1</span><span class="sc">-</span>beta_hat)<span class="sc">/</span>gamma_hat)</span>
<span id="cb53-1982"><a href="#cb53-1982" aria-hidden="true" tabindex="-1"></a>Q_hat</span>
<span id="cb53-1983"><a href="#cb53-1983" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-1984"><a href="#cb53-1984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1985"><a href="#cb53-1985" aria-hidden="true" tabindex="-1"></a>The straightforward way of determining if costs are higher than they need to be as a result of vertical integration is by counting the number of firms for which $Q &gt; \hat Q^*$. This doesn't account for the degree to which firms are exceeding the efficient scale. To do this, let's consider the total output produced by firms which are exceeding the efficient scale, opposed to those which are not. </span>
<span id="cb53-1986"><a href="#cb53-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-1989"><a href="#cb53-1989" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb53-1990"><a href="#cb53-1990" aria-hidden="true" tabindex="-1"></a>CG_1976 <span class="sc">%&gt;%</span> </span>
<span id="cb53-1991"><a href="#cb53-1991" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">inefficient =</span> (output <span class="sc">&gt;</span> Q_hat)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1992"><a href="#cb53-1992" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(inefficient) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1993"><a href="#cb53-1993" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb53-1994"><a href="#cb53-1994" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_firms =</span> <span class="fu">n</span>(),</span>
<span id="cb53-1995"><a href="#cb53-1995" aria-hidden="true" tabindex="-1"></a>    <span class="at">total_output =</span> <span class="fu">sum</span>(output)</span>
<span id="cb53-1996"><a href="#cb53-1996" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1997"><a href="#cb53-1997" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prop_output =</span> total_output<span class="sc">/</span><span class="fu">sum</span>(total_output)) <span class="sc">%&gt;%</span> </span>
<span id="cb53-1998"><a href="#cb53-1998" aria-hidden="true" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>()</span>
<span id="cb53-1999"><a href="#cb53-1999" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb53-2000"><a href="#cb53-2000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2001"><a href="#cb53-2001" aria-hidden="true" tabindex="-1"></a>57% of output is attributed to the 25 firms producing over the efficient scale. This seems to indicate that the market is too vertically integrated.</span>
<span id="cb53-2002"><a href="#cb53-2002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2003"><a href="#cb53-2003" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb53-2004"><a href="#cb53-2004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2005"><a href="#cb53-2005" aria-hidden="true" tabindex="-1"></a>**Conditional Expectation and Linear Projection**: Chapter 2 of @wooldridge2010econometric, Chapter 2 of @hansen2022econometrics, Chapter 3 of @angrist2008mostly</span>
<span id="cb53-2006"><a href="#cb53-2006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2007"><a href="#cb53-2007" aria-hidden="true" tabindex="-1"></a>**Structural Modeling**: Chapter 1 of @greene2003econometric , @reiss2007structural, @goldberger1972structural, portions of Chapter 2 of @cameron2005microeconometrics </span>
<span id="cb53-2008"><a href="#cb53-2008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2009"><a href="#cb53-2009" aria-hidden="true" tabindex="-1"></a>**OLS**: Chapter 4 of @wooldridge2010econometric, Chapters 3-5 of @greene2003econometric,  portions of Chapter 1-2 of @hayashi2011econometrics</span>
<span id="cb53-2010"><a href="#cb53-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2011"><a href="#cb53-2011" aria-hidden="true" tabindex="-1"></a>**Model selection**: @phillips1996econometric @hansen2005challenges @leamer1978specification @hendry2000econometrics @davidson1981several @hendry1995dynamic</span>
<span id="cb53-2012"><a href="#cb53-2012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2013"><a href="#cb53-2013" aria-hidden="true" tabindex="-1"></a><span class="fu">## Math Appendix: Projection {#sec-proj}</span></span>
<span id="cb53-2014"><a href="#cb53-2014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2015"><a href="#cb53-2015" aria-hidden="true" tabindex="-1"></a>Linear projection can be generalized far beyond the setting of a random vector $(Y,\X)$. I'll *quickly* define projection in the general setting of Hilbert spaces. For details, see @rudin, @royden1988real, or @folland1999real.</span>
<span id="cb53-2016"><a href="#cb53-2016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2017"><a href="#cb53-2017" aria-hidden="true" tabindex="-1"></a>A normed vector space $V$ defined over a field $F$ is, as the name implies, a vector space equipped with a norm $\norm{\cdot}:V\mapsto [0,\infty)$ satisfying:</span>
<span id="cb53-2018"><a href="#cb53-2018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2019"><a href="#cb53-2019" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\norm{v} = 0 \iff v = 0$;</span>
<span id="cb53-2020"><a href="#cb53-2020" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\norm{av} = \abs{a}\norm{v}$;</span>
<span id="cb53-2021"><a href="#cb53-2021" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\norm{w + v} \le \norm{w} + \norm{v}$;</span>
<span id="cb53-2022"><a href="#cb53-2022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2023"><a href="#cb53-2023" aria-hidden="true" tabindex="-1"></a>for all vectors $v,w\in V$ and scalars $a\in F$. The norm tells us how "far" a vector $v\in V$ is from the zero element ("origin") $0\in V$. Any normed vector space is also a metric space if we define a metric as $d(v,w)=\norm{v-w}$. With this metric comes the familiar definitions of convergence. If $V$ is a complete metric space (all Cauchy sequences converge in $V$), then we say $V$ is a Banach space. </span>
<span id="cb53-2024"><a href="#cb53-2024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2025"><a href="#cb53-2025" aria-hidden="true" tabindex="-1"></a>A Hilbert space is a complete vector space $H$ (defined over a field of scalars $F$) equipped with an inner product $\langle\cdot,\cdot\rangle: H\times H\to F$ which satisfies:</span>
<span id="cb53-2026"><a href="#cb53-2026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2027"><a href="#cb53-2027" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\langle x,y \rangle = \langle y,x \rangle$;</span>
<span id="cb53-2028"><a href="#cb53-2028" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\langle cx,y \rangle = c\langle y,x \rangle$</span>
<span id="cb53-2029"><a href="#cb53-2029" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\langle x+z,y \rangle = \langle x,z \rangle + \langle y,z \rangle$;</span>
<span id="cb53-2030"><a href="#cb53-2030" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$\langle x,x \rangle &gt; 0 \iff x\neq 0$;</span>
<span id="cb53-2031"><a href="#cb53-2031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2032"><a href="#cb53-2032" aria-hidden="true" tabindex="-1"></a>for all $x,y\in H$ and $c\in F$. All Hilbert spaces are normed vector spaces, as $\norm{x} = \sqrt{\langle x,x \rangle}$ satisfies all the properties of a norm. This makes a Hilbert space a Banach space, as we've assumed $H$ is complete. The development of Hilbert spaces was motivated by Euclidean space, as the vector space $\mathbb R^k$ (over the field of scalars $\mathbb R$) is a Hilbert space:</span>
<span id="cb53-2033"><a href="#cb53-2033" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-2034"><a href="#cb53-2034" aria-hidden="true" tabindex="-1"></a>\langle \x,\y \rangle &amp;= \x\cdot\y = \sum_{i=1}^k x_iy_i,<span class="sc">\\</span></span>
<span id="cb53-2035"><a href="#cb53-2035" aria-hidden="true" tabindex="-1"></a>\norm{\x} &amp;= \left(\sum_{i=1}^k x_i^2\right)^{1/2},<span class="sc">\\</span></span>
<span id="cb53-2036"><a href="#cb53-2036" aria-hidden="true" tabindex="-1"></a>d(\x,\y) &amp;= \left(\sum_{i=1}^k(y_i - x_i)^2\right)^{1/2}.</span>
<span id="cb53-2037"><a href="#cb53-2037" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-2038"><a href="#cb53-2038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2039"><a href="#cb53-2039" aria-hidden="true" tabindex="-1"></a>Another useful Hilbert space is a special case of a normed vector space known as an $L^p$ space. For a measure space $(X,\mathcal N, \mu)$, define $L^p$ to be the set of all measurable (real) functions $f:X\to\mathbb R$. The norm for this space is </span>
<span id="cb53-2040"><a href="#cb53-2040" aria-hidden="true" tabindex="-1"></a>$$ \norm{f}_p = \left(\int\abs{f}^p\ d\mu\right)^{1/p}.$$ In general $L^p$ spaces are not Hilbert spaces, but if $p=2$, then they are. In this case the inner product is </span>
<span id="cb53-2041"><a href="#cb53-2041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2042"><a href="#cb53-2042" aria-hidden="true" tabindex="-1"></a>$$\langle f,g \rangle = \int\abs{f}\abs{g}\ d\mu.$$</span>
<span id="cb53-2043"><a href="#cb53-2043" aria-hidden="true" tabindex="-1"></a>In the event that the measure space is $(\mathbb Z^+, \mathbb Z^+, \mu)$ where $\mu$ is the counting measure, then a measurable function $f:\mathbb Z^+\to\mathbb R$ is a sequence of numbers real $<span class="sc">\{</span>x_1,x_2,\ldots<span class="sc">\}</span>$. In this case we denote the $L^p$ space as $\ell^p$ and have </span>
<span id="cb53-2044"><a href="#cb53-2044" aria-hidden="true" tabindex="-1"></a>$$ \norm{f}_p = \left(\int\abs{f}^p\ d\mu\right)^{1/p} = \left(\sum_{i=1}^\infty |x_i|^p\right)^{1/2}.$$ If we $<span class="sc">\{</span>x_1,x_2,\ldots,x_k, 0,0,0,\ldots <span class="sc">\}</span>\in\ell ^k$, we could also consider this an element of $\mathbb R^k$ because the trailing zeros. In this sense, Euclidean space is an $L^p$ space:</span>
<span id="cb53-2045"><a href="#cb53-2045" aria-hidden="true" tabindex="-1"></a>$$\norm{\x}_2 = \left(\sum_{i=1}^k x_i^2\right)^{1/2}.$$</span>
<span id="cb53-2046"><a href="#cb53-2046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2047"><a href="#cb53-2047" aria-hidden="true" tabindex="-1"></a>Two elements of a Hilbert space are orthogonal if $\langle x,y \rangle = 0$. If $S\subset H$ is a subspace of a Hilbert space $H$, there exists a unique element $\hat y\in S$ such that </span>
<span id="cb53-2048"><a href="#cb53-2048" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb53-2049"><a href="#cb53-2049" aria-hidden="true" tabindex="-1"></a>\norm{x - \hat y} &amp;= \inf_{y\in S}\norm{x - y},<span class="sc">\\</span></span>
<span id="cb53-2050"><a href="#cb53-2050" aria-hidden="true" tabindex="-1"></a>\langle x-\hat y,z \rangle &amp;= 0 &amp; (\forall z\in S).</span>
<span id="cb53-2051"><a href="#cb53-2051" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb53-2052"><a href="#cb53-2052" aria-hidden="true" tabindex="-1"></a> We refer to $\hat y$ as the projection of $x$ onto $S$.</span>
<span id="cb53-2053"><a href="#cb53-2053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-2054"><a href="#cb53-2054" aria-hidden="true" tabindex="-1"></a>For a probability space $(\Omega, \mathcal F, P)$. If we define a $L^2$ space as all the random variables (measurable functions) with squared values that are integrable, then </span>
<span id="cb53-2055"><a href="#cb53-2055" aria-hidden="true" tabindex="-1"></a>$$ \langle X,Y \rangle = \int\ xy\ dP = \E{XY}.$$</span>
<span id="cb53-2056"><a href="#cb53-2056" aria-hidden="true" tabindex="-1"></a>This is why we refer to random variables as orthogonal when $\E{XY} = 0$! In the event we have a random vector $\Z = (Y,\X)$, the projection of $Y$ onto $\X$ happens to be $\E{Y\mid \X}$.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>