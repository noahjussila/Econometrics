<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanved Econometrics with Examples - 1&nbsp; Finite Sample Properties of Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./asymptotics.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#models-and-parameterizations" id="toc-models-and-parameterizations" class="nav-link active" data-scroll-target="#models-and-parameterizations"><span class="toc-section-number">1.1</span>  Models and Parameterizations</a></li>
  <li><a href="#statistics-and-estimators" id="toc-statistics-and-estimators" class="nav-link" data-scroll-target="#statistics-and-estimators"><span class="toc-section-number">1.2</span>  Statistics and Estimators</a></li>
  <li><a href="#unbiasedness" id="toc-unbiasedness" class="nav-link" data-scroll-target="#unbiasedness"><span class="toc-section-number">1.3</span>  Unbiasedness</a></li>
  <li><a href="#relative-efficiency" id="toc-relative-efficiency" class="nav-link" data-scroll-target="#relative-efficiency"><span class="toc-section-number">1.4</span>  Relative Efficiency</a></li>
  <li><a href="#efficient-estimators-fisher-information" id="toc-efficient-estimators-fisher-information" class="nav-link" data-scroll-target="#efficient-estimators-fisher-information"><span class="toc-section-number">1.5</span>  Efficient Estimators, Fisher Information</a></li>
  <li><a href="#the-distribution-of-an-estimator" id="toc-the-distribution-of-an-estimator" class="nav-link" data-scroll-target="#the-distribution-of-an-estimator"><span class="toc-section-number">1.6</span>  The Distribution of an Estimator</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="toc-section-number">1.7</span>  Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-est" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-1_b8eb78e88ceb84ca99cf5a47395aa211">

</div>
<section id="models-and-parameterizations" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="models-and-parameterizations"><span class="header-section-number">1.1</span> Models and Parameterizations</h2>
<p>The term “estimation” is used so often, that it’s often easy to forget precisely what it means. It’s worth briefly presenting the formal definition of (point) estimation in accordance with statistical theory and formulate the performance of an estimator using tools from decision theory. A comprehensive treatment of the theory of estimation, see <span class="citation" data-cites="bickel2015mathematical">Bickel and Doksum (<a href="references.html#ref-bickel2015mathematical" role="doc-biblioref">2015</a>)</span> or <span class="citation" data-cites="lehmann2006theory">Lehmann and Casella (<a href="references.html#ref-lehmann2006theory" role="doc-biblioref">1998</a>)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 </strong></span>Given a random experiment with sample space <span class="math inline">\(\mathcal X\)</span> over which a random vector <span class="math inline">\({\mathbf{X}}=(X_1,\ldots. X_n)\)</span> is defined, a <span style="color:red"><strong><em>model</em></strong></span> <span class="math inline">\(\mathcal P\)</span> is the collection of probability distributions (or densities), or sets of distributions (or densities), from which <span class="math inline">\(\mathbf{X}\)</span> may be distributed. We call an element of a model <span class="math inline">\(P\in\mathcal P\)</span>, <span style="color:red"><strong><em>model value</em></strong></span>.</p>
</div>
<p>In order to keep track of model values, we need to introduce labels for model values <span class="math inline">\(P\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 </strong></span>For every element of <span class="math inline">\(\mathcal P\)</span>, we assign a label called a <span style="color:red"><strong><em>parameter</em></strong></span> <span class="math inline">\(\boldsymbol{\theta}\)</span> from a <span style="color:red"><strong><em>parameter space</em></strong></span> <span class="math inline">\(\boldsymbol{\Theta}= \Theta_1 \times\cdots\times\Theta_k\)</span>, denoting the labeled element as <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span>. The function <span class="math inline">\(\theta \mapsto P\)</span> which assigns these labels is a <span style="color:red"><strong><em>parameter space</em></strong></span>.</p>
</div>
<p>A parameterization enables to write the model as <span class="math inline">\(\mathcal P =\{P_{\boldsymbol{\theta}} \mid \boldsymbol{\theta}\in \boldsymbol{\Theta}\}\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Normally Distributed Random Variable) </strong></span>Assume that we know <em>a priori</em> that <span class="math inline">\(X\)</span> is a normally distributed random variable (<span class="math inline">\(n=1\)</span> here). The model <span class="math inline">\(\mathcal P\)</span> is the collection of all normal distributions. How do we label this entirely family of models? The most common way is via the mean and variance of the normal distribution. In this case, <span class="math inline">\(\boldsymbol{\Theta}= \mathbb R \times \mathbb R^+\)</span>, <span class="math inline">\(\boldsymbol{\theta}= (\mu, \sigma^2)\)</span>, and <span class="math display">\[f_X(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right].\]</span> By parameterizing <span class="math inline">\(\mathcal P\)</span>, we can now easily refer to each element of <span class="math inline">\(\mathcal P\)</span> using <span class="math inline">\(\boldsymbol{\theta}=(\mu,\sigma^2)\)</span>.</p>
<p><span class="math display">\[\mathcal P = \{f_X(x \mid \mu, \sigma^2) \mid  (\mu, \sigma^2) \in \mathbb R \times \mathbb R^+\}.\]</span></p>
<p>Alternatively, we could <strong><em>reparameterize</em></strong> the model using the standard deviation instead of the variance, <span class="math inline">\(\boldsymbol{\theta}= (\mu, \sigma)\)</span>. This gives <span class="math display">\[f_X(x \mid \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right].\]</span> If we wanted to really be crazy, we could parameterize the model using <span class="math inline">\(\boldsymbol{\theta}= (\mu/\sigma^2, -1/2\sigma)\)</span>, where <span class="math inline">\(\Theta = \mathbb R\times \mathbb R^-\)</span>.</p>
<p><span class="math display">\[\begin{align*}
f_X(x \mid \boldsymbol{\theta}) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ &amp; = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[\dfrac{x^2}{2\sigma^2} -\dfrac{2x\mu}{2\sigma^2} + \dfrac{\mu^2}{2\sigma^2}\right] \\
         &amp; =\exp\left[\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right]\\
         &amp; =\exp\left[\log\left( (2\pi\sigma^2)^{-1/2}\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right] \\
         &amp; = \exp\left[-\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right] \\
         &amp; = \exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2} -\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right]\\
         &amp; = \exp\left[\dfrac{\mu}{\sigma^2}x-\dfrac{1}{2\sigma^2}x^2 -\frac{1}{2}\log\left(\frac{\mu^2}{\sigma^2}+\log\left( 2\pi\sigma^2\right)\right)\right]\\
         &amp; = \exp\left[\theta_1x+\theta_2x^2 -\frac{1}{2}\log\left(-\frac{\theta_1}{2\theta_2}+\log\left( \pi\theta_2^{-1}\right)\right)\right]
    \end{align*}\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Exponentially Distributed Random Variable) </strong></span>If <span class="math inline">\(X\)</span> is distributed according to an exponential distribution, the most common parameterization has <span class="math inline">\(\Theta = \mathbb R^+\)</span>, <span class="math inline">\(\theta = \lambda\)</span>, and each element of <span class="math inline">\(\mathcal P\)</span> has a corresponding density of <span class="math display">\[ f_X(x\mid\lambda) = \begin{cases}\lambda e^{-\lambda x}&amp; x\ge 0 \\ 0 &amp; x&lt;0\end{cases}.\]</span> A common alternate parameterization is in terms of <span class="math inline">\(\beta \in \mathbb R^+\)</span> labels elements in <span class="math inline">\(\mathcal P\)</span> such that <span class="math display">\[ f_X(x\mid\beta) = \begin{cases}\frac{1}{\beta} e^{-x/\beta}&amp; x\ge 0 \\ 0 &amp; x&lt;0\end{cases}. \]</span></p>
</div>
<p>These last two examples were simple because each element <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> is a single probability density/distribution. In this case we write <span class="math inline">\({\mathbf{X}} \sim P_{\boldsymbol{\theta}}\)</span>, and the difference between <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> and <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\boldsymbol{\theta})\)</span> is purely a matter of notation. It is the same difference between writing a uniform distribution as <span class="math inline">\(\text{Uni}(a,b)\)</span> instead of <span class="math inline">\(f_X(x\mid a,b) = 1/(b-a)\)</span> or <span class="math inline">\(F_X(x\mid a,b)=x/(b-a)\)</span>. We may also write the probability measure associated with the sample space <span class="math inline">\({\mathcal X}\)</span> and density as <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span>, so the expectation of <span class="math inline">\({\mathbf{X}}\)</span> can be written as <span class="math display">\[ \text{E}\left[\mathbf{X}\right] = \int_{\mathcal{X}} {\mathbf{X}} f_{\mathbf{X}}({\mathbf{X}}\mid\boldsymbol{\theta})\ d{\mathbf{X}} = \int_{\mathcal{X}} {\mathbf{X}} \ dF_{\mathbf{X}}({\mathbf{X}}\mid\boldsymbol{\theta}) = \int_{\mathcal{X}} {\mathbf{X}} \ dP_{\boldsymbol{\theta}}.\]</span></p>
<p>It will often be the case, especially so in econometrics, that an element of <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> is itself a set of distributions defined by some common properties.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (IID Sample Parameterized by Mean) </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1,\ldots, X_n)\)</span> is a sample such that <span class="math inline">\(X_i \overset{iid}{\sim}F_X(x)\)</span> and <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span> for all <span class="math inline">\(i\)</span>. This data generating process constitutes a model parameterized by <span class="math inline">\(\mu\)</span>. An element of <span class="math inline">\(\mathcal P\)</span> takes the form <span class="math display">\[P_\mu = \left\{f_{\mathbf{X}}({\mathbf{X}})\ \Bigg|\ f_{\mathbf{X}}({\mathbf{X}}) = \prod_{i=1}^n f_{X_i}(x) \text{ and } f_{X_i} = f_{X_j}\ \forall i,j \text{ and }\text{E}\left[X_i\right]=\mu\ \forall i\right\}.\]</span> For example, if we let <span class="math inline">\(\mu = 5\)</span>, then <span class="math inline">\(P_5\in\mathcal P\)</span> is the set of all joint distributions such that the elements of <span class="math inline">\(\mathbf{X}\)</span> are independent and have an expected value of <span class="math inline">\(5\)</span>. This is <em>a lot</em> of possible distributions. One possible distribution in this set is <span class="math display">\[f_{\mathbf{X}}({\mathbf{X}}\mid \mu) = \prod_{i=1}^n\frac{1}{2\mu} = \frac{1}{(2\mu)^n},\]</span> which is the joint density of <span class="math inline">\({\mathbf{X}}\)</span> when <span class="math inline">\(X_i \overset{iid}{\sim}\text{Uni}(0,2\mu)\)</span>. Because <span class="math inline">\({\mathbf{X}}\)</span> is comprised of an iid sample, we could also define the elements of <span class="math inline">\(\mathcal P\)</span> using the marginal density <span class="math inline">\(f_{X_i}(x\mid\theta)\)</span>, as this density is identical for all <span class="math inline">\(X_i\)</span>. <span class="math display">\[ P_\mu = \{f_{X_i}(x) \mid \text{E}\left[X_i\right] = \mu\} .\]</span> What is important here is that each <span class="math inline">\(P_\mu\)</span> is an infinite collection of densities all with a common properties related to the parameter <span class="math inline">\(\mu\)</span>.</p>
</div>
<p>To differentiate models where <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> is a single distribution opposed to a collection of distributions, we will introduce a new term (that is not standard across any sources).</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 </strong></span>A model <span class="math inline">\(\mathcal P = \{P_{\boldsymbol{\theta}} \mid \boldsymbol{\theta}\in \boldsymbol{\Theta}\}\)</span> is a <span style="color:red"><strong><em>regular model</em></strong></span> if each <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> is a probability distribution with density <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\boldsymbol{\theta})\)</span>.</p>
</div>
<p>Not only can we consider just how many distributions make up one model value <span class="math inline">\(P\in\mathcal P\)</span>, but we can also consider the dimension of the parameter space <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4 </strong></span>Suppose a model <span class="math inline">\(\mathcal P\)</span> is parameterized by a mapping <span class="math inline">\(\boldsymbol{\theta}\mapsto P_\boldsymbol{\theta}\)</span> where <span class="math inline">\(\dim(\boldsymbol{\Theta}) = k\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(\dim(\Theta_j)\)</span> is finite for all <span class="math inline">\(j=1,\ldots, k\)</span>, then <span class="math inline">\(\mathcal P = \{P_{\boldsymbol{\theta}} \mid \boldsymbol{\theta}\in \boldsymbol{\Theta}\}\)</span> is <span style="color:red"><strong><em>parametric</em></strong></span>.</li>
<li>If <span class="math inline">\(\dim(\Theta_j)\)</span> is infinite for all <span class="math inline">\(j=1,\ldots, k\)</span>, then <span class="math inline">\(\mathcal P = \{P_{\boldsymbol{\theta}} \mid \boldsymbol{\theta}\in \boldsymbol{\Theta}\}\)</span> is <span style="color:red"><strong><em>nonparametric</em></strong></span>.</li>
<li>If <span class="math inline">\(\mathcal P\)</span> is neither parametric, nor non-parametric, it is <span style="color:red"><strong><em>semiparametric</em></strong></span>. This means that some components of <span class="math inline">\(\boldsymbol{\Theta}\)</span> have finite dimension, while others have infinite dimension.</li>
</ol>
</div>
<p>For the classic parameterization of the normal distribution, <span class="math inline">\(\mu\in\mathbb{R}\)</span> and <span class="math inline">\(\sigma^2 \in \mathbb{R}^+\)</span>, where <span class="math inline">\(\dim(\mathbb{R}) = \dim(\mathbb{R}^+) = 1\)</span>. For the exponential distribution, <span class="math inline">\(\lambda \in \mathbb{R}^+\)</span>, where <span class="math inline">\(\dim(\mathbb{R}) \in \mathbb{R}\)</span>. Where things get subjective is when we deal with models that are not regular (model values <span class="math inline">\(P\in\mathcal P\)</span> consist of entire sets of distributions), as we could take one of our parameters to be an infinite space of distributions.</p>
<div id="exm-iidmean" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (IID Sample Parameterized by Mean) </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1,\ldots, X_n)\)</span> is a sample such that <span class="math inline">\(X_i \overset{iid}{\sim}F_X(x)\)</span> and <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span> for all <span class="math inline">\(i\)</span>. Originally, we defined an element <span class="math inline">\(P_\mu \in \mathcal P\)</span> to be a collection of distributions. We could instead reparameterize this model such that it is a regular model. Parameterize <span class="math inline">\(\mathcal P\)</span> with the mapping <span class="math inline">\((\mu, f_X)\to P_{(\mu, f_X)}\)</span> where <span class="math inline">\(\text{E}\left[X_i\right]=\mu\)</span> and <span class="math inline">\(f_X\)</span> is the common density function of all <span class="math inline">\(X_i\)</span>. Now <span class="math inline">\(P_{\mu, f_X}\in\mathcal P\)</span> corresponds to the density that gives rise to the iid random sample <span class="math inline">\({\mathbf{X}}\)</span>, along with the mean <span class="math inline">\(\mu\)</span> of that density. The model is now semiparametric, as <span class="math inline">\(\mu\in\mathbb{R}\)</span> where <span class="math inline">\(\dim(\mathbb{R}) = 1\)</span>, and <span class="math inline">\(f_X\)</span> is an element of an infinite dimensional space of functions. We could also parameterize this model only simply with the mapping <span class="math inline">\(f_X \mapsto P_{f_X}\)</span>, as the parameter <span class="math inline">\(\mu\)</span> is implicitly given by <span class="math inline">\(f_X\)</span>: <span class="math display">\[ \mu = \int_{\mathcal X} xf_X(x)\ dx.\]</span> In this case, the model is nonparametric.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>In the last example, we reparameterized <a href="#exm-iidmean">Example&nbsp;<span>1.4</span></a> by introducing the common density of the <span class="math inline">\(X_i\)</span>, <span class="math inline">\(f_X(x)\)</span>, as its own parameter. This made a model that was not regular (each element <span class="math inline">\(P_\mu\in\mathcal P\)</span> was an infinite collection of densities), regular and semiparametric (the space of all densities <span class="math inline">\(f_X(x)\)</span> with mean <span class="math inline">\(\mu\)</span> has infinite dimension). In general if we have a model <span class="math inline">\(\mathcal P\)</span> of the form <span class="math display">\[\begin{align*}
\mathcal P &amp;= \{P_{\boldsymbol{\theta}} \mid \boldsymbol{\theta}\in \boldsymbol{\Theta}\}, \\
P_{\boldsymbol{\theta}} &amp; = \{f_{\mathbf{X}}({\mathbf{X}}) \mid f_{\mathbf{X}}({\mathbf{X}})\text{ satisfies some condition which may involve }\boldsymbol{\theta}\},
\end{align*}\]</span> we can write reparameterize it as a regular semiparametric model where <span class="math inline">\((\boldsymbol{\theta}, f_{\mathbf{X}}) \mapsto P_{(\boldsymbol{\theta}, f_{\mathbf{X}})}\)</span>, taking the density <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}})\)</span> to be its own parameter. The difference between these two does not amount to much practice, but it can be a tad confusing when one author considers a model parametric while another considers it semiparametric. This is particularly relevant in econometrics, because many models do not specify the entire distribution (ex: the data is normally distributed), instead opting for weaker assumptions related to the distribution (ex: the distribution of the data is symmetric and centered at zero).</p>
</div>
</section>
<section id="statistics-and-estimators" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="statistics-and-estimators"><span class="header-section-number">1.2</span> Statistics and Estimators</h2>
<p>Given a realizations of a random vector <span class="math inline">\({\mathbf{X}}\)</span>, we can calculate statistics</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 </strong></span>A <span style="color:red"><strong><em>statistic</em></strong></span> <span class="math inline">\(T\)</span> is a function <span class="math inline">\(T:{\mathcal X} \to \mathcal T\)</span>, where <span class="math inline">\(\mathcal T\)</span> is some space of values.</p>
</div>
<p>In almost every case we are interested in <span class="math inline">\(\mathcal T = \mathbb R^k\)</span> for some <span class="math inline">\(k\)</span>. Most of the descriptive statistics we think of (sample mean, sample variance, median, mode, etc.) are all statistics which take on values in <span class="math inline">\(\mathbb R\)</span>. Because statistics are nothing more than functions of random variables, they themselves are random variables. This fact is paramount for a specific type of statistic.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.6 </strong></span>Suppose <span class="math inline">\({\mathbf{X}} \sim P_{\theta_0}\)</span>, where <span class="math inline">\(P_{\theta_0}\in \mathcal P\)</span> for a known <span class="math inline">\(\mathcal P\)</span>. An <span style="color:red"><strong><em>estimator</em></strong></span> <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is a statistic which maps the sample space <span class="math inline">\({\mathcal X}\)</span> to the parameter space <span class="math inline">\(\Theta\)</span>, whose goal is to estimate the <span style="color:red"><strong><em>estimand</em></strong></span> <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> A realization of an estimator, <span class="math inline">\(\hat{\theta}({\mathbf{X}})\)</span> is an <span style="color:red"><strong><em>estimate</em></strong></span>.</p>
</div>
<p>The goal of an estimator is to “guess” the true value <span class="math inline">\(P_{\boldsymbol{\theta}_0}\in\mathcal P\)</span> which generated the data, and do so by “guessing” the corresponding parameter <span class="math inline">\(\boldsymbol{\theta}_0\)</span>. To emphasize the fact that <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is the true value we have added a subscript 0, a practice that will be employed when. All this notation can be a bit confusing at first, so it helps to work through it with a very familiar setting.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 </strong></span>Suppose for a random vector <span class="math inline">\({\mathbf{X}} = (X_1,\ldots, X_n)\)</span> that <span class="math inline">\(X_i\sim N(\mu_0,\sigma_0^2)\)</span>. This is equivalent to <span class="math inline">\({\mathbf{X}}\)</span> being distributed according to a multivariate normal distribution <span class="math inline">\(N(\boldsymbol\mu,\boldsymbol{\Sigma})\)</span>, where <span class="math inline">\(\boldsymbol\mu_i = \mu\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>, and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix comprised entirely of <span class="math inline">\(\sigma^2\)</span>. The sample space of our random vector is <span class="math inline">\(\mathbb R^n\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> If <span class="math inline">\(\varphi(x)\)</span> denotes the standard normal distribution, then <span class="math display">\[\mathcal P = \left\{ \prod_{i=1}^n \frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right)\  \biggr\vert\  (\mu,\sigma)\in \mathbb R\times\mathbb R^+\right\}.\]</span> In order to estimate <span class="math inline">\(\boldsymbol{\theta}_0 = (\mu_0,\sigma_0)\)</span>, we define the following estimator: <span class="math display">\[\begin{align*}
\hat{\boldsymbol{\theta}}({\mathbf{X}}) &amp; = (\hat\theta_1({\mathbf{X}}), \hat\theta_2({\mathbf{X}}))\\
\hat\theta_1({\mathbf{X}}) &amp; = \frac{1}{n}\sum_{i=1}^n X_i\\
\hat\theta_2({\mathbf{X}}) &amp; = \left[\frac{1}{n-1}\sum_{i=1}^n (X_i - \hat\theta_1({\mathbf{X}}))^2\right]^{1/2}
\end{align*}\]</span> This is of course how we’ve been estimating <span class="math inline">\((\mu_0,\sigma_0)\)</span> since we were in high school – using the sample mean and sample standard deviation! These estimators are so important that they have their own special notation, <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(S({\mathbf{X}})\)</span>.</p>
</div>
<div id="exm-unident" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 </strong></span>Let’s modify the previous problem by changing our parameterization of <span class="math inline">\(\mathcal P\)</span>. Instead of labeling each normal distribution with <span class="math inline">\((\mu,\sigma)\)</span>, let’s instead use <span class="math inline">\((\alpha, \beta, \sigma)\)</span> where <span class="math inline">\(\alpha + \beta\)</span> replaces <span class="math inline">\(\mu\)</span>. That is, each <span class="math inline">\(X_i\)</span> is distributed according to <span class="math inline">\(N(\alpha_0 + \beta_0, \sigma_0^2)\)</span>. Is it possible to estimate all three parameters? It seems that we can still estimate <span class="math inline">\(\sigma_0\)</span> with no issues, but the same cannot be said for <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span>. What happens if we attempt to use the sample mean <span class="math inline">\(\bar X\)</span> and find that <span class="math inline">\(\bar x = 3\)</span>? The best we can do is use this information to estimate <span class="math inline">\(\alpha_0 + \beta_0\)</span>, but it’s impossible to distinguish the individual values of each parameter, as there are infinite pairs of <span class="math inline">\((\alpha, \beta)\)</span> which sum to 3. We could have <span class="math inline">\(\alpha_0 = 0\)</span> and <span class="math inline">\(\beta_0 = 3\)</span>, or <span class="math inline">\(\alpha_0 = 1\)</span> and <span class="math inline">\(\beta_0 = 2\)</span>, or <span class="math inline">\(\alpha_0 = -412\)</span> and <span class="math inline">\(\beta_0 = 415\)</span>, etc. Even if we had an infinite amount of data, this problem would persist!</p>
</div>
<p>This last example highlights what will be one of the central topics in econometric theory, and it arose in how our model was parameterized. The catalyst of the problem we faced in estimating <span class="math inline">\(\alpha_0\)</span> and <span class="math inline">\(\beta_0\)</span>, is that we did a poor job at “labeling” the model <span class="math inline">\(\mathcal P\)</span> with parameters. To be precise, the “labels” given to elements of <span class="math inline">\(\mathcal P\)</span> by the parameterization were not unique. If our goal is uncovering the true <span class="math inline">\(P_{\boldsymbol{\theta}_0}\in \mathcal P\)</span> via estimating <span class="math inline">\(\boldsymbol{\theta}_0\)</span>, we need to make sure that there is a one-to-one relationship between <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span>.</p>
<div id="def-ident" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.7 </strong></span>A parameterization <span class="math inline">\(\boldsymbol{\theta}\mapsto P_{\boldsymbol{\theta}}\)</span> is <span style="color:red"><strong><em>identifiable</em></strong></span> if it is an injective mapping. That is <span class="math display">\[P_{\boldsymbol{\theta}} \neq P_{\boldsymbol{\theta}'} \implies \boldsymbol{\theta}\neq \boldsymbol{\theta}',\]</span> which is equivalent to <span class="math display">\[ \boldsymbol{\theta}= \boldsymbol{\theta}'\implies P_{\boldsymbol{\theta}} = P_{\boldsymbol{\theta}'}.\]</span> We say a parameter <span class="math inline">\(\theta_j\)</span> (a component of the vector <span class="math inline">\(\boldsymbol{\theta}\)</span>) is <span style="color:red"><strong><em>identified</em></strong></span> if the <span class="math inline">\(j\)</span>-th component of the parameterization <span class="math inline">\(\boldsymbol{\theta}\mapsto P_{\boldsymbol{\theta}}\)</span> is injective.</p>
</div>
<p>The parameterization in the previous problem is not identifiable, as multiple vectors of parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> map to a single element of <span class="math inline">\(\mathcal P\)</span>. In situations like this, estimation is a nonstarter! We always need to make sure our model (and it’s accompanying parameterization) is identified before we can even attempt to estimate any parameters.</p>
<p>It’s often necessary to adopt assumptions in order to add additional structure to a model, facilitating identification. For example, if we revisit @exm-unident, and assumed that <span class="math inline">\(\alpha = 2\beta\)</span>, then we can identify <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. We were able to estimate <span class="math inline">\(\alpha + \beta\)</span> using <span class="math inline">\(\bar X\)</span>, and combining this with the additional assumption of <span class="math inline">\(\alpha = 2\beta\)</span> gives <span class="math inline">\(\hat\beta = \bar X/3\)</span> and <span class="math inline">\(\hat\alpha = \bar X/3\)</span>. A hallmark of econometrics is determining which assumptions are required to identify parameters of interest, and if those assumptions are reasonable and consistent with economic theory and the observed behavior of economic agents.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (IID Sample, Identifying the Mean) </strong></span>What parameters are and are not identified when our model generated data according to <span class="math inline">\(X_i\overset{iid}{\sim}F_X(x)\)</span> where <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>? This depends on how we parameterize <span class="math inline">\(\mathcal P\)</span>.</p>
<ol type="1">
<li>Define a regular model <span class="math inline">\(\mathcal P\)</span> such that each element <span class="math inline">\(P_\mu\)</span> is a distribution with expectation <span class="math inline">\(\mu\)</span>. <span class="math display">\[ \mathcal{P} = \left\{f_{X}(x) \mid \text{E}\left[X\right] = \mu \right\}\]</span> With the parameterization <span class="math inline">\(\mu\mapsto P_\mu\)</span>, the model is not identified, as any <span class="math inline">\(\mu\)</span> maps to an infinite number of distributions with an expectation of <span class="math inline">\(\mu\)</span>. For example, <span class="math inline">\(\mu = 5\)</span> maps to <span class="math inline">\(\text{Uni}(0,10)\)</span>, <span class="math inline">\(\text{Uni}(4,6)\)</span>, <span class="math inline">\(N(5,1)\)</span>, <span class="math inline">\(\text{Exp}(1/5)\)</span>, and every other density such that <span class="math inline">\(\int xf_X(x)\ dx = 5\)</span>.<br>
</li>
<li>But do we really care about the precise distribution of <span class="math inline">\(X_i\)</span>, or are we just interested in estimating the mean <span class="math inline">\(\mu\)</span>? If we only want to estimate the mean <span class="math inline">\(\mu\)</span>, then we can define our model such that <span class="math inline">\(P_\mu \in \mathcal P\)</span> is itself the entire collection of density with mean <span class="math inline">\(\mu\)</span>. <span class="math display">\[\begin{align*}
\mathcal P &amp;= \{P_\mu \mid \mu \in \mathbb{R}\} \\
P_\mu &amp; = \left\{f_{\mathbf{X}}({\mathbf{X}})\ \bigg| \ \int xf_{\mathbf{X}}({\mathbf{X}})\ dx = \mu \right\}
\end{align*}\]</span> This model is not regular, but it is identified.</li>
<li>We could define redefine our identified model as a a semiparametric model by taking <span class="math inline">\(f_X\)</span> to be its own parameter, and using the parameterization <span class="math inline">\((\mu, f_X)\mapsto P_{(\mu, f_X)}\)</span>. In this case the model is still identified, as <span class="math inline">\((\mu, f_X)\)</span> uniquely determine an element of <span class="math inline">\(\mathcal P\)</span>.</li>
</ol>
<p>So if the models from 2 and 3 are both identified, which one do we pick? As discussed earlier, it doesn’t really matter in practice, <em>but</em> strictly speaking model 2 may be the better choice. The goal of estimating some parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> is to “guess” <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span>. If <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> does not specify a distribution which generates the data, then <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> should include all the possible distributions that could have generated the data.</p>
</div>
</section>
<section id="unbiasedness" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="unbiasedness"><span class="header-section-number">1.3</span> Unbiasedness</h2>
<p>Before estimating <span class="math inline">\(\boldsymbol{\theta}\)</span>, we need to determine which estimator to use. How do we assess the quality of an estimator <em>ex-ante</em> (prior to calculating an estimate for observed data <span class="math inline">\({\mathbf{X}}\)</span>)? For the sake of simplicity, assume that <span class="math inline">\(\Theta = \mathbb R\)</span>. Ideally, an estimator <span class="math inline">\(\hat \theta\)</span> will be “close” to the estimand <span class="math inline">\(\theta\)</span>, which may look like <span class="math inline">\(\hat\theta({\mathbf{X}}) -\theta\approx 0\)</span> for a realization of a random vector <span class="math inline">\({\mathbf{X}}\)</span>,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> but there is no way to know <span class="math inline">\({\mathbf{X}}\)</span>. We instead will calculate this discrepancy <em>in expectation</em>.</p>
<div id="def-bias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.8 </strong></span>The <span style="color:red"><strong><em>bias</em></strong></span> of an estimator <span class="math inline">\(\hat\theta : {\mathcal X} \to \mathbb R\)</span> is defined as <span class="math display">\[\text{Bias}(\hat\theta, \theta) = \text{Bias}(\hat\theta) = \text{E}\left[\hat\theta - \theta\right] = \text{E}\left[\hat\theta\right] - \theta .\]</span> An estimator is <span style="color:red"><strong><em>unbiased</em></strong></span> if <span class="math inline">\(\text{Bias}(\hat\theta) = 0\)</span>, which is equivalent to <span class="math inline">\(\text{E}\left[\hat\theta\right] = \theta\)</span>. This is readily extended to an estimator of a vector of estimands, <span class="math inline">\(\hat{\boldsymbol{\theta}({\mathbf{X}})}\)</span>.</p>
</div>
<p>An unbiased estimator is, <em>on average</em>, correct.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1, \ldots, X_n)\)</span> is a random sample from a distribution with mean <span class="math inline">\(\mu\)</span> (<span class="math inline">\(\text{E}\left[X_i\right]=\mu\)</span> for all <span class="math inline">\(i\)</span>). The estimator <span class="math display">\[\hat\theta(X) = \frac{1}{n}\sum_{i=1}^n X_i\]</span> is an unbiased estimator for <span class="math inline">\(\mu\)</span>. <span class="math display">\[\begin{align*}
\text{E}\left[\hat\theta\right] &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n X_i\right] \\
              &amp; = \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i\right]  &amp; (\text{Expectation is linear}) \\
              &amp; = \frac{1}{n}\sum_{i=1}^n\mu &amp; (\text{E}\left[X_i\right]=\mu) \\
              &amp; = \frac{1}{n}(n\mu)\\
              &amp; = \mu
\end{align*}\]</span></p>
</div>
<div id="exm-var" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1, \ldots, X_n)\)</span> is a random sample from a distribution with variance <span class="math inline">\(\sigma^2\)</span> and mean <span class="math inline">\(\mu\)</span>. If the sample mean is an unbiased estimator of the mean, then perhaps the sample analogue of the variance, <span class="math display">\[ \hat\theta(X) = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 ,\]</span> is unbiased as well? Recalling that <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/m\)</span> <span class="math display">\[\begin{align*}
\text{E}\left[\hat\theta\right] &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2\right] \\
              &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \bar X + (\mu - \mu))^2\right]\\
              &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n ((X_i - \mu) - (\bar X - \mu))^2\right] \\
              &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n ((X_i - \mu)^2 - 2(X_i - \mu)(\bar X - \mu) + (\bar X - \mu)^2)\right] \\
              &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\sum_{i=1}^n (X_i - \mu) + (\bar X - \mu)^2\frac{1}{n}\sum_{i=1}^n 1\right] \\
               &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\left[\sum_{i=1}^n X_i - \sum_{i=1}^n \mu\right] + (\bar X - \mu)^2(1/n)n\right]\\
               &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)(n\bar X - n\mu) + (\bar X - \mu)^2\right] \\
                &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)n(\bar X - \mu) + (\bar X - \mu)^2\right] \\
                &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - 2(\bar X - \mu)^2 + (\bar X - \mu)^2)\right]\\
                &amp; = \text{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar X - \mu)^2 \right]\\
                &amp; = \frac{1}{n}\text{E}\left[\sum_{i=1}^n (X_i - \mu)^2\right] -  \text{E}\left[(\bar X - \mu)^2 \right] \\
                &amp; = \frac{1}{n}\sum_{i=1}^n\text{Var}\left(X_i\right) - \frac{\sigma^2}{n}\\
                &amp; =  \frac{1}{n}(n\sigma^2) - \frac{\sigma^2}{n}\\
                &amp; = \sigma^2 - \frac{\sigma^2}{n}\\
                &amp; = \frac{n-1}{n}\sigma^2\\
                &amp; \neq \sigma^2
\end{align*}\]</span> Our estimator is biased! We can “modify” <span class="math inline">\(\hat\theta(X)\)</span> in order to correct it’s bias. Define a second estimator <span class="math inline">\(\hat\theta^*\)</span> as <span class="math display">\[ \hat\theta^*({\mathbf{X}}) = \frac{n}{n-1}\hat\theta(X) = \frac{n}{n-1}\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.\]</span> This correction gives an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\text{E}\left[\hat\theta^*\right] = \text{E}\left[\frac{n}{n-1}\hat\theta\right] =  \frac{n}{n-1}\text{E}\left[\hat\theta\right]= \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2. \]</span> This correction is known as <strong><em>Bessel’s correction</em></strong>, and its effect is easily demonstrated with a Monte Carlo simulation. Suppose <span class="math inline">\(X_i \sim N(0,1)\)</span> for <span class="math inline">\(i=1,\ldots, 20\)</span>. Let’s calculate <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\hat\theta^*\)</span> for our sample, repeat this for one million simulations, and look the sample mean of each estimator over the one million simulations.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> For <span class="math inline">\(n = 20\)</span>, we have <span class="math display">\[\begin{align*}
\text{E}\left[\hat\theta\right] &amp; = \frac{n-1}{n}\sigma^2 = \frac{20-1}{20}= 0.95, \\
\text{E}\left[\hat\theta^*\right] &amp; = 1,
\end{align*}\]</span> so we should see that <span class="math display">\[\begin{align*}
\frac{1}{1000000}\sum_{k}\hat\theta &amp; \approx 0.95, \\
\frac{1}{1000000}\sum_{k}\hat\theta^* &amp; \approx  1,
\end{align*}\]</span> where simulations are indexed by <span class="math inline">\(k\)</span>.</p>
<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-2_3842d34948428749b25922ed23f453dd">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Define estimators (including sample mean)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x_bar <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(x)<span class="sc">/</span><span class="fu">length</span>(x)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>theta_biased <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">x_bar</span>(x))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(x)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>theta_unbiased <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">x_bar</span>(x))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(x) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Create vectors to store estimates from each simulation</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>unbiased_estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>biased_estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Perform 100,000 simulations (R sticklers will be angry I didn't use apply())</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="co">#draw 20 observations from N(0,1)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  biased_estimates[k] <span class="ot">&lt;-</span> <span class="fu">theta_biased</span>(X)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  unbiased_estimates[k] <span class="ot">&lt;-</span> <span class="fu">theta_unbiased</span>(X)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">x_bar</span>(biased_estimates), <span class="fu">x_bar</span>(unbiased_estimates))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9498406 0.9998323</code></pre>
</div>
</div>
<p>Alternatively, we can calculate the bias of each estimator:</p>
<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-3_967de3869449d7e629c90d10a3e235c7">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">c</span>(<span class="fu">x_bar</span>(biased_estimates), <span class="fu">x_bar</span>(unbiased_estimates))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.050159362 0.000167749</code></pre>
</div>
</div>
<p>It is also is informative to plot the density of our estimates, in effect illustrating the probability distributions of the estimators <span class="math inline">\(\hat\theta(X)\)</span> and <span class="math inline">\(\hat\theta^*(X)\)</span></p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="estimators_cache/html/fig-plot11_0c2ea07ad8eaec0e9afe519b2489589e">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimate =</span> <span class="fu">c</span>(biased_estimates, unbiased_estimates),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Estimator =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Biased"</span>, N_sim), <span class="fu">rep</span>(<span class="st">"Unbiased"</span>, N_sim))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) <span class="sc">+</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(biased_estimates), <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span> </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(unbiased_estimates), <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot11" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="estimators_files/figure-html/fig-plot11-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.1: Distribution of estimators for the population variance</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Because <span class="math inline">\(\hat\theta^*\)</span> is unbiased, it is the most frequent way of calculating the variance of a random variable using a sample, and is often notated by <span class="math inline">\(S^2\)</span>. While informative, this doesn’t really pin down <em>why</em> the uncorrected estimator is biased. The bias arises from the fact that <span class="math inline">\(\hat\theta({\mathbf{X}})\)</span> is a function of another estimator – <span class="math inline">\(\bar X\)</span>. In the event we knew <span class="math inline">\(\mu\)</span> and didn’t need to estimate it with <span class="math inline">\(\bar X\)</span>, then <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n (X_i -\mu)^2\)</span> is actually unbiased. It’s the “intermediate” estimation of <span class="math inline">\(\mu\)</span> via <span class="math inline">\(\bar X\)</span>, as the discrepancy between <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\mu\)</span> factors in when taking the sum of squared deviations. Assuming we know <span class="math inline">\(\mu\)</span>, we can estimate <span class="math inline">\(\sigma\)</span> as <span class="math display">\[\hat\sigma({\mathbf{X}}) = \frac{1}{n}\sum_{i=1}^n [(X_i - \mu)]^2 = \frac{1}{n}\sum_{i=1}^n [(X_i - \bar X) + (\bar X - \mu)]^2,\]</span> which highlights that <span class="math inline">\(X_i\)</span>’s deviation from the sample mean can be written as the sum of the discrepancy between <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\bar X\)</span>’s discrepancy from <span class="math inline">\(\mu\)</span>. If we expand it we have <span class="math display">\[\begin{align*}
\hat\sigma^2({\mathbf{X}}) &amp;= \frac{1}{n}\sum_{i=1}^n [(X_i - \bar X)^2 + 2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2] \\
              &amp; = \frac{1}{n}\sum_{i=1}^n (X_i - \bar X)^2 + \frac{1}{n}\sum_{i=1}^n [2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2]\\
              &amp; = \hat\theta({\mathbf{X}}) + \frac{1}{n}\sum_{i=1}^n [2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2]
\end{align*}\]</span></p>
</div>
<p>If we try to use <span class="math inline">\(\hat\theta({\mathbf{X}})\)</span>, we don’t capture the second term, and are underestimating the variance.</p>
</section>
<section id="relative-efficiency" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="relative-efficiency"><span class="header-section-number">1.4</span> Relative Efficiency</h2>
<p>While unbiasedness is important, it isn’t the end all be all when selecting an estimator. The following pathological example will highlight another concern.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.10 </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1, \ldots, X_n)\)</span> is a random sample from a distribution with mean <span class="math inline">\(\mu\)</span>. Let’s define two estimators for <span class="math inline">\(\mu\)</span>: <span class="math display">\[\begin{align*}
\hat\theta_1(X) &amp; = \bar X = \frac{1}{n}\sum_{i=1}^nX_i,\\
\hat\theta_2(X) &amp; = \sum_{i=1}^n\frac{1}{2^i}X_i.
\end{align*}\]</span></p>
<p>We’re familiar with the unbiased estimator <span class="math inline">\(\hat\theta_2(X)\)</span>. The estimator <span class="math inline">\(\hat\theta_2(X)\)</span> is a weighted average, where weights are determined by the geometric series <span class="math inline">\(1/2^i\)</span>. This estimator is also unbiased, <span class="math display">\[ \text{E}\left[\hat\theta_2\right] = \text{E}\left[\sum_{i=1}^n\frac{1}{2^i}X_i\right] = \sum_{i=1}^n\frac{1}{2^i}\text{E}\left[X_i\right]  = \sum_{i=1}^n\frac{1}{2^i}\mu = \mu \sum_{i=1}^n\frac{1}{2^i} = \mu(1)=\mu.\]</span> In fact, <em>any</em> weighted average where the (normalized) weights sum to one is unbiased. If both estimators are unbiased, how do we determine which is superior? To gain some insight, let’s simulate some estimates, assuming <span class="math inline">\(X_i \sim N(0,1)\)</span> for <span class="math inline">\(i = 1,\ldots, 20\)</span>.</p>
<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-5_fa4b42e2de2d8a72310f64ec8db16b65">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#define estimator</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>weighted_xbar <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate weights</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x)))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(w<span class="sc">*</span>x)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Create vectors to store estimates from each simulation</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>xbar_est <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>weighted_xbar_est <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="co">#draw 20 observations from N(0,1)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  xbar_est[k] <span class="ot">&lt;-</span> <span class="fu">x_bar</span>(X)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  weighted_xbar_est[k] <span class="ot">&lt;-</span> <span class="fu">weighted_xbar</span>(X)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we use the simulated estimates to plot the (approximate) densities of the estimators, one fact sticks out – <span class="math inline">\(\text{Var}\left(\hat\theta_1\right) &lt; \text{Var}\left(\hat\theta_2\right)\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="estimators_cache/html/fig-plot12_792725423765f29bd9b8ef67f4192a30">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimate =</span> <span class="fu">c</span>(xbar_est, weighted_xbar_est), </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Estimator =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Sample Mean"</span>, N_sim), <span class="fu">rep</span>(<span class="st">"Weighted Sample Mean"</span>, N_sim))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Mean, True Value = 0"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot12" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="estimators_files/figure-html/fig-plot12-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.2: Distribution of estimators for the population mean</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p>The variance of an estimator gives us a sense for the dispersion of our estimates, but isn’t desirable in and of itself. We can <em>always</em> find an estimator with a small variance, and by small variance I mean no variance! Suppose we set our estimator <span class="math inline">\(\hat\theta(X) = 1\)</span>. Regardless of the realized sample <span class="math inline">\({\mathbf{X}}\)</span>, we have an estimate of <span class="math inline">\(\hat\theta({\mathbf{X}}) = 1\)</span>, and <span class="math inline">\(\text{Var}\left(\hat\theta\right) = 0\)</span>. This approach completely ignores the fact that <span class="math inline">\(1\)</span> may not even be remotely close to <span class="math inline">\(\theta_0\)</span>, something addressed by <span class="math inline">\(\text{Bias}(\hat\theta)\)</span>. Not only do we want a precise estimator (low variance), but we want an accurate estimator (low bias). The next example shows that there is a balancing act between these two properties.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.11 </strong></span>Return to the problem of estimating <span class="math inline">\(\sigma^2\)</span> using realizations of <span class="math inline">\(X_i\sim N(0,1)\)</span> for <span class="math inline">\(i=1,\ldots,20.\)</span> We’ve already considered two estimators: <span class="math display">\[\begin{align*}
S^2({\mathbf{X}}) &amp; = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.\\
\hat\theta({\mathbf{X}}) &amp; = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.
\end{align*}\]</span></p>
<p>If we return to our estimates from <a href="#exm-var">Example&nbsp;<span>1.9</span></a> we can estimate the variance of each estimator using <span class="math inline">\(S^2\)</span>. Just for clarification, we are using <span class="math inline">\(S^2(S^2({\mathbf{X}})\)</span> to estimate the variances of the estimators <span class="math inline">\(S^2({\mathbf{X}})\)</span> and <span class="math inline">\(\hat\theta({\mathbf{X}})\)</span>.</p>
<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-7_ac53c4ab206dd087f33e23a990821d64">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">var</span>(biased_estimates), <span class="fu">var</span>(unbiased_estimates))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.09496095 0.10521989</code></pre>
</div>
</div>
<p>It seems that the biased estimator is more efficient than the unbiased estimator. It can be shown that <span class="math display">\[\begin{align*}
\text{Var}\left(S^2\right) &amp; = \frac{2\sigma^4}{n-1},\\
\text{Var}\left(\hat\theta\right) &amp; = \frac{2(n-1)\sigma^4}{n^2}
\end{align*}\]</span> which gives <span class="math display">\[\begin{align*}
&amp; 1  \le \frac{n-1}{n} \\
\implies &amp; 1  &lt; \frac{(n-1)^2}{n^2} \\
\implies &amp; \frac{1}{n-1} &lt; \frac{n-1}{n^2} \\
\implies &amp; \frac{2\sigma^4}{n-1} &lt; \frac{2(n-1)\sigma^4}{n^2} \\
\implies &amp; \text{Var}\left(S^2\right) &lt; \text{Var}\left(\hat\theta\right).
\end{align*}\]</span></p>
</div>
<p>If bias and variance are at odds, perhaps we can conceive of a third measure of an estimator’s performance that supersedes both. We can quantify the <strong><em>cost</em></strong> of an estimator with a <strong><em>loss function</em></strong> <span class="math inline">\(l:\Theta\times\Theta \to \mathbb R^+\)</span> which captures how “wrong” an estimate <span class="math inline">\(\hat\theta\)</span> is for an estimand <span class="math inline">\(\theta\)</span>. A loss function is a function of an estimator, making it a random variable. The expected loss of an estimator is given by a <strong><em>risk function</em></strong> <span class="math inline">\(R:\Theta\times\Theta \to \mathbb R^+\)</span>, <span class="math display">\[R(\hat\theta, \theta) =  \text{E}\left[l(\hat\theta,\theta)\right] = \int_{{\mathcal X}} l(\hat\theta(x), \theta)\ dF(x\mid\theta).\]</span> We’ve already seen one special case of a risk function in the form of <span class="math inline">\(\text{Bias}(\hat\theta,\theta)\)</span>. If we define <span class="math inline">\(l(\hat\theta,\theta) = \hat\theta - \theta\)</span>, we have <span class="math display">\[ \text{E}\left[l(\hat\theta,\theta)\right] = \text{E}\left[\hat\theta - \theta\right] = \text{Bias}(\hat\theta,\theta).\]</span></p>
<p>One possible issue with the loss function <span class="math inline">\(l(\hat\theta,\theta) = \hat\theta - \theta\)</span> is that it is linear. This means that if <span class="math inline">\(\theta_0 = 0\)</span>, the estimate <span class="math inline">\(\hat\theta = 2\)</span> is twice as costly as <span class="math inline">\(\hat\theta =1\)</span>. In many situations, we think that a very bad estimate is not much worse than a bad estimate, but <em>much much</em> worse. We want a loss function that assigns more weight to poor estimates, capturing the idea that the marginal cost/loss of a underestimating/overestimating <span class="math inline">\(\theta\)</span> is increasing. This is achieved with a quadratic loss function <span class="math inline">\(l(\hat\theta,\theta) = (\hat\theta-\theta)^2\)</span>. The risk function which corresponds to this choice of <span class="math inline">\(l(\hat\theta,\theta)\)</span> is so important that it gets its own name.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="estimators_cache/html/fig-plot13_04c9856d85035095941465dc3390fd29">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> (<span class="sc">-</span><span class="dv">1000</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_void</span>() <span class="sc">+</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.96</span>, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Estimated θ"</span>) <span class="sc">+</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.07</span>, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"True θ"</span>) <span class="sc">+</span> </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.75</span>, <span class="at">y =</span> <span class="fl">0.2</span>, <span class="at">label =</span> <span class="st">"Loss(Estimaed θ, True θ)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot13" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="estimators_files/figure-html/fig-plot13-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.3: Quadratic loss function</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div id="def-mse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.9 </strong></span>Suppose <span class="math inline">\(\hat\theta:{\mathcal X} \to \Theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span>. The <span style="color:red"><strong><em>mean squared error (MSE)</em></strong></span> of <span class="math inline">\(\hat\theta\)</span> is <span class="math display">\[\text{MSE}\left(\hat\theta\right) = \text{E}\left[(\hat\theta - \theta)^2\right].\]</span> In the event our estimator is a vector <span class="math inline">\(\hat{\boldsymbol{\theta}} = (\hat\theta_1,\ldots,\hat\theta_k)\)</span>, then <span class="math display">\[ \text{MSE}\left(\hat{\boldsymbol{\theta}}\right) = \text{E}\left[\textstyle\sum_{i=1}^n(\hat\theta_i - \theta_i)^2\right].\]</span></p>
</div>
<p>How does this third measure help us? Don’t we need to compare bias, variance, and MSE now? Fortunately, we do not, as MSE captures both the variance and bias of an estimator! By expanding <span class="math inline">\((\hat\theta - \theta)^2\)</span> in the definition of MSE, we have <span class="math display">\[\begin{align*}
\text{MSE}\left(\hat\theta\right) &amp; = \text{E}\left[\hat\theta^2 - 2\hat\theta\theta + \theta^2\right] \\
                 &amp; = \text{E}\left[\hat\theta^2\right] - 2\theta\text{E}\left[\hat\theta\right] + \text{E}\left[\theta^2\right] &amp; (\text{Expectation is linear})\\
                 &amp; = \text{E}\left[\hat\theta^2\right] - 2\theta\text{E}\left[\hat\theta\right] + \text{E}\left[\theta^2\right] + \left(\text{E}\left[\hat\theta\right]^2 - \text{E}\left[\hat\theta\right]^2\right) \\
                 &amp; = \left(\text{E}\left[\hat\theta^2\right] - \text{E}\left[\hat\theta\right]^2\right) - 2\theta\text{E}\left[\hat\theta\right] + \text{E}\left[\theta^2\right] + \text{E}\left[\hat\theta\right]^2 \\
                 &amp; = \text{Var}\left(\hat\theta\right) + \left(\text{E}\left[\hat\theta\right]^2 - 2\theta\text{E}\left[\hat\theta\right] + \theta^2 \right)\\
                 &amp; = \text{Var}\left(\hat\theta\right) + \left(\text{E}\left[\hat\theta\right] - \theta\right)^2 \\
                 &amp; = \text{Var}\left(\hat\theta\right) + \text{Bias}(\hat\theta)^2.
\end{align*}\]</span> In a sense, mean square error is an aggregate of variance and bias, where we are more concerned with bias than variance (hence it being squared). We would rather have a very precise (low variance) estimator that is inaccurate (high bias) then an accurate estimator (low bias) that is imprecise (high variance). What does this mean for our two estimators of <span class="math inline">\(\sigma^2\)</span>?</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.12 </strong></span>Once again, define <span class="math display">\[\begin{align*}
S^2({\mathbf{X}}) &amp; = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.\\
\hat\theta({\mathbf{X}}) &amp; = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.
\end{align*}\]</span> The MSE of these estimators are <span class="math display">\[\begin{align*}
\text{MSE}\left(S^2\right) &amp;= \text{Var}\left(S^2\right) + \text{Bias}(S^2)^2 = \frac{2\sigma^4}{n-1} + 0 = \frac{2\sigma^4}{n-1},\\
\text{MSE}\left(\hat\theta\right) &amp;= \text{Var}\left(\hat\theta\right) + \text{Bias}(\hat\theta)^2 = \frac{2\sigma^4(n-1)}{n-1} + \left(\frac{n-1}{n}\sigma^2 - \sigma^2\right)^2 = \frac{\sigma^4(2n-1)}{n^2}.
\end{align*}\]</span> For all <span class="math inline">\(n \ge 1\)</span> we have, <span class="math display">\[\begin{align*}
&amp; 3n &gt; 1 \\
\implies &amp; -1 &gt; -3n \\
\implies &amp; 0 &gt; 1-3n \\
\implies &amp; 2n^2 &gt; 2n^2 - 3n + 1 \\
\implies &amp; 2n^2 &gt; (2n-1)(n-1) \\
\implies &amp; \frac{2n^2}{(n-1)n^2} &gt; \frac{(2n-1)(n-1)}{(n-1)n^2}\\
\implies &amp; \frac{2n^2}{n-1} &gt; \frac{(2n-1)}{n^2}\\
\implies &amp; \frac{2\sigma^4n^2}{n-1} &gt; \frac{\sigma^4(2n-1)}{n^2}\\
\implies &amp; \text{MSE}\left(S^2\right) &gt; \text{MSE}\left(\hat\theta\right).
\end{align*}\]</span></p>
<p>If our sole criterion for an estimator is mean-squared error, than we should opt to estimate <span class="math inline">\(\sigma^2\)</span> with the biased estimator.</p>
</div>
<p>This example brings attention to an immediate result of Definition <a href="#def-mse">Definition&nbsp;<span>1.9</span></a>.</p>
<div id="prp-biasmse" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.1 (MSE of an Unbiased Estimator) </strong></span>If an estimator <span class="math inline">\(\hat\theta : {\mathcal X} \to \Theta\)</span> is unbiased, then <span class="math display">\[\text{MSE}\left(\hat\theta\right)=\text{Var}\left(\hat\theta\right).\]</span></p>
</div>
</section>
<section id="efficient-estimators-fisher-information" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="efficient-estimators-fisher-information"><span class="header-section-number">1.5</span> Efficient Estimators, Fisher Information</h2>
<p>With MSE in mind, we can define what it means to have an “optimal” estimator.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.10 </strong></span>An estimator <span class="math inline">\(\hat\theta\)</span> is <span style="color:red"><strong><em>efficient</em></strong></span> if <span class="math inline">\(\text{MSE}\left(\hat\theta\right) &lt; \text{MSE}\left(\hat\theta'\right)\)</span> (for all values of <span class="math inline">\(\theta\)</span>) for all other estimators <span class="math inline">\(\hat\theta'\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> If <span class="math inline">\(\text{MSE}\left(\hat\theta_1\right) &lt; \text{MSE}\left(\hat\theta_2\right)\)</span>, we say <span class="math inline">\(\hat\theta_1\)</span> <strong><em>is more efficient</em></strong> <span class="math inline">\(\hat\theta_2\)</span></p>
</div>
<p>Calculating the most efficient estimator can be quite difficult considering how many possible estimators there are, so it’s common to look for the most efficient estimator among a smaller class of estimators that satisfy other desirable properties. The most common estimators to discard when finding an estimators are biased estimators. This approach takes unbiasedness as the lowest common denominator, and then picks the most efficient of the unbiased estimators. Note that by Proposition <a href="#prp-biasmse">Proposition&nbsp;<span>1.1</span></a>, the MSE of all unbiased estimators is simply their respective variances, so by restricting attention to this class of estimators, minimizing the MSE is the same thing as minimizing variance.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.11 </strong></span>An estimator <span class="math inline">\(\hat\theta\)</span> is the <span style="color:red"><strong><em>(uniformly) minimum-variance unbiased estimator (MVUE)</em></strong></span> if it is the most efficient estimator among all unbiased estimators.</p>
</div>
<p>Even with attention restricted to unbiased estimators, we will need an additional tool to determine if an estimator is efficient. This comes is a key result which bounds the variance of an estimator.</p>
<div id="thm-CRbound" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Information Inequality) </strong></span>Suppose <span class="math inline">\(\hat\theta :{\mathcal X} \to \Theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span> where <span class="math inline">\(\text{E}\left[\hat\theta\right]=\psi(\theta)\)</span>. If the support of <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, then <span class="math display">\[ \text{Var}\left(\hat\theta\right) \ge \frac{\left\lvert\psi'(\theta)\right\rvert^2}{I(\theta)} \]</span> where <span class="math inline">\(I(\theta)\)</span> is defined as <span class="math display">\[I(\theta) = \text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2\right],\]</span> and known as the <span style="color:red"><strong><em>Fisher information</em></strong></span> of the sample.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(\hat\theta\)</span> is an estimator such that <span class="math inline">\(\text{E}\left[\hat\theta\right]=\psi(\theta)\)</span>, and <span class="math inline">\(f_X(x\mid\theta)\)</span> satisfies the aforementioned properties. First, we’ll define a function of <span class="math inline">\(f_X(x\mid \theta)\)</span>. <span class="math display">\[ V(x) = \frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\]</span></p>
<p>Apply the chain rule to <span class="math inline">\(V(x)\)</span> gives: <span class="math display">\[V(x) = \frac{\partial}{\partial \theta} \log f_{\mathbf{X}}({\mathbf{X}}\mid\theta) = \frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right].\]</span> Because <span class="math inline">\(V\)</span> depends on the random variable <span class="math inline">\(X\)</span>, we can take it’s expectation over the sample space <span class="math inline">\({\mathcal X}\)</span>. <span class="math display">\[\begin{align*}
\text{E}\left[V\right] &amp; = \int\frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]\ dF_{\mathbf{X}}({\mathbf{X}}\mid\theta)\\
&amp; = \int f_{\mathbf{X}}({\mathbf{X}}\mid\theta) \cdot \frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]\ d{\mathbf{X}}\\
&amp; = \int \frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}}.
\end{align*}\]</span> This integral is taken over the support of <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\)</span>. We’ve assumed the support is not a function of <span class="math inline">\(\theta\)</span>, so the bounds of integration do not depend on <span class="math inline">\(\theta\)</span> and we can use Liebniz’s integral rule to interchange integration and differentiation: <span class="math display">\[ \text{E}\left[V\right] = \int \frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}} = \frac{\partial }{\partial \theta} \underbrace{\int  f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}}}_1 = 0.\]</span> Because <span class="math inline">\(V\)</span> has an expectation of zero, it’s variance is the expectation of its square: <span id="eq-vvar"><span class="math display">\[\text{Var}\left(V\right) = \text{E}\left[V^2\right] - \underbrace{\text{E}\left[V\right]^2}_0 = \cdot\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid \theta)\right)^2\right] = I(\theta) \tag{1.1}\]</span></span> Using the fact that <span class="math inline">\(\text{E}\left[V\right] = 0\)</span>, the covariance between <span class="math inline">\(V\)</span> and <span class="math inline">\(\hat\theta\)</span> is <span class="math display">\[\begin{align*}
\text{Cov}\left(V, \hat\theta\right) &amp;= \text{E}\left[V\hat\theta\right] -\underbrace{\text{E}\left[V\right]}_0\text{E}\left[\hat\theta\right]\\
&amp; = \text{E}\left[V\hat\theta\right]\\
&amp; = \text{E}\left[\frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]\hat\theta\right] \\
&amp; = \int \frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]\hat\theta({\mathbf{X}}) \ dF_{\mathbf{X}}({\mathbf{X}}\mid \theta) \\
&amp; = \frac{\partial }{\partial \theta} \int \frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\hat\theta({\mathbf{X}}) \ dF_{\mathbf{X}}({\mathbf{X}}\mid \theta) \\
&amp; = \frac{\partial }{\partial \theta} \int \hat\theta({\mathbf{X}}) \ dF_{\mathbf{X}}({\mathbf{X}}\mid \theta) \\
&amp; = \frac{\partial }{\partial \theta} \text{E}\left[\hat\theta\right] \\
&amp; = \psi'(\theta).
\end{align*}\]</span> Again we were able to interchange differentiation and integration due to the assumption about <span class="math inline">\(f_X(x\mid\theta)\)</span>’s support. In the context of probability, the famed Cauchy-Schwarz inequality takes the form <span class="math display">\[ \left\lvert{\text{Cov}\left(V,\hat\theta\right)}^2\right\rvert \le \text{Var}\left(V\right)\text{Var}\left(\hat\theta\right).\]</span> Using this along with the fact that <span class="math inline">\(\text{Cov}\left(V, \hat\theta\right) = \psi'(\theta)\)</span> and <a href="#eq-vvar">Equation&nbsp;<span>1.1</span></a>, gives: <span class="math display">\[\begin{align*}
&amp; \left\lvert{\text{Cov}\left(V,\hat\theta\right)}^2\right\rvert \le \text{Var}\left(V\right)\text{Var}\left(\hat\theta\right) \\
\implies &amp; \left\lvert\psi'(\theta)\right\rvert^2 \le I(\theta)\text{Var}\left(\hat\theta\right) \\
\implies &amp; \frac{\left\lvert\psi'(\theta)\right\rvert^2}{ I(\theta)} \le \text{Var}\left(\hat\theta\right)
\end{align*}\]</span> This is the desired inequality</p>
</div>
<p>This bound on the variance of an estimator is often called the <strong><em>Cramér–Rao (CR) lower bound</em></strong>. Theorem <a href="#thm-CRbound">Theorem&nbsp;<span>1.1</span></a> presents this bound in more generality than is often required. We can simplify it in three cases:</p>
<ol type="1">
<li><span class="math inline">\(\hat\theta\)</span> is unbiased, allowing us to eliminate reference to <span class="math inline">\(\psi'(\theta)\)</span>.</li>
<li>The random vector <span class="math inline">\({\mathbf{X}}\)</span> is comprised of iid random variables <span class="math inline">\(X_i\)</span>, allowing us to write the Fisher information in terms to common marginal density <span class="math inline">\(f_X(x\mid\theta)\)</span>.</li>
<li>If the second derivative of <span class="math inline">\(\log f_{\mathbf{X}}({\mathbf{X}}\mid \theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> exists, we can write the fisher information with respect to this second derivative.</li>
</ol>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1.1 (Information Inequality for Unbiased Estimators) </strong></span>Suppose <span class="math inline">\(\hat\theta :{\mathcal X} \to \Theta\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span>. If the support of <span class="math inline">\(f_X(x\mid\theta)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, then <span class="math display">\[ \text{Var}\left(\hat\theta\right) \ge I(\theta)^{-1}.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(\hat\theta\)</span> is unbiased, then <span class="math inline">\(\text{E}\left[\hat\theta\right] = \theta\)</span>, and <span class="math inline">\(\psi'(\theta) = 1\)</span>.</p>
</div>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1.2 (Information Inequality for IID Samples) </strong></span>Suppose <span class="math inline">\(\hat\theta :{\mathcal X} \to \Theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(X_i \overset{iid}{\sim} P_{\boldsymbol{\theta}}a\)</span> for all <span class="math inline">\(i=1,\ldots,n\)</span>. If the support of <span class="math inline">\(f_X(x\mid\theta)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, then <span class="math display">\[\text{Var}\left(\hat\theta\right) \ge \frac{\left\lvert\psi'(\theta)\right\rvert^2}{n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2\right]}\]</span> where <span class="math inline">\(f_X(x\mid\theta)\)</span> is the distribution shared by all <span class="math inline">\(X_i\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The joint distribution <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\)</span> can be written as the product of the <span class="math inline">\(n\)</span> identical marginal distributions. <span class="math display">\[f_{\mathbf{X}}({\mathbf{X}}\mid\theta) = \prod_{i=1}^nf_X(x_i\mid\theta)\]</span> If we use this equality we can write the Fisher information as, <span class="math display">\[\begin{align*}
I(\theta) &amp;= \text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2\right]\\
&amp; = \text{E}\left[\left(\frac{\partial}{\partial \theta}\log \prod_{i=1}^nf_X(x_i\mid\theta)\right)^2\right] \\
&amp; = \text{E}\left[\left(\frac{\partial}{\partial \theta}\sum_{i=1}^n\log f_X(x_i\mid\theta)\right)^2\right] &amp; (\text{log properties})\\
&amp; = \text{E}\left[\left(\sum_{i=1}^n\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2\right] &amp; (\text{linearity of derivative})\\
&amp; = \text{E}\left[\sum_{i=1}^n\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2 + \sum_{i\neq j}\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)\right] &amp; (\text{expand square})\\
&amp; = \sum_{i=1}^n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2\right] + \sum_{i\neq j}\text{E}\left[{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)}\right]&amp; (\text{expectation is linear})\\
&amp; = \sum_{i=1}^n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2\right] + \sum_{i\neq j}\underbrace{\text{E}\left[{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}\right]}_0\underbrace{\text{E}\left[\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)\right]}_0&amp; (X_i\perp X_j) \\
&amp; = n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2\right] &amp; (\text{identical distributions}).
\end{align*}\]</span> The fact that <span class="math inline">\(\text{E}\left[{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}\right] = 0\)</span> follows from the same argument we used to conclude <span class="math inline">\(\text{E}\left[V\right] = 0\)</span> in the proof of Theorem <a href="#thm-CRbound">Theorem&nbsp;<span>1.1</span></a>.</p>
</div>
<div id="cor-crb2nd" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1.3 </strong></span>Suppose <span class="math inline">\(\hat\theta :{\mathcal X} \to \Theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span> where <span class="math inline">\(\text{E}\left[\hat\theta\right]=\psi(\theta)\)</span>. If the support of <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}} \mid \theta)\)</span> exists, then <span class="math display">\[\text{Var}\left(\hat\theta\right) \ge \frac{\left\lvert\psi'(\theta)\right\rvert^2}{-\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that in the proof of <a href="#thm-CRbound">Theorem&nbsp;<span>1.1</span></a> we established <span class="math inline">\(\text{E}\left[{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}\right] = 0\)</span>. Using this, along with the Liebniz’s rule and the assumption about the support of <span class="math inline">\(f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\)</span> we have</p>
<p><span class="math display">\[\begin{align*}
&amp; \text{E}\left[{\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right] = 0\\
\implies &amp; \frac{\partial}{\partial \theta}\text{E}\left[{\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right] = 0 \\
\implies &amp; \frac{\partial}{\partial \theta}\int \frac{1}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ dF_{\mathbf{X}}({\mathbf{X}}\mid\theta) = 0\\
\implies &amp; \frac{\partial}{\partial \theta}\int \frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}} = 0\\
\implies &amp; \int \frac{\partial^2}{\partial \theta^2}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}} = 0\\
\implies &amp; \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\ d{\mathbf{X}} = 0\\
\implies &amp; \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\ dF_{\mathbf{X}}({\mathbf{X}}\mid\theta) = 0 \\
\implies &amp; \text{E}\left[\frac{\frac{\partial^2}{\partial \theta^2}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right] = 0
\end{align*}\]</span> This equality allows us to conclude <span class="math inline">\(-\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right] = \text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2\right]\)</span>.</p>
<p><span class="math display">\[\begin{align*}
-\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right] &amp; = -\text{E}\left[\frac{\partial}{\partial \theta}\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)\right] \\
  &amp; = -\text{E}\left[\frac{\partial}{\partial \theta}\left(\frac{\frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right)\right]&amp; (\text{chain rule})\\
  &amp; = -\text{E}\left[\frac{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\frac{\partial^2}{\partial^2 \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta) - \frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)^2}\right]&amp; (\text{quotient rule})\\
  &amp; =-\text{E}\left[\frac{\frac{\partial^2}{\partial^2 \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)} - \left(\frac{\frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right)^2 \right]\\
  &amp; = -\underbrace{\text{E}\left[\frac{\frac{\partial^2}{\partial^2 \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right]}_0 + \text{E}\left[\left(\frac{\frac{\partial}{\partial \theta}f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}{f_{\mathbf{X}}({\mathbf{X}}\mid\theta)}\right)^2 \right] &amp; (\text{expectation is linear}) \\
  &amp; =  \text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2 \right]
\end{align*}\]</span> If we apply the information inequality, the result follows.</p>
</div>
<p>These three corollaries can make the Cramér–Rao lower bound a headache because it takes so many forms. The following table helps us make sense of the various cases, assuming that <span class="math inline">\(\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}} \mid \theta)\)</span> exists.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 35%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(\hat\theta\)</span> is unbiased</th>
<th><span class="math inline">\(\text{E}\left[\hat\theta\right] = \psi(\theta)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X_i \overset{iid}{\sim}{P_{\boldsymbol{\theta}}}\)</span></td>
<td><span class="math inline">\(\text{Var}\left(\hat\theta\right) \le \frac{1}{n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2\right]} = -\frac{1}{n\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)\right]}\)</span></td>
<td><span class="math inline">\(\text{Var}\left(\hat\theta\right) \le \frac{\left\lvert\psi'(\theta)\right\rvert^2}{n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2\right]} = -\frac{\left\lvert\psi'(\theta)\right\rvert^2}{n\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)\right]}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(X_i {\overset{iid}{\not\sim}}{P_{\boldsymbol{\theta}}}\)</span></td>
<td><span class="math inline">\(\text{Var}\left(\hat\theta\right) \le \frac{1}{\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2\right]} = -\frac{1}{\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]}\)</span></td>
<td><span class="math inline">\(\text{Var}\left(\hat\theta\right) \le \frac{\left\lvert\psi'(\theta)\right\rvert^2}{\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right)^2\right]} = -\frac{\left\lvert\psi'(\theta)\right\rvert^2}{\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_{\mathbf{X}}({\mathbf{X}}\mid\theta)\right]}\)</span></td>
</tr>
</tbody>
</table>
<p>Because we’re often concerned with finding a MVUE, we will rarely be concerned with the case where <span class="math inline">\(\text{E}\left[[\right]\hat\theta]\)</span>. Additionally, the assumption that the random sample is used to calculate estimator is exceedingly common. For these two reasons, the Cramér–Rao lower bound will almost always take the form <span class="math display">\[\text{Var}\left(\hat\theta\right) \le \frac{1}{n\text{E}\left[\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2\right]} = -\frac{1}{n\text{E}\left[\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)\right]}.\]</span></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.13 </strong></span>Suppose <span class="math inline">\(X_i\overset{iid}{\sim}\text{Binom}(k,p)\)</span>, recalling <span class="math display">\[f_X(x\mid p) = \binom{k}{p}p^x(1-p)^{k-x}\]</span> where <span class="math inline">\(x\)</span> is the number of realized successes of <span class="math inline">\(k\)</span> binomial trials. This random variable has an expected value of <span class="math inline">\(kp\)</span> and variance of <span class="math inline">\(kp(1-p)\)</span>. Our goal is to estimate the probability of success <span class="math inline">\(p\)</span>. If we observe one sequence of <span class="math inline">\(k\)</span> trials (<span class="math inline">\(n=1\)</span>) A natural estimator for this is ratio of successes to trials, <span class="math inline">\(\hat p = X/k\)</span>. This estimator is unbiased: <span class="math display">\[\text{E}\left[\hat p\right] = \frac{\text{E}\left[X\right]}{k} = \frac{kp}{k} = p.\]</span> The variance of the estimator is: <span class="math display">\[\text{Var}\left(\hat p\right) = \text{Var}\left(\frac{X}{k}\right) = \frac{\text{Var}\left(X\right)}{k^2} = \frac{kp(1-p)}{k^2} = \frac{p(1-p)}{k}.\]</span> Is <span class="math inline">\(\hat p\)</span> an MVUE? In order to determine this, we must calculate the Cramér–Rao lower bound. <span class="math display">\[\begin{align*}
&amp; \log f_X(x\mid p) = \log \binom{k}{x} + x\log p + (k-x)\log (1-p)\\
\implies &amp;  \frac{\partial}{\partial p}  \log f_X(x\mid p) = \frac{x-kp}{p(1-p)}\\
\implies &amp; \left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 = \frac{(x-kp)^2}{p^2(1-p)^2}\\
\implies &amp; \text{E}\left[\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 \right] = \frac{\text{E}\left[(x-kp)^2\right]}{p^2(1-p)^2} = \frac{\text{Var}\left(X\right)}{p^2(1-p)^2} =  \frac{kp(1-p)}{p^2(1-p)^2} = \frac{k}{p(1-p)}\\
\implies &amp; n\text{E}\left[\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 \right] = 1\cdot \frac{k}{p(1-p)} = \frac{k}{p(1-p)}\\
\implies &amp; \text{Var}\left(\hat p\right) \le \frac{p(1-p)}{k}.
\end{align*}\]</span> We have that <span class="math inline">\(\hat p =X/k\)</span> is the MVUE. We could have calculated this bound using the second derivative of <span class="math inline">\(\log f_X(x\mid\theta)\)</span>:</p>
<p><span class="math display">\[ \frac{\partial^2}{\partial p^2}  \log f_X(x\mid p)= \frac{\partial}{\partial p}\left[\frac{x-kp}{p(1-p)}\right]=-\frac{x}{p^2}-\frac{m-x}{(1-p)^2}=-\frac{(x-mp)^2}{p^2(1-p^2)} \]</span></p>
<p>It’s important to remember that this means <span class="math inline">\(\hat p\)</span> is the most efficient estimator among unbiased estimators. It is entirely possible that there exists a biased estimator for <span class="math inline">\(p\)</span> that has a lower MSE than <span class="math inline">\(\hat p\)</span> for certain parameter values of <span class="math inline">\((k,p)\)</span>. Take for instance the estimator <span class="math inline">\(\hat p'\)</span>, <span class="math display">\[\hat p' = \frac{X+1}{k+2}.\]</span> For this estimator, we have <span class="math display">\[\begin{align*}
\text{Bias}(\hat p ')&amp; = \text{E}\left[\hat p '\right] - p = \frac{\text{E}\left[X\right] + 1}{k + 2} - p = \frac{kp + 1}{k + 2} - p = \frac{1-2p}{m+2}\\
\text{Var}\left(\hat p'\right) &amp;= \text{Var}\left(\frac{X + 1}{k+2}\right) = \frac{\text{Var}\left(X+1\right)}{(k+2)^2}= \frac{kp(1-p)}{(k+2)^2}\\
\text{MSE}\left(\hat p '\right)&amp; = \text{Var}\left(\hat p '\right) + \text{Bias}(\hat p')^2 = \frac{1 + (k-4)p - (k-4)p^2}{(k+2)^2}.
\end{align*}\]</span> We can graph <span class="math inline">\(\text{MSE}\left(\hat p\right) = \text{Var}\left(\hat p\right)\)</span> along with <span class="math inline">\(\text{MSE}\left(\hat p'\right)\)</span> for various values of <span class="math inline">\(k\)</span>.</p>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-warnings="false" data-hash="estimators_cache/html/fig-plot14_7b67cf937fe9517e64aef2e8c6baa4aa">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">100</span><span class="sc">/</span><span class="dv">100</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>, <span class="dv">20</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>est <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Biased"</span>, <span class="st">"MVUE"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(p, k, est) <span class="sc">%&gt;%</span> </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">p =</span> Var1,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">k =</span> Var2,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">Estimator =</span> Var3</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse =</span> <span class="fu">ifelse</span>(Estimator <span class="sc">==</span> <span class="st">"Biased"</span>, (<span class="dv">1</span><span class="sc">+</span>(k<span class="dv">-4</span>)<span class="sc">*</span>p <span class="sc">-</span> (k<span class="dv">-4</span>)<span class="sc">*</span>p<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>((k<span class="sc">+</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>) , p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">/</span>k)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, mse, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>k) <span class="sc">+</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"True Value of Estimad, p"</span>, <span class="at">y =</span> <span class="st">"Mean Squared Error"</span>) <span class="sc">+</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot14" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="estimators_files/figure-html/fig-plot14-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.4: Distribution of estimates of variance for an unbiased estimator and a biased estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The estimator <span class="math inline">\(\hat p'\)</span> does have a lower MSE than the UMVE at times.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.14 </strong></span>If we observe realizations of <span class="math inline">\(X_i\overset{iid}{\sim}\text{Uni}(0,\theta)\)</span>, then the distribution <span class="math inline">\(f_X(x\mid\theta) = 1/\theta\)</span> (with a support of <span class="math inline">\([0,\theta]\)</span>) <em>does not</em> satisfy the information inequality’s necessary condition – the support of <span class="math inline">\(f_X(x\mid\theta)\)</span> is a function of the parameter <span class="math inline">\(\theta\)</span>. Without this assumption, we can not apply Leibniz’s rule in the proof for @ref(thm:CRbound), and shouldn’t expect the result to hold.</p>
<p>An unbiased estimator for <span class="math inline">\(\theta\)</span> is <span class="math display">\[\hat\theta({\mathbf{X}})=\frac{n+1}{n}X_{(n)}\]</span> where <span class="math inline">\(X_{(n)} = \max\{X_1,\ldots, X_n\}\)</span> is the maximum order statistic. If the random variable <span class="math inline">\(X_i\)</span> has an upper bound of <span class="math inline">\(\theta\)</span>, then using the largest value from the sample to estimate <span class="math inline">\(\theta\)</span> is rather intuitive. The order statistic <span class="math inline">\(X_{(n)}\)</span> has a density function of <span class="math display">\[f_{X_{(n)}}(x\mid\theta) = nf_X(x\mid\theta)[F_X(x\mid\theta)]^{n-1} = n\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1} = n\frac{x^{n-1}}{\theta^n}.\]</span> The expectation of <span class="math inline">\(X_{(n)}\)</span> is <span class="math display">\[\text{E}\left[X_{(n)}\right] = \int_0^\theta x\ dF_{X_{(n)}}(x\mid\theta) = \frac{n}{\theta^n} \int_0^\theta x n\frac{x^{n-1}}{\theta^n}\ dx = \frac{n}{n+1} \theta,\]</span> so it’s a biased estimator. We can correct for this bias by multiplying by <span class="math inline">\(X_{(n)}\)</span> by <span class="math inline">\((n+1)/n\)</span>, which gives <span class="math inline">\(\hat\theta({\mathbf{X}})\)</span>. The variance of <span class="math inline">\(\hat\theta\)</span> is <span class="math display">\[\begin{align*}
\text{Var}\left(\hat\theta\right)&amp;=\text{Var}\left(\frac{n+1}{n}X_{(n)}\right)\\
&amp;= \frac{(n+1)^2}{n^2} \left(\text{E}\left[X_{(n)}^2\right] - \text{E}\left[X_{(n)}\right]^2\right)\\
&amp;= \frac{(n+1)^2}{n^2} \left(\int_0^\theta x^2\ dF_{X_{(n)}}(x\mid\theta) - \left(\frac{n}{n+1} \theta\right)^2\right)\\
&amp; = \frac{\theta^2}{n(n+2)},
\end{align*}\]</span> while the Fisher information of the sample is <span class="math display">\[\begin{align*}
&amp; \log f_X(x\mid \theta) = -\log(\theta)\\
\implies &amp; \frac{\partial}{\partial \theta} \log f_X(x\mid \theta) = -\frac{1}{\theta}\\
\implies &amp;\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2 = \frac{1}{\theta^2}\\
\implies &amp; \text{E}\left[\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2\right] = \frac{1}{\theta^2}\\
\implies &amp; n\text{E}\left[\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2\right] = \frac{n}{\theta^2}\\
\implies &amp; I(\theta) = \frac{n}{\theta^2}.
\end{align*}\]</span></p>
<p>In this case, <span class="math inline">\(\text{Var}\left(\hat\theta\right) &lt; I(\theta)^{-1}\)</span>.</p>
</div>
<p>We can extend @ref(thm:CRbound) to the case where we estimate multiple parameters. Because our interest will almost always be in unbiased estimators calculated with IID samples, the multiparameter version of the information inequality will be presented in this context.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (Information Inequality for Multiple Parameters) </strong></span>Suppose <span class="math inline">\(\hat{\boldsymbol{\theta}} :{\mathcal X} \to \Theta\)</span> is an unbiased estimator for <span class="math inline">\(\boldsymbol{\theta}\)</span>, and <span class="math inline">\(X_i \overset{iid}{\sim} P_{\boldsymbol{\theta}}a\)</span> for all <span class="math inline">\(i=1,\ldots,n\)</span>. If the support of <span class="math inline">\(f_X(x\mid\boldsymbol{\theta})\)</span> does not depend on <span class="math inline">\(\boldsymbol{\theta}\)</span>, then <span class="math display">\[ \text{Var}\left(\hat{\boldsymbol{\theta}}\right) \ge \mathbf I(\boldsymbol{\theta})^{-1},\]</span> where <span class="math inline">\(\mathbf I(\boldsymbol{\theta})\)</span> is known as the <span style="color:red"><strong><em>information matrix</em></strong></span> and defined as <span class="math display">\[\mathbf I(\boldsymbol{\theta})_{i,j} = -n \text{E}\left[\frac{\partial}{\partial \theta_i\partial\theta_j}\log f_X(x\mid\theta)\right]\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.15 </strong></span>If <span class="math inline">\(X_i\sim N(\mu,\sigma^2)\)</span>, we can estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> jointly with an estimator <span class="math inline">\(\hat{\boldsymbol{\theta}} = (\hat\mu, \hat\sigma^2)\)</span>.</p>
<p>This can be verified by using the information inequality for unbiased estimators and IID samples. In this case, <span class="math display">\[\begin{align*} f_X(x\mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]
\implies \log f_X(x\mid \mu, \sigma^2) = \log(1) - \frac{1}{2}\log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}.
\end{align*}\]</span></p>
<p>Differentiating gives:</p>
<p><span class="math display">\[\begin{align*}
-n \text{E}\left[\frac{\partial^2}{\partial \mu^2}\log f_X(x\mid\theta)\right] &amp;= -n\text{E}\left[-\sigma^{-2}\right] = n\sigma^{-2}\\
-n \text{E}\left[\frac{\partial}{\partial \mu\partial\sigma^2}\log f_X(x\mid\theta)\right] &amp;= -n \text{E}\left[\frac{\partial}{\partial\sigma^2\partial \mu}\log f_X(x\mid\theta)\right]=-n\sigma^{-4}\text{E}\left[x-\mu\right]=0\\
-n \text{E}\left[\frac{\partial^2}{\partial (\sigma^2)^2}\log f_X(x\mid\theta)\right] &amp;= -n\text{E}\left[\sigma^{-4}/2\right] = n\sigma^{-4}/2\\
\mathbf I(\mu, \sigma) &amp; = \begin{bmatrix} n/\sigma^{-2} &amp; 0 \\ 0  &amp; n\sigma^{-4}/2 \end{bmatrix}.
\end{align*}\]</span> The information inequality takes the form <span class="math display">\[\text{Var}\left(\hat\mu, \hat\sigma^2\right) \ge  \begin{bmatrix} \sigma^{2}/n &amp; 0 \\ 0  &amp; 2\sigma^{4}/n \end{bmatrix}.\]</span> Notice that <span class="math inline">\(\text{Var}\left(\hat\mu\right) \ge \sigma^2/n\)</span>. This means that <span class="math inline">\(\hat\mu = \bar X\)</span> is the MVUE, as <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/n\)</span>.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span><a href="#thm-CRbound">Theorem&nbsp;<span>1.1</span></a> and its resulting corollaries have one main drawback. It is not constructive and provides no guidance as to constructing efficient estimators, and if such an estimator even exists. Addressing this is a matter of introducing a new property of estimators related to data reduction and information (<strong><em>sufficiency</em></strong>) and the Lehmann–Scheffé theorem (which is related to the perhaps more familiar Rao–Blackwell theorem).</p>
</div>
</section>
<section id="the-distribution-of-an-estimator" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="the-distribution-of-an-estimator"><span class="header-section-number">1.6</span> The Distribution of an Estimator</h2>
<p>Finally, we turn to the matter of an estimator’s probability distribution. Like any other random variable, estimators have a distribution and density function. Knowing the distribution of an estimator allows us to put a point estimate in a broader context by using an estimator’s distribution to calculate the probability of observing the estimate in question.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.16 </strong></span>Suppose <span class="math inline">\({\mathbf{X}} = (X_1, \ldots, X_n)\)</span> is a random sample from a normal distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span>. What is the distribution of <span class="math inline">\(\bar X\)</span>? The sum of <em>independent</em> normally distributed random variables is normally distributed as follows: <span class="math display">\[\sum_{i=1}^n X_i \sim N(u_1 + \mu_2 + \cdots + \mu_n, \sigma_1^2 + \sigma_2^2 + \cdots \sigma_n^2).\]</span> In our case, each <span class="math inline">\(X_i\)</span> are identically distributed as well, so <span class="math inline">\(\mu_i = \mu\)</span> and <span class="math inline">\(\sigma_i^2 = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. <span class="math display">\[ \sum_{i=1}^n X_i\sim N(n\mu, n\sigma^2)\]</span> Finally if we scale the sum by <span class="math inline">\(1/n\)</span>, giving <span class="math inline">\(\bar X\)</span>, we have <span class="math display">\[\bar X\sim N(\mu, \sigma^2/n)\]</span> by the properties of expectation and variance. If we simulate realizations of this estimator, the resulting histogram should betray that <span class="math inline">\(\bar X\)</span> is normally distributed. For these simulations, we will take <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(\mu = 0\)</span>, and <span class="math inline">\(\sigma^2 = 1\)</span>. We should see that <span class="math display">\[\begin{align*}
\text{E}\left[\bar X\right] &amp;\approx 0\\
\text{Var}\left(\bar X\right) &amp;\approx 0.01
\end{align*}\]</span></p>
<div class="cell" data-hash="estimators_cache/html/unnamed-chunk-10_97227ee2e3c46f417b5351f728df0427">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  estimates[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">mean</span>(estimates), <span class="fu">var</span>(estimates))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -6.754554e-05  9.993609e-03</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-fig.asp="0.7" data-hash="estimators_cache/html/fig-plot15_c85350552876c1b7c7c4e14f8e9aaa9e">
<details>
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(estimates) <span class="sc">%&gt;%</span> </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimates)) <span class="sc">+</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">colour =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of μ"</span>) <span class="sc">+</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">100</span>)), <span class="at">color =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot15" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="estimators_files/figure-html/fig-plot15-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1.5: Distribution of estimates of variance for an unbiased estimator and a biased estimator</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Note that for this to hold, it must be the case that <span class="math inline">\(X_i\)</span> are iid normally distributed, which is rather restrictive. Ideally, we will be able to make some statements about estimators’ distributions regardless of the underlying distribution which generates the observable data. Fortunately, the next section will equip us with the tools to do this.</p>
</div>
</section>
<section id="further-reading" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">1.7</span> Further Reading</h2>
<ul>
<li><span class="citation" data-cites="mccullagh2002statistical">McCullagh (<a href="references.html#ref-mccullagh2002statistical" role="doc-biblioref">2002</a>)</span></li>
<li><span class="citation" data-cites="bickel2015mathematical">Bickel and Doksum (<a href="references.html#ref-bickel2015mathematical" role="doc-biblioref">2015</a>)</span></li>
<li><span class="citation" data-cites="lehmann2006theory">Lehmann and Casella (<a href="references.html#ref-lehmann2006theory" role="doc-biblioref">1998</a>)</span></li>
<li><span class="citation" data-cites="greene2003econometric">Greene (<a href="references.html#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Appendix C</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-bickel2015mathematical" class="csl-entry" role="doc-biblioentry">
Bickel, Peter J, and Kjell A Doksum. 2015. <em>Mathematical Statistics: Basic Ideas and Selected Topics, Volume i</em>. 2nd ed. CRC Press.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="doc-biblioentry">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-lehmann2006theory" class="csl-entry" role="doc-biblioentry">
Lehmann, Erich L, and George Casella. 1998. <em>Theory of Point Estimation</em>. 2nd ed. Springer.
</div>
<div id="ref-mccullagh2002statistical" class="csl-entry" role="doc-biblioentry">
McCullagh, Peter. 2002. <span>“What Is a Statistical Model?”</span> <em>The Annals of Statistics</em> 30 (5): 1225–1310.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Sets with this type of structure are quite common in mathematics and can be formalized with equivelence relations and quotient groups.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>An estimand could also be a function of the true underlying parameter <span class="math inline">\(\boldsymbol{\theta}_0\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Okay, it actually isn’t <span class="math inline">\(\mathbb R^n\)</span>. It’s sigma-algebra generated by the Borel sets of <span class="math inline">\(\mathbb R^n\)</span>, but measure theory is not our concern at the moment.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>We could also use <span class="math inline">\(\left\lvert\hat\theta({\mathbf{X}}) -\theta\right\rvert\approx 0\)</span>, but this measure is unsigned. It may be useful to know whether or not an estimator is overestimating or underestimating <span class="math inline">\(\theta\)</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>These sample means are themselves estimators of <span class="math inline">\(\text{E}\left[\hat\theta\right]\)</span> and <span class="math inline">\(\text{E}\left[\hat\theta^*\right]\)</span>, but one million simulations is so many that our estimates will be very good. Why this works is a discussion for the next section on asymptotics.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In general, efficiency is subject to our choice of loss function, and is by no means restricted to a quadratic loss function.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preliminaries</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./asymptotics.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># Finite Sample Properties of Estimators {#sec-est}</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a><span class="fu">## Models and Parameterizations</span></span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a>The term "estimation" is used so often, that it's often easy to forget precisely what it means. It's worth briefly presenting the formal definition of (point) estimation in accordance with statistical theory and formulate the performance of an estimator using tools from decision theory. A comprehensive treatment of the theory of estimation, see @bickel2015mathematical or @lehmann2006theory. </span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a>Given a random experiment with sample space $\mathcal X$ over which a random vector ${\X}=(X_1,\ldots. X_n)$ is defined, a <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_model_**&lt;/span&gt; $\mathcal P$ is the collection of probability distributions (or densities), or sets of distributions (or densities), from which $\X$ may be distributed. We call an element of a model $P\in\mathcal P$, &lt;span style="color:red"&gt;**_model value_**<span class="kw">&lt;/span&gt;</span>.</span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>In order to keep track of model values, we need to introduce labels for model values $P$. </span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>For every element of $\mathcal P$, we assign a label called a <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_parameter_**&lt;/span&gt; $\thet$ from a &lt;span style="color:red"&gt;**_parameter space_**&lt;/span&gt;  $\Thet = \Theta_1 \times\cdots\times\Theta_k$, denoting the labeled element as $P_{\thet}$. The function $\theta \mapsto P$ which assigns these labels is a &lt;span style="color:red"&gt;**_parameter space_**<span class="kw">&lt;/span&gt;</span>. </span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>A parameterization enables to write the model as $\mathcal P =<span class="sc">\{</span>P_{\thet} \mid \thet\in \Thet<span class="sc">\}</span>$. </span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## Normally Distributed Random Variable</span></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a>Assume that we know *a priori* that $X$ is a normally distributed random variable ($n=1$ here). The model $\mathcal P$ is the collection of all normal distributions. How do we label this entirely family of models? The most common way is via the mean and variance of the normal distribution. In this case, $\Thet = \mathbb R \times \mathbb R^+$, $\thet = (\mu, \sigma^2)$, and </span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>$$f_X(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left<span class="co">[</span><span class="ot">-\frac{(x-\mu)^2}{2\sigma^2}\right</span><span class="co">]</span>.$$ By parameterizing $\mathcal P$, we can now easily refer to each element of $\mathcal P$ using $\thet=(\mu,\sigma^2)$.</span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a>$$\mathcal P = <span class="sc">\{</span>f_X(x \mid \mu, \sigma^2) \mid  (\mu, \sigma^2) \in \mathbb R \times \mathbb R^+<span class="sc">\}</span>.$$</span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a>Alternatively, we could **_reparameterize_** the model using the standard deviation instead of the variance, $\thet = (\mu, \sigma)$. This gives </span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a>$$f_X(x \mid \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right</span><span class="co">]</span>.$$ If we wanted to really be crazy, we could parameterize the model using $\thet = (\mu/\sigma^2, -1/2\sigma)$, where $\Theta = \mathbb R\times \mathbb R^-$.</span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a>f_X(x \mid \thet) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left<span class="co">[</span><span class="ot">-\frac{(x-\mu)^2}{2\sigma^2}\right</span><span class="co">]</span> <span class="sc">\\</span> &amp; = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left<span class="co">[</span><span class="ot">\dfrac{x^2}{2\sigma^2} -\dfrac{2x\mu}{2\sigma^2} + \dfrac{\mu^2}{2\sigma^2}\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a>         &amp; =\exp\left<span class="co">[</span><span class="ot">\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a>         &amp; =\exp\left<span class="co">[</span><span class="ot">\log\left( (2\pi\sigma^2)^{-1/2}\right)\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right</span><span class="co">]</span> <span class="sc">\\</span> </span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a>         &amp; = \exp\left<span class="co">[</span><span class="ot">-\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a>         &amp; = \exp\left<span class="co">[</span><span class="ot">-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2} -\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a>         &amp; = \exp\left<span class="co">[</span><span class="ot">\dfrac{\mu}{\sigma^2}x-\dfrac{1}{2\sigma^2}x^2 -\frac{1}{2}\log\left(\frac{\mu^2}{\sigma^2}+\log\left( 2\pi\sigma^2\right)\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a>         &amp; = \exp\left<span class="co">[</span><span class="ot">\theta_1x+\theta_2x^2 -\frac{1}{2}\log\left(-\frac{\theta_1}{2\theta_2}+\log\left( \pi\theta_2^{-1}\right)\right)\right</span><span class="co">]</span></span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a>    \end{align*}</span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exponentially Distributed Random Variable</span></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a>If $X$ is distributed according to an exponential distribution, the most common parameterization has $\Theta = \mathbb R^+$, $\theta = \lambda$, and each element of $\mathcal P$ has a corresponding density of </span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a>$$ f_X(x\mid\lambda) = \begin{cases}\lambda e^{-\lambda x}&amp; x\ge 0 <span class="sc">\\</span> 0 &amp; x&lt;0\end{cases}.$$ A common alternate parameterization is in terms of $\beta \in \mathbb R^+$ labels elements in $\mathcal P$ such that </span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a>$$ f_X(x\mid\beta) = \begin{cases}\frac{1}{\beta} e^{-x/\beta}&amp; x\ge 0 <span class="sc">\\</span> 0 &amp; x&lt;0\end{cases}. $$</span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a>These last two examples were simple because each element $P_\thet \in \mathcal P$ is a single probability density/distribution. In this case we write ${\X} \sim P_{\thet}$, and the difference between $P_{\thet}$ and $f_{\X}({\X}\mid\thet)$ is purely a matter of notation. It is the same difference between writing a uniform distribution as $\text{Uni}(a,b)$ instead of $f_X(x\mid a,b) = 1/(b-a)$ or $F_X(x\mid a,b)=x/(b-a)$. We may also write the probability measure associated with the sample space ${\mathcal X}$ and density as $P_{\thet}$, so the expectation of ${\X}$ can be written as </span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a>$$ \E{\X} = \int_{\mathcal{X}} {\X} f_{\X}({\X}\mid\thet)\ d{\X} = \int_{\mathcal{X}} {\X} \ dF_{\X}({\X}\mid\thet) = \int_{\mathcal{X}} {\X} \ dP_{\thet}.$$ </span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a>It will often be the case, especially so in econometrics, that an element of $P_\thet \in \mathcal P$ is itself a set of distributions defined by some common properties.^<span class="co">[</span><span class="ot">Sets with this type of structure are quite common in mathematics and can be formalized with equivelence relations and quotient groups.</span><span class="co">]</span> </span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a><span class="fu">## IID Sample Parameterized by Mean</span></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1,\ldots, X_n)$ is a sample such that $X_i \iid F_X(x)$ and $\E{X_i} = \mu$ for all $i$. This data generating process constitutes a model parameterized by $\mu$. An element of $\mathcal P$ takes the form </span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a>$$P_\mu = \left<span class="sc">\{</span>f_{\X}({\X})\ \Bigg|\ f_{\X}({\X}) = \prod_{i=1}^n f_{X_i}(x) \text{ and } f_{X_i} = f_{X_j}\ \forall i,j \text{ and }\E{X_i}=\mu\ \forall i\right<span class="sc">\}</span>.$$ For example, if we let $\mu = 5$, then $P_5\in\mathcal P$ is the set of all joint distributions such that the elements of $\X$ are independent and have an expected value of $5$. This is *a lot* of possible distributions. One possible distribution in this set is $$f_{\X}({\X}\mid \mu) = \prod_{i=1}^n\frac{1}{2\mu} = \frac{1}{(2\mu)^n},$$ which is the joint density of ${\X}$ when $X_i \iid \text{Uni}(0,2\mu)$. Because ${\X}$ is comprised of an iid sample, we could also define the elements of $\mathcal P$ using the marginal density $f_{X_i}(x\mid\theta)$, as this density is identical for all $X_i$. </span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a>$$ P_\mu = <span class="sc">\{</span>f_{X_i}(x) \mid \E{X_i} = \mu<span class="sc">\}</span> .$$ What is important here is that each $P_\mu$ is an infinite collection of densities all with a common properties related to the parameter $\mu$. </span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a>To differentiate models where $P_\thet \in \mathcal P$ is a single distribution opposed to a collection of distributions, we will introduce a new term (that is not standard across any sources).  </span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb15-148"><a href="#cb15-148" aria-hidden="true" tabindex="-1"></a>A model $\mathcal P = <span class="sc">\{</span>P_{\thet} \mid \thet \in \Thet<span class="sc">\}</span>$ is a <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_regular model_**<span class="kw">&lt;/span&gt;</span> if each $P_{\thet}$ is a probability distribution with density $f_{\X}({\X}\mid\thet)$.</span>
<span id="cb15-149"><a href="#cb15-149" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-150"><a href="#cb15-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a>Not only can we consider just how many distributions make up one model value $P\in\mathcal P$, but we can also consider the dimension of the parameter space $\Thet$. </span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a>Suppose a model $\mathcal P$ is parameterized by a mapping $\thet \mapsto P_\thet$ where $\dim(\Thet) = k$.</span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If $\dim(\Theta_j)$ is finite for all $j=1,\ldots, k$, then $\mathcal P = \{P_{\thet} \mid \thet \in \Thet\}$ is &lt;span style="color:red"&gt;**_parametric_**<span class="kw">&lt;/span&gt;</span>. </span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If $\dim(\Theta_j)$ is infinite for all $j=1,\ldots, k$, then $\mathcal P = \{P_{\thet} \mid \thet \in \Thet\}$ is &lt;span style="color:red"&gt;**_nonparametric_**<span class="kw">&lt;/span&gt;</span>. </span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If $\mathcal P$ is neither parametric, nor non-parametric, it is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_semiparametric_**<span class="kw">&lt;/span&gt;</span>. This means that some components of $\Thet$ have finite dimension, while others have infinite dimension. </span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a>For the classic parameterization of the normal distribution, $\mu\in\R$ and $\sigma^2 \in \R^+$, where $\dim(\R) = \dim(\R^+) = 1$. For the exponential distribution, $\lambda \in \R^+$, where $\dim(\R) \in \R$. Where things get subjective is when we deal with models that are not regular (model values $P\in\mathcal P$ consist of entire sets of distributions), as we could take one of our parameters to be an infinite space of distributions.</span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a>::: {#exm-iidmean}</span>
<span id="cb15-164"><a href="#cb15-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a><span class="fu">## IID Sample Parameterized by Mean</span></span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1,\ldots, X_n)$ is a sample such that $X_i \iid F_X(x)$ and $\E{X_i} = \mu$ for all $i$. Originally, we defined an element $P_\mu \in \mathcal P$ to be a collection of distributions. We could instead reparameterize this model such that it is a regular model. Parameterize $\mathcal P$ with the mapping $(\mu, f_X)\to P_{(\mu, f_X)}$ where $\E{X_i}=\mu$ and $f_X$ is the common density function of all $X_i$. Now $P_{\mu, f_X}\in\mathcal P$ corresponds to the density that gives rise to the iid random sample ${\X}$, along with the mean $\mu$ of that density. The model is now semiparametric, as $\mu\in\R$ where $\dim(\R) = 1$, and $f_X$ is an element of an infinite dimensional space of functions. We could also parameterize this model only simply with the mapping $f_X \mapsto P_{f_X}$, as the parameter $\mu$ is implicitly given by $f_X$:</span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a>$$ \mu = \int_{\mathcal X} xf_X(x)\ dx.$$ In this case, the model is nonparametric.</span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a>In the last example, we reparameterized @exm-iidmean by introducing the common density of the $X_i$, $f_X(x)$, as its own parameter. This made a model that was not regular (each element $P_\mu\in\mathcal P$ was an infinite collection of densities), regular and semiparametric (the space of all densities $f_X(x)$ with mean $\mu$ has infinite dimension). In general if we have a model $\mathcal P$ of the form </span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a>\mathcal P &amp;= <span class="sc">\{</span>P_{\thet} \mid \thet \in \Thet<span class="sc">\}</span>, <span class="sc">\\</span></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a>P_{\thet} &amp; = <span class="sc">\{</span>f_{\X}({\X}) \mid f_{\X}({\X})\text{ satisfies some condition which may involve }\thet <span class="sc">\}</span>,</span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-178"><a href="#cb15-178" aria-hidden="true" tabindex="-1"></a>we can write reparameterize it as a regular semiparametric model where $(\thet, f_{\X}) \mapsto P_{(\thet, f_{\X})}$, taking the density $f_{\X}({\X})$ to be its own parameter. The difference between these two does not amount to much practice, but it can be a tad confusing when one author considers a model parametric while another considers it semiparametric. This is particularly relevant in econometrics, because many models do not specify the entire distribution (ex: the data is normally distributed), instead opting for weaker assumptions related to the distribution (ex: the distribution of the data is symmetric and centered at zero).</span>
<span id="cb15-179"><a href="#cb15-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Statistics and Estimators</span></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a>Given a realizations of a random vector ${\X}$, we can calculate statistics </span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a>A <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_statistic_**<span class="kw">&lt;/span&gt;</span> $T$ is a function $T:{\mathcal X} \to \mathcal T$, where $\mathcal T$ is some space of values.</span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-189"><a href="#cb15-189" aria-hidden="true" tabindex="-1"></a>In almost every case we are interested in $\mathcal T = \mathbb R^k$ for some $k$. Most of the descriptive statistics we think of (sample mean, sample variance, median, mode, etc.) are all statistics which take on values in $\mathbb R$. Because statistics are nothing more than functions of random variables, they themselves are random variables. This fact is paramount for a specific type of statistic.</span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-191"><a href="#cb15-191" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb15-192"><a href="#cb15-192" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} \sim P_{\theta_0}$, where $P_{\theta_0}\in \mathcal P$ for a known $\mathcal P$. An <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_estimator_**&lt;/span&gt; $\hat{\thet}$ is a statistic which maps the sample space ${\mathcal X}$ to the parameter space $\Theta$, whose goal is to estimate the &lt;span style="color:red"&gt;**_estimand_**&lt;/span&gt; $\thet_0$.^[An estimand could also be a function of the true underlying parameter $\thet_0$.] A realization of an estimator, $\hat{\theta}({\X})$ is an &lt;span style="color:red"&gt;**_estimate_**<span class="kw">&lt;/span&gt;</span>.</span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a>The goal of an estimator is to "guess" the true value $P_{\thet_0}\in\mathcal P$ which generated the data, and do so by "guessing" the corresponding parameter $\thet_0$. To emphasize the fact that $\thet_0$ is the true value we have added a subscript 0, a practice that will be employed when. All this notation can be a bit confusing at first, so it helps to work through it with a very familiar setting.</span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a>Suppose for a random vector ${\X} = (X_1,\ldots, X_n)$ that $X_i\sim N(\mu_0,\sigma_0^2)$. This is equivalent to ${\X}$ being distributed according to a multivariate normal distribution $N(\boldsymbol\mu,\Sig)$, where $\boldsymbol\mu_i = \mu$ for $i=1,\ldots,n$, and $\Sig$ is a diagonal matrix comprised entirely of $\sigma^2$. The sample space of our random vector is $\mathbb R^n$.^<span class="co">[</span><span class="ot">Okay, it actually isn't $\mathbb R^n$. It's sigma-algebra generated by the Borel sets of $\mathbb R^n$, but measure theory is not our concern at the moment.</span><span class="co">]</span> If $\varphi(x)$ denotes the standard normal distribution, then </span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a>$$\mathcal P = \left<span class="sc">\{</span> \prod_{i=1}^n \frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right)\  \biggr\vert\  (\mu,\sigma)\in \mathbb R\times\mathbb R^+\right<span class="sc">\}</span>.$$ In order to estimate $\thet_0 = (\mu_0,\sigma_0)$, we define the following estimator:</span>
<span id="cb15-200"><a href="#cb15-200" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-201"><a href="#cb15-201" aria-hidden="true" tabindex="-1"></a>\hat{\thet}({\X}) &amp; = (\hat\theta_1({\X}), \hat\theta_2({\X}))<span class="sc">\\</span></span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a>\hat\theta_1({\X}) &amp; = \frac{1}{n}\sum_{i=1}^n X_i<span class="sc">\\</span> </span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a>\hat\theta_2({\X}) &amp; = \left<span class="co">[</span><span class="ot">\frac{1}{n-1}\sum_{i=1}^n (X_i - \hat\theta_1({\X}))^2\right</span><span class="co">]</span>^{1/2}</span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a>This is of course how we've been estimating $(\mu_0,\sigma_0)$ since we were in high school -- using the sample mean and sample standard deviation! These estimators are so important that they have their own special notation, $\bar X$ and $S({\X})$.</span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a>::: {#exm-unident}</span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>Let's modify the previous problem by changing our parameterization of $\mathcal P$. Instead of labeling each normal distribution with $(\mu,\sigma)$, let's instead use $(\alpha, \beta, \sigma)$ where $\alpha + \beta$ replaces $\mu$. That is, each $X_i$ is distributed according to $N(\alpha_0 + \beta_0, \sigma_0^2)$. Is it possible to estimate all three parameters? It seems that we can still estimate $\sigma_0$ with no issues, but the same cannot be said for $\alpha_0$ and $\beta_0$. What happens if we attempt to use the sample mean $\bar X$ and find that $\bar x = 3$? The best we can do is use this information to estimate $\alpha_0 + \beta_0$, but it's impossible to distinguish the individual values of each parameter, as there are infinite pairs of $(\alpha, \beta)$ which sum to 3. We could have $\alpha_0 = 0$ and $\beta_0 = 3$, or $\alpha_0 = 1$ and $\beta_0 = 2$, or $\alpha_0 = -412$ and $\beta_0 = 415$, etc. Even if we had an infinite amount of data, this problem would persist! </span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a>This last example highlights what will be one of the central topics in econometric theory, and it arose in how our model was parameterized. The catalyst of the problem we faced in estimating $\alpha_0$ and $\beta_0$, is that we did a poor job at "labeling" the model $\mathcal P$ with parameters. To be precise, the "labels" given to elements of $\mathcal P$ by the parameterization were not unique. If our goal is uncovering the true $P_{\thet_0}\in \mathcal P$ via estimating $\thet_0$, we need to make sure that there is a one-to-one relationship between $\thet$ and $P_{\thet}$.</span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a>::: {#def-ident}</span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a>A parameterization $\thet \mapsto P_{\thet}$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_identifiable_**<span class="kw">&lt;/span&gt;</span> if it is an injective mapping. That is $$P_{\thet} \neq P_{\thet'} \implies \thet \neq \thet',$$ which is equivalent to </span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a>$$ \thet = \thet'\implies P_{\thet} = P_{\thet'}.$$ We say a parameter $\theta_j$ (a component of the vector $\thet$) is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_identified_**<span class="kw">&lt;/span&gt;</span> if the $j$-th component of the parameterization  $\thet \mapsto P_{\thet}$ is injective.  </span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-218"><a href="#cb15-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-219"><a href="#cb15-219" aria-hidden="true" tabindex="-1"></a>The parameterization in the previous problem is not identifiable, as multiple vectors of parameters $\thet$ map to a single element of $\mathcal P$. In situations like this, estimation is a nonstarter! We always need to make sure our model (and it's accompanying parameterization) is identified before we can even attempt to estimate any parameters.</span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a>It's often necessary to adopt assumptions in order to add additional structure to a model, facilitating identification. For example, if we revisit \@exm-unident, and assumed that $\alpha = 2\beta$, then we can identify $\alpha$ and $\beta$. We were able to estimate $\alpha + \beta$ using $\bar X$, and combining this with the additional assumption of $\alpha = 2\beta$ gives $\hat\beta = \bar X/3$ and $\hat\alpha = \bar X/3$. A hallmark of econometrics is determining which assumptions are required to identify parameters of interest, and if those assumptions are reasonable and consistent with economic theory and the observed behavior of economic agents.</span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a><span class="fu">## IID Sample, Identifying the Mean</span></span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-227"><a href="#cb15-227" aria-hidden="true" tabindex="-1"></a>What parameters are and are not identified when our model generated data according to $X_i\iid F_X(x)$ where $\E{X_i} = \mu$? This depends on how we parameterize $\mathcal P$.</span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Define a regular model $\mathcal P$ such that each element $P_\mu$ is a distribution with expectation $\mu$. $$ \mathcal{P} = \left\{f_{X}(x) \mid \E{X} = \mu \right\}$$ With the parameterization $\mu\mapsto P_\mu$, the model is not identified, as any $\mu$ maps to an infinite number of distributions with an expectation of $\mu$. For example, $\mu = 5$ maps to $\text{Uni}(0,10)$, $\text{Uni}(4,6)$, $N(5,1)$, $\text{Exp}(1/5)$, and every other density such that $\int xf_X(x)\ dx = 5$.   </span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>But do we really care about the precise distribution of $X_i$, or are we just interested in estimating the mean $\mu$? If we only want to estimate the mean $\mu$, then we can define our model such that $P_\mu \in \mathcal P$ is itself the entire collection of density with mean $\mu$.</span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a>\mathcal P &amp;= <span class="sc">\{</span>P_\mu \mid \mu \in \R<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a>P_\mu &amp; = \left\{f_{\X}({\X})\ \bigg| \ \int xf_{\X}({\X})\ dx = \mu \right<span class="sc">\}</span></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a>This model is not regular, but it is identified. </span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We could define redefine our identified model as a a semiparametric model by taking $f_X$ to be its own parameter, and using the parameterization $(\mu, f_X)\mapsto P_{(\mu, f_X)}$. In this case the model is still identified, as $(\mu, f_X)$ uniquely determine an element of $\mathcal P$.    </span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a>So if the models from 2 and 3 are both identified, which one do we pick? As discussed earlier, it doesn't really matter in practice, *but* strictly speaking model 2 may be the better choice. The goal of estimating some parameter $\thet$ is to "guess" $P_{\thet}$. If $P_{\thet}$ does not specify a distribution which generates the data, then $P_{\thet}$ should include all the possible distributions that could have generated the data.</span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## Unbiasedness</span></span>
<span id="cb15-242"><a href="#cb15-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a>Before estimating $\thet$, we need to determine which estimator to use. How do we assess the quality of an estimator *ex-ante* (prior to calculating an estimate for observed data ${\X}$)? For the sake of simplicity, assume that $\Theta = \mathbb R$. Ideally, an estimator $\hat \theta$ will be "close" to the estimand $\theta$, which may look like $\hat\theta({\X}) -\theta\approx 0$ for a realization of a random vector ${\X}$,^[We could also use $\abs{\hat\theta({\X}) -\theta}\approx 0$, but this measure is unsigned. It may be useful to know whether or not an estimator is overestimating or underestimating $\theta$] but there is no way to know ${\X}$. We instead will calculate this discrepancy *in expectation*.</span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a>::: {#def-bias}</span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_bias_**<span class="kw">&lt;/span&gt;</span> of an estimator $\hat\theta : {\mathcal X} \to \mathbb R$ is defined as </span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a>$$\text{Bias}(\hat\theta, \theta) = \text{Bias}(\hat\theta) = \E{\hat\theta - \theta} = \E{\hat\theta} - \theta .$$ An estimator is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_unbiased_**<span class="kw">&lt;/span&gt;</span> if $\text{Bias}(\hat\theta) = 0$, which is equivalent to $\E{\hat\theta} = \theta$. This is readily extended to an estimator of a vector of estimands, $\hat{\thet({\X})}$. </span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a>An unbiased estimator is, *on average*, correct. </span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with mean $\mu$ ($\E{X_i}=\mu$ for all $i$). The estimator $$\hat\theta(X) = \frac{1}{n}\sum_{i=1}^n X_i$$ is an unbiased estimator for $\mu$. </span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a>\E{\hat\theta} &amp; = \E{\frac{1}{n}\sum_{i=1}^n X_i} <span class="sc">\\</span></span>
<span id="cb15-257"><a href="#cb15-257" aria-hidden="true" tabindex="-1"></a>              &amp; = \frac{1}{n}\sum_{i=1}^n\E{X_i}  &amp; (\text{Expectation is linear}) <span class="sc">\\</span></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a>              &amp; = \frac{1}{n}\sum_{i=1}^n\mu &amp; (\E{X_i}=\mu) <span class="sc">\\</span></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a>              &amp; = \frac{1}{n}(n\mu)<span class="sc">\\</span></span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a>              &amp; = \mu</span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a>::: {#exm-var}</span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with variance $\sigma^2$ and mean $\mu$. If the sample mean is an unbiased estimator of the mean, then perhaps the sample analogue of the variance, $$ \hat\theta(X) = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 ,$$ is unbiased as well? Recalling that $\var{\bar X} = \sigma^2/m$</span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a>\E{\hat\theta} &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2} <span class="sc">\\</span></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a>              &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \bar X + (\mu - \mu))^2}<span class="sc">\\</span></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a>              &amp; = \E{\frac{1}{n}\sum_{i=1}^n ((X_i - \mu) - (\bar X - \mu))^2} <span class="sc">\\</span></span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a>              &amp; = \E{\frac{1}{n}\sum_{i=1}^n ((X_i - \mu)^2 - 2(X_i - \mu)(\bar X - \mu) + (\bar X - \mu)^2)} <span class="sc">\\</span></span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a>              &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\sum_{i=1}^n (X_i - \mu) + (\bar X - \mu)^2\frac{1}{n}\sum_{i=1}^n 1} <span class="sc">\\</span></span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a>               &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\left<span class="co">[</span><span class="ot">\sum_{i=1}^n X_i - \sum_{i=1}^n \mu\right</span><span class="co">]</span> + (\bar X - \mu)^2(1/n)n}<span class="sc">\\</span></span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a>               &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)(n\bar X - n\mu) + (\bar X - \mu)^2} <span class="sc">\\</span></span>
<span id="cb15-274"><a href="#cb15-274" aria-hidden="true" tabindex="-1"></a>                &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)n(\bar X - \mu) + (\bar X - \mu)^2} <span class="sc">\\</span></span>
<span id="cb15-275"><a href="#cb15-275" aria-hidden="true" tabindex="-1"></a>                &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - 2(\bar X - \mu)^2 + (\bar X - \mu)^2)}<span class="sc">\\</span></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a>                &amp; = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar X - \mu)^2 }<span class="sc">\\</span></span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a>                &amp; = \frac{1}{n}\E{\sum_{i=1}^n (X_i - \mu)^2} -  \E{(\bar X - \mu)^2 } <span class="sc">\\</span> </span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a>                &amp; = \frac{1}{n}\sum_{i=1}^n\var{X_i} - \frac{\sigma^2}{n}<span class="sc">\\</span></span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a>                &amp; =  \frac{1}{n}(n\sigma^2) - \frac{\sigma^2}{n}<span class="sc">\\</span></span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a>                &amp; = \sigma^2 - \frac{\sigma^2}{n}<span class="sc">\\</span></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a>                &amp; = \frac{n-1}{n}\sigma^2<span class="sc">\\</span></span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a>                &amp; \neq \sigma^2</span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a>Our estimator is biased! We can "modify" $\hat\theta(X)$ in order to correct it's bias. Define a second estimator $\hat\theta^*$ as $$ \hat\theta^*({\X}) = \frac{n}{n-1}\hat\theta(X) = \frac{n}{n-1}\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.$$ This correction gives an unbiased estimator for $\sigma^2$:</span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a>$$\E{\hat\theta^*} = \E{\frac{n}{n-1}\hat\theta} =  \frac{n}{n-1}\E{\hat\theta}= \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2. $$</span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a>This correction is known as **_Bessel's correction_**, and its effect is easily demonstrated with a Monte Carlo simulation. Suppose $X_i \sim N(0,1)$ for $i=1,\ldots, 20$. Let's calculate $\hat\theta$ and $\hat\theta^*$ for our sample, repeat this for one million simulations, and look the sample mean of each estimator over the one million simulations.^[These sample means are themselves estimators of $\E{\hat\theta}$ and $\E{\hat\theta^*}$, but one million simulations is so many that our estimates will be very good. Why this works is a discussion for the next section on asymptotics.]</span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a>For $n = 20$, we have</span>
<span id="cb15-288"><a href="#cb15-288" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-289"><a href="#cb15-289" aria-hidden="true" tabindex="-1"></a>\E{\hat\theta} &amp; = \frac{n-1}{n}\sigma^2 = \frac{20-1}{20}= 0.95, <span class="sc">\\</span> </span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a>\E{\hat\theta^*} &amp; = 1,</span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a>so we should see that </span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a>\frac{1}{1000000}\sum_{k}\hat\theta &amp; \approx 0.95, <span class="sc">\\</span> </span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a>\frac{1}{1000000}\sum_{k}\hat\theta^* &amp; \approx  1,</span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a>where simulations are indexed by $k$.</span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-299"><a href="#cb15-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-302"><a href="#cb15-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-303"><a href="#cb15-303" aria-hidden="true" tabindex="-1"></a><span class="co">#Define estimators (including sample mean)</span></span>
<span id="cb15-304"><a href="#cb15-304" aria-hidden="true" tabindex="-1"></a>x_bar <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb15-305"><a href="#cb15-305" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(x)<span class="sc">/</span><span class="fu">length</span>(x)</span>
<span id="cb15-306"><a href="#cb15-306" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-307"><a href="#cb15-307" aria-hidden="true" tabindex="-1"></a>theta_biased <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb15-308"><a href="#cb15-308" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">x_bar</span>(x))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(x)</span>
<span id="cb15-309"><a href="#cb15-309" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-310"><a href="#cb15-310" aria-hidden="true" tabindex="-1"></a>theta_unbiased <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb15-311"><a href="#cb15-311" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">x_bar</span>(x))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(x) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb15-312"><a href="#cb15-312" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-313"><a href="#cb15-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-314"><a href="#cb15-314" aria-hidden="true" tabindex="-1"></a><span class="co">#Create vectors to store estimates from each simulation</span></span>
<span id="cb15-315"><a href="#cb15-315" aria-hidden="true" tabindex="-1"></a>N_sim <span class="ot">&lt;-</span> <span class="fl">1e6</span></span>
<span id="cb15-316"><a href="#cb15-316" aria-hidden="true" tabindex="-1"></a>unbiased_estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb15-317"><a href="#cb15-317" aria-hidden="true" tabindex="-1"></a>biased_estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb15-318"><a href="#cb15-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-319"><a href="#cb15-319" aria-hidden="true" tabindex="-1"></a><span class="co">#Perform 100,000 simulations (R sticklers will be angry I didn't use apply())</span></span>
<span id="cb15-320"><a href="#cb15-320" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb15-321"><a href="#cb15-321" aria-hidden="true" tabindex="-1"></a>  <span class="co">#draw 20 observations from N(0,1)</span></span>
<span id="cb15-322"><a href="#cb15-322" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb15-323"><a href="#cb15-323" aria-hidden="true" tabindex="-1"></a>  biased_estimates[k] <span class="ot">&lt;-</span> <span class="fu">theta_biased</span>(X)</span>
<span id="cb15-324"><a href="#cb15-324" aria-hidden="true" tabindex="-1"></a>  unbiased_estimates[k] <span class="ot">&lt;-</span> <span class="fu">theta_unbiased</span>(X)</span>
<span id="cb15-325"><a href="#cb15-325" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-326"><a href="#cb15-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-327"><a href="#cb15-327" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">x_bar</span>(biased_estimates), <span class="fu">x_bar</span>(unbiased_estimates))</span>
<span id="cb15-328"><a href="#cb15-328" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-329"><a href="#cb15-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-330"><a href="#cb15-330" aria-hidden="true" tabindex="-1"></a>Alternatively, we can calculate the bias of each estimator:</span>
<span id="cb15-331"><a href="#cb15-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-334"><a href="#cb15-334" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-335"><a href="#cb15-335" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">c</span>(<span class="fu">x_bar</span>(biased_estimates), <span class="fu">x_bar</span>(unbiased_estimates))</span>
<span id="cb15-336"><a href="#cb15-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-337"><a href="#cb15-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-338"><a href="#cb15-338" aria-hidden="true" tabindex="-1"></a>It is also is informative to plot the density of our estimates, in effect illustrating the probability distributions of the estimators $\hat\theta(X)$ and $\hat\theta^*(X)$</span>
<span id="cb15-339"><a href="#cb15-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-342"><a href="#cb15-342" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-343"><a href="#cb15-343" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb15-344"><a href="#cb15-344" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot11</span></span>
<span id="cb15-345"><a href="#cb15-345" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb15-346"><a href="#cb15-346" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb15-347"><a href="#cb15-347" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb15-348"><a href="#cb15-348" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of estimators for the population variance"</span></span>
<span id="cb15-349"><a href="#cb15-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb15-350"><a href="#cb15-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-351"><a href="#cb15-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-352"><a href="#cb15-352" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb15-353"><a href="#cb15-353" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimate =</span> <span class="fu">c</span>(biased_estimates, unbiased_estimates),</span>
<span id="cb15-354"><a href="#cb15-354" aria-hidden="true" tabindex="-1"></a>  <span class="at">Estimator =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Biased"</span>, N_sim), <span class="fu">rep</span>(<span class="st">"Unbiased"</span>, N_sim))</span>
<span id="cb15-355"><a href="#cb15-355" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb15-356"><a href="#cb15-356" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb15-357"><a href="#cb15-357" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb15-358"><a href="#cb15-358" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb15-359"><a href="#cb15-359" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb15-360"><a href="#cb15-360" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb15-361"><a href="#cb15-361" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) <span class="sc">+</span></span>
<span id="cb15-362"><a href="#cb15-362" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(biased_estimates), <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span> </span>
<span id="cb15-363"><a href="#cb15-363" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">mean</span>(unbiased_estimates), <span class="at">size =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) </span>
<span id="cb15-364"><a href="#cb15-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-365"><a href="#cb15-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-366"><a href="#cb15-366" aria-hidden="true" tabindex="-1"></a>Because $\hat\theta^*$ is unbiased, it is the most frequent way of calculating the variance of a random variable using a sample, and is often notated by $S^2$. While informative, this doesn't really pin down *why* the uncorrected estimator is biased. The bias arises from the fact that $\hat\theta({\X})$ is a function of another estimator -- $\bar X$. In the event we knew $\mu$ and didn't need to estimate it with $\bar X$, then $\frac{1}{n}\sum_{i=1}^n (X_i -\mu)^2$ is actually unbiased. It's the "intermediate" estimation of $\mu$ via $\bar X$, as the discrepancy between $\bar X$ and $\mu$ factors in when taking the sum of squared deviations. Assuming we know $\mu$, we can estimate $\sigma$  as </span>
<span id="cb15-367"><a href="#cb15-367" aria-hidden="true" tabindex="-1"></a>$$\hat\sigma({\X}) = \frac{1}{n}\sum_{i=1}^n <span class="co">[</span><span class="ot">(X_i - \mu)</span><span class="co">]</span>^2 = \frac{1}{n}\sum_{i=1}^n <span class="co">[</span><span class="ot">(X_i - \bar X) + (\bar X - \mu)</span><span class="co">]</span>^2,$$ which highlights that $X_i$'s deviation from the sample mean can be written as the sum of the discrepancy between $\mu$ and $\bar X$'s discrepancy from $\mu$. If we expand it we have </span>
<span id="cb15-368"><a href="#cb15-368" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-369"><a href="#cb15-369" aria-hidden="true" tabindex="-1"></a>\hat\sigma^2({\X}) &amp;= \frac{1}{n}\sum_{i=1}^n <span class="co">[</span><span class="ot">(X_i - \bar X)^2 + 2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb15-370"><a href="#cb15-370" aria-hidden="true" tabindex="-1"></a>              &amp; = \frac{1}{n}\sum_{i=1}^n (X_i - \bar X)^2 + \frac{1}{n}\sum_{i=1}^n <span class="co">[</span><span class="ot">2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb15-371"><a href="#cb15-371" aria-hidden="true" tabindex="-1"></a>              &amp; = \hat\theta({\X}) + \frac{1}{n}\sum_{i=1}^n <span class="co">[</span><span class="ot">2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2</span><span class="co">]</span></span>
<span id="cb15-372"><a href="#cb15-372" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-373"><a href="#cb15-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-374"><a href="#cb15-374" aria-hidden="true" tabindex="-1"></a>If we try to use $\hat\theta({\X})$, we don't capture the second term, and are underestimating the variance.</span>
<span id="cb15-375"><a href="#cb15-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-376"><a href="#cb15-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Relative Efficiency </span></span>
<span id="cb15-377"><a href="#cb15-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-378"><a href="#cb15-378" aria-hidden="true" tabindex="-1"></a>While unbiasedness is important, it isn't the end all be all when selecting an estimator. The following pathological example will highlight another concern.</span>
<span id="cb15-379"><a href="#cb15-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-380"><a href="#cb15-380" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-381"><a href="#cb15-381" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with mean $\mu$. Let's define two estimators for $\mu$:</span>
<span id="cb15-382"><a href="#cb15-382" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-383"><a href="#cb15-383" aria-hidden="true" tabindex="-1"></a>\hat\theta_1(X) &amp; = \bar X = \frac{1}{n}\sum_{i=1}^nX_i,<span class="sc">\\</span></span>
<span id="cb15-384"><a href="#cb15-384" aria-hidden="true" tabindex="-1"></a>\hat\theta_2(X) &amp; = \sum_{i=1}^n\frac{1}{2^i}X_i.</span>
<span id="cb15-385"><a href="#cb15-385" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-386"><a href="#cb15-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-387"><a href="#cb15-387" aria-hidden="true" tabindex="-1"></a>We're familiar with the unbiased estimator $\hat\theta_2(X)$. The estimator $\hat\theta_2(X)$ is a weighted average, where weights are determined by the geometric series $1/2^i$. This estimator is also unbiased, $$ \E{\hat\theta_2} = \E{\sum_{i=1}^n\frac{1}{2^i}X_i} = \sum_{i=1}^n\frac{1}{2^i}\E{X_i}  = \sum_{i=1}^n\frac{1}{2^i}\mu = \mu \sum_{i=1}^n\frac{1}{2^i} = \mu(1)=\mu.$$ In fact, *any* weighted average where the (normalized) weights sum to one is unbiased. If both estimators are unbiased, how do we determine which is superior? To gain some insight, let's simulate some estimates, assuming $X_i \sim N(0,1)$ for $i = 1,\ldots, 20$.</span>
<span id="cb15-388"><a href="#cb15-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-391"><a href="#cb15-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-392"><a href="#cb15-392" aria-hidden="true" tabindex="-1"></a><span class="co">#define estimator</span></span>
<span id="cb15-393"><a href="#cb15-393" aria-hidden="true" tabindex="-1"></a>weighted_xbar <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb15-394"><a href="#cb15-394" aria-hidden="true" tabindex="-1"></a>  <span class="co">#calculate weights</span></span>
<span id="cb15-395"><a href="#cb15-395" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x)))</span>
<span id="cb15-396"><a href="#cb15-396" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(w<span class="sc">*</span>x)</span>
<span id="cb15-397"><a href="#cb15-397" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-398"><a href="#cb15-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-399"><a href="#cb15-399" aria-hidden="true" tabindex="-1"></a><span class="co">#Create vectors to store estimates from each simulation</span></span>
<span id="cb15-400"><a href="#cb15-400" aria-hidden="true" tabindex="-1"></a>xbar_est <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb15-401"><a href="#cb15-401" aria-hidden="true" tabindex="-1"></a>weighted_xbar_est <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb15-402"><a href="#cb15-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-403"><a href="#cb15-403" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb15-404"><a href="#cb15-404" aria-hidden="true" tabindex="-1"></a>  <span class="co">#draw 20 observations from N(0,1)</span></span>
<span id="cb15-405"><a href="#cb15-405" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>)</span>
<span id="cb15-406"><a href="#cb15-406" aria-hidden="true" tabindex="-1"></a>  xbar_est[k] <span class="ot">&lt;-</span> <span class="fu">x_bar</span>(X)</span>
<span id="cb15-407"><a href="#cb15-407" aria-hidden="true" tabindex="-1"></a>  weighted_xbar_est[k] <span class="ot">&lt;-</span> <span class="fu">weighted_xbar</span>(X)</span>
<span id="cb15-408"><a href="#cb15-408" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-409"><a href="#cb15-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-410"><a href="#cb15-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-411"><a href="#cb15-411" aria-hidden="true" tabindex="-1"></a>If we use the simulated estimates to plot the (approximate) densities of the estimators, one fact sticks out -- $\var{\hat\theta_1} &lt; \var{\hat\theta_2}$.</span>
<span id="cb15-412"><a href="#cb15-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-415"><a href="#cb15-415" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-416"><a href="#cb15-416" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb15-417"><a href="#cb15-417" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot12</span></span>
<span id="cb15-418"><a href="#cb15-418" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb15-419"><a href="#cb15-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb15-420"><a href="#cb15-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb15-421"><a href="#cb15-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of estimators for the population mean"</span></span>
<span id="cb15-422"><a href="#cb15-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb15-423"><a href="#cb15-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-424"><a href="#cb15-424" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb15-425"><a href="#cb15-425" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimate =</span> <span class="fu">c</span>(xbar_est, weighted_xbar_est), </span>
<span id="cb15-426"><a href="#cb15-426" aria-hidden="true" tabindex="-1"></a>  <span class="at">Estimator =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Sample Mean"</span>, N_sim), <span class="fu">rep</span>(<span class="st">"Weighted Sample Mean"</span>, N_sim))</span>
<span id="cb15-427"><a href="#cb15-427" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb15-428"><a href="#cb15-428" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb15-429"><a href="#cb15-429" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb15-430"><a href="#cb15-430" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb15-431"><a href="#cb15-431" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb15-432"><a href="#cb15-432" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Mean, True Value = 0"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb15-433"><a href="#cb15-433" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) </span>
<span id="cb15-434"><a href="#cb15-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-435"><a href="#cb15-435" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-436"><a href="#cb15-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-437"><a href="#cb15-437" aria-hidden="true" tabindex="-1"></a>The variance of an estimator gives us a sense for the dispersion of our estimates, but isn't desirable in and of itself. We can *always* find an estimator with a small variance, and by small variance I mean no variance! Suppose we set our estimator $\hat\theta(X) = 1$. Regardless of the realized sample ${\X}$, we have an estimate of $\hat\theta({\X}) = 1$, and $\var{\hat\theta} = 0$. This approach completely ignores the fact that $1$ may not even be remotely close to $\theta_0$, something addressed by $\text{Bias}(\hat\theta)$. Not only do we want a precise estimator (low variance), but we want an accurate estimator (low bias). The next example shows that there is a balancing act between these two properties.</span>
<span id="cb15-438"><a href="#cb15-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-439"><a href="#cb15-439" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-440"><a href="#cb15-440" aria-hidden="true" tabindex="-1"></a>Return to the problem of estimating $\sigma^2$ using realizations of $X_i\sim N(0,1)$ for $i=1,\ldots,20.$ We've already considered two estimators: </span>
<span id="cb15-441"><a href="#cb15-441" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-442"><a href="#cb15-442" aria-hidden="true" tabindex="-1"></a>S^2({\X}) &amp; = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.<span class="sc">\\</span></span>
<span id="cb15-443"><a href="#cb15-443" aria-hidden="true" tabindex="-1"></a>\hat\theta({\X}) &amp; = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.</span>
<span id="cb15-444"><a href="#cb15-444" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-445"><a href="#cb15-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-446"><a href="#cb15-446" aria-hidden="true" tabindex="-1"></a>If we return to our estimates from @exm-var we can estimate the variance of each estimator using $S^2$. Just for clarification, we are using $S^2(S^2({\X})$ to estimate the variances of the estimators $S^2({\X})$ and $\hat\theta({\X})$. </span>
<span id="cb15-447"><a href="#cb15-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-450"><a href="#cb15-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-451"><a href="#cb15-451" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">var</span>(biased_estimates), <span class="fu">var</span>(unbiased_estimates))</span>
<span id="cb15-452"><a href="#cb15-452" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-453"><a href="#cb15-453" aria-hidden="true" tabindex="-1"></a>It seems that the biased estimator is more efficient than the unbiased estimator. It can be shown that </span>
<span id="cb15-454"><a href="#cb15-454" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-455"><a href="#cb15-455" aria-hidden="true" tabindex="-1"></a>\var{S^2} &amp; = \frac{2\sigma^4}{n-1},<span class="sc">\\</span></span>
<span id="cb15-456"><a href="#cb15-456" aria-hidden="true" tabindex="-1"></a>\var{\hat\theta} &amp; = \frac{2(n-1)\sigma^4}{n^2}</span>
<span id="cb15-457"><a href="#cb15-457" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-458"><a href="#cb15-458" aria-hidden="true" tabindex="-1"></a>which gives</span>
<span id="cb15-459"><a href="#cb15-459" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-460"><a href="#cb15-460" aria-hidden="true" tabindex="-1"></a>&amp; 1  \le \frac{n-1}{n} <span class="sc">\\</span></span>
<span id="cb15-461"><a href="#cb15-461" aria-hidden="true" tabindex="-1"></a>\implies &amp; 1  &lt; \frac{(n-1)^2}{n^2} <span class="sc">\\</span></span>
<span id="cb15-462"><a href="#cb15-462" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{1}{n-1} &lt; \frac{n-1}{n^2} <span class="sc">\\</span></span>
<span id="cb15-463"><a href="#cb15-463" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{2\sigma^4}{n-1} &lt; \frac{2(n-1)\sigma^4}{n^2} <span class="sc">\\</span></span>
<span id="cb15-464"><a href="#cb15-464" aria-hidden="true" tabindex="-1"></a>\implies &amp; \var{S^2} &lt; \var{\hat\theta}.</span>
<span id="cb15-465"><a href="#cb15-465" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-466"><a href="#cb15-466" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-467"><a href="#cb15-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-468"><a href="#cb15-468" aria-hidden="true" tabindex="-1"></a>If bias and variance are at odds, perhaps we can conceive of a third measure of an estimator's performance that supersedes both. We can quantify the **_cost_** of an estimator with a **_loss function_** $l:\Theta\times\Theta \to \mathbb R^+$ which captures how "wrong" an estimate $\hat\theta$ is for an estimand $\theta$. A loss function is a function of an estimator, making it a random variable. The expected loss of an estimator is given by a **_risk function_** $R:\Theta\times\Theta \to \mathbb R^+$,</span>
<span id="cb15-469"><a href="#cb15-469" aria-hidden="true" tabindex="-1"></a>$$R(\hat\theta, \theta) =  \E{l(\hat\theta,\theta)} = \int_{{\mathcal X}} l(\hat\theta(x), \theta)\ dF(x\mid\theta).$$ We've already seen one special case of a risk function in the form of $\text{Bias}(\hat\theta,\theta)$. If we define $l(\hat\theta,\theta) = \hat\theta - \theta$, we have $$ \E{l(\hat\theta,\theta)} = \E{\hat\theta - \theta} = \text{Bias}(\hat\theta,\theta).$$ </span>
<span id="cb15-470"><a href="#cb15-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-471"><a href="#cb15-471" aria-hidden="true" tabindex="-1"></a>One possible issue with the loss function $l(\hat\theta,\theta) = \hat\theta - \theta$ is that it is linear. This means that if $\theta_0 = 0$, the estimate $\hat\theta = 2$ is twice as costly as $\hat\theta =1$. In many situations, we think that a very bad estimate is not much worse than a bad estimate, but *much much* worse. We want a loss function that assigns more weight to poor estimates, capturing the idea that the marginal cost/loss of a underestimating/overestimating $\theta$ is increasing. This is achieved with a quadratic loss function $l(\hat\theta,\theta) = (\hat\theta-\theta)^2$. The risk function which corresponds to this choice of $l(\hat\theta,\theta)$ is so important that it gets its own name.     </span>
<span id="cb15-474"><a href="#cb15-474" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-475"><a href="#cb15-475" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb15-476"><a href="#cb15-476" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot13</span></span>
<span id="cb15-477"><a href="#cb15-477" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb15-478"><a href="#cb15-478" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb15-479"><a href="#cb15-479" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb15-480"><a href="#cb15-480" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Quadratic loss function"</span></span>
<span id="cb15-481"><a href="#cb15-481" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb15-482"><a href="#cb15-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-483"><a href="#cb15-483" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">x =</span> (<span class="sc">-</span><span class="dv">1000</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-484"><a href="#cb15-484" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-485"><a href="#cb15-485" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb15-486"><a href="#cb15-486" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb15-487"><a href="#cb15-487" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_void</span>() <span class="sc">+</span></span>
<span id="cb15-488"><a href="#cb15-488" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb15-489"><a href="#cb15-489" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">size =</span> <span class="fl">0.25</span>) <span class="sc">+</span></span>
<span id="cb15-490"><a href="#cb15-490" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.96</span>, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Estimated θ"</span>) <span class="sc">+</span></span>
<span id="cb15-491"><a href="#cb15-491" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.07</span>, <span class="at">y =</span> <span class="sc">-</span><span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"True θ"</span>) <span class="sc">+</span> </span>
<span id="cb15-492"><a href="#cb15-492" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.75</span>, <span class="at">y =</span> <span class="fl">0.2</span>, <span class="at">label =</span> <span class="st">"Loss(Estimaed θ, True θ)"</span>)</span>
<span id="cb15-493"><a href="#cb15-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-494"><a href="#cb15-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-495"><a href="#cb15-495" aria-hidden="true" tabindex="-1"></a>::: {#def-mse}</span>
<span id="cb15-496"><a href="#cb15-496" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta:{\mathcal X} \to \Theta$ is an estimator for $\theta$. The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_mean squared error (MSE)_**<span class="kw">&lt;/span&gt;</span> of $\hat\theta$ is $$\mse{\hat\theta} = \E{(\hat\theta - \theta)^2}.$$ In the event our estimator is a vector $\hat{\thet} = (\hat\theta_1,\ldots,\hat\theta_k)$, then </span>
<span id="cb15-497"><a href="#cb15-497" aria-hidden="true" tabindex="-1"></a>$$ \mse{\hat{\thet}} = \E{\textstyle\sum_{i=1}^n(\hat\theta_i - \theta_i)^2}.$$</span>
<span id="cb15-498"><a href="#cb15-498" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-499"><a href="#cb15-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-500"><a href="#cb15-500" aria-hidden="true" tabindex="-1"></a>How does this third measure help us? Don't we need to compare bias, variance, and MSE now? Fortunately, we do not, as MSE captures both the variance and bias of an estimator! By expanding $(\hat\theta - \theta)^2$ in the definition of MSE, we have </span>
<span id="cb15-501"><a href="#cb15-501" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-502"><a href="#cb15-502" aria-hidden="true" tabindex="-1"></a>\mse{\hat\theta} &amp; = \E{\hat\theta^2 - 2\hat\theta\theta + \theta^2} <span class="sc">\\</span></span>
<span id="cb15-503"><a href="#cb15-503" aria-hidden="true" tabindex="-1"></a>                 &amp; = \E{\hat\theta^2} - 2\theta\E{\hat\theta} + \E{\theta^2} &amp; (\text{Expectation is linear})<span class="sc">\\</span></span>
<span id="cb15-504"><a href="#cb15-504" aria-hidden="true" tabindex="-1"></a>                 &amp; = \E{\hat\theta^2} - 2\theta\E{\hat\theta} + \E{\theta^2} + \left(\E{\hat\theta}^2 - \E{\hat\theta}^2\right) <span class="sc">\\</span> </span>
<span id="cb15-505"><a href="#cb15-505" aria-hidden="true" tabindex="-1"></a>                 &amp; = \left(\E{\hat\theta^2} - \E{\hat\theta}^2\right) - 2\theta\E{\hat\theta} + \E{\theta^2} + \E{\hat\theta}^2 <span class="sc">\\</span> </span>
<span id="cb15-506"><a href="#cb15-506" aria-hidden="true" tabindex="-1"></a>                 &amp; = \var{\hat\theta} + \left(\E{\hat\theta}^2 - 2\theta\E{\hat\theta} + \theta^2 \right)<span class="sc">\\</span></span>
<span id="cb15-507"><a href="#cb15-507" aria-hidden="true" tabindex="-1"></a>                 &amp; = \var{\hat\theta} + \left(\E{\hat\theta} - \theta\right)^2 <span class="sc">\\</span></span>
<span id="cb15-508"><a href="#cb15-508" aria-hidden="true" tabindex="-1"></a>                 &amp; = \var{\hat\theta} + \text{Bias}(\hat\theta)^2.</span>
<span id="cb15-509"><a href="#cb15-509" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-510"><a href="#cb15-510" aria-hidden="true" tabindex="-1"></a>In a sense, mean square error is an aggregate of variance and bias, where we are more concerned with bias than variance (hence it being squared).  We would rather have a very precise (low variance) estimator that is inaccurate (high bias) then an accurate estimator (low bias) that is imprecise (high variance). What does this mean for our two estimators of $\sigma^2$?  </span>
<span id="cb15-511"><a href="#cb15-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-512"><a href="#cb15-512" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-513"><a href="#cb15-513" aria-hidden="true" tabindex="-1"></a>Once again, define </span>
<span id="cb15-514"><a href="#cb15-514" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-515"><a href="#cb15-515" aria-hidden="true" tabindex="-1"></a>S^2({\X}) &amp; = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.<span class="sc">\\</span></span>
<span id="cb15-516"><a href="#cb15-516" aria-hidden="true" tabindex="-1"></a>\hat\theta({\X}) &amp; = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.</span>
<span id="cb15-517"><a href="#cb15-517" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-518"><a href="#cb15-518" aria-hidden="true" tabindex="-1"></a>The MSE of these estimators are</span>
<span id="cb15-519"><a href="#cb15-519" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-520"><a href="#cb15-520" aria-hidden="true" tabindex="-1"></a>\mse{S^2} &amp;= \var{S^2} + \text{Bias}(S^2)^2 = \frac{2\sigma^4}{n-1} + 0 = \frac{2\sigma^4}{n-1},<span class="sc">\\</span></span>
<span id="cb15-521"><a href="#cb15-521" aria-hidden="true" tabindex="-1"></a>\mse{\hat\theta} &amp;= \var{\hat\theta} + \text{Bias}(\hat\theta)^2 = \frac{2\sigma^4(n-1)}{n-1} + \left(\frac{n-1}{n}\sigma^2 - \sigma^2\right)^2 = \frac{\sigma^4(2n-1)}{n^2}.</span>
<span id="cb15-522"><a href="#cb15-522" aria-hidden="true" tabindex="-1"></a>\end{align*} </span>
<span id="cb15-523"><a href="#cb15-523" aria-hidden="true" tabindex="-1"></a>For all $n \ge 1$ we have, </span>
<span id="cb15-524"><a href="#cb15-524" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-525"><a href="#cb15-525" aria-hidden="true" tabindex="-1"></a>&amp; 3n &gt; 1 <span class="sc">\\</span> </span>
<span id="cb15-526"><a href="#cb15-526" aria-hidden="true" tabindex="-1"></a>\implies &amp; -1 &gt; -3n <span class="sc">\\</span></span>
<span id="cb15-527"><a href="#cb15-527" aria-hidden="true" tabindex="-1"></a>\implies &amp; 0 &gt; 1-3n <span class="sc">\\</span></span>
<span id="cb15-528"><a href="#cb15-528" aria-hidden="true" tabindex="-1"></a>\implies &amp; 2n^2 &gt; 2n^2 - 3n + 1 <span class="sc">\\</span></span>
<span id="cb15-529"><a href="#cb15-529" aria-hidden="true" tabindex="-1"></a>\implies &amp; 2n^2 &gt; (2n-1)(n-1) <span class="sc">\\</span></span>
<span id="cb15-530"><a href="#cb15-530" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{2n^2}{(n-1)n^2} &gt; \frac{(2n-1)(n-1)}{(n-1)n^2}<span class="sc">\\</span></span>
<span id="cb15-531"><a href="#cb15-531" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{2n^2}{n-1} &gt; \frac{(2n-1)}{n^2}<span class="sc">\\</span></span>
<span id="cb15-532"><a href="#cb15-532" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{2\sigma^4n^2}{n-1} &gt; \frac{\sigma^4(2n-1)}{n^2}<span class="sc">\\</span></span>
<span id="cb15-533"><a href="#cb15-533" aria-hidden="true" tabindex="-1"></a>\implies &amp; \mse{S^2} &gt; \mse{\hat\theta}.</span>
<span id="cb15-534"><a href="#cb15-534" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-535"><a href="#cb15-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-536"><a href="#cb15-536" aria-hidden="true" tabindex="-1"></a>If our sole criterion for an estimator is mean-squared error, than we should opt to estimate $\sigma^2$ with the biased estimator.</span>
<span id="cb15-537"><a href="#cb15-537" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-538"><a href="#cb15-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-539"><a href="#cb15-539" aria-hidden="true" tabindex="-1"></a>This example brings attention to an immediate result of Definition @def-mse.</span>
<span id="cb15-540"><a href="#cb15-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-541"><a href="#cb15-541" aria-hidden="true" tabindex="-1"></a>::: {#prp-biasmse}</span>
<span id="cb15-542"><a href="#cb15-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-543"><a href="#cb15-543" aria-hidden="true" tabindex="-1"></a><span class="fu">## MSE of an Unbiased Estimator</span></span>
<span id="cb15-544"><a href="#cb15-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-545"><a href="#cb15-545" aria-hidden="true" tabindex="-1"></a>If an estimator $\hat\theta : {\mathcal X} \to \Theta$ is unbiased, then </span>
<span id="cb15-546"><a href="#cb15-546" aria-hidden="true" tabindex="-1"></a>$$\mse{\hat\theta}=\var{\hat\theta}.$$</span>
<span id="cb15-547"><a href="#cb15-547" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-548"><a href="#cb15-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-549"><a href="#cb15-549" aria-hidden="true" tabindex="-1"></a><span class="fu">## Efficient Estimators, Fisher Information</span></span>
<span id="cb15-550"><a href="#cb15-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-551"><a href="#cb15-551" aria-hidden="true" tabindex="-1"></a>With MSE in mind, we can define what it means to have an "optimal" estimator.</span>
<span id="cb15-552"><a href="#cb15-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-553"><a href="#cb15-553" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb15-554"><a href="#cb15-554" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_efficient_**&lt;/span&gt; if $\mse{\hat\theta} &lt; \mse{\hat\theta'}$ (for all values of $\theta$) for all other estimators $\hat\theta'$.^[In general, efficiency is subject to our choice of loss function, and is by no means restricted to a quadratic loss function.] If $\mse{\hat\theta_1} &lt; \mse{\hat\theta_2}$, we say $\hat\theta_1$ **_is more efficient_** $\hat\theta_2$</span>
<span id="cb15-555"><a href="#cb15-555" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-556"><a href="#cb15-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-557"><a href="#cb15-557" aria-hidden="true" tabindex="-1"></a>Calculating the most efficient estimator can be quite difficult considering how many possible estimators there are, so it's common to look for the most efficient estimator among a smaller class of estimators that satisfy other desirable properties. The most common estimators to discard when finding an estimators are biased estimators. This approach takes unbiasedness as the lowest common denominator, and then picks the most efficient of the unbiased estimators. Note that by Proposition @prp-biasmse, the MSE of all unbiased estimators is simply their respective variances, so by restricting attention to this class of estimators, minimizing the MSE is the same thing as minimizing variance.</span>
<span id="cb15-558"><a href="#cb15-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-559"><a href="#cb15-559" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb15-560"><a href="#cb15-560" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_(uniformly) minimum-variance unbiased estimator (MVUE)_**<span class="kw">&lt;/span&gt;</span> if it is the most efficient estimator among all unbiased estimators.</span>
<span id="cb15-561"><a href="#cb15-561" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-562"><a href="#cb15-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-563"><a href="#cb15-563" aria-hidden="true" tabindex="-1"></a>Even with attention restricted to unbiased estimators, we will need an additional tool to determine if an estimator is efficient. This comes is a key result which bounds the variance of an estimator.</span>
<span id="cb15-564"><a href="#cb15-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-565"><a href="#cb15-565" aria-hidden="true" tabindex="-1"></a>::: {#thm-CRbound}</span>
<span id="cb15-566"><a href="#cb15-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-567"><a href="#cb15-567" aria-hidden="true" tabindex="-1"></a><span class="fu">## Information Inequality</span></span>
<span id="cb15-568"><a href="#cb15-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-569"><a href="#cb15-569" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$ where $\E{\hat\theta}=\psi(\theta)$. If the support of $f_{\X}({\X}\mid\theta)$ does not depend on $\theta$, then </span>
<span id="cb15-570"><a href="#cb15-570" aria-hidden="true" tabindex="-1"></a>$$ \var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{I(\theta)} $$ where $I(\theta)$ is defined as  $$I(\theta) = \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2},$$ and known as the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_Fisher information_**<span class="kw">&lt;/span&gt;</span> of the sample. </span>
<span id="cb15-571"><a href="#cb15-571" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-572"><a href="#cb15-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-573"><a href="#cb15-573" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb15-574"><a href="#cb15-574" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta$ is an estimator such that $\E{\hat\theta}=\psi(\theta)$, and $f_X(x\mid\theta)$ satisfies the aforementioned properties. First, we'll define a function of $f_X(x\mid \theta)$.</span>
<span id="cb15-575"><a href="#cb15-575" aria-hidden="true" tabindex="-1"></a>$$ V(x) = \frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)$$</span>
<span id="cb15-576"><a href="#cb15-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-577"><a href="#cb15-577" aria-hidden="true" tabindex="-1"></a>Apply the chain rule to $V(x)$ gives:</span>
<span id="cb15-578"><a href="#cb15-578" aria-hidden="true" tabindex="-1"></a>$$V(x) = \frac{\partial}{\partial \theta} \log f_{\X}({\X}\mid\theta) = \frac{1}{f_{\X}({\X}\mid\theta)}\left<span class="co">[</span><span class="ot">\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right</span><span class="co">]</span>.$$ Because $V$ depends on the random variable $X$, we can take it's expectation over the sample space ${\mathcal X}$.</span>
<span id="cb15-579"><a href="#cb15-579" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-580"><a href="#cb15-580" aria-hidden="true" tabindex="-1"></a>\E{V} &amp; = \int\frac{1}{f_{\X}({\X}\mid\theta)}\left<span class="co">[</span><span class="ot">\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right</span><span class="co">]</span>\ dF_{\X}({\X}\mid\theta)<span class="sc">\\</span></span>
<span id="cb15-581"><a href="#cb15-581" aria-hidden="true" tabindex="-1"></a> &amp; = \int f_{\X}({\X}\mid\theta) \cdot \frac{1}{f_{\X}({\X}\mid\theta)}\left<span class="co">[</span><span class="ot">\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right</span><span class="co">]</span>\ d{\X}<span class="sc">\\</span></span>
<span id="cb15-582"><a href="#cb15-582" aria-hidden="true" tabindex="-1"></a> &amp; = \int \frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\ d{\X}.</span>
<span id="cb15-583"><a href="#cb15-583" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-584"><a href="#cb15-584" aria-hidden="true" tabindex="-1"></a>This integral is taken over the support of $f_{\X}({\X}\mid\theta)$. We've assumed the support is not a function of $\theta$, so the bounds of integration do not depend on $\theta$ and we can use Liebniz's integral rule to interchange integration and differentiation:</span>
<span id="cb15-585"><a href="#cb15-585" aria-hidden="true" tabindex="-1"></a>$$ \E{V} = \int \frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\ d{\X} = \frac{\partial }{\partial \theta} \underbrace{\int  f_{\X}({\X}\mid\theta)\ d{\X}}_1 = 0.$$ Because $V$ has an expectation of zero, it's variance is the expectation of its square:</span>
<span id="cb15-586"><a href="#cb15-586" aria-hidden="true" tabindex="-1"></a>$$\var{V} = \E{V^2} - \underbrace{\E{V}^2}_0 = \cdot\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid \theta)\right)^2} = I(\theta)$$ {#eq-vvar}</span>
<span id="cb15-587"><a href="#cb15-587" aria-hidden="true" tabindex="-1"></a>Using the fact that $\E{V} = 0$, the covariance between $V$ and $\hat\theta$ is </span>
<span id="cb15-588"><a href="#cb15-588" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-589"><a href="#cb15-589" aria-hidden="true" tabindex="-1"></a>\cov{V, \hat\theta} &amp;= \E{V\hat\theta} -\underbrace{\E{V}}_0\E{\hat\theta}<span class="sc">\\</span></span>
<span id="cb15-590"><a href="#cb15-590" aria-hidden="true" tabindex="-1"></a>&amp; = \E{V\hat\theta}<span class="sc">\\</span></span>
<span id="cb15-591"><a href="#cb15-591" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\frac{1}{f_{\X}({\X}\mid\theta)}\left<span class="co">[</span><span class="ot">\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right</span><span class="co">]</span>\hat\theta} <span class="sc">\\</span> </span>
<span id="cb15-592"><a href="#cb15-592" aria-hidden="true" tabindex="-1"></a>&amp; = \int \frac{1}{f_{\X}({\X}\mid\theta)}\left<span class="co">[</span><span class="ot">\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right</span><span class="co">]</span>\hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) <span class="sc">\\</span></span>
<span id="cb15-593"><a href="#cb15-593" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\partial }{\partial \theta} \int \frac{1}{f_{\X}({\X}\mid\theta)}f_{\X}({\X}\mid\theta)\hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) <span class="sc">\\</span></span>
<span id="cb15-594"><a href="#cb15-594" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\partial }{\partial \theta} \int \hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) <span class="sc">\\</span></span>
<span id="cb15-595"><a href="#cb15-595" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\partial }{\partial \theta} \E{\hat\theta} <span class="sc">\\</span></span>
<span id="cb15-596"><a href="#cb15-596" aria-hidden="true" tabindex="-1"></a>&amp; = \psi'(\theta).</span>
<span id="cb15-597"><a href="#cb15-597" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-598"><a href="#cb15-598" aria-hidden="true" tabindex="-1"></a>Again we were able to interchange differentiation and integration due to the assumption about $f_X(x\mid\theta)$'s support. In the context of probability, the famed Cauchy-Schwarz inequality takes the form </span>
<span id="cb15-599"><a href="#cb15-599" aria-hidden="true" tabindex="-1"></a>$$ \abs{{\cov{V,\hat\theta}}^2} \le \var{V}\var{\hat\theta}.$$</span>
<span id="cb15-600"><a href="#cb15-600" aria-hidden="true" tabindex="-1"></a>Using this along with the fact that $\cov{V, \hat\theta} = \psi'(\theta)$ and @eq-vvar, gives:</span>
<span id="cb15-601"><a href="#cb15-601" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-602"><a href="#cb15-602" aria-hidden="true" tabindex="-1"></a>&amp; \abs{{\cov{V,\hat\theta}}^2} \le \var{V}\var{\hat\theta} <span class="sc">\\</span></span>
<span id="cb15-603"><a href="#cb15-603" aria-hidden="true" tabindex="-1"></a>\implies &amp; \abs{\psi'(\theta)}^2 \le I(\theta)\var{\hat\theta} <span class="sc">\\</span> </span>
<span id="cb15-604"><a href="#cb15-604" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\abs{\psi'(\theta)}^2}{ I(\theta)} \le \var{\hat\theta} </span>
<span id="cb15-605"><a href="#cb15-605" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-606"><a href="#cb15-606" aria-hidden="true" tabindex="-1"></a>This is the desired inequality</span>
<span id="cb15-607"><a href="#cb15-607" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-608"><a href="#cb15-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-609"><a href="#cb15-609" aria-hidden="true" tabindex="-1"></a>This bound on the variance of an estimator is often called the **_Cramér–Rao (CR) lower bound_**. Theorem @thm-CRbound presents this bound in more generality than is often required. We can simplify it in three cases:</span>
<span id="cb15-610"><a href="#cb15-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-611"><a href="#cb15-611" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\hat\theta$ is unbiased, allowing us to eliminate reference to $\psi'(\theta)$.</span>
<span id="cb15-612"><a href="#cb15-612" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The random vector ${\X}$ is comprised of iid random variables $X_i$, allowing us to write the Fisher information in terms to common marginal density $f_X(x\mid\theta)$.</span>
<span id="cb15-613"><a href="#cb15-613" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If the second derivative of $\log f_{\X}({\X}\mid \theta)$ with respect to $\theta$ exists, we can write the fisher information with respect to this second derivative.</span>
<span id="cb15-614"><a href="#cb15-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-615"><a href="#cb15-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-616"><a href="#cb15-616" aria-hidden="true" tabindex="-1"></a>::: {#cor-}</span>
<span id="cb15-617"><a href="#cb15-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-618"><a href="#cb15-618" aria-hidden="true" tabindex="-1"></a><span class="fu">## Information Inequality for Unbiased Estimators</span></span>
<span id="cb15-619"><a href="#cb15-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-620"><a href="#cb15-620" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an unbiased estimator for $\theta$. If the support of $f_X(x\mid\theta)$ does not depend on $\theta$, then </span>
<span id="cb15-621"><a href="#cb15-621" aria-hidden="true" tabindex="-1"></a>$$ \var{\hat\theta} \ge I(\theta)^{-1}.$$</span>
<span id="cb15-622"><a href="#cb15-622" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-623"><a href="#cb15-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-624"><a href="#cb15-624" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb15-625"><a href="#cb15-625" aria-hidden="true" tabindex="-1"></a>If $\hat\theta$ is unbiased, then $\E{\hat\theta} = \theta$, and $\psi'(\theta) = 1$.</span>
<span id="cb15-626"><a href="#cb15-626" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-627"><a href="#cb15-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-628"><a href="#cb15-628" aria-hidden="true" tabindex="-1"></a>::: {#cor-}</span>
<span id="cb15-629"><a href="#cb15-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-630"><a href="#cb15-630" aria-hidden="true" tabindex="-1"></a><span class="fu">## Information Inequality for IID Samples</span></span>
<span id="cb15-631"><a href="#cb15-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-632"><a href="#cb15-632" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$, and $X_i \overset{iid}{\sim} P_{\thet}a$ for all $i=1,\ldots,n$. If the support of $f_X(x\mid\theta)$ does not depend on $\theta$, then </span>
<span id="cb15-633"><a href="#cb15-633" aria-hidden="true" tabindex="-1"></a>$$\var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2}}$$ where $f_X(x\mid\theta)$ is the distribution shared by all $X_i$. </span>
<span id="cb15-634"><a href="#cb15-634" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-635"><a href="#cb15-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-636"><a href="#cb15-636" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb15-637"><a href="#cb15-637" aria-hidden="true" tabindex="-1"></a>The joint distribution $f_{\X}({\X}\mid\theta)$ can be written as the product of the $n$ identical marginal distributions.</span>
<span id="cb15-638"><a href="#cb15-638" aria-hidden="true" tabindex="-1"></a>$$f_{\X}({\X}\mid\theta) = \prod_{i=1}^nf_X(x_i\mid\theta)$$</span>
<span id="cb15-639"><a href="#cb15-639" aria-hidden="true" tabindex="-1"></a>If we use this equality we can write the Fisher information as,</span>
<span id="cb15-640"><a href="#cb15-640" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-641"><a href="#cb15-641" aria-hidden="true" tabindex="-1"></a>I(\theta) &amp;= \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}<span class="sc">\\</span></span>
<span id="cb15-642"><a href="#cb15-642" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\left(\frac{\partial}{\partial \theta}\log \prod_{i=1}^nf_X(x_i\mid\theta)\right)^2} <span class="sc">\\</span></span>
<span id="cb15-643"><a href="#cb15-643" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\left(\frac{\partial}{\partial \theta}\sum_{i=1}^n\log f_X(x_i\mid\theta)\right)^2} &amp; (\text{log properties})<span class="sc">\\</span></span>
<span id="cb15-644"><a href="#cb15-644" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\left(\sum_{i=1}^n\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} &amp; (\text{linearity of derivative})<span class="sc">\\</span></span>
<span id="cb15-645"><a href="#cb15-645" aria-hidden="true" tabindex="-1"></a>&amp; = \E{\sum_{i=1}^n\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2 + \sum_{i\neq j}\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)} &amp; (\text{expand square})<span class="sc">\\</span></span>
<span id="cb15-646"><a href="#cb15-646" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} + \sum_{i\neq j}\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)}}&amp; (\text{expectation is linear})<span class="sc">\\</span></span>
<span id="cb15-647"><a href="#cb15-647" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} + \sum_{i\neq j}\underbrace{\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}}}_0\underbrace{\E{\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)}}_0&amp; (X_i\perp X_j) <span class="sc">\\</span> </span>
<span id="cb15-648"><a href="#cb15-648" aria-hidden="true" tabindex="-1"></a>&amp; = n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} &amp; (\text{identical distributions}).</span>
<span id="cb15-649"><a href="#cb15-649" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-650"><a href="#cb15-650" aria-hidden="true" tabindex="-1"></a>The fact that $\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}} = 0$ follows from the same argument we used to conclude $\E{V} = 0$ in the proof of Theorem @thm-CRbound.</span>
<span id="cb15-651"><a href="#cb15-651" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-652"><a href="#cb15-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-653"><a href="#cb15-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-654"><a href="#cb15-654" aria-hidden="true" tabindex="-1"></a>::: {#cor-crb2nd}</span>
<span id="cb15-655"><a href="#cb15-655" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$ where $\E{\hat\theta}=\psi(\theta)$. If the support of $f_{\X}({\X}\mid\theta)$ does not depend on $\theta$, and $\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X} \mid \theta)$ exists, then </span>
<span id="cb15-656"><a href="#cb15-656" aria-hidden="true" tabindex="-1"></a>$$\var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$$ </span>
<span id="cb15-657"><a href="#cb15-657" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-658"><a href="#cb15-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-659"><a href="#cb15-659" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb15-660"><a href="#cb15-660" aria-hidden="true" tabindex="-1"></a>Recall that in the proof of @thm-CRbound we established $\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}} = 0$. Using this, along with the Liebniz's rule and the assumption about the support of $f_{\X}({\X}\mid\theta)$ we have </span>
<span id="cb15-661"><a href="#cb15-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-662"><a href="#cb15-662" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-663"><a href="#cb15-663" aria-hidden="true" tabindex="-1"></a>&amp; \E{{\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)}} = 0<span class="sc">\\</span></span>
<span id="cb15-664"><a href="#cb15-664" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial}{\partial \theta}\E{{\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)}} = 0 <span class="sc">\\</span></span>
<span id="cb15-665"><a href="#cb15-665" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial}{\partial \theta}\int \frac{1}{f_{\X}({\X}\mid\theta)}\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\ dF_{\X}({\X}\mid\theta) = 0<span class="sc">\\</span></span>
<span id="cb15-666"><a href="#cb15-666" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial}{\partial \theta}\int \frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\ d{\X} = 0<span class="sc">\\</span></span>
<span id="cb15-667"><a href="#cb15-667" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int \frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)\ d{\X} = 0<span class="sc">\\</span></span>
<span id="cb15-668"><a href="#cb15-668" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}f_{\X}({\X}\mid\theta)\ d{\X} = 0<span class="sc">\\</span></span>
<span id="cb15-669"><a href="#cb15-669" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\ dF_{\X}({\X}\mid\theta) = 0 <span class="sc">\\</span></span>
<span id="cb15-670"><a href="#cb15-670" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}} = 0</span>
<span id="cb15-671"><a href="#cb15-671" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-672"><a href="#cb15-672" aria-hidden="true" tabindex="-1"></a>This equality allows us to conclude $-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)} = \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}$.</span>
<span id="cb15-673"><a href="#cb15-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-674"><a href="#cb15-674" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-675"><a href="#cb15-675" aria-hidden="true" tabindex="-1"></a>-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)} &amp; = -\E{\frac{\partial}{\partial \theta}\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)} <span class="sc">\\</span></span>
<span id="cb15-676"><a href="#cb15-676" aria-hidden="true" tabindex="-1"></a>  &amp; = -\E{\frac{\partial}{\partial \theta}\left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)}&amp; (\text{chain rule})<span class="sc">\\</span></span>
<span id="cb15-677"><a href="#cb15-677" aria-hidden="true" tabindex="-1"></a>  &amp; = -\E{\frac{f_{\X}({\X}\mid\theta)\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta) - \frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)^2}}&amp; (\text{quotient rule})<span class="sc">\\</span></span>
<span id="cb15-678"><a href="#cb15-678" aria-hidden="true" tabindex="-1"></a>  &amp; =-\E{\frac{\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)} - \left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)^2 }<span class="sc">\\</span></span>
<span id="cb15-679"><a href="#cb15-679" aria-hidden="true" tabindex="-1"></a>  &amp; = -\underbrace{\E{\frac{\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}}}_0 + \E{\left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)^2 } &amp; (\text{expectation is linear}) <span class="sc">\\</span></span>
<span id="cb15-680"><a href="#cb15-680" aria-hidden="true" tabindex="-1"></a>  &amp; =  \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2 } </span>
<span id="cb15-681"><a href="#cb15-681" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-682"><a href="#cb15-682" aria-hidden="true" tabindex="-1"></a>If we apply the information inequality, the result follows.</span>
<span id="cb15-683"><a href="#cb15-683" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-684"><a href="#cb15-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-685"><a href="#cb15-685" aria-hidden="true" tabindex="-1"></a>These three corollaries can make the Cramér–Rao lower bound a headache because it takes so many forms. The following table helps us make sense of the various cases, assuming that $\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X} \mid \theta)$ exists.</span>
<span id="cb15-686"><a href="#cb15-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-687"><a href="#cb15-687" aria-hidden="true" tabindex="-1"></a>|                                   | $\hat\theta$ is unbiased | $\E{\hat\theta} = \psi(\theta)$ |</span>
<span id="cb15-688"><a href="#cb15-688" aria-hidden="true" tabindex="-1"></a>|---------------|--------------------------|---------------------------------|</span>
<span id="cb15-689"><a href="#cb15-689" aria-hidden="true" tabindex="-1"></a>| $X_i \overset{iid}{\sim}{P_{\thet}}$     | $\var{\hat\theta} \le \frac{1}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{1}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}$                     | $\var{\hat\theta} \le \frac{\abs{\psi'(\theta)}^2}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{\abs{\psi'(\theta)}^2}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}$                            |</span>
<span id="cb15-690"><a href="#cb15-690" aria-hidden="true" tabindex="-1"></a>| $X_i {\overset{iid}{\not\sim}}{P_{\thet}}$ |  $\var{\hat\theta} \le \frac{1}{\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}} = -\frac{1}{\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$                          |  $\var{\hat\theta} \le \frac{\abs{\psi'(\theta)}^2}{\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}} = -\frac{\abs{\psi'(\theta)}^2}{\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$                                |</span>
<span id="cb15-691"><a href="#cb15-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-692"><a href="#cb15-692" aria-hidden="true" tabindex="-1"></a>Because we're often concerned with finding a MVUE, we will rarely be concerned with the case where $\E<span class="co">[</span><span class="ot">\hat\theta</span><span class="co">]</span>$. Additionally, the assumption that the random sample is used to calculate estimator is exceedingly common. For these two reasons, the Cramér–Rao lower bound will almost always take the form </span>
<span id="cb15-693"><a href="#cb15-693" aria-hidden="true" tabindex="-1"></a>$$\var{\hat\theta} \le \frac{1}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{1}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}.$$</span>
<span id="cb15-694"><a href="#cb15-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-695"><a href="#cb15-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-696"><a href="#cb15-696" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-697"><a href="#cb15-697" aria-hidden="true" tabindex="-1"></a>Suppose $X_i\overset{iid}{\sim}\text{Binom}(k,p)$, recalling $$f_X(x\mid p) = \binom{k}{p}p^x(1-p)^{k-x}$$ where $x$ is the number of realized successes of $k$ binomial trials. This random variable has an expected value of $kp$ and variance of $kp(1-p)$. Our goal is to estimate the probability of success $p$. If we observe one sequence of $k$ trials ($n=1$) A natural estimator for this is ratio of successes to trials, $\hat p = X/k$. This estimator is unbiased:</span>
<span id="cb15-698"><a href="#cb15-698" aria-hidden="true" tabindex="-1"></a>$$\E{\hat p} = \frac{\E{X}}{k} = \frac{kp}{k} = p.$$</span>
<span id="cb15-699"><a href="#cb15-699" aria-hidden="true" tabindex="-1"></a>The variance of the estimator is:</span>
<span id="cb15-700"><a href="#cb15-700" aria-hidden="true" tabindex="-1"></a>$$\var{\hat p} = \var{\frac{X}{k}} = \frac{\var{X}}{k^2} = \frac{kp(1-p)}{k^2} = \frac{p(1-p)}{k}.$$</span>
<span id="cb15-701"><a href="#cb15-701" aria-hidden="true" tabindex="-1"></a>Is $\hat p$ an MVUE? In order to determine this, we must calculate the Cramér–Rao lower bound.</span>
<span id="cb15-702"><a href="#cb15-702" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-703"><a href="#cb15-703" aria-hidden="true" tabindex="-1"></a>&amp; \log f_X(x\mid p) = \log \binom{k}{x} + x\log p + (k-x)\log (1-p)<span class="sc">\\</span></span>
<span id="cb15-704"><a href="#cb15-704" aria-hidden="true" tabindex="-1"></a>\implies &amp;  \frac{\partial}{\partial p}  \log f_X(x\mid p) = \frac{x-kp}{p(1-p)}<span class="sc">\\</span></span>
<span id="cb15-705"><a href="#cb15-705" aria-hidden="true" tabindex="-1"></a>\implies &amp; \left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 = \frac{(x-kp)^2}{p^2(1-p)^2}<span class="sc">\\</span></span>
<span id="cb15-706"><a href="#cb15-706" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 } = \frac{\E{(x-kp)^2}}{p^2(1-p)^2} = \frac{\var{X}}{p^2(1-p)^2} =  \frac{kp(1-p)}{p^2(1-p)^2} = \frac{k}{p(1-p)}<span class="sc">\\</span> </span>
<span id="cb15-707"><a href="#cb15-707" aria-hidden="true" tabindex="-1"></a>\implies &amp; n\E{\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 } = 1\cdot \frac{k}{p(1-p)} = \frac{k}{p(1-p)}<span class="sc">\\</span></span>
<span id="cb15-708"><a href="#cb15-708" aria-hidden="true" tabindex="-1"></a>\implies &amp; \var{\hat p} \le \frac{p(1-p)}{k}.</span>
<span id="cb15-709"><a href="#cb15-709" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-710"><a href="#cb15-710" aria-hidden="true" tabindex="-1"></a>We have that $\hat p =X/k$ is the MVUE. We could have calculated this bound using the second derivative of $\log f_X(x\mid\theta)$:</span>
<span id="cb15-711"><a href="#cb15-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-712"><a href="#cb15-712" aria-hidden="true" tabindex="-1"></a>$$ \frac{\partial^2}{\partial p^2}  \log f_X(x\mid p)= \frac{\partial}{\partial p}\left<span class="co">[</span><span class="ot">\frac{x-kp}{p(1-p)}\right</span><span class="co">]</span>=-\frac{x}{p^2}-\frac{m-x}{(1-p)^2}=-\frac{(x-mp)^2}{p^2(1-p^2)} $$</span>
<span id="cb15-713"><a href="#cb15-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-714"><a href="#cb15-714" aria-hidden="true" tabindex="-1"></a>It's important to remember that this means $\hat p$ is the most efficient estimator among unbiased estimators. It is entirely possible that there exists a biased estimator for $p$ that has a lower MSE than $\hat p$ for certain parameter values of $(k,p)$. Take for instance the estimator $\hat p'$, </span>
<span id="cb15-715"><a href="#cb15-715" aria-hidden="true" tabindex="-1"></a>$$\hat p' = \frac{X+1}{k+2}.$$</span>
<span id="cb15-716"><a href="#cb15-716" aria-hidden="true" tabindex="-1"></a>For this estimator, we have </span>
<span id="cb15-717"><a href="#cb15-717" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-718"><a href="#cb15-718" aria-hidden="true" tabindex="-1"></a>\text{Bias}(\hat p ')&amp; = \E{\hat p '} - p = \frac{\E{X} + 1}{k + 2} - p = \frac{kp + 1}{k + 2} - p = \frac{1-2p}{m+2}<span class="sc">\\</span></span>
<span id="cb15-719"><a href="#cb15-719" aria-hidden="true" tabindex="-1"></a>\var{\hat p'} &amp;= \var{\frac{X + 1}{k+2}} = \frac{\var{X+1}}{(k+2)^2}= \frac{kp(1-p)}{(k+2)^2}<span class="sc">\\</span> </span>
<span id="cb15-720"><a href="#cb15-720" aria-hidden="true" tabindex="-1"></a>\mse{\hat p '}&amp; = \var{\hat p '} + \text{Bias}(\hat p')^2 = \frac{1 + (k-4)p - (k-4)p^2}{(k+2)^2}.</span>
<span id="cb15-721"><a href="#cb15-721" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-722"><a href="#cb15-722" aria-hidden="true" tabindex="-1"></a>We can graph $\mse{\hat p} = \var{\hat p}$ along with $\mse{\hat p'}$ for various values of $k$.</span>
<span id="cb15-723"><a href="#cb15-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-726"><a href="#cb15-726" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-727"><a href="#cb15-727" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb15-728"><a href="#cb15-728" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot14</span></span>
<span id="cb15-729"><a href="#cb15-729" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb15-730"><a href="#cb15-730" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb15-731"><a href="#cb15-731" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb15-732"><a href="#cb15-732" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of estimates of variance for an unbiased estimator and a biased estimator"</span></span>
<span id="cb15-733"><a href="#cb15-733" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb15-734"><a href="#cb15-734" aria-hidden="true" tabindex="-1"></a><span class="co">#| warnings: false</span></span>
<span id="cb15-735"><a href="#cb15-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-736"><a href="#cb15-736" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">100</span><span class="sc">/</span><span class="dv">100</span></span>
<span id="cb15-737"><a href="#cb15-737" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>, <span class="dv">20</span>)</span>
<span id="cb15-738"><a href="#cb15-738" aria-hidden="true" tabindex="-1"></a>est <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Biased"</span>, <span class="st">"MVUE"</span>)</span>
<span id="cb15-739"><a href="#cb15-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-740"><a href="#cb15-740" aria-hidden="true" tabindex="-1"></a><span class="fu">expand.grid</span>(p, k, est) <span class="sc">%&gt;%</span> </span>
<span id="cb15-741"><a href="#cb15-741" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(</span>
<span id="cb15-742"><a href="#cb15-742" aria-hidden="true" tabindex="-1"></a>    <span class="at">p =</span> Var1,</span>
<span id="cb15-743"><a href="#cb15-743" aria-hidden="true" tabindex="-1"></a>    <span class="at">k =</span> Var2,</span>
<span id="cb15-744"><a href="#cb15-744" aria-hidden="true" tabindex="-1"></a>    <span class="at">Estimator =</span> Var3</span>
<span id="cb15-745"><a href="#cb15-745" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb15-746"><a href="#cb15-746" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb15-747"><a href="#cb15-747" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse =</span> <span class="fu">ifelse</span>(Estimator <span class="sc">==</span> <span class="st">"Biased"</span>, (<span class="dv">1</span><span class="sc">+</span>(k<span class="dv">-4</span>)<span class="sc">*</span>p <span class="sc">-</span> (k<span class="dv">-4</span>)<span class="sc">*</span>p<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>((k<span class="sc">+</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>) , p<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">/</span>k)</span>
<span id="cb15-748"><a href="#cb15-748" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb15-749"><a href="#cb15-749" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, mse, <span class="at">color =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb15-750"><a href="#cb15-750" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb15-751"><a href="#cb15-751" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>k) <span class="sc">+</span></span>
<span id="cb15-752"><a href="#cb15-752" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb15-753"><a href="#cb15-753" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb15-754"><a href="#cb15-754" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"True Value of Estimad, p"</span>, <span class="at">y =</span> <span class="st">"Mean Squared Error"</span>) <span class="sc">+</span></span>
<span id="cb15-755"><a href="#cb15-755" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)) </span>
<span id="cb15-756"><a href="#cb15-756" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-757"><a href="#cb15-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-758"><a href="#cb15-758" aria-hidden="true" tabindex="-1"></a>The estimator $\hat p'$ does have a lower MSE than the UMVE at times.</span>
<span id="cb15-759"><a href="#cb15-759" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-760"><a href="#cb15-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-761"><a href="#cb15-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-762"><a href="#cb15-762" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-763"><a href="#cb15-763" aria-hidden="true" tabindex="-1"></a>If we observe realizations of $X_i\overset{iid}{\sim}\text{Uni}(0,\theta)$, then the distribution $f_X(x\mid\theta) = 1/\theta$ (with a support of $<span class="co">[</span><span class="ot">0,\theta</span><span class="co">]</span>$) *does not* satisfy the information inequality's necessary condition -- the support of $f_X(x\mid\theta)$ is a function of the parameter $\theta$. Without this assumption, we can not apply Leibniz's rule in the proof for \@ref(thm:CRbound), and shouldn't expect the result to hold.</span>
<span id="cb15-764"><a href="#cb15-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-765"><a href="#cb15-765" aria-hidden="true" tabindex="-1"></a>An unbiased estimator for $\theta$ is </span>
<span id="cb15-766"><a href="#cb15-766" aria-hidden="true" tabindex="-1"></a>$$\hat\theta({\X})=\frac{n+1}{n}X_{(n)}$$ where $X_{(n)} = \max<span class="sc">\{</span>X_1,\ldots, X_n<span class="sc">\}</span>$ is the maximum order statistic. If the random variable $X_i$ has an upper bound of $\theta$, then using the largest value from the sample to estimate $\theta$ is rather intuitive. The order statistic $X_{(n)}$ has a density function of </span>
<span id="cb15-767"><a href="#cb15-767" aria-hidden="true" tabindex="-1"></a>$$f_{X_{(n)}}(x\mid\theta) = nf_X(x\mid\theta)<span class="co">[</span><span class="ot">F_X(x\mid\theta)</span><span class="co">]</span>^{n-1} = n\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1} = n\frac{x^{n-1}}{\theta^n}.$$</span>
<span id="cb15-768"><a href="#cb15-768" aria-hidden="true" tabindex="-1"></a>The expectation of $X_{(n)}$ is </span>
<span id="cb15-769"><a href="#cb15-769" aria-hidden="true" tabindex="-1"></a>$$\E{X_{(n)}} = \int_0^\theta x\ dF_{X_{(n)}}(x\mid\theta) = \frac{n}{\theta^n} \int_0^\theta x n\frac{x^{n-1}}{\theta^n}\ dx = \frac{n}{n+1} \theta,$$ so it's a biased estimator. We can correct for this bias by multiplying by $X_{(n)}$ by $(n+1)/n$, which gives $\hat\theta({\X})$. The variance of $\hat\theta$ is </span>
<span id="cb15-770"><a href="#cb15-770" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-771"><a href="#cb15-771" aria-hidden="true" tabindex="-1"></a>\var{\hat\theta}&amp;=\var{\frac{n+1}{n}X_{(n)}}<span class="sc">\\</span></span>
<span id="cb15-772"><a href="#cb15-772" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(n+1)^2}{n^2} \left(\E{X_{(n)}^2} - \E{X_{(n)}}^2\right)<span class="sc">\\</span></span>
<span id="cb15-773"><a href="#cb15-773" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(n+1)^2}{n^2} \left(\int_0^\theta x^2\ dF_{X_{(n)}}(x\mid\theta) - \left(\frac{n}{n+1} \theta\right)^2\right)<span class="sc">\\</span></span>
<span id="cb15-774"><a href="#cb15-774" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\theta^2}{n(n+2)},</span>
<span id="cb15-775"><a href="#cb15-775" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-776"><a href="#cb15-776" aria-hidden="true" tabindex="-1"></a>while the Fisher information of the sample is </span>
<span id="cb15-777"><a href="#cb15-777" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-778"><a href="#cb15-778" aria-hidden="true" tabindex="-1"></a>&amp; \log f_X(x\mid \theta) = -\log(\theta)<span class="sc">\\</span></span>
<span id="cb15-779"><a href="#cb15-779" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{\partial}{\partial \theta} \log f_X(x\mid \theta) = -\frac{1}{\theta}<span class="sc">\\</span></span>
<span id="cb15-780"><a href="#cb15-780" aria-hidden="true" tabindex="-1"></a>\implies &amp;\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2 = \frac{1}{\theta^2}<span class="sc">\\</span></span>
<span id="cb15-781"><a href="#cb15-781" aria-hidden="true" tabindex="-1"></a>\implies &amp; \E{\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2} = \frac{1}{\theta^2}<span class="sc">\\</span></span>
<span id="cb15-782"><a href="#cb15-782" aria-hidden="true" tabindex="-1"></a>\implies &amp; n\E{\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2} = \frac{n}{\theta^2}<span class="sc">\\</span></span>
<span id="cb15-783"><a href="#cb15-783" aria-hidden="true" tabindex="-1"></a>\implies &amp; I(\theta) = \frac{n}{\theta^2}.</span>
<span id="cb15-784"><a href="#cb15-784" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-785"><a href="#cb15-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-786"><a href="#cb15-786" aria-hidden="true" tabindex="-1"></a>In this case, $\var{\hat\theta} &lt; I(\theta)^{-1}$.</span>
<span id="cb15-787"><a href="#cb15-787" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-788"><a href="#cb15-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-789"><a href="#cb15-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-790"><a href="#cb15-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-791"><a href="#cb15-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-792"><a href="#cb15-792" aria-hidden="true" tabindex="-1"></a>We can extend \@ref(thm:CRbound) to the case where we estimate multiple parameters. Because our interest will almost always be in unbiased estimators calculated with IID samples, the multiparameter version of the information inequality will be presented in this context. </span>
<span id="cb15-793"><a href="#cb15-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-794"><a href="#cb15-794" aria-hidden="true" tabindex="-1"></a>::: {#thm-}</span>
<span id="cb15-795"><a href="#cb15-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-796"><a href="#cb15-796" aria-hidden="true" tabindex="-1"></a><span class="fu">## Information Inequality for Multiple Parameters</span></span>
<span id="cb15-797"><a href="#cb15-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-798"><a href="#cb15-798" aria-hidden="true" tabindex="-1"></a>Suppose $\hat{\thet} :{\mathcal X} \to \Theta$ is an unbiased estimator for $\thet$, and $X_i \overset{iid}{\sim} P_{\thet}a$ for all $i=1,\ldots,n$. If the support of $f_X(x\mid\thet)$ does not depend on $\thet$, then </span>
<span id="cb15-799"><a href="#cb15-799" aria-hidden="true" tabindex="-1"></a>$$ \var{\hat{\thet}} \ge \mathbf I(\thet)^{-1},$$</span>
<span id="cb15-800"><a href="#cb15-800" aria-hidden="true" tabindex="-1"></a>where $\mathbf I(\thet)$ is known as the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_information matrix_**<span class="kw">&lt;/span&gt;</span> and defined as </span>
<span id="cb15-801"><a href="#cb15-801" aria-hidden="true" tabindex="-1"></a>$$\mathbf I(\thet)_{i,j} = -n \E{\frac{\partial}{\partial \theta_i\partial\theta_j}\log f_X(x\mid\theta)}$$</span>
<span id="cb15-802"><a href="#cb15-802" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb15-803"><a href="#cb15-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-804"><a href="#cb15-804" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-805"><a href="#cb15-805" aria-hidden="true" tabindex="-1"></a>If $X_i\sim N(\mu,\sigma^2)$, we can estimate $\mu$ and $\sigma^2$ jointly with an estimator $\hat{\thet}  = (\hat\mu, \hat\sigma^2)$.</span>
<span id="cb15-806"><a href="#cb15-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-807"><a href="#cb15-807" aria-hidden="true" tabindex="-1"></a>This can be verified by using the information inequality for unbiased estimators and IID samples. In this case,</span>
<span id="cb15-808"><a href="#cb15-808" aria-hidden="true" tabindex="-1"></a>\begin{align*} f_X(x\mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left<span class="co">[</span><span class="ot">-\frac{(x-\mu)^2}{2\sigma^2}\right</span><span class="co">]</span></span>
<span id="cb15-809"><a href="#cb15-809" aria-hidden="true" tabindex="-1"></a>\implies \log f_X(x\mid \mu, \sigma^2) = \log(1) - \frac{1}{2}\log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}. </span>
<span id="cb15-810"><a href="#cb15-810" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-811"><a href="#cb15-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-812"><a href="#cb15-812" aria-hidden="true" tabindex="-1"></a>Differentiating gives:</span>
<span id="cb15-813"><a href="#cb15-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-814"><a href="#cb15-814" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-815"><a href="#cb15-815" aria-hidden="true" tabindex="-1"></a>-n \E{\frac{\partial^2}{\partial \mu^2}\log f_X(x\mid\theta)} &amp;= -n\E{-\sigma^{-2}} = n\sigma^{-2}<span class="sc">\\</span></span>
<span id="cb15-816"><a href="#cb15-816" aria-hidden="true" tabindex="-1"></a>-n \E{\frac{\partial}{\partial \mu\partial\sigma^2}\log f_X(x\mid\theta)} &amp;= -n \E{\frac{\partial}{\partial\sigma^2\partial \mu}\log f_X(x\mid\theta)}=-n\sigma^{-4}\E{x-\mu}=0<span class="sc">\\</span></span>
<span id="cb15-817"><a href="#cb15-817" aria-hidden="true" tabindex="-1"></a>-n \E{\frac{\partial^2}{\partial (\sigma^2)^2}\log f_X(x\mid\theta)} &amp;= -n\E{\sigma^{-4}/2} = n\sigma^{-4}/2<span class="sc">\\</span></span>
<span id="cb15-818"><a href="#cb15-818" aria-hidden="true" tabindex="-1"></a>\mathbf I(\mu, \sigma) &amp; = \begin{bmatrix} n/\sigma^{-2} &amp; 0 <span class="sc">\\</span> 0  &amp; n\sigma^{-4}/2 \end{bmatrix}.</span>
<span id="cb15-819"><a href="#cb15-819" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-820"><a href="#cb15-820" aria-hidden="true" tabindex="-1"></a>The information inequality takes the form </span>
<span id="cb15-821"><a href="#cb15-821" aria-hidden="true" tabindex="-1"></a>$$\var{\hat\mu, \hat\sigma^2} \ge  \begin{bmatrix} \sigma^{2}/n &amp; 0 <span class="sc">\\</span> 0  &amp; 2\sigma^{4}/n \end{bmatrix}.$$</span>
<span id="cb15-822"><a href="#cb15-822" aria-hidden="true" tabindex="-1"></a>Notice that $\var{\hat\mu} \ge \sigma^2/n$. This means that $\hat\mu = \bar X$ is the MVUE, as $\var{\bar X} =  \sigma^2/n$.</span>
<span id="cb15-823"><a href="#cb15-823" aria-hidden="true" tabindex="-1"></a>:::  </span>
<span id="cb15-824"><a href="#cb15-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-825"><a href="#cb15-825" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb15-826"><a href="#cb15-826" aria-hidden="true" tabindex="-1"></a>@thm-CRbound and its resulting corollaries have one main drawback. It is not constructive and provides no guidance as to constructing efficient estimators, and if such an estimator even exists. Addressing this is a matter of introducing a new property of estimators related to data reduction and information (**_sufficiency_**) and the Lehmann–Scheffé theorem (which is related to the perhaps more familiar Rao–Blackwell theorem).</span>
<span id="cb15-827"><a href="#cb15-827" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-828"><a href="#cb15-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-829"><a href="#cb15-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-830"><a href="#cb15-830" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Distribution of an Estimator</span></span>
<span id="cb15-831"><a href="#cb15-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-832"><a href="#cb15-832" aria-hidden="true" tabindex="-1"></a>Finally, we turn to the matter of an estimator's probability distribution. Like any other random variable, estimators have a distribution and density function. Knowing the distribution of an estimator allows us to put a point estimate in a broader context by using an estimator's distribution to calculate the probability of observing the estimate in question. </span>
<span id="cb15-833"><a href="#cb15-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-834"><a href="#cb15-834" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb15-835"><a href="#cb15-835" aria-hidden="true" tabindex="-1"></a>Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a normal distribution $N(\mu, \sigma^2)$. What is the distribution of $\bar X$?  The sum of *independent* normally distributed random variables is normally distributed as follows:</span>
<span id="cb15-836"><a href="#cb15-836" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^n X_i \sim N(u_1 + \mu_2 + \cdots + \mu_n, \sigma_1^2 + \sigma_2^2 + \cdots \sigma_n^2).$$ In our case, each $X_i$ are identically distributed as well, so $\mu_i = \mu$ and $\sigma_i^2 = \sigma^2$ for all $i$.</span>
<span id="cb15-837"><a href="#cb15-837" aria-hidden="true" tabindex="-1"></a>$$ \sum_{i=1}^n X_i\sim N(n\mu, n\sigma^2)$$</span>
<span id="cb15-838"><a href="#cb15-838" aria-hidden="true" tabindex="-1"></a>Finally if we scale the sum by $1/n$, giving $\bar X$, we have </span>
<span id="cb15-839"><a href="#cb15-839" aria-hidden="true" tabindex="-1"></a>$$\bar X\sim N(\mu, \sigma^2/n)$$</span>
<span id="cb15-840"><a href="#cb15-840" aria-hidden="true" tabindex="-1"></a>by the properties of expectation and variance. If we simulate realizations of this estimator, the resulting histogram should betray that $\bar X$ is normally distributed. For these simulations, we will take $n = 100$, $\mu = 0$, and $\sigma^2 = 1$. We should see that </span>
<span id="cb15-841"><a href="#cb15-841" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb15-842"><a href="#cb15-842" aria-hidden="true" tabindex="-1"></a>\E{\bar X} &amp;\approx 0<span class="sc">\\</span> </span>
<span id="cb15-843"><a href="#cb15-843" aria-hidden="true" tabindex="-1"></a>\var{\bar X} &amp;\approx 0.01</span>
<span id="cb15-844"><a href="#cb15-844" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb15-845"><a href="#cb15-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-848"><a href="#cb15-848" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-849"><a href="#cb15-849" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">"numeric"</span>, N_sim)</span>
<span id="cb15-850"><a href="#cb15-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-851"><a href="#cb15-851" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N_sim) {</span>
<span id="cb15-852"><a href="#cb15-852" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb15-853"><a href="#cb15-853" aria-hidden="true" tabindex="-1"></a>  estimates[k] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb15-854"><a href="#cb15-854" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-855"><a href="#cb15-855" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">mean</span>(estimates), <span class="fu">var</span>(estimates))</span>
<span id="cb15-856"><a href="#cb15-856" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-857"><a href="#cb15-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-860"><a href="#cb15-860" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-861"><a href="#cb15-861" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb15-862"><a href="#cb15-862" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot15</span></span>
<span id="cb15-863"><a href="#cb15-863" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb15-864"><a href="#cb15-864" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb15-865"><a href="#cb15-865" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb15-866"><a href="#cb15-866" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of estimates of variance for an unbiased estimator and a biased estimator"</span></span>
<span id="cb15-867"><a href="#cb15-867" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb15-868"><a href="#cb15-868" aria-hidden="true" tabindex="-1"></a><span class="co">#| </span></span>
<span id="cb15-869"><a href="#cb15-869" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(estimates) <span class="sc">%&gt;%</span> </span>
<span id="cb15-870"><a href="#cb15-870" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimates)) <span class="sc">+</span></span>
<span id="cb15-871"><a href="#cb15-871" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">colour =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb15-872"><a href="#cb15-872" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of μ"</span>) <span class="sc">+</span></span>
<span id="cb15-873"><a href="#cb15-873" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb15-874"><a href="#cb15-874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">100</span>)), <span class="at">color =</span> <span class="st">"red"</span>)</span>
<span id="cb15-875"><a href="#cb15-875" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-876"><a href="#cb15-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-877"><a href="#cb15-877" aria-hidden="true" tabindex="-1"></a>Note that for this to hold, it must be the case that $X_i$ are iid normally distributed, which is rather restrictive. Ideally, we will be able to make some statements about estimators' distributions regardless of the underlying distribution which generates the observable data. Fortunately, the next section will equip us with the tools to do this. </span>
<span id="cb15-878"><a href="#cb15-878" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb15-879"><a href="#cb15-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-880"><a href="#cb15-880" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb15-881"><a href="#cb15-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-882"><a href="#cb15-882" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@mccullagh2002statistical</span>
<span id="cb15-883"><a href="#cb15-883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@bickel2015mathematical</span>
<span id="cb15-884"><a href="#cb15-884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@lehmann2006theory</span>
<span id="cb15-885"><a href="#cb15-885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@greene2003econometric, Appendix C</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>