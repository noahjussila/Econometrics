[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanved Econometrics with Examples",
    "section": "",
    "text": "We cannot define probability over the entire power set \\(2^{\\mathcal X}\\), or else we run into some technical problems.↩︎\nAnytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra↩︎\nMore generally, such functions are called measurable.↩︎\nThe converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are many functions which are not Riemann integrable but are Lebesgue integrable.↩︎\nBut is this really new? The Riemann integral is just the limit of a sum.↩︎\nIn general, this type of construction is called the Radon–Nikodym derivative, and its existence is outlined by the Radon–Nikodym theorem.↩︎\nIt won’t save your life in emergency situations.↩︎\nThese are the two standard references used for a PhD-level statistical theory course.↩︎"
  },
  {
    "objectID": "estimators.html#models-and-parameterizations",
    "href": "estimators.html#models-and-parameterizations",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.1 Models and Parameterizations",
    "text": "1.1 Models and Parameterizations\nThe term “estimation” is used so often, that it’s often easy to forget precisely what it means. It’s worth briefly presenting the formal definition of (point) estimation in accordance with statistical theory and formulate the performance of an estimator using tools from decision theory. A comprehensive treatment of the theory of estimation, see Bickel and Doksum (2015) or Lehmann and Casella (1998).\n\nDefinition 1.1 Given a random experiment with sample space \\(\\mathcal X\\) over which a random vector \\({\\mathbf{X}}=(X_1,\\ldots. X_n)\\) is defined, a model \\(\\mathcal P\\) is the collection of probability distributions (or densities), or sets of distributions (or densities), from which \\(\\mathbf{X}\\) may be distributed. We call an element of a model \\(P\\in\\mathcal P\\), model value.\n\nIn order to keep track of model values, we need to introduce labels for model values \\(P\\).\n\nDefinition 1.2 For every element of \\(\\mathcal P\\), we assign a label called a parameter \\(\\boldsymbol{\\theta}\\) from a parameter space \\(\\boldsymbol{\\Theta}= \\Theta_1 \\times\\cdots\\times\\Theta_k\\), denoting the labeled element as \\(P_{\\boldsymbol{\\theta}}\\). The function \\(\\theta \\mapsto P\\) which assigns these labels is a parameter space.\n\nA parameterization enables to write the model as \\(\\mathcal P =\\{P_{\\boldsymbol{\\theta}} \\mid \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\).\n\nExample 1.1 (Normally Distributed Random Variable) Assume that we know a priori that \\(X\\) is a normally distributed random variable (\\(n=1\\) here). The model \\(\\mathcal P\\) is the collection of all normal distributions. How do we label this entirely family of models? The most common way is via the mean and variance of the normal distribution. In this case, \\(\\boldsymbol{\\Theta}= \\mathbb R \\times \\mathbb R^+\\), \\(\\boldsymbol{\\theta}= (\\mu, \\sigma^2)\\), and \\[f_X(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right].\\] By parameterizing \\(\\mathcal P\\), we can now easily refer to each element of \\(\\mathcal P\\) using \\(\\boldsymbol{\\theta}=(\\mu,\\sigma^2)\\).\n\\[\\mathcal P = \\{f_X(x \\mid \\mu, \\sigma^2) \\mid  (\\mu, \\sigma^2) \\in \\mathbb R \\times \\mathbb R^+\\}.\\]\nAlternatively, we could reparameterize the model using the standard deviation instead of the variance, \\(\\boldsymbol{\\theta}= (\\mu, \\sigma)\\). This gives \\[f_X(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right].\\] If we wanted to really be crazy, we could parameterize the model using \\(\\boldsymbol{\\theta}= (\\mu/\\sigma^2, -1/2\\sigma)\\), where \\(\\Theta = \\mathbb R\\times \\mathbb R^-\\).\n\\[\\begin{align*}\nf_X(x \\mid \\boldsymbol{\\theta}) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] \\\\ & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[\\dfrac{x^2}{2\\sigma^2} -\\dfrac{2x\\mu}{2\\sigma^2} + \\dfrac{\\mu^2}{2\\sigma^2}\\right] \\\\\n         & =\\exp\\left[\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)\\right]\\exp\\left[-\\dfrac{x^2}{2\\sigma^2} + \\dfrac{2x\\mu}{2\\sigma^2} - \\dfrac{\\mu^2}{2\\sigma^2}\\right]\\\\\n         & =\\exp\\left[\\log\\left( (2\\pi\\sigma^2)^{-1/2}\\right)\\right]\\exp\\left[-\\dfrac{x^2}{2\\sigma^2} + \\dfrac{2x\\mu}{2\\sigma^2} - \\dfrac{\\mu^2}{2\\sigma^2}\\right] \\\\\n         & = \\exp\\left[-\\frac{1}{2}\\log\\left( 2\\pi\\sigma^2\\right)\\right]\\exp\\left[-\\dfrac{x^2}{2\\sigma^2} + \\dfrac{2x\\mu}{2\\sigma^2} - \\dfrac{\\mu^2}{2\\sigma^2}\\right] \\\\\n         & = \\exp\\left[-\\dfrac{x^2}{2\\sigma^2} + \\dfrac{2x\\mu}{2\\sigma^2} - \\dfrac{\\mu^2}{2\\sigma^2} -\\frac{1}{2}\\log\\left( 2\\pi\\sigma^2\\right)\\right]\\\\\n         & = \\exp\\left[\\dfrac{\\mu}{\\sigma^2}x-\\dfrac{1}{2\\sigma^2}x^2 -\\frac{1}{2}\\log\\left(\\frac{\\mu^2}{\\sigma^2}+\\log\\left( 2\\pi\\sigma^2\\right)\\right)\\right]\\\\\n         & = \\exp\\left[\\theta_1x+\\theta_2x^2 -\\frac{1}{2}\\log\\left(-\\frac{\\theta_1}{2\\theta_2}+\\log\\left( \\pi\\theta_2^{-1}\\right)\\right)\\right]\n    \\end{align*}\\]\n\n\nExample 1.2 (Exponentially Distributed Random Variable) If \\(X\\) is distributed according to an exponential distribution, the most common parameterization has \\(\\Theta = \\mathbb R^+\\), \\(\\theta = \\lambda\\), and each element of \\(\\mathcal P\\) has a corresponding density of \\[ f_X(x\\mid\\lambda) = \\begin{cases}\\lambda e^{-\\lambda x}& x\\ge 0 \\\\ 0 & x<0\\end{cases}.\\] A common alternate parameterization is in terms of \\(\\beta \\in \\mathbb R^+\\) labels elements in \\(\\mathcal P\\) such that \\[ f_X(x\\mid\\beta) = \\begin{cases}\\frac{1}{\\beta} e^{-x/\\beta}& x\\ge 0 \\\\ 0 & x<0\\end{cases}. \\]\n\nThese last two examples were simple because each element \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) is a single probability density/distribution. In this case we write \\({\\mathbf{X}} \\sim P_{\\boldsymbol{\\theta}}\\), and the difference between \\(P_{\\boldsymbol{\\theta}}\\) and \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\boldsymbol{\\theta})\\) is purely a matter of notation. It is the same difference between writing a uniform distribution as \\(\\text{Uni}(a,b)\\) instead of \\(f_X(x\\mid a,b) = 1/(b-a)\\) or \\(F_X(x\\mid a,b)=x/(b-a)\\). We may also write the probability measure associated with the sample space \\({\\mathcal X}\\) and density as \\(P_{\\boldsymbol{\\theta}}\\), so the expectation of \\({\\mathbf{X}}\\) can be written as \\[ \\text{E}\\left[\\mathbf{X}\\right] = \\int_{\\mathcal{X}} {\\mathbf{X}} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\boldsymbol{\\theta})\\ d{\\mathbf{X}} = \\int_{\\mathcal{X}} {\\mathbf{X}} \\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid\\boldsymbol{\\theta}) = \\int_{\\mathcal{X}} {\\mathbf{X}} \\ dP_{\\boldsymbol{\\theta}}.\\]\nIt will often be the case, especially so in econometrics, that an element of \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) is itself a set of distributions defined by some common properties.1\n\nExample 1.3 (IID Sample Parameterized by Mean) Suppose \\({\\mathbf{X}} = (X_1,\\ldots, X_n)\\) is a sample such that \\(X_i \\overset{iid}{\\sim}F_X(x)\\) and \\(\\text{E}\\left[X_i\\right] = \\mu\\) for all \\(i\\). This data generating process constitutes a model parameterized by \\(\\mu\\). An element of \\(\\mathcal P\\) takes the form \\[P_\\mu = \\left\\{f_{\\mathbf{X}}({\\mathbf{X}})\\ \\Bigg|\\ f_{\\mathbf{X}}({\\mathbf{X}}) = \\prod_{i=1}^n f_{X_i}(x) \\text{ and } f_{X_i} = f_{X_j}\\ \\forall i,j \\text{ and }\\text{E}\\left[X_i\\right]=\\mu\\ \\forall i\\right\\}.\\] For example, if we let \\(\\mu = 5\\), then \\(P_5\\in\\mathcal P\\) is the set of all joint distributions such that the elements of \\(\\mathbf{X}\\) are independent and have an expected value of \\(5\\). This is a lot of possible distributions. One possible distribution in this set is \\[f_{\\mathbf{X}}({\\mathbf{X}}\\mid \\mu) = \\prod_{i=1}^n\\frac{1}{2\\mu} = \\frac{1}{(2\\mu)^n},\\] which is the joint density of \\({\\mathbf{X}}\\) when \\(X_i \\overset{iid}{\\sim}\\text{Uni}(0,2\\mu)\\). Because \\({\\mathbf{X}}\\) is comprised of an iid sample, we could also define the elements of \\(\\mathcal P\\) using the marginal density \\(f_{X_i}(x\\mid\\theta)\\), as this density is identical for all \\(X_i\\). \\[ P_\\mu = \\{f_{X_i}(x) \\mid \\text{E}\\left[X_i\\right] = \\mu\\} .\\] What is important here is that each \\(P_\\mu\\) is an infinite collection of densities all with a common properties related to the parameter \\(\\mu\\).\n\nTo differentiate models where \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) is a single distribution opposed to a collection of distributions, we will introduce a new term (that is not standard across any sources).\n\nDefinition 1.3 A model \\(\\mathcal P = \\{P_{\\boldsymbol{\\theta}} \\mid \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\) is a regular model if each \\(P_{\\boldsymbol{\\theta}}\\) is a probability distribution with density \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\boldsymbol{\\theta})\\).\n\nNot only can we consider just how many distributions make up one model value \\(P\\in\\mathcal P\\), but we can also consider the dimension of the parameter space \\(\\boldsymbol{\\Theta}\\).\n\nDefinition 1.4 Suppose a model \\(\\mathcal P\\) is parameterized by a mapping \\(\\boldsymbol{\\theta}\\mapsto P_\\boldsymbol{\\theta}\\) where \\(\\dim(\\boldsymbol{\\Theta}) = k\\).\n\nIf \\(\\dim(\\Theta_j)\\) is finite for all \\(j=1,\\ldots, k\\), then \\(\\mathcal P = \\{P_{\\boldsymbol{\\theta}} \\mid \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\) is parametric.\nIf \\(\\dim(\\Theta_j)\\) is infinite for all \\(j=1,\\ldots, k\\), then \\(\\mathcal P = \\{P_{\\boldsymbol{\\theta}} \\mid \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\) is nonparametric.\nIf \\(\\mathcal P\\) is neither parametric, nor non-parametric, it is semiparametric. This means that some components of \\(\\boldsymbol{\\Theta}\\) have finite dimension, while others have infinite dimension.\n\n\nFor the classic parameterization of the normal distribution, \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma^2 \\in \\mathbb{R}^+\\), where \\(\\dim(\\mathbb{R}) = \\dim(\\mathbb{R}^+) = 1\\). For the exponential distribution, \\(\\lambda \\in \\mathbb{R}^+\\), where \\(\\dim(\\mathbb{R}) \\in \\mathbb{R}\\). Where things get subjective is when we deal with models that are not regular (model values \\(P\\in\\mathcal P\\) consist of entire sets of distributions), as we could take one of our parameters to be an infinite space of distributions.\n\nExample 1.4 (IID Sample Parameterized by Mean) Suppose \\({\\mathbf{X}} = (X_1,\\ldots, X_n)\\) is a sample such that \\(X_i \\overset{iid}{\\sim}F_X(x)\\) and \\(\\text{E}\\left[X_i\\right] = \\mu\\) for all \\(i\\). Originally, we defined an element \\(P_\\mu \\in \\mathcal P\\) to be a collection of distributions. We could instead reparameterize this model such that it is a regular model. Parameterize \\(\\mathcal P\\) with the mapping \\((\\mu, f_X)\\to P_{(\\mu, f_X)}\\) where \\(\\text{E}\\left[X_i\\right]=\\mu\\) and \\(f_X\\) is the common density function of all \\(X_i\\). Now \\(P_{\\mu, f_X}\\in\\mathcal P\\) corresponds to the density that gives rise to the iid random sample \\({\\mathbf{X}}\\), along with the mean \\(\\mu\\) of that density. The model is now semiparametric, as \\(\\mu\\in\\mathbb{R}\\) where \\(\\dim(\\mathbb{R}) = 1\\), and \\(f_X\\) is an element of an infinite dimensional space of functions. We could also parameterize this model only simply with the mapping \\(f_X \\mapsto P_{f_X}\\), as the parameter \\(\\mu\\) is implicitly given by \\(f_X\\): \\[ \\mu = \\int_{\\mathcal X} xf_X(x)\\ dx.\\] In this case, the model is nonparametric.\n\n\nRemark. In the last example, we reparameterized Example 1.4 by introducing the common density of the \\(X_i\\), \\(f_X(x)\\), as its own parameter. This made a model that was not regular (each element \\(P_\\mu\\in\\mathcal P\\) was an infinite collection of densities), regular and semiparametric (the space of all densities \\(f_X(x)\\) with mean \\(\\mu\\) has infinite dimension). In general if we have a model \\(\\mathcal P\\) of the form \\[\\begin{align*}\n\\mathcal P &= \\{P_{\\boldsymbol{\\theta}} \\mid \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}, \\\\\nP_{\\boldsymbol{\\theta}} & = \\{f_{\\mathbf{X}}({\\mathbf{X}}) \\mid f_{\\mathbf{X}}({\\mathbf{X}})\\text{ satisfies some condition which may involve }\\boldsymbol{\\theta}\\},\n\\end{align*}\\] we can write reparameterize it as a regular semiparametric model where \\((\\boldsymbol{\\theta}, f_{\\mathbf{X}}) \\mapsto P_{(\\boldsymbol{\\theta}, f_{\\mathbf{X}})}\\), taking the density \\(f_{\\mathbf{X}}({\\mathbf{X}})\\) to be its own parameter. The difference between these two does not amount to much practice, but it can be a tad confusing when one author considers a model parametric while another considers it semiparametric. This is particularly relevant in econometrics, because many models do not specify the entire distribution (ex: the data is normally distributed), instead opting for weaker assumptions related to the distribution (ex: the distribution of the data is symmetric and centered at zero)."
  },
  {
    "objectID": "estimators.html#statistics-and-estimators",
    "href": "estimators.html#statistics-and-estimators",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.2 Statistics and Estimators",
    "text": "1.2 Statistics and Estimators\nGiven a realizations of a random vector \\({\\mathbf{X}}\\), we can calculate statistics\n\nDefinition 1.5 A statistic \\(T\\) is a function \\(T:{\\mathcal X} \\to \\mathcal T\\), where \\(\\mathcal T\\) is some space of values.\n\nIn almost every case we are interested in \\(\\mathcal T = \\mathbb R^k\\) for some \\(k\\). Most of the descriptive statistics we think of (sample mean, sample variance, median, mode, etc.) are all statistics which take on values in \\(\\mathbb R\\). Because statistics are nothing more than functions of random variables, they themselves are random variables. This fact is paramount for a specific type of statistic.\n\nDefinition 1.6 Suppose \\({\\mathbf{X}} \\sim P_{\\theta_0}\\), where \\(P_{\\theta_0}\\in \\mathcal P\\) for a known \\(\\mathcal P\\). An estimator \\(\\hat{\\boldsymbol{\\theta}}\\) is a statistic which maps the sample space \\({\\mathcal X}\\) to the parameter space \\(\\Theta\\), whose goal is to estimate the estimand \\(\\boldsymbol{\\theta}_0\\).2 A realization of an estimator, \\(\\hat{\\theta}({\\mathbf{X}})\\) is an estimate.\n\nThe goal of an estimator is to “guess” the true value \\(P_{\\boldsymbol{\\theta}_0}\\in\\mathcal P\\) which generated the data, and do so by “guessing” the corresponding parameter \\(\\boldsymbol{\\theta}_0\\). To emphasize the fact that \\(\\boldsymbol{\\theta}_0\\) is the true value we have added a subscript 0, a practice that will be employed when. All this notation can be a bit confusing at first, so it helps to work through it with a very familiar setting.\n\nExample 1.5 Suppose for a random vector \\({\\mathbf{X}} = (X_1,\\ldots, X_n)\\) that \\(X_i\\sim N(\\mu_0,\\sigma_0^2)\\). This is equivalent to \\({\\mathbf{X}}\\) being distributed according to a multivariate normal distribution \\(N(\\boldsymbol\\mu,\\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol\\mu_i = \\mu\\) for \\(i=1,\\ldots,n\\), and \\(\\boldsymbol{\\Sigma}\\) is a diagonal matrix comprised entirely of \\(\\sigma^2\\). The sample space of our random vector is \\(\\mathbb R^n\\).3 If \\(\\varphi(x)\\) denotes the standard normal distribution, then \\[\\mathcal P = \\left\\{ \\prod_{i=1}^n \\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\  \\biggr\\vert\\  (\\mu,\\sigma)\\in \\mathbb R\\times\\mathbb R^+\\right\\}.\\] In order to estimate \\(\\boldsymbol{\\theta}_0 = (\\mu_0,\\sigma_0)\\), we define the following estimator: \\[\\begin{align*}\n\\hat{\\boldsymbol{\\theta}}({\\mathbf{X}}) & = (\\hat\\theta_1({\\mathbf{X}}), \\hat\\theta_2({\\mathbf{X}}))\\\\\n\\hat\\theta_1({\\mathbf{X}}) & = \\frac{1}{n}\\sum_{i=1}^n X_i\\\\\n\\hat\\theta_2({\\mathbf{X}}) & = \\left[\\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\hat\\theta_1({\\mathbf{X}}))^2\\right]^{1/2}\n\\end{align*}\\] This is of course how we’ve been estimating \\((\\mu_0,\\sigma_0)\\) since we were in high school – using the sample mean and sample standard deviation! These estimators are so important that they have their own special notation, \\(\\bar X\\) and \\(S({\\mathbf{X}})\\).\n\n\nExample 1.6 Let’s modify the previous problem by changing our parameterization of \\(\\mathcal P\\). Instead of labeling each normal distribution with \\((\\mu,\\sigma)\\), let’s instead use \\((\\alpha, \\beta, \\sigma)\\) where \\(\\alpha + \\beta\\) replaces \\(\\mu\\). That is, each \\(X_i\\) is distributed according to \\(N(\\alpha_0 + \\beta_0, \\sigma_0^2)\\). Is it possible to estimate all three parameters? It seems that we can still estimate \\(\\sigma_0\\) with no issues, but the same cannot be said for \\(\\alpha_0\\) and \\(\\beta_0\\). What happens if we attempt to use the sample mean \\(\\bar X\\) and find that \\(\\bar x = 3\\)? The best we can do is use this information to estimate \\(\\alpha_0 + \\beta_0\\), but it’s impossible to distinguish the individual values of each parameter, as there are infinite pairs of \\((\\alpha, \\beta)\\) which sum to 3. We could have \\(\\alpha_0 = 0\\) and \\(\\beta_0 = 3\\), or \\(\\alpha_0 = 1\\) and \\(\\beta_0 = 2\\), or \\(\\alpha_0 = -412\\) and \\(\\beta_0 = 415\\), etc. Even if we had an infinite amount of data, this problem would persist!\n\nThis last example highlights what will be one of the central topics in econometric theory, and it arose in how our model was parameterized. The catalyst of the problem we faced in estimating \\(\\alpha_0\\) and \\(\\beta_0\\), is that we did a poor job at “labeling” the model \\(\\mathcal P\\) with parameters. To be precise, the “labels” given to elements of \\(\\mathcal P\\) by the parameterization were not unique. If our goal is uncovering the true \\(P_{\\boldsymbol{\\theta}_0}\\in \\mathcal P\\) via estimating \\(\\boldsymbol{\\theta}_0\\), we need to make sure that there is a one-to-one relationship between \\(\\boldsymbol{\\theta}\\) and \\(P_{\\boldsymbol{\\theta}}\\).\n\nDefinition 1.7 A parameterization \\(\\boldsymbol{\\theta}\\mapsto P_{\\boldsymbol{\\theta}}\\) is identifiable if it is an injective mapping. That is \\[P_{\\boldsymbol{\\theta}} \\neq P_{\\boldsymbol{\\theta}'} \\implies \\boldsymbol{\\theta}\\neq \\boldsymbol{\\theta}',\\] which is equivalent to \\[ \\boldsymbol{\\theta}= \\boldsymbol{\\theta}'\\implies P_{\\boldsymbol{\\theta}} = P_{\\boldsymbol{\\theta}'}.\\] We say a parameter \\(\\theta_j\\) (a component of the vector \\(\\boldsymbol{\\theta}\\)) is identified if the \\(j\\)-th component of the parameterization \\(\\boldsymbol{\\theta}\\mapsto P_{\\boldsymbol{\\theta}}\\) is injective.\n\nThe parameterization in the previous problem is not identifiable, as multiple vectors of parameters \\(\\boldsymbol{\\theta}\\) map to a single element of \\(\\mathcal P\\). In situations like this, estimation is a nonstarter! We always need to make sure our model (and it’s accompanying parameterization) is identified before we can even attempt to estimate any parameters.\nIt’s often necessary to adopt assumptions in order to add additional structure to a model, facilitating identification. For example, if we revisit @exm-unident, and assumed that \\(\\alpha = 2\\beta\\), then we can identify \\(\\alpha\\) and \\(\\beta\\). We were able to estimate \\(\\alpha + \\beta\\) using \\(\\bar X\\), and combining this with the additional assumption of \\(\\alpha = 2\\beta\\) gives \\(\\hat\\beta = \\bar X/3\\) and \\(\\hat\\alpha = \\bar X/3\\). A hallmark of econometrics is determining which assumptions are required to identify parameters of interest, and if those assumptions are reasonable and consistent with economic theory and the observed behavior of economic agents.\n\nExample 1.7 (IID Sample, Identifying the Mean) What parameters are and are not identified when our model generated data according to \\(X_i\\overset{iid}{\\sim}F_X(x)\\) where \\(\\text{E}\\left[X_i\\right] = \\mu\\)? This depends on how we parameterize \\(\\mathcal P\\).\n\nDefine a regular model \\(\\mathcal P\\) such that each element \\(P_\\mu\\) is a distribution with expectation \\(\\mu\\). \\[ \\mathcal{P} = \\left\\{f_{X}(x) \\mid \\text{E}\\left[X\\right] = \\mu \\right\\}\\] With the parameterization \\(\\mu\\mapsto P_\\mu\\), the model is not identified, as any \\(\\mu\\) maps to an infinite number of distributions with an expectation of \\(\\mu\\). For example, \\(\\mu = 5\\) maps to \\(\\text{Uni}(0,10)\\), \\(\\text{Uni}(4,6)\\), \\(N(5,1)\\), \\(\\text{Exp}(1/5)\\), and every other density such that \\(\\int xf_X(x)\\ dx = 5\\).\n\nBut do we really care about the precise distribution of \\(X_i\\), or are we just interested in estimating the mean \\(\\mu\\)? If we only want to estimate the mean \\(\\mu\\), then we can define our model such that \\(P_\\mu \\in \\mathcal P\\) is itself the entire collection of density with mean \\(\\mu\\). \\[\\begin{align*}\n\\mathcal P &= \\{P_\\mu \\mid \\mu \\in \\mathbb{R}\\} \\\\\nP_\\mu & = \\left\\{f_{\\mathbf{X}}({\\mathbf{X}})\\ \\bigg| \\ \\int xf_{\\mathbf{X}}({\\mathbf{X}})\\ dx = \\mu \\right\\}\n\\end{align*}\\] This model is not regular, but it is identified.\nWe could define redefine our identified model as a a semiparametric model by taking \\(f_X\\) to be its own parameter, and using the parameterization \\((\\mu, f_X)\\mapsto P_{(\\mu, f_X)}\\). In this case the model is still identified, as \\((\\mu, f_X)\\) uniquely determine an element of \\(\\mathcal P\\).\n\nSo if the models from 2 and 3 are both identified, which one do we pick? As discussed earlier, it doesn’t really matter in practice, but strictly speaking model 2 may be the better choice. The goal of estimating some parameter \\(\\boldsymbol{\\theta}\\) is to “guess” \\(P_{\\boldsymbol{\\theta}}\\). If \\(P_{\\boldsymbol{\\theta}}\\) does not specify a distribution which generates the data, then \\(P_{\\boldsymbol{\\theta}}\\) should include all the possible distributions that could have generated the data."
  },
  {
    "objectID": "estimators.html#unbiasedness",
    "href": "estimators.html#unbiasedness",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.3 Unbiasedness",
    "text": "1.3 Unbiasedness\nBefore estimating \\(\\boldsymbol{\\theta}\\), we need to determine which estimator to use. How do we assess the quality of an estimator ex-ante (prior to calculating an estimate for observed data \\({\\mathbf{X}}\\))? For the sake of simplicity, assume that \\(\\Theta = \\mathbb R\\). Ideally, an estimator \\(\\hat \\theta\\) will be “close” to the estimand \\(\\theta\\), which may look like \\(\\hat\\theta({\\mathbf{X}}) -\\theta\\approx 0\\) for a realization of a random vector \\({\\mathbf{X}}\\),4 but there is no way to know \\({\\mathbf{X}}\\). We instead will calculate this discrepancy in expectation.\n\nDefinition 1.8 The bias of an estimator \\(\\hat\\theta : {\\mathcal X} \\to \\mathbb R\\) is defined as \\[\\text{Bias}(\\hat\\theta, \\theta) = \\text{Bias}(\\hat\\theta) = \\text{E}\\left[\\hat\\theta - \\theta\\right] = \\text{E}\\left[\\hat\\theta\\right] - \\theta .\\] An estimator is unbiased if \\(\\text{Bias}(\\hat\\theta) = 0\\), which is equivalent to \\(\\text{E}\\left[\\hat\\theta\\right] = \\theta\\). This is readily extended to an estimator of a vector of estimands, \\(\\hat{\\boldsymbol{\\theta}({\\mathbf{X}})}\\).\n\nAn unbiased estimator is, on average, correct.\n\nExample 1.8 Suppose \\({\\mathbf{X}} = (X_1, \\ldots, X_n)\\) is a random sample from a distribution with mean \\(\\mu\\) (\\(\\text{E}\\left[X_i\\right]=\\mu\\) for all \\(i\\)). The estimator \\[\\hat\\theta(X) = \\frac{1}{n}\\sum_{i=1}^n X_i\\] is an unbiased estimator for \\(\\mu\\). \\[\\begin{align*}\n\\text{E}\\left[\\hat\\theta\\right] & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] \\\\\n              & = \\frac{1}{n}\\sum_{i=1}^n\\text{E}\\left[X_i\\right]  & (\\text{Expectation is linear}) \\\\\n              & = \\frac{1}{n}\\sum_{i=1}^n\\mu & (\\text{E}\\left[X_i\\right]=\\mu) \\\\\n              & = \\frac{1}{n}(n\\mu)\\\\\n              & = \\mu\n\\end{align*}\\]\n\n\nExample 1.9 Suppose \\({\\mathbf{X}} = (X_1, \\ldots, X_n)\\) is a random sample from a distribution with variance \\(\\sigma^2\\) and mean \\(\\mu\\). If the sample mean is an unbiased estimator of the mean, then perhaps the sample analogue of the variance, \\[ \\hat\\theta(X) = \\frac{1}{n}\\sum_{i=1}^n (X_i -\\bar X)^2 ,\\] is unbiased as well? Recalling that \\(\\text{Var}\\left(\\bar X\\right) = \\sigma^2/m\\) \\[\\begin{align*}\n\\text{E}\\left[\\hat\\theta\\right] & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i -\\bar X)^2\\right] \\\\\n              & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar X + (\\mu - \\mu))^2\\right]\\\\\n              & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n ((X_i - \\mu) - (\\bar X - \\mu))^2\\right] \\\\\n              & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n ((X_i - \\mu)^2 - 2(X_i - \\mu)(\\bar X - \\mu) + (\\bar X - \\mu)^2)\\right] \\\\\n              & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\frac{2}{n}(\\bar X - \\mu)\\sum_{i=1}^n (X_i - \\mu) + (\\bar X - \\mu)^2\\frac{1}{n}\\sum_{i=1}^n 1\\right] \\\\\n               & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\frac{2}{n}(\\bar X - \\mu)\\left[\\sum_{i=1}^n X_i - \\sum_{i=1}^n \\mu\\right] + (\\bar X - \\mu)^2(1/n)n\\right]\\\\\n               & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\frac{2}{n}(\\bar X - \\mu)(n\\bar X - n\\mu) + (\\bar X - \\mu)^2\\right] \\\\\n                & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - \\frac{2}{n}(\\bar X - \\mu)n(\\bar X - \\mu) + (\\bar X - \\mu)^2\\right] \\\\\n                & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - 2(\\bar X - \\mu)^2 + (\\bar X - \\mu)^2)\\right]\\\\\n                & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n (X_i - \\mu)^2 - (\\bar X - \\mu)^2 \\right]\\\\\n                & = \\frac{1}{n}\\text{E}\\left[\\sum_{i=1}^n (X_i - \\mu)^2\\right] -  \\text{E}\\left[(\\bar X - \\mu)^2 \\right] \\\\\n                & = \\frac{1}{n}\\sum_{i=1}^n\\text{Var}\\left(X_i\\right) - \\frac{\\sigma^2}{n}\\\\\n                & =  \\frac{1}{n}(n\\sigma^2) - \\frac{\\sigma^2}{n}\\\\\n                & = \\sigma^2 - \\frac{\\sigma^2}{n}\\\\\n                & = \\frac{n-1}{n}\\sigma^2\\\\\n                & \\neq \\sigma^2\n\\end{align*}\\] Our estimator is biased! We can “modify” \\(\\hat\\theta(X)\\) in order to correct it’s bias. Define a second estimator \\(\\hat\\theta^*\\) as \\[ \\hat\\theta^*({\\mathbf{X}}) = \\frac{n}{n-1}\\hat\\theta(X) = \\frac{n}{n-1}\\frac{1}{n}\\sum_{i=1}^n (X_i -\\bar X)^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i -\\bar X)^2.\\] This correction gives an unbiased estimator for \\(\\sigma^2\\): \\[\\text{E}\\left[\\hat\\theta^*\\right] = \\text{E}\\left[\\frac{n}{n-1}\\hat\\theta\\right] =  \\frac{n}{n-1}\\text{E}\\left[\\hat\\theta\\right]= \\frac{n}{n-1}\\frac{n-1}{n}\\sigma^2 = \\sigma^2. \\] This correction is known as Bessel’s correction, and its effect is easily demonstrated with a Monte Carlo simulation. Suppose \\(X_i \\sim N(0,1)\\) for \\(i=1,\\ldots, 20\\). Let’s calculate \\(\\hat\\theta\\) and \\(\\hat\\theta^*\\) for our sample, repeat this for one million simulations, and look the sample mean of each estimator over the one million simulations.5 For \\(n = 20\\), we have \\[\\begin{align*}\n\\text{E}\\left[\\hat\\theta\\right] & = \\frac{n-1}{n}\\sigma^2 = \\frac{20-1}{20}= 0.95, \\\\\n\\text{E}\\left[\\hat\\theta^*\\right] & = 1,\n\\end{align*}\\] so we should see that \\[\\begin{align*}\n\\frac{1}{1000000}\\sum_{k}\\hat\\theta & \\approx 0.95, \\\\\n\\frac{1}{1000000}\\sum_{k}\\hat\\theta^* & \\approx  1,\n\\end{align*}\\] where simulations are indexed by \\(k\\).\n\n#Define estimators (including sample mean)\nx_bar <- function(x){\n  sum(x)/length(x)\n}\ntheta_biased <- function(x){\n  sum((x - x_bar(x))^2)/length(x)\n}\ntheta_unbiased <- function(x){\n  sum((x - x_bar(x))^2)/(length(x) - 1)\n}\n\n#Create vectors to store estimates from each simulation\nN_sim <- 1e6\nunbiased_estimates <- vector(\"numeric\", N_sim)\nbiased_estimates <- vector(\"numeric\", N_sim)\n\n#Perform 100,000 simulations (R sticklers will be angry I didn't use apply())\nfor (k in 1:N_sim) {\n  #draw 20 observations from N(0,1)\n  X <- rnorm(20)\n  biased_estimates[k] <- theta_biased(X)\n  unbiased_estimates[k] <- theta_unbiased(X)\n}\n\nc(x_bar(biased_estimates), x_bar(unbiased_estimates))\n\n[1] 0.9498406 0.9998323\n\n\nAlternatively, we can calculate the bias of each estimator:\n\n1 - c(x_bar(biased_estimates), x_bar(unbiased_estimates))\n\n[1] 0.050159362 0.000167749\n\n\nIt is also is informative to plot the density of our estimates, in effect illustrating the probability distributions of the estimators \\(\\hat\\theta(X)\\) and \\(\\hat\\theta^*(X)\\)\n\n\nShow code which generates figure\ntibble(\n  estimate = c(biased_estimates, unbiased_estimates),\n  Estimator = c(rep(\"Biased\", N_sim), rep(\"Unbiased\", N_sim))\n  ) %>% \n  ggplot(aes(estimate, color = Estimator)) +\n  geom_density() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Estimates of Variance, True Value = 1\", y = \"Density\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  geom_vline(xintercept = mean(biased_estimates), size = 0.5, color = \"red\", linetype = \"dashed\") + \n  geom_vline(xintercept = mean(unbiased_estimates), size = 0.5, color = \"blue\", linetype = \"dashed\") \n\n\n\n\n\nFigure 1.1: Distribution of estimators for the population variance\n\n\n\n\nBecause \\(\\hat\\theta^*\\) is unbiased, it is the most frequent way of calculating the variance of a random variable using a sample, and is often notated by \\(S^2\\). While informative, this doesn’t really pin down why the uncorrected estimator is biased. The bias arises from the fact that \\(\\hat\\theta({\\mathbf{X}})\\) is a function of another estimator – \\(\\bar X\\). In the event we knew \\(\\mu\\) and didn’t need to estimate it with \\(\\bar X\\), then \\(\\frac{1}{n}\\sum_{i=1}^n (X_i -\\mu)^2\\) is actually unbiased. It’s the “intermediate” estimation of \\(\\mu\\) via \\(\\bar X\\), as the discrepancy between \\(\\bar X\\) and \\(\\mu\\) factors in when taking the sum of squared deviations. Assuming we know \\(\\mu\\), we can estimate \\(\\sigma\\) as \\[\\hat\\sigma({\\mathbf{X}}) = \\frac{1}{n}\\sum_{i=1}^n [(X_i - \\mu)]^2 = \\frac{1}{n}\\sum_{i=1}^n [(X_i - \\bar X) + (\\bar X - \\mu)]^2,\\] which highlights that \\(X_i\\)’s deviation from the sample mean can be written as the sum of the discrepancy between \\(\\mu\\) and \\(\\bar X\\)’s discrepancy from \\(\\mu\\). If we expand it we have \\[\\begin{align*}\n\\hat\\sigma^2({\\mathbf{X}}) &= \\frac{1}{n}\\sum_{i=1}^n [(X_i - \\bar X)^2 + 2(X_i - \\bar X)(\\bar X - \\mu) +(\\bar X - \\mu)^2] \\\\\n              & = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar X)^2 + \\frac{1}{n}\\sum_{i=1}^n [2(X_i - \\bar X)(\\bar X - \\mu) +(\\bar X - \\mu)^2]\\\\\n              & = \\hat\\theta({\\mathbf{X}}) + \\frac{1}{n}\\sum_{i=1}^n [2(X_i - \\bar X)(\\bar X - \\mu) +(\\bar X - \\mu)^2]\n\\end{align*}\\]\n\nIf we try to use \\(\\hat\\theta({\\mathbf{X}})\\), we don’t capture the second term, and are underestimating the variance."
  },
  {
    "objectID": "estimators.html#relative-efficiency",
    "href": "estimators.html#relative-efficiency",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.4 Relative Efficiency",
    "text": "1.4 Relative Efficiency\nWhile unbiasedness is important, it isn’t the end all be all when selecting an estimator. The following pathological example will highlight another concern.\n\nExample 1.10 Suppose \\({\\mathbf{X}} = (X_1, \\ldots, X_n)\\) is a random sample from a distribution with mean \\(\\mu\\). Let’s define two estimators for \\(\\mu\\): \\[\\begin{align*}\n\\hat\\theta_1(X) & = \\bar X = \\frac{1}{n}\\sum_{i=1}^nX_i,\\\\\n\\hat\\theta_2(X) & = \\sum_{i=1}^n\\frac{1}{2^i}X_i.\n\\end{align*}\\]\nWe’re familiar with the unbiased estimator \\(\\hat\\theta_2(X)\\). The estimator \\(\\hat\\theta_2(X)\\) is a weighted average, where weights are determined by the geometric series \\(1/2^i\\). This estimator is also unbiased, \\[ \\text{E}\\left[\\hat\\theta_2\\right] = \\text{E}\\left[\\sum_{i=1}^n\\frac{1}{2^i}X_i\\right] = \\sum_{i=1}^n\\frac{1}{2^i}\\text{E}\\left[X_i\\right]  = \\sum_{i=1}^n\\frac{1}{2^i}\\mu = \\mu \\sum_{i=1}^n\\frac{1}{2^i} = \\mu(1)=\\mu.\\] In fact, any weighted average where the (normalized) weights sum to one is unbiased. If both estimators are unbiased, how do we determine which is superior? To gain some insight, let’s simulate some estimates, assuming \\(X_i \\sim N(0,1)\\) for \\(i = 1,\\ldots, 20\\).\n\n#define estimator\nweighted_xbar <- function(x){\n  #calculate weights\n  w <- 1/(2^(1:length(x)))\n  sum(w*x)\n}\n\n#Create vectors to store estimates from each simulation\nxbar_est <- vector(\"numeric\", N_sim)\nweighted_xbar_est <- vector(\"numeric\", N_sim)\n\nfor (k in 1:N_sim) {\n  #draw 20 observations from N(0,1)\n  X <- rnorm(20)\n  xbar_est[k] <- x_bar(X)\n  weighted_xbar_est[k] <- weighted_xbar(X)\n}\n\nIf we use the simulated estimates to plot the (approximate) densities of the estimators, one fact sticks out – \\(\\text{Var}\\left(\\hat\\theta_1\\right) < \\text{Var}\\left(\\hat\\theta_2\\right)\\).\n\n\nShow code which generates figure\ntibble(\n  estimate = c(xbar_est, weighted_xbar_est), \n  Estimator = c(rep(\"Sample Mean\", N_sim), rep(\"Weighted Sample Mean\", N_sim))\n  ) %>% \n  ggplot(aes(estimate, color = Estimator)) +\n  geom_density() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Estimates of Mean, True Value = 0\", y = \"Density\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) \n\n\n\n\n\nFigure 1.2: Distribution of estimators for the population mean\n\n\n\n\n\nThe variance of an estimator gives us a sense for the dispersion of our estimates, but isn’t desirable in and of itself. We can always find an estimator with a small variance, and by small variance I mean no variance! Suppose we set our estimator \\(\\hat\\theta(X) = 1\\). Regardless of the realized sample \\({\\mathbf{X}}\\), we have an estimate of \\(\\hat\\theta({\\mathbf{X}}) = 1\\), and \\(\\text{Var}\\left(\\hat\\theta\\right) = 0\\). This approach completely ignores the fact that \\(1\\) may not even be remotely close to \\(\\theta_0\\), something addressed by \\(\\text{Bias}(\\hat\\theta)\\). Not only do we want a precise estimator (low variance), but we want an accurate estimator (low bias). The next example shows that there is a balancing act between these two properties.\n\nExample 1.11 Return to the problem of estimating \\(\\sigma^2\\) using realizations of \\(X_i\\sim N(0,1)\\) for \\(i=1,\\ldots,20.\\) We’ve already considered two estimators: \\[\\begin{align*}\nS^2({\\mathbf{X}}) & = \\frac{1}{n-1}\\sum_{i=1}^n (X_i -\\bar X)^2.\\\\\n\\hat\\theta({\\mathbf{X}}) & = \\frac{1}{n}\\sum_{i=1}^n (X_i -\\bar X)^2.\n\\end{align*}\\]\nIf we return to our estimates from Example 1.9 we can estimate the variance of each estimator using \\(S^2\\). Just for clarification, we are using \\(S^2(S^2({\\mathbf{X}})\\) to estimate the variances of the estimators \\(S^2({\\mathbf{X}})\\) and \\(\\hat\\theta({\\mathbf{X}})\\).\n\nc(var(biased_estimates), var(unbiased_estimates))\n\n[1] 0.09496095 0.10521989\n\n\nIt seems that the biased estimator is more efficient than the unbiased estimator. It can be shown that \\[\\begin{align*}\n\\text{Var}\\left(S^2\\right) & = \\frac{2\\sigma^4}{n-1},\\\\\n\\text{Var}\\left(\\hat\\theta\\right) & = \\frac{2(n-1)\\sigma^4}{n^2}\n\\end{align*}\\] which gives \\[\\begin{align*}\n& 1  \\le \\frac{n-1}{n} \\\\\n\\implies & 1  < \\frac{(n-1)^2}{n^2} \\\\\n\\implies & \\frac{1}{n-1} < \\frac{n-1}{n^2} \\\\\n\\implies & \\frac{2\\sigma^4}{n-1} < \\frac{2(n-1)\\sigma^4}{n^2} \\\\\n\\implies & \\text{Var}\\left(S^2\\right) < \\text{Var}\\left(\\hat\\theta\\right).\n\\end{align*}\\]\n\nIf bias and variance are at odds, perhaps we can conceive of a third measure of an estimator’s performance that supersedes both. We can quantify the cost of an estimator with a loss function \\(l:\\Theta\\times\\Theta \\to \\mathbb R^+\\) which captures how “wrong” an estimate \\(\\hat\\theta\\) is for an estimand \\(\\theta\\). A loss function is a function of an estimator, making it a random variable. The expected loss of an estimator is given by a risk function \\(R:\\Theta\\times\\Theta \\to \\mathbb R^+\\), \\[R(\\hat\\theta, \\theta) =  \\text{E}\\left[l(\\hat\\theta,\\theta)\\right] = \\int_{{\\mathcal X}} l(\\hat\\theta(x), \\theta)\\ dF(x\\mid\\theta).\\] We’ve already seen one special case of a risk function in the form of \\(\\text{Bias}(\\hat\\theta,\\theta)\\). If we define \\(l(\\hat\\theta,\\theta) = \\hat\\theta - \\theta\\), we have \\[ \\text{E}\\left[l(\\hat\\theta,\\theta)\\right] = \\text{E}\\left[\\hat\\theta - \\theta\\right] = \\text{Bias}(\\hat\\theta,\\theta).\\]\nOne possible issue with the loss function \\(l(\\hat\\theta,\\theta) = \\hat\\theta - \\theta\\) is that it is linear. This means that if \\(\\theta_0 = 0\\), the estimate \\(\\hat\\theta = 2\\) is twice as costly as \\(\\hat\\theta =1\\). In many situations, we think that a very bad estimate is not much worse than a bad estimate, but much much worse. We want a loss function that assigns more weight to poor estimates, capturing the idea that the marginal cost/loss of a underestimating/overestimating \\(\\theta\\) is increasing. This is achieved with a quadratic loss function \\(l(\\hat\\theta,\\theta) = (\\hat\\theta-\\theta)^2\\). The risk function which corresponds to this choice of \\(l(\\hat\\theta,\\theta)\\) is so important that it gets its own name.\n\n\nShow code which generates figure\ntibble(x = (-1000:1000)/1000) %>% \n  mutate(y = x^2) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_void() +\n  geom_vline(xintercept = 0, size = 0.25) +\n  geom_hline(yintercept = 0, size = 0.25) +\n  annotate(\"text\", x = 0.96, y = -0.05, label = \"Estimated θ\") +\n  annotate(\"text\", x = 0.07, y = -0.05, label = \"True θ\") + \n  annotate(\"text\", x = 0.75, y = 0.2, label = \"Loss(Estimaed θ, True θ)\")\n\n\n\n\n\nFigure 1.3: Quadratic loss function\n\n\n\n\n\nDefinition 1.9 Suppose \\(\\hat\\theta:{\\mathcal X} \\to \\Theta\\) is an estimator for \\(\\theta\\). The mean squared error (MSE) of \\(\\hat\\theta\\) is \\[\\text{MSE}\\left(\\hat\\theta\\right) = \\text{E}\\left[(\\hat\\theta - \\theta)^2\\right].\\] In the event our estimator is a vector \\(\\hat{\\boldsymbol{\\theta}} = (\\hat\\theta_1,\\ldots,\\hat\\theta_k)\\), then \\[ \\text{MSE}\\left(\\hat{\\boldsymbol{\\theta}}\\right) = \\text{E}\\left[\\textstyle\\sum_{i=1}^n(\\hat\\theta_i - \\theta_i)^2\\right].\\]\n\nHow does this third measure help us? Don’t we need to compare bias, variance, and MSE now? Fortunately, we do not, as MSE captures both the variance and bias of an estimator! By expanding \\((\\hat\\theta - \\theta)^2\\) in the definition of MSE, we have \\[\\begin{align*}\n\\text{MSE}\\left(\\hat\\theta\\right) & = \\text{E}\\left[\\hat\\theta^2 - 2\\hat\\theta\\theta + \\theta^2\\right] \\\\\n                 & = \\text{E}\\left[\\hat\\theta^2\\right] - 2\\theta\\text{E}\\left[\\hat\\theta\\right] + \\text{E}\\left[\\theta^2\\right] & (\\text{Expectation is linear})\\\\\n                 & = \\text{E}\\left[\\hat\\theta^2\\right] - 2\\theta\\text{E}\\left[\\hat\\theta\\right] + \\text{E}\\left[\\theta^2\\right] + \\left(\\text{E}\\left[\\hat\\theta\\right]^2 - \\text{E}\\left[\\hat\\theta\\right]^2\\right) \\\\\n                 & = \\left(\\text{E}\\left[\\hat\\theta^2\\right] - \\text{E}\\left[\\hat\\theta\\right]^2\\right) - 2\\theta\\text{E}\\left[\\hat\\theta\\right] + \\text{E}\\left[\\theta^2\\right] + \\text{E}\\left[\\hat\\theta\\right]^2 \\\\\n                 & = \\text{Var}\\left(\\hat\\theta\\right) + \\left(\\text{E}\\left[\\hat\\theta\\right]^2 - 2\\theta\\text{E}\\left[\\hat\\theta\\right] + \\theta^2 \\right)\\\\\n                 & = \\text{Var}\\left(\\hat\\theta\\right) + \\left(\\text{E}\\left[\\hat\\theta\\right] - \\theta\\right)^2 \\\\\n                 & = \\text{Var}\\left(\\hat\\theta\\right) + \\text{Bias}(\\hat\\theta)^2.\n\\end{align*}\\] In a sense, mean square error is an aggregate of variance and bias, where we are more concerned with bias than variance (hence it being squared). We would rather have a very precise (low variance) estimator that is inaccurate (high bias) then an accurate estimator (low bias) that is imprecise (high variance). What does this mean for our two estimators of \\(\\sigma^2\\)?\n\nExample 1.12 Once again, define \\[\\begin{align*}\nS^2({\\mathbf{X}}) & = \\frac{1}{n-1}\\sum_{i=1}^n (X_i -\\bar X)^2.\\\\\n\\hat\\theta({\\mathbf{X}}) & = \\frac{1}{n}\\sum_{i=1}^n (X_i -\\bar X)^2.\n\\end{align*}\\] The MSE of these estimators are \\[\\begin{align*}\n\\text{MSE}\\left(S^2\\right) &= \\text{Var}\\left(S^2\\right) + \\text{Bias}(S^2)^2 = \\frac{2\\sigma^4}{n-1} + 0 = \\frac{2\\sigma^4}{n-1},\\\\\n\\text{MSE}\\left(\\hat\\theta\\right) &= \\text{Var}\\left(\\hat\\theta\\right) + \\text{Bias}(\\hat\\theta)^2 = \\frac{2\\sigma^4(n-1)}{n-1} + \\left(\\frac{n-1}{n}\\sigma^2 - \\sigma^2\\right)^2 = \\frac{\\sigma^4(2n-1)}{n^2}.\n\\end{align*}\\] For all \\(n \\ge 1\\) we have, \\[\\begin{align*}\n& 3n > 1 \\\\\n\\implies & -1 > -3n \\\\\n\\implies & 0 > 1-3n \\\\\n\\implies & 2n^2 > 2n^2 - 3n + 1 \\\\\n\\implies & 2n^2 > (2n-1)(n-1) \\\\\n\\implies & \\frac{2n^2}{(n-1)n^2} > \\frac{(2n-1)(n-1)}{(n-1)n^2}\\\\\n\\implies & \\frac{2n^2}{n-1} > \\frac{(2n-1)}{n^2}\\\\\n\\implies & \\frac{2\\sigma^4n^2}{n-1} > \\frac{\\sigma^4(2n-1)}{n^2}\\\\\n\\implies & \\text{MSE}\\left(S^2\\right) > \\text{MSE}\\left(\\hat\\theta\\right).\n\\end{align*}\\]\nIf our sole criterion for an estimator is mean-squared error, than we should opt to estimate \\(\\sigma^2\\) with the biased estimator.\n\nThis example brings attention to an immediate result of Definition Definition 1.9.\n\nProposition 1.1 (MSE of an Unbiased Estimator) If an estimator \\(\\hat\\theta : {\\mathcal X} \\to \\Theta\\) is unbiased, then \\[\\text{MSE}\\left(\\hat\\theta\\right)=\\text{Var}\\left(\\hat\\theta\\right).\\]"
  },
  {
    "objectID": "estimators.html#efficient-estimators-fisher-information",
    "href": "estimators.html#efficient-estimators-fisher-information",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.5 Efficient Estimators, Fisher Information",
    "text": "1.5 Efficient Estimators, Fisher Information\nWith MSE in mind, we can define what it means to have an “optimal” estimator.\n\nDefinition 1.10 An estimator \\(\\hat\\theta\\) is efficient if \\(\\text{MSE}\\left(\\hat\\theta\\right) < \\text{MSE}\\left(\\hat\\theta'\\right)\\) (for all values of \\(\\theta\\)) for all other estimators \\(\\hat\\theta'\\).6 If \\(\\text{MSE}\\left(\\hat\\theta_1\\right) < \\text{MSE}\\left(\\hat\\theta_2\\right)\\), we say \\(\\hat\\theta_1\\) is more efficient \\(\\hat\\theta_2\\)\n\nCalculating the most efficient estimator can be quite difficult considering how many possible estimators there are, so it’s common to look for the most efficient estimator among a smaller class of estimators that satisfy other desirable properties. The most common estimators to discard when finding an estimators are biased estimators. This approach takes unbiasedness as the lowest common denominator, and then picks the most efficient of the unbiased estimators. Note that by Proposition Proposition 1.1, the MSE of all unbiased estimators is simply their respective variances, so by restricting attention to this class of estimators, minimizing the MSE is the same thing as minimizing variance.\n\nDefinition 1.11 An estimator \\(\\hat\\theta\\) is the (uniformly) minimum-variance unbiased estimator (MVUE) if it is the most efficient estimator among all unbiased estimators.\n\nEven with attention restricted to unbiased estimators, we will need an additional tool to determine if an estimator is efficient. This comes is a key result which bounds the variance of an estimator.\n\nTheorem 1.1 (Information Inequality) Suppose \\(\\hat\\theta :{\\mathcal X} \\to \\Theta\\) is an estimator for \\(\\theta\\) where \\(\\text{E}\\left[\\hat\\theta\\right]=\\psi(\\theta)\\). If the support of \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\) does not depend on \\(\\theta\\), then \\[ \\text{Var}\\left(\\hat\\theta\\right) \\ge \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{I(\\theta)} \\] where \\(I(\\theta)\\) is defined as \\[I(\\theta) = \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2\\right],\\] and known as the Fisher information of the sample.\n\n\nProof. Suppose \\(\\hat\\theta\\) is an estimator such that \\(\\text{E}\\left[\\hat\\theta\\right]=\\psi(\\theta)\\), and \\(f_X(x\\mid\\theta)\\) satisfies the aforementioned properties. First, we’ll define a function of \\(f_X(x\\mid \\theta)\\). \\[ V(x) = \\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\]\nApply the chain rule to \\(V(x)\\) gives: \\[V(x) = \\frac{\\partial}{\\partial \\theta} \\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) = \\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\left[\\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right].\\] Because \\(V\\) depends on the random variable \\(X\\), we can take it’s expectation over the sample space \\({\\mathcal X}\\). \\[\\begin{align*}\n\\text{E}\\left[V\\right] & = \\int\\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\left[\\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]\\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\\\\n& = \\int f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) \\cdot \\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\left[\\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]\\ d{\\mathbf{X}}\\\\\n& = \\int \\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}}.\n\\end{align*}\\] This integral is taken over the support of \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\). We’ve assumed the support is not a function of \\(\\theta\\), so the bounds of integration do not depend on \\(\\theta\\) and we can use Liebniz’s integral rule to interchange integration and differentiation: \\[ \\text{E}\\left[V\\right] = \\int \\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}} = \\frac{\\partial }{\\partial \\theta} \\underbrace{\\int  f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}}}_1 = 0.\\] Because \\(V\\) has an expectation of zero, it’s variance is the expectation of its square: \\[\\text{Var}\\left(V\\right) = \\text{E}\\left[V^2\\right] - \\underbrace{\\text{E}\\left[V\\right]^2}_0 = \\cdot\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid \\theta)\\right)^2\\right] = I(\\theta) \\tag{1.1}\\] Using the fact that \\(\\text{E}\\left[V\\right] = 0\\), the covariance between \\(V\\) and \\(\\hat\\theta\\) is \\[\\begin{align*}\n\\text{Cov}\\left(V, \\hat\\theta\\right) &= \\text{E}\\left[V\\hat\\theta\\right] -\\underbrace{\\text{E}\\left[V\\right]}_0\\text{E}\\left[\\hat\\theta\\right]\\\\\n& = \\text{E}\\left[V\\hat\\theta\\right]\\\\\n& = \\text{E}\\left[\\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\left[\\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]\\hat\\theta\\right] \\\\\n& = \\int \\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\left[\\frac{\\partial }{\\partial \\theta} f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]\\hat\\theta({\\mathbf{X}}) \\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid \\theta) \\\\\n& = \\frac{\\partial }{\\partial \\theta} \\int \\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\hat\\theta({\\mathbf{X}}) \\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid \\theta) \\\\\n& = \\frac{\\partial }{\\partial \\theta} \\int \\hat\\theta({\\mathbf{X}}) \\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid \\theta) \\\\\n& = \\frac{\\partial }{\\partial \\theta} \\text{E}\\left[\\hat\\theta\\right] \\\\\n& = \\psi'(\\theta).\n\\end{align*}\\] Again we were able to interchange differentiation and integration due to the assumption about \\(f_X(x\\mid\\theta)\\)’s support. In the context of probability, the famed Cauchy-Schwarz inequality takes the form \\[ \\left\\lvert{\\text{Cov}\\left(V,\\hat\\theta\\right)}^2\\right\\rvert \\le \\text{Var}\\left(V\\right)\\text{Var}\\left(\\hat\\theta\\right).\\] Using this along with the fact that \\(\\text{Cov}\\left(V, \\hat\\theta\\right) = \\psi'(\\theta)\\) and Equation 1.1, gives: \\[\\begin{align*}\n& \\left\\lvert{\\text{Cov}\\left(V,\\hat\\theta\\right)}^2\\right\\rvert \\le \\text{Var}\\left(V\\right)\\text{Var}\\left(\\hat\\theta\\right) \\\\\n\\implies & \\left\\lvert\\psi'(\\theta)\\right\\rvert^2 \\le I(\\theta)\\text{Var}\\left(\\hat\\theta\\right) \\\\\n\\implies & \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{ I(\\theta)} \\le \\text{Var}\\left(\\hat\\theta\\right)\n\\end{align*}\\] This is the desired inequality\n\nThis bound on the variance of an estimator is often called the Cramér–Rao (CR) lower bound. Theorem Theorem 1.1 presents this bound in more generality than is often required. We can simplify it in three cases:\n\n\\(\\hat\\theta\\) is unbiased, allowing us to eliminate reference to \\(\\psi'(\\theta)\\).\nThe random vector \\({\\mathbf{X}}\\) is comprised of iid random variables \\(X_i\\), allowing us to write the Fisher information in terms to common marginal density \\(f_X(x\\mid\\theta)\\).\nIf the second derivative of \\(\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid \\theta)\\) with respect to \\(\\theta\\) exists, we can write the fisher information with respect to this second derivative.\n\n\nCorollary 1.1 (Information Inequality for Unbiased Estimators) Suppose \\(\\hat\\theta :{\\mathcal X} \\to \\Theta\\) is an unbiased estimator for \\(\\theta\\). If the support of \\(f_X(x\\mid\\theta)\\) does not depend on \\(\\theta\\), then \\[ \\text{Var}\\left(\\hat\\theta\\right) \\ge I(\\theta)^{-1}.\\]\n\n\nProof. If \\(\\hat\\theta\\) is unbiased, then \\(\\text{E}\\left[\\hat\\theta\\right] = \\theta\\), and \\(\\psi'(\\theta) = 1\\).\n\n\nCorollary 1.2 (Information Inequality for IID Samples) Suppose \\(\\hat\\theta :{\\mathcal X} \\to \\Theta\\) is an estimator for \\(\\theta\\), and \\(X_i \\overset{iid}{\\sim} P_{\\boldsymbol{\\theta}}a\\) for all \\(i=1,\\ldots,n\\). If the support of \\(f_X(x\\mid\\theta)\\) does not depend on \\(\\theta\\), then \\[\\text{Var}\\left(\\hat\\theta\\right) \\ge \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2\\right]}\\] where \\(f_X(x\\mid\\theta)\\) is the distribution shared by all \\(X_i\\).\n\n\nProof. The joint distribution \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\) can be written as the product of the \\(n\\) identical marginal distributions. \\[f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) = \\prod_{i=1}^nf_X(x_i\\mid\\theta)\\] If we use this equality we can write the Fisher information as, \\[\\begin{align*}\nI(\\theta) &= \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2\\right]\\\\\n& = \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log \\prod_{i=1}^nf_X(x_i\\mid\\theta)\\right)^2\\right] \\\\\n& = \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\sum_{i=1}^n\\log f_X(x_i\\mid\\theta)\\right)^2\\right] & (\\text{log properties})\\\\\n& = \\text{E}\\left[\\left(\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2\\right] & (\\text{linearity of derivative})\\\\\n& = \\text{E}\\left[\\sum_{i=1}^n\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2 + \\sum_{i\\neq j}\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\frac{\\partial}{\\partial \\theta}\\log f_X(x_j\\mid\\theta)\\right] & (\\text{expand square})\\\\\n& = \\sum_{i=1}^n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2\\right] + \\sum_{i\\neq j}\\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\frac{\\partial}{\\partial \\theta}\\log f_X(x_j\\mid\\theta)}\\right]& (\\text{expectation is linear})\\\\\n& = \\sum_{i=1}^n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2\\right] + \\sum_{i\\neq j}\\underbrace{\\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)}\\right]}_0\\underbrace{\\text{E}\\left[\\frac{\\partial}{\\partial \\theta}\\log f_X(x_j\\mid\\theta)\\right]}_0& (X_i\\perp X_j) \\\\\n& = n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)\\right)^2\\right] & (\\text{identical distributions}).\n\\end{align*}\\] The fact that \\(\\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)}\\right] = 0\\) follows from the same argument we used to conclude \\(\\text{E}\\left[V\\right] = 0\\) in the proof of Theorem Theorem 1.1.\n\n\nCorollary 1.3 Suppose \\(\\hat\\theta :{\\mathcal X} \\to \\Theta\\) is an estimator for \\(\\theta\\) where \\(\\text{E}\\left[\\hat\\theta\\right]=\\psi(\\theta)\\). If the support of \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\) does not depend on \\(\\theta\\), and \\(\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}} \\mid \\theta)\\) exists, then \\[\\text{Var}\\left(\\hat\\theta\\right) \\ge \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{-\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]}\\]\n\n\nProof. Recall that in the proof of Theorem 1.1 we established \\(\\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_X(x_i\\mid\\theta)}\\right] = 0\\). Using this, along with the Liebniz’s rule and the assumption about the support of \\(f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\) we have\n\\[\\begin{align*}\n& \\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right] = 0\\\\\n\\implies & \\frac{\\partial}{\\partial \\theta}\\text{E}\\left[{\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right] = 0 \\\\\n\\implies & \\frac{\\partial}{\\partial \\theta}\\int \\frac{1}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) = 0\\\\\n\\implies & \\frac{\\partial}{\\partial \\theta}\\int \\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}} = 0\\\\\n\\implies & \\int \\frac{\\partial^2}{\\partial \\theta^2}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}} = 0\\\\\n\\implies & \\int \\frac{\\frac{\\partial^2}{\\partial \\theta^2}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\ d{\\mathbf{X}} = 0\\\\\n\\implies & \\int \\frac{\\frac{\\partial^2}{\\partial \\theta^2}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\ dF_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) = 0 \\\\\n\\implies & \\text{E}\\left[\\frac{\\frac{\\partial^2}{\\partial \\theta^2}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right] = 0\n\\end{align*}\\] This equality allows us to conclude \\(-\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right] = \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2\\right]\\).\n\\[\\begin{align*}\n-\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right] & = -\\text{E}\\left[\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)\\right] \\\\\n  & = -\\text{E}\\left[\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right)\\right]& (\\text{chain rule})\\\\\n  & = -\\text{E}\\left[\\frac{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\frac{\\partial^2}{\\partial^2 \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta) - \\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)^2}\\right]& (\\text{quotient rule})\\\\\n  & =-\\text{E}\\left[\\frac{\\frac{\\partial^2}{\\partial^2 \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)} - \\left(\\frac{\\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right)^2 \\right]\\\\\n  & = -\\underbrace{\\text{E}\\left[\\frac{\\frac{\\partial^2}{\\partial^2 \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right]}_0 + \\text{E}\\left[\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}{f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)}\\right)^2 \\right] & (\\text{expectation is linear}) \\\\\n  & =  \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2 \\right]\n\\end{align*}\\] If we apply the information inequality, the result follows.\n\nThese three corollaries can make the Cramér–Rao lower bound a headache because it takes so many forms. The following table helps us make sense of the various cases, assuming that \\(\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}} \\mid \\theta)\\) exists.\n\n\n\n\n\n\n\n\n\n\\(\\hat\\theta\\) is unbiased\n\\(\\text{E}\\left[\\hat\\theta\\right] = \\psi(\\theta)\\)\n\n\n\n\n\\(X_i \\overset{iid}{\\sim}{P_{\\boldsymbol{\\theta}}}\\)\n\\(\\text{Var}\\left(\\hat\\theta\\right) \\le \\frac{1}{n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x\\mid\\theta)\\right)^2\\right]} = -\\frac{1}{n\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_X(x\\mid\\theta)\\right]}\\)\n\\(\\text{Var}\\left(\\hat\\theta\\right) \\le \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x\\mid\\theta)\\right)^2\\right]} = -\\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{n\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_X(x\\mid\\theta)\\right]}\\)\n\n\n\\(X_i {\\overset{iid}{\\not\\sim}}{P_{\\boldsymbol{\\theta}}}\\)\n\\(\\text{Var}\\left(\\hat\\theta\\right) \\le \\frac{1}{\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2\\right]} = -\\frac{1}{\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]}\\)\n\\(\\text{Var}\\left(\\hat\\theta\\right) \\le \\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right)^2\\right]} = -\\frac{\\left\\lvert\\psi'(\\theta)\\right\\rvert^2}{\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_{\\mathbf{X}}({\\mathbf{X}}\\mid\\theta)\\right]}\\)\n\n\n\nBecause we’re often concerned with finding a MVUE, we will rarely be concerned with the case where \\(\\text{E}\\left[[\\right]\\hat\\theta]\\). Additionally, the assumption that the random sample is used to calculate estimator is exceedingly common. For these two reasons, the Cramér–Rao lower bound will almost always take the form \\[\\text{Var}\\left(\\hat\\theta\\right) \\le \\frac{1}{n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\log f_X(x\\mid\\theta)\\right)^2\\right]} = -\\frac{1}{n\\text{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\log f_X(x\\mid\\theta)\\right]}.\\]\n\nExample 1.13 Suppose \\(X_i\\overset{iid}{\\sim}\\text{Binom}(k,p)\\), recalling \\[f_X(x\\mid p) = \\binom{k}{p}p^x(1-p)^{k-x}\\] where \\(x\\) is the number of realized successes of \\(k\\) binomial trials. This random variable has an expected value of \\(kp\\) and variance of \\(kp(1-p)\\). Our goal is to estimate the probability of success \\(p\\). If we observe one sequence of \\(k\\) trials (\\(n=1\\)) A natural estimator for this is ratio of successes to trials, \\(\\hat p = X/k\\). This estimator is unbiased: \\[\\text{E}\\left[\\hat p\\right] = \\frac{\\text{E}\\left[X\\right]}{k} = \\frac{kp}{k} = p.\\] The variance of the estimator is: \\[\\text{Var}\\left(\\hat p\\right) = \\text{Var}\\left(\\frac{X}{k}\\right) = \\frac{\\text{Var}\\left(X\\right)}{k^2} = \\frac{kp(1-p)}{k^2} = \\frac{p(1-p)}{k}.\\] Is \\(\\hat p\\) an MVUE? In order to determine this, we must calculate the Cramér–Rao lower bound. \\[\\begin{align*}\n& \\log f_X(x\\mid p) = \\log \\binom{k}{x} + x\\log p + (k-x)\\log (1-p)\\\\\n\\implies &  \\frac{\\partial}{\\partial p}  \\log f_X(x\\mid p) = \\frac{x-kp}{p(1-p)}\\\\\n\\implies & \\left(\\frac{\\partial}{\\partial p}  \\log f_X(x\\mid p)\\right)^2 = \\frac{(x-kp)^2}{p^2(1-p)^2}\\\\\n\\implies & \\text{E}\\left[\\left(\\frac{\\partial}{\\partial p}  \\log f_X(x\\mid p)\\right)^2 \\right] = \\frac{\\text{E}\\left[(x-kp)^2\\right]}{p^2(1-p)^2} = \\frac{\\text{Var}\\left(X\\right)}{p^2(1-p)^2} =  \\frac{kp(1-p)}{p^2(1-p)^2} = \\frac{k}{p(1-p)}\\\\\n\\implies & n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial p}  \\log f_X(x\\mid p)\\right)^2 \\right] = 1\\cdot \\frac{k}{p(1-p)} = \\frac{k}{p(1-p)}\\\\\n\\implies & \\text{Var}\\left(\\hat p\\right) \\le \\frac{p(1-p)}{k}.\n\\end{align*}\\] We have that \\(\\hat p =X/k\\) is the MVUE. We could have calculated this bound using the second derivative of \\(\\log f_X(x\\mid\\theta)\\):\n\\[ \\frac{\\partial^2}{\\partial p^2}  \\log f_X(x\\mid p)= \\frac{\\partial}{\\partial p}\\left[\\frac{x-kp}{p(1-p)}\\right]=-\\frac{x}{p^2}-\\frac{m-x}{(1-p)^2}=-\\frac{(x-mp)^2}{p^2(1-p^2)} \\]\nIt’s important to remember that this means \\(\\hat p\\) is the most efficient estimator among unbiased estimators. It is entirely possible that there exists a biased estimator for \\(p\\) that has a lower MSE than \\(\\hat p\\) for certain parameter values of \\((k,p)\\). Take for instance the estimator \\(\\hat p'\\), \\[\\hat p' = \\frac{X+1}{k+2}.\\] For this estimator, we have \\[\\begin{align*}\n\\text{Bias}(\\hat p ')& = \\text{E}\\left[\\hat p '\\right] - p = \\frac{\\text{E}\\left[X\\right] + 1}{k + 2} - p = \\frac{kp + 1}{k + 2} - p = \\frac{1-2p}{m+2}\\\\\n\\text{Var}\\left(\\hat p'\\right) &= \\text{Var}\\left(\\frac{X + 1}{k+2}\\right) = \\frac{\\text{Var}\\left(X+1\\right)}{(k+2)^2}= \\frac{kp(1-p)}{(k+2)^2}\\\\\n\\text{MSE}\\left(\\hat p '\\right)& = \\text{Var}\\left(\\hat p '\\right) + \\text{Bias}(\\hat p')^2 = \\frac{1 + (k-4)p - (k-4)p^2}{(k+2)^2}.\n\\end{align*}\\] We can graph \\(\\text{MSE}\\left(\\hat p\\right) = \\text{Var}\\left(\\hat p\\right)\\) along with \\(\\text{MSE}\\left(\\hat p'\\right)\\) for various values of \\(k\\).\n\n\nShow code which generates figure\np <- 0:100/100\nk <- c(5,10,15, 20)\nest <- c(\"Biased\", \"MVUE\")\n\nexpand.grid(p, k, est) %>% \n  rename(\n    p = Var1,\n    k = Var2,\n    Estimator = Var3\n  ) %>% \n  mutate(\n    mse = ifelse(Estimator == \"Biased\", (1+(k-4)*p - (k-4)*p^2)/((k+2)^2) , p*(1-p)/k)\n  ) %>% \n  ggplot(aes(p, mse, color = Estimator)) +\n  geom_line() +\n  facet_wrap(~k) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"True Value of Estimad, p\", y = \"Mean Squared Error\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) \n\n\n\n\n\nFigure 1.4: Distribution of estimates of variance for an unbiased estimator and a biased estimator\n\n\n\n\nThe estimator \\(\\hat p'\\) does have a lower MSE than the UMVE at times.\n\n\nExample 1.14 If we observe realizations of \\(X_i\\overset{iid}{\\sim}\\text{Uni}(0,\\theta)\\), then the distribution \\(f_X(x\\mid\\theta) = 1/\\theta\\) (with a support of \\([0,\\theta]\\)) does not satisfy the information inequality’s necessary condition – the support of \\(f_X(x\\mid\\theta)\\) is a function of the parameter \\(\\theta\\). Without this assumption, we can not apply Leibniz’s rule in the proof for @ref(thm:CRbound), and shouldn’t expect the result to hold.\nAn unbiased estimator for \\(\\theta\\) is \\[\\hat\\theta({\\mathbf{X}})=\\frac{n+1}{n}X_{(n)}\\] where \\(X_{(n)} = \\max\\{X_1,\\ldots, X_n\\}\\) is the maximum order statistic. If the random variable \\(X_i\\) has an upper bound of \\(\\theta\\), then using the largest value from the sample to estimate \\(\\theta\\) is rather intuitive. The order statistic \\(X_{(n)}\\) has a density function of \\[f_{X_{(n)}}(x\\mid\\theta) = nf_X(x\\mid\\theta)[F_X(x\\mid\\theta)]^{n-1} = n\\left(\\frac{1}{\\theta}\\right)\\left(\\frac{x}{\\theta}\\right)^{n-1} = n\\frac{x^{n-1}}{\\theta^n}.\\] The expectation of \\(X_{(n)}\\) is \\[\\text{E}\\left[X_{(n)}\\right] = \\int_0^\\theta x\\ dF_{X_{(n)}}(x\\mid\\theta) = \\frac{n}{\\theta^n} \\int_0^\\theta x n\\frac{x^{n-1}}{\\theta^n}\\ dx = \\frac{n}{n+1} \\theta,\\] so it’s a biased estimator. We can correct for this bias by multiplying by \\(X_{(n)}\\) by \\((n+1)/n\\), which gives \\(\\hat\\theta({\\mathbf{X}})\\). The variance of \\(\\hat\\theta\\) is \\[\\begin{align*}\n\\text{Var}\\left(\\hat\\theta\\right)&=\\text{Var}\\left(\\frac{n+1}{n}X_{(n)}\\right)\\\\\n&= \\frac{(n+1)^2}{n^2} \\left(\\text{E}\\left[X_{(n)}^2\\right] - \\text{E}\\left[X_{(n)}\\right]^2\\right)\\\\\n&= \\frac{(n+1)^2}{n^2} \\left(\\int_0^\\theta x^2\\ dF_{X_{(n)}}(x\\mid\\theta) - \\left(\\frac{n}{n+1} \\theta\\right)^2\\right)\\\\\n& = \\frac{\\theta^2}{n(n+2)},\n\\end{align*}\\] while the Fisher information of the sample is \\[\\begin{align*}\n& \\log f_X(x\\mid \\theta) = -\\log(\\theta)\\\\\n\\implies & \\frac{\\partial}{\\partial \\theta} \\log f_X(x\\mid \\theta) = -\\frac{1}{\\theta}\\\\\n\\implies &\\left(\\frac{\\partial}{\\partial \\theta} \\log f_X(x\\mid \\theta)\\right)^2 = \\frac{1}{\\theta^2}\\\\\n\\implies & \\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f_X(x\\mid \\theta)\\right)^2\\right] = \\frac{1}{\\theta^2}\\\\\n\\implies & n\\text{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f_X(x\\mid \\theta)\\right)^2\\right] = \\frac{n}{\\theta^2}\\\\\n\\implies & I(\\theta) = \\frac{n}{\\theta^2}.\n\\end{align*}\\]\nIn this case, \\(\\text{Var}\\left(\\hat\\theta\\right) < I(\\theta)^{-1}\\).\n\nWe can extend @ref(thm:CRbound) to the case where we estimate multiple parameters. Because our interest will almost always be in unbiased estimators calculated with IID samples, the multiparameter version of the information inequality will be presented in this context.\n\nTheorem 1.2 (Information Inequality for Multiple Parameters) Suppose \\(\\hat{\\boldsymbol{\\theta}} :{\\mathcal X} \\to \\Theta\\) is an unbiased estimator for \\(\\boldsymbol{\\theta}\\), and \\(X_i \\overset{iid}{\\sim} P_{\\boldsymbol{\\theta}}a\\) for all \\(i=1,\\ldots,n\\). If the support of \\(f_X(x\\mid\\boldsymbol{\\theta})\\) does not depend on \\(\\boldsymbol{\\theta}\\), then \\[ \\text{Var}\\left(\\hat{\\boldsymbol{\\theta}}\\right) \\ge \\mathbf I(\\boldsymbol{\\theta})^{-1},\\] where \\(\\mathbf I(\\boldsymbol{\\theta})\\) is known as the information matrix and defined as \\[\\mathbf I(\\boldsymbol{\\theta})_{i,j} = -n \\text{E}\\left[\\frac{\\partial}{\\partial \\theta_i\\partial\\theta_j}\\log f_X(x\\mid\\theta)\\right]\\]\n\n\nExample 1.15 If \\(X_i\\sim N(\\mu,\\sigma^2)\\), we can estimate \\(\\mu\\) and \\(\\sigma^2\\) jointly with an estimator \\(\\hat{\\boldsymbol{\\theta}} = (\\hat\\mu, \\hat\\sigma^2)\\).\nThis can be verified by using the information inequality for unbiased estimators and IID samples. In this case, \\[\\begin{align*} f_X(x\\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\n\\implies \\log f_X(x\\mid \\mu, \\sigma^2) = \\log(1) - \\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}.\n\\end{align*}\\]\nDifferentiating gives:\n\\[\\begin{align*}\n-n \\text{E}\\left[\\frac{\\partial^2}{\\partial \\mu^2}\\log f_X(x\\mid\\theta)\\right] &= -n\\text{E}\\left[-\\sigma^{-2}\\right] = n\\sigma^{-2}\\\\\n-n \\text{E}\\left[\\frac{\\partial}{\\partial \\mu\\partial\\sigma^2}\\log f_X(x\\mid\\theta)\\right] &= -n \\text{E}\\left[\\frac{\\partial}{\\partial\\sigma^2\\partial \\mu}\\log f_X(x\\mid\\theta)\\right]=-n\\sigma^{-4}\\text{E}\\left[x-\\mu\\right]=0\\\\\n-n \\text{E}\\left[\\frac{\\partial^2}{\\partial (\\sigma^2)^2}\\log f_X(x\\mid\\theta)\\right] &= -n\\text{E}\\left[\\sigma^{-4}/2\\right] = n\\sigma^{-4}/2\\\\\n\\mathbf I(\\mu, \\sigma) & = \\begin{bmatrix} n/\\sigma^{-2} & 0 \\\\ 0  & n\\sigma^{-4}/2 \\end{bmatrix}.\n\\end{align*}\\] The information inequality takes the form \\[\\text{Var}\\left(\\hat\\mu, \\hat\\sigma^2\\right) \\ge  \\begin{bmatrix} \\sigma^{2}/n & 0 \\\\ 0  & 2\\sigma^{4}/n \\end{bmatrix}.\\] Notice that \\(\\text{Var}\\left(\\hat\\mu\\right) \\ge \\sigma^2/n\\). This means that \\(\\hat\\mu = \\bar X\\) is the MVUE, as \\(\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n\\).\n\n\nRemark. Theorem 1.1 and its resulting corollaries have one main drawback. It is not constructive and provides no guidance as to constructing efficient estimators, and if such an estimator even exists. Addressing this is a matter of introducing a new property of estimators related to data reduction and information (sufficiency) and the Lehmann–Scheffé theorem (which is related to the perhaps more familiar Rao–Blackwell theorem)."
  },
  {
    "objectID": "estimators.html#the-distribution-of-an-estimator",
    "href": "estimators.html#the-distribution-of-an-estimator",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.6 The Distribution of an Estimator",
    "text": "1.6 The Distribution of an Estimator\nFinally, we turn to the matter of an estimator’s probability distribution. Like any other random variable, estimators have a distribution and density function. Knowing the distribution of an estimator allows us to put a point estimate in a broader context by using an estimator’s distribution to calculate the probability of observing the estimate in question.\n\nExample 1.16 Suppose \\({\\mathbf{X}} = (X_1, \\ldots, X_n)\\) is a random sample from a normal distribution \\(N(\\mu, \\sigma^2)\\). What is the distribution of \\(\\bar X\\)? The sum of independent normally distributed random variables is normally distributed as follows: \\[\\sum_{i=1}^n X_i \\sim N(u_1 + \\mu_2 + \\cdots + \\mu_n, \\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_n^2).\\] In our case, each \\(X_i\\) are identically distributed as well, so \\(\\mu_i = \\mu\\) and \\(\\sigma_i^2 = \\sigma^2\\) for all \\(i\\). \\[ \\sum_{i=1}^n X_i\\sim N(n\\mu, n\\sigma^2)\\] Finally if we scale the sum by \\(1/n\\), giving \\(\\bar X\\), we have \\[\\bar X\\sim N(\\mu, \\sigma^2/n)\\] by the properties of expectation and variance. If we simulate realizations of this estimator, the resulting histogram should betray that \\(\\bar X\\) is normally distributed. For these simulations, we will take \\(n = 100\\), \\(\\mu = 0\\), and \\(\\sigma^2 = 1\\). We should see that \\[\\begin{align*}\n\\text{E}\\left[\\bar X\\right] &\\approx 0\\\\\n\\text{Var}\\left(\\bar X\\right) &\\approx 0.01\n\\end{align*}\\]\n\nestimates <- vector(\"numeric\", N_sim)\n\nfor (k in 1:N_sim) {\n  X <- rnorm(100)\n  estimates[k] <- mean(X)\n}\nc(mean(estimates), var(estimates))\n\n[1] -6.754554e-05  9.993609e-03\n\n\n\n\nShow code which generates figure\ntibble(estimates) %>% \n  ggplot(aes(estimates)) +\n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 100) + \n  labs(x = \"Estimates of μ\") +\n  theme_minimal() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(1/100)), color = \"red\")\n\n\n\n\n\nFigure 1.5: Distribution of estimates of variance for an unbiased estimator and a biased estimator\n\n\n\n\nNote that for this to hold, it must be the case that \\(X_i\\) are iid normally distributed, which is rather restrictive. Ideally, we will be able to make some statements about estimators’ distributions regardless of the underlying distribution which generates the observable data. Fortunately, the next section will equip us with the tools to do this."
  },
  {
    "objectID": "estimators.html#further-reading",
    "href": "estimators.html#further-reading",
    "title": "1  Finite Sample Properties of Estimators",
    "section": "1.7 Further Reading",
    "text": "1.7 Further Reading\n\nMcCullagh (2002)\nBickel and Doksum (2015)\nLehmann and Casella (1998)\nGreene (2018), Appendix C\n\n\n\n\n\n\n\nBickel, Peter J, and Kjell A Doksum. 2015. Mathematical Statistics: Basic Ideas and Selected Topics, Volume i. 2nd ed. CRC Press.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson Education.\n\n\nLehmann, Erich L, and George Casella. 1998. Theory of Point Estimation. 2nd ed. Springer.\n\n\nMcCullagh, Peter. 2002. “What Is a Statistical Model?” The Annals of Statistics 30 (5): 1225–1310."
  },
  {
    "objectID": "asymptotics.html",
    "href": "asymptotics.html",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "",
    "text": "When considering estimators in Chapter 1, we kept the sample size \\(n\\) fixed when assessing estimators. We now consider how estimators behave as \\(n\\to\\infty.\\) In practice, we will never have infinite data, asymptotics gives us an approximate idea of how estimators perform for large data sets. A comprehensive reference in asymptotic theory is due to Van der Vaart (2000). For a treatment concerned purely with econometrics, Newey and McFadden (1994) provide a phenomenal survey, most of which we will touch on when discussing general classes of estimators.\nWith loss of some generality, we will assume that all random variables have finite expectation and variances. Dispensing with this assumption is something for a probability course."
  },
  {
    "objectID": "asymptotics.html#convergence",
    "href": "asymptotics.html#convergence",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.1 Convergence",
    "text": "2.1 Convergence\nAt some point in high school, most students encounter the concept of a numeric sequence, and how they can converge to a limit. Later on, perhaps when taking a real analysis course, sequences are generalized to spaces of functions. A sequence of functions may also converge to a limit, whether that be converging pointwise and/or converging uniformly (for details see Rudin (1976)). Random variables are functions from a sample space to \\(\\mathbb R\\), so we can consider how these functions converge. For the most part, the dimension of the random variable (i.e whether it takes on scalar values of vector values) won’t really matter, so we’ll remain agnostic about the exact case.\n\n2.1.1 Convergence in MSE\nThe first type of convergence we’ll work with deals with MSE.\n\nDefinition 2.1 A sequence of random variables \\(X_n\\) converges in mean square to a random variable \\(X\\), written as \\(X_n\\overset{ms}{\\to} X\\), if \\[\\lim_{n\\to\\infty} \\text{E}\\left[(X_n - X)^2\\right] = 0.\\]\n\n\\(X_n \\overset{ms}{\\to}X\\) if the average distance between \\(X_n\\) and \\(X\\) shrinks as \\(n\\to\\infty\\) where distance is measured as \\((X_n - X)^2\\). We can also have \\(X_n \\overset{ms}{\\to}c\\) for some constant \\(c\\), as \\(c\\) is a trivial random variable.\n\nExample 2.1 Suppose we draw a sample of \\(n\\) iid random variables \\(Z_i\\) and define \\(X_n\\) to be the sample mean of our observations. \\[X_n = \\frac{1}{n}\\sum_{i=1}^n Z_i\\] If \\(\\text{E}\\left[Z_i\\right] = \\mu\\) and \\(\\text{Var}\\left(Z_i\\right) = \\sigma^2\\) for all \\(i\\), we have \\(X_n\\overset{ms}{\\to}\\mu\\): \\[\\begin{align*}\n\\lim_{n\\to\\infty}\\text{E}\\left[(X_n - \\mu)^2\\right] & = \\lim_{n\\to\\infty}\\text{Var}\\left(X_n\\right) + \\text{Bias}(X_n) \\\\\n&= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} + 0 & (X_n \\text{ unbiased}) \\\\\n& = \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} \\\\\n& = 0.\n\\end{align*}\\]\nWhat does this convergence “look like”? If \\(Z_i\\overset{iid}{\\sim}N(0,1)\\), we know that \\(X_n = \\bar Z \\sim N(\\mu, \\sigma^2/n)\\). Let’s plot this distribution for increasing values of \\(n\\).\n\n\nShow code which generates figure\nexpand.grid(\n  x = seq(-3, 3, length = 500),\n  n = c(1, 5, 10, 50, 100)\n) %>% \n  mutate(\n    y = dnorm(x, 0, sqrt(1/n)),\n    n = as.factor(n)\n  ) %>% \n  ggplot(aes(x,y, color = n)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Value of X_n\", y = \"Density\", color = \"Sample Size\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.1: The distribution of X_n for various values of n\n\n\n\n\n\nThis example betrays a useful property related to variables which converge in mean square.\n\nProposition 2.1 A sequence of random variables \\(X_n\\) converges in mean square to a constant \\(c\\) if and only if \\(\\text{E}\\left[X_n\\right]\\to c\\) and \\(\\text{Var}\\left(X_n\\right)\\to 0\\).\n\n\nProof. \\((\\Longrightarrow)\\) Suppose \\(X_n \\overset{ms}{\\to}c\\). Then \\[\\begin{align*}\n& \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n\\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n\\implies& \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right]^2 -2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0\\\\\n\\implies& \\lim_{n\\to\\infty}\\left[(\\text{E}\\left[X_n\\right]^2 -\\text{E}\\left[X_n\\right]^2) + \\text{E}\\left[X_n\\right]^2 - 2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0 \\\\\n\\implies &\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0\n\\end{align*}\\] This final equality gives the desired result.\n\\((\\Longleftarrow)\\) Suppose \\(\\text{E}\\left[X_n\\right]\\to c\\) and \\(\\text{Var}\\left(X_n\\right)\\to 0\\). We have \\[\\begin{align*}\n&\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0 \\\\\n\\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n\\implies & X_n\\overset{ms}{\\to}c\n\\end{align*}\\]\n\n\nCorollary 2.1 Suppose \\(X_n\\) is a sequence of random variables such that \\(\\text{E}\\left[X_n\\right] = c\\) for all \\(n\\). Then \\(X_n\\overset{ms}{\\to}c\\) if and only if \\(\\text{Var}\\left(X_n\\right)\\to 0\\).\n\n\n\n2.1.2 Convergence in Probability\nConvergence in mean square captures the idea that a random variable gets “closer” to some value \\(c,\\) but it is hardly the only way to define this behavior. A more “traditional” approach would be defining convergence using an inequality involving an arbitrarily small \\(\\varepsilon >0\\) (akin the to \\(\\varepsilon-\\delta\\) definition of a limit).\n\nDefinition 2.2 A sequence of random variables \\(X_n\\) converges in probability to a random variable \\(X\\), written as \\(X_n\\overset{p}{\\to}X\\) or \\(\\mathop{\\mathrm{plim}}X_n = X\\), if \\[\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon)= 0\\] for all \\(\\varepsilon > 0\\). Equivalently, \\(X_n\\overset{p}{\\to}X\\) if for all \\(\\varepsilon > 0\\) and \\(\\delta > 0\\), there exists some \\(N\\) such that for all \\(n \\ge N\\), \\[ \\Pr (|X_n - X| > \\varepsilon) < \\delta.\\]\n\nIntuitively, \\(X_n \\overset{p}{\\to}X\\) if the probability that the difference \\(|X_n - X|\\) is not small (greater than some \\(\\varepsilon\\)) goes to zero as \\(n\\to\\infty\\).\n\nExample 2.2 Return to the previous example where \\(X_n = \\bar Z\\), and assume \\(Z_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). We will verify that \\(X_n\\overset{p}{\\to}\\mu\\) using the definition of convergence in probability using the fact that \\(X_n \\sim N(\\mu, \\sigma^2/n)\\).\n\n\n\n\n\nFigure 2.2: For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small\n\n\n\n\nFor some \\(\\varepsilon > 0\\), \\[\\begin{align*}\n\\Pr (|X_n - \\mu| > \\varepsilon) & = 1 - \\Pr (\\mu - \\varepsilon < X_n < \\mu + \\varepsilon)\\\\\n& = 1 - (F_{X_n}(\\mu + \\varepsilon) + F_{X_n}(\\mu - \\varepsilon))\\\\\n& = 1 - 2\\left[F_{X_n}(\\mu + \\varepsilon) - \\frac{1}{2}\\right] & (F_{X_n} \\text{symmetric about }\\mu)\\\\\n& = 1 - 2\\left[\\Phi\\left(\\frac{(\\mu + \\varepsilon) - \\mu}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] & (\\Phi\\text{ standard normal distribution})\\\\\n& = 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right].\n\\end{align*}\\] Given some \\(\\delta >0\\), we can solve for the lowest value of \\(n\\) that satisfies \\(\\Pr (|X_n - c| > \\varepsilon) < \\delta\\). \\[\\begin{align*}\n&\\Pr (|X_n - c| > \\varepsilon) < \\delta \\\\\n\\implies & 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] < \\delta \\\\\n\\implies&  n > \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2\n\\\\\\implies & n > \\left\\lceil \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2 \\right\\rceil\n\\end{align*}\\] Just to be excruciatingly pedantic, we rounded our solution up to the closest positive integer, as \\(n\\) corresponds to a sample size. For fixed values of \\(\\mu\\) and \\(\\sigma^2\\) (say 3 and 2, respectively), we can define a function of \\((\\varepsilon, \\delta)\\) which calculates the sample size required to satisfy \\(\\Pr(|X_n - c|>\\varepsilon)<\\delta\\).\n\nmu <- 3\nsigma <- sqrt(2)\n\nn_fun <- function(delta, ep){\n ceiling(((sigma*qnorm(1-delta /2))/ep)^2) \n}\n\nLet’s plot this function for various values of \\((\\varepsilon, \\delta)\\).\n\n\nShow code which generates figure\nexpand.grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  d = 1:9999/10000\n) %>% \n  mutate(sample = n_fun(d,e)) %>% \n  ggplot(aes(d, sample, color = as.factor(e))) +\n  scale_x_reverse() +\n  scale_y_log10() + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"δ\", y = \"Sample Size\", color = \"ε\")+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2.3: The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)\n\n\n\n\nWe can also verify that \\(\\lim_{n\\to\\infty}\\Pr (|X_n - \\mu| > \\varepsilon) = 0\\) for various values of \\(\\boldsymbol{\\varepsilon}\\).\n\nprob_ep <- function(n, ep){\n  1 - 2*(pnorm(ep / (sigma / sqrt(n))) - 1/2)\n}\n\n\n\nShow code which generates figure\nexpand.grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  n = 1:100\n) %>% \n  mutate(prob = prob_ep(e,n)) %>% \n  ggplot(aes(n, prob, color = as.factor(e))) +\n  geom_line() +\n  scale_x_log10() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Pr(|X_n - mu| > ε)\", color = \"ε\") +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 2.4: The probability that X_n falls outside the interval |μ-ε| for various values of (ε,n)\n\n\n\n\n\nHow does convergence in mean square related to convergence in probability? As it turns out the latter is a weaker condition implied by the prior. Before stating and proving this result, we will need a lemma.\n\nLemma 2.1 (Markov’s inequality) If \\(X\\) is a nonnegative random variable, and \\(a > 0\\), then \\[\\Pr(X\\ge a) \\le \\frac{\\text{E}\\left[X\\right]}{a}\\]\n\n\nProof. The expectation of \\(X\\) can be written as \\[\\begin{align*}\n\\text{E}\\left[X\\right] & = \\int_{-\\infty}^\\infty x\\ dF_X(x) \\\\\n& = \\int_{0}^\\infty x\\ dF_X(x) & (X\\text{ is nonnegative}) \\\\\n& = \\int_{0}^a x\\ dF_X(x) + \\int_{a}^\\infty x\\ dF_X(x) \\\\\n& \\ge \\int_a^\\infty x\\ dF_X(x)\\\\\n& \\ge \\int_a^\\infty a\\ dF_X(x) & (a \\ge x \\text{ on }(a,\\infty))\\\\\n& = a \\int_a^\\infty\\ dF_X(x) \\\\\n& = a\\Pr(X \\ge a).\n\\end{align*}\\] Dividing both sides of this inequality by \\(a\\) gives \\(\\Pr(X\\ge a) \\le \\text{E}\\left[X\\right]/a\\).\n\n\nProposition 2.2 (Convergence in MSE –> Convergence in Probability) Let \\(X_n\\) be a sequence of random variables. If \\(X_n\\overset{ms}{\\to}X\\), then \\(X_n\\overset{p}{\\to}X\\).\n\n\nProof. Suppose \\(X_n\\overset{ms}{\\to}X\\). For all \\(\\varepsilon > 0\\) \\[\\begin{align*}\n\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon) & = \\lim_{n\\to\\infty} \\Pr ((X_n - X)^2 > \\varepsilon^2) \\\\\n& \\le \\lim_{n\\to\\infty} \\frac{\\text{E}\\left[(X_n - X)^2\\right]}{\\varepsilon^2} & (\\text{Markov's Inequality}) \\\\\n& = \\frac{0}{\\varepsilon^2} & (X_n\\overset{ms}{\\to}X)\\\\\n& = 0.\n\\end{align*}\\] Therefore \\(X_n\\overset{p}{\\to}c\\).\n\nThe usefulness of Proposition 2.2 cannot be emphasized enough. Proving convergence in probability using the definition is cumbersome, so we will almost show convergence in mean square and then appeal to Proposition 2.2 to verify convergence in probability. Nevertheless, situations can arise where \\(X_n\\overset{p}{\\to}c\\), but \\(X_n \\not\\overset{ms}{\\to} c\\).\n\nExample 2.3 (Convergence in Probability but not in Mean Square) Suppose there is a sequence of random variables \\(X_n\\) that take on the values \\(0\\) and \\(n^2\\) with probabilities \\(\\Pr(X_n = 0) = 1-1/n\\) and \\(\\Pr(X_n=n^2) = 1/n\\). The expected value of this random variable is \\[\\text{E}\\left[X_n\\right] = 0(1-1/n) + n^2(1/n) = n,\\] so \\(\\text{E}\\left[X_n\\right]\\to\\infty\\) as \\(n\\to \\infty\\). This rules out \\(X_n\\) converging in mean square to any value. Nevertheless, we have \\(X_n\\overset{p}{\\to}0\\). For all \\(\\varepsilon > 0\\), \\[\\Pr(|X_n - 0| > \\varepsilon) = \\Pr(X_n \\neq 0) = \\Pr(X_n = n^2) = 1/n \\to 0.\\]\n\n\n\n2.1.3 Almost Sure Convergence\nAnother form of convergence arises if we remember \\(X_n:\\mathcal X\\to\\mathbb{R}\\) is just a function from a sample space to \\(\\mathbb{R}\\) (or \\(\\mathbb{R}^k\\)). While we usually write \\(X_n = x\\),\n\n\n2.1.4 Convergence in Distribution\nThe final notion of convergence we will use related to the probability distribution of random variables.\n\nDefinition 2.3 A sequence of random variables \\(X_n\\) converges in distribution (converges weakly) to a random variable \\(X\\), written as \\(X_n \\overset{d}{\\to}X\\), if \\[\\lim_{n\\to\\infty} F_{X_n}(x)= F_X(x).\\] In this case, we refer to \\(F_X\\) as the asymptotic distribution of \\(X_n\\), and sometimes write \\(X_n \\overset{a}{\\sim}F_X\\).\n\nFor our purposes, \\(X_n\\overset{d}{\\to}X\\) means the distribution of \\(X_n\\) can be approximated by \\(F_X\\), and this approximation becomes increasingly better as \\(n\\to\\infty\\).\n\nOne example of convergence in distribution you may be familiar with deals with the student’s \\(t-\\)distribution where the degrees of freedom \\(n\\to\\infty\\). If \\(X_n\\sim t_n\\), then \\(X_n \\overset{d}{\\to}X\\) where \\(X\\sim N(0,1)\\).\n\n\nShow code which generates figure\nexpand.grid(\n  x = seq(-4, 4, length = 10000),\n  n = c(1, 5, 10, 50, 100),\n  dist = \"Student's t\") %>% \n  mutate(val = dt(x, n)) %>% \n  ggplot(aes(x, val, color = as.factor(n))) +\n  geom_line() +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = 0, sd = 1), \n    color = \"black\", \n    size = 0.5, \n    linetype=\"dashed\"\n  ) +\n  theme_minimal() +\n  labs(color = \"t-distribution degrees of freedom, n\", y = \"Density\") +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 2.5: The t-distribution converges to the standard normal distribution (represented by the dashed black line) as the degrees of freedom increase.\n\n\n\n\n\n\nProposition 2.3 (Convergence in Probability –> Convergence in Distribution) Let \\(X_n\\) be a sequence of random variables. If \\(X_n\\overset{p}{\\to}X\\), then \\(X_n\\overset{d}{\\to}X\\).\n\n\nProof. Suppose \\(X_n\\overset{p}{\\to}X\\) and let \\(\\varepsilon > 0\\). We have, \\[\\begin{align*}\n\\Pr(X_n \\le x) & = \\Pr(X_n\\le x \\text{ and } X \\le x + \\varepsilon) + \\Pr(X_n\\le x \\text{ and } X > x + \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X\\le x - X \\text{ and } x - X < -\\varepsilon)\\\\\n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon)\\\\\n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon) + \\Pr(X - X_n > \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon)\n\\end{align*}\\] Similarly, \\[ \\Pr(X \\le x-\\varepsilon) \\le \\Pr(X_n \\le x) + \\Pr(|X_n - X| > \\varepsilon).\\] We can use these inequalities to find an upper and lower bound of \\(\\Pr(X_n \\le x)\\): \\[\\begin{align*}\n& \\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)\\le \\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) \\\\\n\\implies & \\lim_{n\\to\\infty}[\\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)]\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\lim_{n\\to\\infty}[\\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) ]\\\\\n\\implies & \\Pr(X \\le x-\\varepsilon) - \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon) }_0\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon)}_0 \\\\\n\\implies & \\Pr(X \\le x-\\varepsilon)\\le \\lim_{n\\to\\infty} \\Pr(X_n \\le x) \\le  \\Pr(X \\le x + \\varepsilon) & (X_n\\overset{p}{\\to}X)\\\\\n\\implies & F_X(x-\\varepsilon)\\le \\lim_{n\\to\\infty} F_{X_n}(x) \\le  F_X(x-\\varepsilon)\n\\end{align*}\\] This holds for all \\(\\varepsilon > 0\\), so it must be the case that \\(\\lim_{n\\to\\infty} F_{X_n}(x) = F_X(x)\\)\n\n\n\n2.1.5 Putting the Pieces Together\nThe biggest takeaway from this should be the following relation: \\[ X_n \\overset{ms}{\\to}X \\implies X_n \\overset{p}{\\to}X \\implies X_n \\overset{d}{\\to}X\\]"
  },
  {
    "objectID": "asymptotics.html#consistency",
    "href": "asymptotics.html#consistency",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.2 Consistency",
    "text": "2.2 Consistency\nOur three modes of convergence were defined for any sequence of random variables. It should come as no surprise, considering the previous examples considering whether the sample mean converged, that we are interested in the convergence of estimators \\(\\hat\\theta(\\mathbf{X})\\) as sample size increases. In particular we are interested in whether \\(\\hat\\theta(\\mathbf{X})\\) converges to the constant \\(\\theta\\in\\Theta\\) it is estimating.\n\nDefinition 2.4 An estimator \\(\\hat\\theta\\) is consistent (for estimand \\(\\theta\\)) if \\(\\hat\\theta \\overset{p}{\\to}\\theta\\).\n\nWe’ve already seen that \\(\\bar X\\) is a consistent estimator for \\(\\mu\\) when we take an iid sample from a normal distribution. Let’s investigate it’s variance-counterpart \\(S^2\\).\n\nExample 2.4 For \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), \\(S^2 = \\sum_{i=1}^n (X_i - \\bar X)/(n-1)\\) is an unbiased estimator for \\(\\sigma^2\\). This estimator’s MSE (which is just its variance as it is unbiased) is \\(2\\sigma^4/(n-1)\\) which converges to \\(0\\) as \\(n\\to\\infty\\), so \\(S^2\\) is consistent by Proposition 2.2.\n\nThis example highlights the fact that proving an unbiased estimator is consistent is a matter of showing its variance converges to 0.\n\nCorollary 2.2 Suppose \\(\\hat\\theta\\) is an unbiased estimator for \\(\\theta\\). Then \\(\\hat\\theta\\) is consistent if and only if \\(\\text{Var}\\left(\\hat\\theta\\right)\\to 0\\).\n\n\nProof. Apply Corollary 2.1 to an unbiased estimator.\n\nA second type of convergence related to estimators pertains to the bias of an estimator. In Chapter 1 we saw a few estimators that were biased, but this bias was such that it diminished as \\(n\\to\\infty\\). In effect, they were unbiased in an asymptotic sense.\n\nDefinition 2.5 An estimator \\(\\hat\\theta\\) is asymptotically unbiased if \\(\\lim_{n\\to\\infty}\\text{Bias}(\\hat\\theta, \\theta) = 0\\).\n\n\nExample 2.5 For \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), \\(\\hat\\theta = \\sum_{i=1}^n (X_i - \\bar X)/n\\) is a biased estimator for \\(\\sigma^2\\). Its bias is \\[\\text{Bias}(\\hat\\theta, \\sigma^2) = \\frac{n-1}{n}\\sigma^2 - \\sigma^2.\\] As \\(n\\to\\infty\\), this bias vanishes. To illustrate this, we can simulate estimates for various sample sizes, taking \\(X_i \\sim N(0,1)\\).\n\ntheta <- function(X){\n  sum((X - mean(X))^2)/length(X)\n}\n\nN_sim <- 1e6\nsample_sizes <- c(10, 25, 50, 100, 500)\nstore_estimates <- matrix(NA, nrow = N_sim, ncol = length(sample_sizes))\ncolnames(store_estimates) <- c(\"n=10\", \"n=25\", \"n=50\", \"n=100\", \"n=500\")\n\nfor (n in sample_sizes) {\n  col_index <- which(sample_sizes == n)\n  for (k in 1:N_sim){\n    X <- rnorm(n)\n    store_estimates[k, col_index] <- theta(X) \n  }\n}\n\n#Print bias calculated over 1,000,000 simulations\ncolMeans(store_estimates) - 1\n\n        n=10         n=25         n=50        n=100        n=500 \n-0.100384324 -0.040078003 -0.020466531 -0.010057292 -0.002121508 \n\n\n\n\nShow code which generates figure\ncolnames(store_estimates) <- NULL\nstore_estimates %>% \n  as_tibble() %>% \n  gather() %>% \n  mutate(\n    key = ifelse(key == \"V1\", \"10\", \n          ifelse(key == \"V2\", \"25\", \n          ifelse(key == \"V3\", \"50\", \n          ifelse(key == \"V4\", \"100\", \"500\")\n          )))\n  ) %>% \n  mutate(key = as.numeric(key)) %>% \n  rename(n = key)  %>% \n  ggplot(aes(value, color = as.factor(n))) +\n  geom_density() +\n  xlim(0,2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Estimates of Variance, True Value = 1\", y = \"Density\", color = \"Sample Size\")\n\n\n\n\n\nFigure 2.6: As the sample size increases, the bias of our estimator converges to zero.\n\n\n\n\nThe estimator \\(\\hat\\theta\\) is also consistent, as it converges in mean square.\n\nNeither asymptotic unbiasedness does not imply consistency, nor does consistency imply asymptotic unbiasedness.\n\nExample 2.6 (Consistent, Not Asymptotically Unbiased) Recall that the sequence of discrete random variables \\(X_n\\) with denisty \\[ f_{X_n}(x) = \\begin{cases}1-1/n& x=0\\\\ 1/n & x = n^2 \\end{cases}.\\] We established that \\(X_n\\overset{p}{\\to}0\\), so an estimator with this distribution would be a consistent estimator for \\(0\\). Despite this, the estimator would not be asymptotically unbiased, as \\(\\text{E}\\left[X_n\\right] = n\\), which tends to infinity as \\(n\\) grows.\n\n\nExample 2.7 (Asymptotically Unbiased, Not Consistent) For \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), define the estimator \\(\\hat\\mu(\\mathbf{X}) = X_1\\). We simply take the first observation to be our estimate of \\(\\mu\\). This estimator is unbiased, \\[\\text{E}\\left[\\hat\\mu\\right] = \\text{E}\\left[X_1\\right] = \\mu,\\] so it is asymptotically unbiased. Nevertheless, the estimator fails to be consistent, as \\[\\lim_{n\\to\\infty} \\Pr (|\\hat\\mu - \\mu| > \\varepsilon)= \\lim_{n\\to\\infty} \\left\\{1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma}\\right) - \\frac{1}{2}\\right]\\right\\} \\neq 0 .\\]\n\nThe incompatibility of asymptotic unbiasedness and consistency is due to the behavoir of \\(\\text{Var}\\left(X_n\\right)\\) as \\(n\\to\\infty\\).\n\nProposition 2.4 (Relating Consistency and Asymptotic Unbiasedness) Suppose \\(\\hat\\theta\\) is an estimator for \\(\\theta\\).\n\nIf \\(\\hat\\theta\\) is consistent and there exists some \\(M\\) such that \\(\\text{Var}\\left(\\hat\\theta\\right) \\le M\\) for all \\(n\\) (bounded variance), then \\(\\hat\\theta\\) is asymptotically unbiased.\nIf \\(\\hat\\theta\\) is asymptotically unbiased and \\(\\lim_{n\\to\\infty}\\text{Var}\\left(\\hat\\theta\\right) = 0\\) (vanishing variance), then \\(\\hat\\theta\\) is consistent\n\n\n\nProof. test\n\n\n\n\n\n\nFigure 2.7: Relationship between various concepts of convergence in the context of estimators"
  },
  {
    "objectID": "asymptotics.html#law-of-large-numbers",
    "href": "asymptotics.html#law-of-large-numbers",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.3 Law of Large Numbers",
    "text": "2.3 Law of Large Numbers\nIn most examples until now, the properties of estimators were implicitly a function of the underlying model the data is generated from. We established that \\(\\bar X\\) and \\(S^2\\) are consistent estimators for \\(\\mu\\) and \\(\\sigma^2\\), respectively, when \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). We know the distribution of \\(\\bar X\\), when \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). In an ideal world, we would be able to establish desirable properties of estimators under more robust settings where our specified model may include a wide array of distributions. Our first step in doing this will be introducing one of the most important results in all of probability – the law of large numbers. Proving this requires an inequality similar to Lemma 2.1.\n\nLemma 2.2 (Chebyshev’s Inequality) If \\(X\\) is a random variable with an expected value \\(\\mu\\) and variance \\(\\sigma^2\\), then for all \\(a > 0\\) \\[\\Pr(|X - \\mu| \\ge k) \\le \\frac{\\sigma^2}{k^2}.\\]\n\n\nProof. \\[\\begin{align*}\n\\Pr(|X - \\mu| \\ge k) &= \\Pr((X - \\mu)^2 \\ge k^2)\\\\\n& \\le \\frac{\\text{E}\\left[(X-\\mu)^2\\right]}{k^2} & (\\text{Markov's Inequality})\\\\\n& = \\frac{\\sigma^2}{k^2}\n\\end{align*}\\]\n\n\nTheorem 2.1 ((Khinchine’s Weak) Law of Large Numbers (LLN)) If \\((X_1,\\ldots, X_n)\\) are a set of iid random variables where \\(\\text{E}\\left[X_i\\right] = \\mu\\), then \\(\\bar X \\overset{p}{\\to}\\mu\\).\n\n\nProof. Recall that \\(\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n\\). By Chebyshev’s Inequality, \\[\\begin{align*}\n\\lim_{n\\to\\infty}\\Pr(|X_n - \\mu| \\ge \\varepsilon) \\le \\lim_{n\\to\\infty}\\frac{(\\sigma^2/n)}{\\varepsilon^2} =   \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n\\varepsilon} = 0.\n\\end{align*}\\] Therefore, \\(\\bar X\\overset{p}{\\to}\\mu\\).\n\nAnother way of thinking about the LLN is that we can approximate \\(\\mu\\) with \\(\\bar X\\), and this approximation gets better as \\(n\\to\\infty\\).\n\nExample 2.8 To illustrate the LLN, let’s simulate realizations of iid random variables from a series of different distributions and show that regardless of the distribution, \\(\\bar X \\to \\mu\\). We will use the following distributions: \\[\\begin{align*}\nX_i & \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\chi_\\mu^2\\\\\nX_i & \\overset{iid}{\\sim}\\text{Uni}(0, 2\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\text{Gamma}(2\\mu, 2)\\\\\nX_i & \\overset{iid}{\\sim}\\text{HyperGeo}(10, 20, 15\\mu)\n\\end{align*}\\] All these distributions have been selected such that \\(\\text{E}\\left[X_i\\right] = \\mu\\). For our simulations, we will take \\(\\mu = 5\\). If we plot the value of the sample mean versus the sample size \\(n\\), we see that the values converge to the true value \\(\\mu = 5\\) regardless of the distribution of \\(X_i\\).\n\n\nShow code which generates figure\nmax_n <- 1e5\nstore_estimates <- matrix(NA, ncol = 5, nrow = max_n/10)\ncolnames(store_estimates) <- c(\"Exponential\", \"Chi-Squared\", \"Uniform\", \"Gamma\", \"Hypergeometric\")\n\nexp_sample <- rexp(max_n, 1/5)\nchisq_sample <- rchisq(max_n, 5)\nunif_sample <- runif(max_n, 0, 10)\ngamma_sample <- rgamma(max_n, 10, 2)\nhyper_sample <- rhyper(max_n, 10, 20, 15)\n\nfor (n in 1:(max_n/10)) {\n  store_estimates[n,1] <- mean(exp_sample[1:(n*10)])\n  store_estimates[n,2] <- mean(chisq_sample[1:(n*10)])\n  store_estimates[n,3] <- mean(unif_sample[1:(n*10)])\n  store_estimates[n,4] <- mean(gamma_sample[1:(n*10)])\n  store_estimates[n,5] <- mean(hyper_sample[1:(n*10)])\n}\n\nstore_estimates %>% \n  as_tibble() %>% \n  gather() %>% \n  group_by(key) %>% \n  mutate(n = 10*row_number()) %>% \n  ggplot(aes(n, value, color = key)) +\n  geom_line(alpha=0.6) +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Sample Mean\", color = \"Distribution of iid Random Sample\") +\n  theme(legend.position = \"bottom\") +\n  geom_hline(yintercept = 5) +\n  ylim(4.5, 5.5)\n\n\n\n\n\nFigure 2.8: The sample mean of all samples converges to the population mean by the LLN\n\n\n\n\n\n\nExample 2.9 (Monte Carlo Simulations) In Example 1.9 we performed a Monte Carlo simulation to illustrate the bias of \\(\\hat\\theta(\\mathbf{X}) = \\sum_{i=1}^n (X_i - \\bar X)/n\\) and unbiasedness of \\(S^2\\). We did this by fixing \\(n=20\\), drawing a random sample, recording estimates \\(\\hat\\theta(\\mathbf{x})\\) and \\(S^2(\\mathbf{x})\\), and repeating this \\(k\\) times. This is nothing more than drawing \\(j=1,\\ldots,k\\) observations from the random variables \\(\\hat\\theta(\\mathbf{X})\\) and \\(S^2(\\mathbf{X})\\). By the LLN, \\[\\begin{align*}\n\\frac{1}{k}\\sum_{i=1}^k \\hat\\theta_j(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[\\hat\\theta(\\mathbf{X})\\right],\\\\\n\\frac{1}{k}\\sum_{i=1}^k S_j^2(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[S^2(\\mathbf{X})\\right],\n\\end{align*}\\] so for a large enough \\(k\\), we can approximate the expected value with its sample counterpart.\n\nThe crucial assumption made by the LLN is that \\(\\bar X\\) is calculated with an iid random sample. If we drop this assumption, then \\(\\bar X\\) needn’t estimate \\(\\mu\\) consistently.\n\nExample 2.10 (LLN Failing with Non-IID Data) Suppose \\(\\mathbf{X}= (X_1, \\ldots, X_n)\\) where \\(X_i \\sim N(-1, i)\\) if \\(i\\) is odd, and \\(X_i \\sim N(1,i)\\) is \\(i\\) is even. The data is independent, but not identically distributed. If some LLN would hold here, we would suspect that \\(\\bar X\\) would converge \\(0\\), the average of the underlying population means \\(1\\) and \\(-1\\).Let’s simulate \\(\\bar X\\) for \\(n\\) ranging from 1 to 100,000.\n\nX <- vector(\"numeric\", 1e5)\nX_bar <- vector(\"numeric\", 1e5)\n\n#draw realizations for each X_i s\nfor (i in 1:length(X)) {\n  #set mu modular arithmetic\n  mu <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  X[i] <- rnorm(1, mu, i)\n  \n  #calculate sample mean \n  X_bar[i] <- mean(X[1:i])\n}\n\nWe see that our estimates very much do not converge to any particular value.\n\n\nShow code which generates figure\nX_bar %>% \n  as_tibble() %>% \n  mutate(n = row_number()) %>% \n  ggplot(aes(n, X_bar)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n\n\n\n\n\nFigure 2.9: The sample mean of non-IID data does not satisfy the LLN\n\n\n\n\n\nThe proof of the LLN relied on Chebyshev’s equality and the fact that \\(\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n \\to 0\\) when \\(\\text{Var}\\left(X_i\\right) = \\sigma^2\\). Perhaps if we added an assumption regarding the variance of a non-iid sample, then we could salvage a result similar to the LLN. This is precisely what Chebyshev’s LLN does.\n\nTheorem 2.2 (Chebyshev’s (Weak) Law of Large Numbers) Suppose \\((X_1,\\ldots, X_n)\\) are a sample such that \\(\\text{E}\\left[X_i\\right] = \\mu_i\\), \\(\\text{Cov}\\left(X_i, X_j\\right) = \\sigma_{ij}^2\\), and \\(\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_{ij}^2 =0\\). If \\(\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu\\), then \\(\\bar X \\overset{p}{\\to}\\mu\\).\n\n\nProof. The expected value of \\(\\bar X\\) is \\[\\text{E}\\left[\\bar X\\right] = \\frac{1}{n}\\sum_{i=1}^n\\text{E}\\left[X_i\\right]= \\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu.\\] The variance is \\[ \\text{Var}\\left(\\bar X\\right) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) = \\frac{1}{n}\\sum_{i=1}^n \\sigma_{ij}^2\\to 0.\\] By Proposition Proposition 2.1, \\(\\bar X\\overset{ms}{\\to}\\mu\\), so \\(\\bar X \\overset{p}{\\to}\\mu\\).\n\n\nSuppose \\((X_1,\\ldots, X_n)\\) are an independent sample such that \\(\\text{E}\\left[X_i\\right] = \\mu_i\\), \\(\\text{Var}\\left(X_i\\right) = \\sigma_{i}^2\\), and \\(\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 =0\\). If \\(\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu\\), then \\(\\bar X \\overset{p}{\\to}\\mu\\).\n\n\nProof. If the sample is independent, then \\[\\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) =\\frac{1}{n^2}\\sum_{i=1}^n\\text{Var}\\left(X_i\\right). \\]\n\nThe reason our non-iid sample in Example Example 2.11 did not converge was because \\[ \\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\frac{1}{n^2}\\sum_{i=1}^n i = \\frac{1}{n^2}\\frac{n(n+1)}{2} = \\frac{n^2 + n}{2n^2} \\to \\frac{1}{2} \\neq 0.\\] Let’s modify it slightly so the sum of the variances does converge to zero.\n\nExample 2.11 (LLN with Non-IID Data) Suppose \\(\\mathbf{X}= (X_1, \\ldots, X_n)\\) where \\(X_i \\sim N(-1,i^{-1})\\) if \\(i\\) is odd, and \\(X_i \\sim N(1,i^{-1})\\) is \\(i\\) is even. Now we have \\[\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\left[\\lim_{n\\to\\infty}\\frac{1}{n^2}\\right]\\sum_{i=1}^\\infty i^{-1} \\to 0 .\\]\n\nX <- vector(\"numeric\", 200)\nX_bar <- vector(\"numeric\", 200)\n\n#draw realizations for each X_i s\nfor (i in 1:length(X)) {\n  #set mu modular arithmetic\n  mu <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  X[i] <- rnorm(1, mu, i^(-1))\n  \n  #calculate sample mean \n  X_bar[i] <- mean(X[1:i])\n}\n\nNow we see that \\(\\bar X\\) is converging to \\(0\\), and doing so rather quickly.\n\n\nShow code which generates figure\nX_bar %>% \n  as_tibble() %>% \n  mutate(n = row_number()) %>% \n  ggplot(aes(n, value)) +\n  geom_line(size = .5) +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n\n\n\n\n\nFigure 2.10: Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way."
  },
  {
    "objectID": "asymptotics.html#the-continuous-mapping-theorem-and-slutskys-theorem",
    "href": "asymptotics.html#the-continuous-mapping-theorem-and-slutskys-theorem",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.4 The Continuous Mapping Theorem and Slutsky’s Theorem",
    "text": "2.4 The Continuous Mapping Theorem and Slutsky’s Theorem\nAt first glance, the LLN may not seem especially useful as it only applies to the sample mean. However, when paired with two key results about convergence, the LLN becomes an indispensable tool to show the convergence of many random variables and estimators. The first of these is an extension of a key result in real analysis. A useful, and defining property, of continuous functions is that they preserve limits of numeric sequences. If \\(\\{a_n\\}\\) is a numeric sequence, then \\[\\lim_{n \\to\\infty} f(a_n) = f\\left(\\lim_{n\\to\\infty} a_n\\right) \\iff f\\text{ continuous}.\\]\n\nTheorem 2.3 (Continuous Mapping Theorem I) Suppose \\(X_n \\overset{p}{\\to}X\\), and let \\(g\\) be a continuous function. Then \\[g(X_n)\\overset{p}{\\to}g(X).\\] In other words we are able to interchange the \\(\\mathop{\\mathrm{plim}}\\) operator with a continuous function: \\[\\mathop{\\mathrm{plim}}g(X_n) = g\\left(\\mathop{\\mathrm{plim}}X_n\\right).\\]\n\nThe proof of this result can be found in Van der Vaart (2000). An immediate corollary follows from the fact that convergence in probability implies convergence in distribution.\n\nCorollary 2.3 (Continuous Mapping Theorem II) Suppose \\(X_n \\overset{p}{\\to}X\\), and let \\(g\\) be a continuous function. Then \\[g(X_n)\\overset{d}{\\to}g(X)\\]\n\nAn equally useful result involves the limit of a sums and products of convergent random variables.\n\nTheorem 2.4 (Slusky’s Theorem) Let \\(X_n\\) and \\(Y_n\\) be sequences of random variables. If \\(X_n\\overset{d}{\\to}X\\) for some random variable \\(X\\), and \\(Y_n\\overset{p}{\\to}c\\) for some constant \\(c\\), then \\[\\begin{align*}\nX_n + Y_n &\\overset{d}{\\to}X + c\\\\\nX_nY_n & \\overset{d}{\\to}Xc.\n\\end{align*}\\] Furthermore, if \\(c\\neq 0\\), \\[ X_n/Y_n \\overset{d}{\\to}X/c.\\]\n\n\nProof. Define a random vector to be \\(\\mathbf Z_n = (X_n,Y_n)\\). We have \\(\\mathbf Z_n \\overset{d}{\\to}(X,c)\\) as \\(X_n\\overset{d}{\\to}X\\) and \\(Y_n \\overset{d}{\\to}c\\) (convergence in probability implies convergence in distribution). We can apply the continuous mapping theorem to \\(g(x,y) = x + y\\), \\(g(x,y)=xy\\), and \\(g(x/y)\\) to establish the result.\n\nSlutsky’s theorem can be a bit hard to remember because it involves a sequence of random variable which converges in distribution to a random variable, and a sequence of random variables which converges in probability to a constant. These asymmetries in mode of convergence and the type of limit are essential, otherwise the result will not hold. Fortunately, the result does hold if we replace all convergences in distribution with convergence in probability (as the later implies the prior).\n\nExample 2.12 Suppose \\(X_n \\sim\\text{Uni}(0,1)\\) and \\(Y_n = - X_n\\). We have \\(X_n \\overset{d}{\\to}\\text{Uni}(0,1)\\) and \\(Y_n \\overset{d}{\\to}\\text{Uni}(-1,0)\\). Despite this \\(X_n + Y_n = 0 \\not\\overset{d}{\\to}\\text{Uni}(0,1) + \\text{Uni}(-1,0).\\)\n\n\nExample 2.13 (Consistency of Sample Variance) Example 2.4 showed that \\(S^2\\) is a consistent estimator for \\(\\sigma^2\\) when \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). We can use the continuous mapping theorem, Slutsky’s theorem, and the LLN to show that \\(S^2\\) is consistent regardless of the distribution of our iid sample. Suppose \\(\\text{E}\\left[X_i\\right] = \\mu\\), \\(\\text{E}\\left[X_i^2\\right]=\\mu_2\\), and \\(\\text{E}\\left[X_i^4\\right]=\\mu_4\\) for all \\(i\\). If we define our continuous function to be \\(g(x) = x^2\\), then \\[\\begin{align*}\nS^2 & = \\frac{1}{n-1} \\sum_{i=1}^nX_i - \\frac{1}{n-1} \\sum_{i=1}^n\\bar X^2 \\\\\n    & = \\frac{1}{n-1} \\sum_{i=1}^nX_i^2 + \\frac{n}{n-1}\\bar X^2  \\\\\n    & = \\frac{n}{n-1}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2\\right]\n\\end{align*}\\] The first term in the brackets is an unbiased estimator of \\(\\mu_2\\) with vanishing variance, so by Proposition 2.4 it is a consistent estimator for \\(\\text{E}\\left[X^2\\right]\\): \\[\\begin{align*}\n\\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right] &= \\frac{1}{n}\\left(n \\mu_2\\right) = \\mu^2\\\\\n\\lim_{n\\to\\infty}\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right) & = \\lim_{n\\to\\infty}\\frac{1}{n^2}n\\left(\\text{E}\\left[X_i^4\\right] - \\text{E}\\left[X_i^2\\right]^2 \\right) = \\lim_{n\\to\\infty}\\frac{\\mu_4 + \\mu_2^2}{n} = 0\n\\end{align*}\\] The second term in the brackets can be written as \\(g(\\bar X)\\), so by the continuous mapping theorem and the LLN, \\[ g(\\bar X) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2 \\overset{p}{\\to}\\mu^2 = g(\\mu).\\] So \\[S^2= \\underbrace{\\frac{n}{n-1}}_{\\to 1}\\Bigg[\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}X_i^2}_{\\overset{p}{\\to}\\mu_2} - \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2}_{\\overset{p}{\\to}\\mu^2} \\Bigg].\\] We can apply Slutsky’s theorem to the sum of sequences of random variables which converge in probability to constants, so \\[ S^2 \\overset{p}{\\to}\\mu_2 - \\mu^2 = \\sigma^2,\\] making \\(S^2\\) consistent."
  },
  {
    "objectID": "asymptotics.html#central-limit-theorem",
    "href": "asymptotics.html#central-limit-theorem",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.5 Central Limit Theorem",
    "text": "2.5 Central Limit Theorem\nThe LLN told us that our favorite estimator for \\(\\mu\\), \\(\\bar X\\), is consistent. We know turn to what is perhaps an even more important result regarding \\(\\bar X\\), one that may in fact be the most important results in all of probability.\n\nTheorem 2.5 (Lindeberg-Lévy Central Limit Theorem (CLT)) Suppose \\(\\mathbf{X}=(X_1,\\ldots, X_n)\\) is a sequence of random variables with \\(\\text{E}\\left[X_i\\right]=\\mu\\) and \\(\\text{Var}\\left(X_i\\right)=\\sigma^2\\). Then \\[\\sqrt{n}(\\bar X - \\mu) \\overset{d}{\\to}N(0,\\sigma^2),\\] which is also often written as \\[\\bar X\\overset{d}{\\to}N(\\mu, \\sigma^2/n),\\] or \\[\\sum_{i=1}^n X_i \\overset{d}{\\to}N(n\\mu, \\sqrt n \\sigma^2) \\]\n\nThe Lindeberg-Lévy central limit theorem is often referred to as the central limit theorem, and is the form which most are familiar with. The proof is a bit technical and requires some measure-theoretic based probability theory. It can be found in Billingsley (2008) or Durrett (2019).\n\nExample 2.14 Suppose we have an iid sample from \\(\\text{Exp}(1)\\). If we simulate 1000 realizations of \\(\\sqrt n(\\bar X - \\mu)\\) for various sample sizes, we should see that the distribution of our realizations becomes approximately normal as we increase the sample size.\n\nsample_sizes <- c(1, 5, 10, 25, 100, 1000)\nN_sim <- 10000\n\nstore_estimates <- matrix(NA, nrow = N_sim, ncol = length(sample_sizes))\ncolnames(store_estimates) <- paste(\"n =\", sample_sizes)\n\ncol_index <- 0\nfor (n in sample_sizes) {\n  col_index <- col_index + 1\n  for (k in 1:N_sim) {\n    X <- rexp(n, 1)\n    store_estimates[k, col_index] <- sqrt(n)*(mean(X) - 1)\n  }\n}\n\nEven for modest values of \\(n\\), we can see that \\(\\sqrt n(\\bar X - \\mu) \\overset{a}{\\sim}N(0, \\sigma^2)\\).\n\n\nShow code which generates figure\ndf <- store_estimates %>% \n  as_tibble() %>% \n  gather() \n\ndf$key <- factor(df$key, levels = c('n = 1','n = 5','n = 10','n = 25', 'n = 100', 'n = 1000'))\n\ndf %>% \n  ggplot(aes(value)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 50) +\n  facet_wrap(~key, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Estimates\", y = \"\")\n\n\n\n\n\nFigure 2.11: As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal.\n\n\n\n\nAn alternate way to visually test whether our estimates are normally distributed is with a quantile-quantile plot (QQ-plot), which graphs the observed quantiles of our estimates against the theoretical quantiles of a normal distribution (or those of any distribution we suspect our data is drawn from). If our estimates are (approximately) normally distributed, then the observed quantiles should be approximately equal to the theoretical quantiles of a normal distribution, forming a 45-degree line.\n\n\nShow code which generates figure\ndf %>% \n  ggplot(aes(sample = value)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  facet_wrap(~key) + \n  theme_minimal() +\n  labs(x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n\n\n\n\n\nFigure 2.12: The QQ-plot for the simulated distribution of the adjusted sample mean\n\n\n\n\n\nThe central limit theorem is similar to the LLN insofar that they only concern the estimator \\(\\bar X\\), so how useful can they really be? Well with the continuous mapping theorem and Slutsky’s theorem, the answer is very useful!\n\nExample 2.15 In Example ?exm-tdist we illustrated the fact that \\(X_n \\overset{d}{\\to}N(0,1)\\) where \\(X_n \\sim t_n\\), but we didn’t actually prove it. Directly proving this result is a matter of verify that \\[\\lim_{n\\to\\infty} F_{X_n}(x) =\\lim_{n\\to\\infty}\\int_{-\\infty}^x \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)\\sqrt{n\\pi}}\\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}} = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-x^2/2} = F_X(x)\\] where \\(\\Gamma\\) is the gamma function defined as \\[\\Gamma(t) = \\int_0^\\infty s^{t-1}e^{-s}\\ ds.\\] A much easier way to prove that \\(X_n \\overset{d}{\\to}N(0,1)\\), is by using Slutsky’s theorem and the continuous mapping theorem. First recall that \\[\\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_n,\\] so we can write \\(X_n\\) as \\(X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}\\) due to the fact that random variables are uniquely determined by their distributions. From Example ?exm-consvar, we know \\(S^2 \\overset{p}{\\to}\\sigma^2\\). By the continuous mapping theorem \\[ \\sqrt{S^2} = S \\overset{p}{\\to}\\sigma = \\sqrt{\\sigma^2}.\\] So we have \\[ X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}= \\underbrace{\\sqrt{n}(\\bar X - \\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)}\\underbrace{\\frac{1}{s}}_{\\overset{p}{\\to}\\sigma}.\\] By Slutsky’s theorem, \\[X_n \\overset{d}{\\to}\\frac{N(0,\\sigma^2)}{\\sigma} = N(0,1).\\]\n\nThe CLT can be generalized to samples of random vectors \\(\\mathbf{X}_i\\).\n\nTheorem 2.6 (CLT in Higher Dimmensions) Suppose \\((\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)\\) is a sequence of iid random vectors with \\(\\text{E}\\left[\\mathbf{X}_i\\right]=\\boldsymbol\\mu\\) and \\(\\text{Var}\\left(\\mathbf{X}_i\\right)=\\boldsymbol\\Sigma\\). Then \\[\\sqrt{n}(\\bar {\\mathbf{X}}- \\boldsymbol\\mu) \\overset{d}{\\to}N(\\mathbf{0},\\boldsymbol\\Sigma).\\]\n\nWe can also generalize the CLT to the case where variables are independent but not necessarily identically distributed.\n\nTheorem 2.7 (Lindeberg-Feller CLT) Suppose \\(\\mathbf{X}=(X_1,\\ldots, X_n)\\) is a sequence of independent random variables with \\(\\text{E}\\left[X_i\\right]=\\mu_i\\) and \\(\\text{Var}\\left(X_i\\right)=\\sigma_i^2\\), and define \\[\\begin{align*}\n\\bar \\mu = \\frac{1}{n}\\sum_{i=1}^n\\mu_i;\\\\\n\\bar \\sigma_n^2 = \\frac{1}{n}\\sum_{i=1}^n\\sigma_i^2.\n\\end{align*}\\] If the collection of variances \\(\\sigma_i^2\\) satisfies: \\[\\begin{align*}\n\\lim_{n\\to\\infty} &\\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0;\\\\\n\\lim_{n\\to\\infty} &\\bar\\sigma_n^2 = \\bar\\sigma^2,\n\\end{align*}\\] then \\(\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2).\\)\n\n\nRemark. The Lindeberg-Feller conditions stipulates that \\(\\lim_{n\\to\\infty}\\bar\\sigma_n^2 = \\bar\\sigma^2\\) and \\(\\lim_{n\\to\\infty} \\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0\\), a condition known as the Lindeberg’s condition. The condition is often presented in more general terms, but the intuition remains the same. For the CLT to hold for random variables that with different variances, we need to makes sure that no single term \\(\\sigma_i\\) dominates the standard deviation. We can think about the sample mean \\(\\bar X\\) as “mixing” many random variables \\(X_i.\\) After mixing these random variables, we hope to have a normal distribution, but that only happens if tails of the various distributions of \\(X_i\\) are negligible as \\(n\\to\\infty\\), giving us the trademark tails of a normal distribution which tapper off. Let’s consider a counterexample. Suppose \\(X_i\\) is defined on the sample space \\(\\{-i,0,i\\}\\) is distributed such that \\[\\Pr(X_i = k) = \\begin{cases} 1/2i^2 & k = -i^2 \\\\ 1/2i^2 & k = i^2 \\\\ 1 - 1/i^2&k=0\\end{cases}.\\] Graphing the density function for a few values of \\(i\\) gives us a better sense of how \\(X_i\\) behaves.\n\n\nShow code which generates figure\nexpand_grid(i = 1:5, x = -100:100) %>% \n  filter(x == i^2 | x == -i^2| x== 0) %>% \n  mutate(y = ifelse(x == 0, 1 - 1/(i^2), 1/(2*i^2))) %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = 0.2) +\n  facet_wrap(~i, ncol = 1) +\n  xlab(\"k\") +\n  geom_segment(aes(x = x, xend = x, y= 0, yend = y)) +\n  theme_minimal()\n\n\n\n\n\nFigure 2.13: Density of X_i for i = 1,…,5.\n\n\n\n\nAs \\(i\\to\\infty\\), nearly all the probability is concentrated as \\(k = 0\\). The remaining probability is at the extreme tails of the distribution \\(\\pm i^2\\), and these tails become more and more extreme (quadritically so) as \\(i\\to\\infty\\). This is the exact type of behavior Lindeberg’s condition rules out. The expectation, expectation squared, and variance of \\(X_i\\) are: \\[\\begin{align*}\n\\text{E}\\left[X_i\\right] & = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0\\\\\n\\text{E}\\left[X_i^2\\right] & = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2\\\\\n\\text{Var}\\left(X_i\\right) & = i^2 - 0^2 = i^2\n\\end{align*}\\]\nWe can verify that Lindeberg’s condition does not hold. \\[\\begin{align*}\n\\lim_{n\\to\\infty}\\bar \\sigma_n = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^n i^2 = \\lim_{n\\to\\infty}\\frac{(n+1)(2n+1)}{6} \\to\\infty\n\\end{align*}\\]\nTo simulate realizations of this random variable, we can define \\(X_i\\) using \\(U_i\\sim\\text{Uni}(0,1)\\).1\n\\[X_i = \\begin{cases}-i^2 & U_i\\in[0, 1/2i^2)\\\\ i^2 & U_i\\in[1/2i^2, 1/i^2) \\\\ 0 & U_i\\in [1/i^2,1]\\end{cases}\\]\n\nN_sim <- 100\ndraw_X <- function(n){\n  U <- runif(n)\n  prob_pos_i <- (U < 1/(2*(1:n)^2))\n  prob_neg_i <- (U > 1/(2*(1:n)^2) & U < 1/((1:n)^2))\n  (1:n)^2*prob_pos_i - (1:n)^2*prob_neg_i\n}\n\nstore_estimates <- matrix(NA, nrow = N_sim, ncol = length(sample_sizes))\ncolnames(store_estimates) <- paste(\"n =\", sample_sizes)\n\ncol_index <- 0\nfor (n in sample_sizes) {\n  col_index <- col_index + 1\n  for (k in 1:N_sim) {\n    X <- draw_X(n)\n    store_estimates[k, col_index] <- sqrt(n)*(mean(X) - 1)\n  }\n}\n\n\n\nShow code which generates figure\ndf <- store_estimates %>% \n  as.data.frame() %>% \n  gather() \n\ndf$key <- factor(df$key, levels = c('n = 1','n = 5','n = 10','n = 25', 'n = 100', 'n = 1000'))\n\ndf %>% \n  ggplot(aes(value)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 50) +\n  facet_wrap(~key, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Estimates\", y = \"\")\n\n\n\n\n\nFigure 2.14: Histograms of estimates as sample size increases.\n\n\n\n\nThe apparent distribution of our estimates is not converging to a normal distribution, as we always have a few outliers that are too plentiful relative to their distance from the mean to be drawn from a normal distribution. As \\(n\\to\\infty\\) these outliers become even more extreme. This is also evident from QQ-plots, where the points far away from the 45-degree line are drawn even farther away as \\(n\\to\\infty\\).\n\n\nShow code which generates figure\ndf %>% \n  ggplot(aes(sample = value)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  facet_wrap(~key, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n\n\n\n\n\nFigure 2.15: The QQ-plot for the simulated distribution of the adjusted sample mean\n\n\n\n\n\nWhile theoretically important, Lindeberg’s condition can be a bit hard to verify. It is much more common to appeal to a stronger assumption which gives rise to a second CLT that holds for independent, but not necessarily identically distributed, random variables. This final CLT may not be as general as the Lindeberg-Feller CLT, but it is much easier to work with.\n\nTheorem 2.8 (Lyapunov CLT) Suppose \\(\\mathbf{X}=(X_1,\\ldots, X_n)\\) is a sequence of independent random variables with \\(\\text{E}\\left[X_i\\right]=\\mu_i\\) and \\(\\text{Var}\\left(X_i\\right)=\\sigma_i^2\\). If \\(\\text{E}\\left[\\left\\lvert X_i - \\mu_i\\right\\rvert^{2+\\kappa}\\right]\\) is finite for some \\(\\kappa > 0,\\) then \\(\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2)\\), where \\(\\bar\\sigma = (1/n)\\sum_{i=1}^n \\sigma_i.\\)"
  },
  {
    "objectID": "asymptotics.html#delta-method",
    "href": "asymptotics.html#delta-method",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.6 Delta Method",
    "text": "2.6 Delta Method\nSlutsky’s theorem and the continuous mapping theorem in tandem with the LLN give us the ability to prove that certain functions of sample means are convergent. Is it possible that we can do something similar with the CLT to find the asymptotic distribution of functions of sample means?\nSuppose \\(g\\) is a function of \\(\\bar X\\), where a CLT applies to the random sample \\(\\mathbf{X}\\). We know \\(\\sqrt n(\\bar X-\\mu)\\overset{a}{\\sim}N(0, \\sigma^2)\\). Is it possible to conclude that \\(\\sqrt n(g(\\bar X)-g(\\mu))\\overset{a}{\\sim}N(0, \\tilde\\sigma^2)\\) for some \\(\\tilde\\sigma^2\\)? Furthermore, can we determine \\(\\tilde\\mu\\) and \\(\\tilde\\sigma^2\\) only knowing \\(g\\), \\(\\mu\\), and \\(\\sigma^2\\)?\nWe have information about the difference \\(\\bar X-\\mu\\) and want information about the difference \\(g(\\bar X)-g(\\mu)\\). Situations where we know something about behavior in the domain of a function and want to relate it to the function’s behavior in the codomain are common place in math, but in particular in real analysis. This is where the mean value theorem saves the day. Assuming \\(g\\) is continuously differentiable and fixing \\(n\\), there exists some \\(T_n\\) in between \\(\\bar X\\) and \\(\\theta\\) (\\(\\bar X< T_n < \\mu\\) or \\(\\mu < T_n < \\bar X\\)) such that: \\[\\begin{align*}\n& \\frac{g(\\bar X)-g(\\mu)}{\\bar X - \\mu} = g'(T_n)\\\\\n\\implies & g(\\bar X) = g(\\mu) + g'(T_n)(\\bar X-\\mu)\\\\\n\\implies & \\sqrt{n}[g(\\bar X) - g(\\mu)] = g'(T_n)\\sqrt{n} (\\bar X-\\mu)\n\\end{align*}\\] If we let \\(n\\) vary, we have a sequence of random variables \\(\\{T_n\\}\\) such that \\(\\left\\lvert T_n -\\mu\\right\\rvert < \\left\\lvert\\bar X - \\mu\\right\\rvert\\). By the LLN \\(\\left\\lvert\\bar X - \\mu\\right\\rvert \\overset{p}{\\to}0\\), so \\(\\left\\lvert T_n -\\mu\\right\\rvert \\overset{p}{\\to}0\\), which is equivalent to \\(T_n \\overset{p}{\\to}\\mu\\). By the continuous mapping theorem, \\(g(T_n) \\overset{p}{\\to}g(\\mu)\\). This means \\[\\sqrt{n}[g(\\bar X) - g(\\mu)] = \\underbrace{g'(T_n)}_{\\overset{p}{\\to}g(\\mu)}\\cdot\\underbrace{\\sqrt{n} (\\bar X-\\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)},\\] so Slutsky’s theorem gives \\[\\sqrt{n}[g(\\bar X) - g(\\mu)] \\overset{d}{\\to}g(\\mu)\\cdot N(0,\\sigma^2) = N(0, \\sigma^2[g(\\mu)]^2).\\] This all is contingent on \\(g\\) not being a function of \\(n\\), otherwise things fall apart when we apply limiting processes.\nThis result is known as the delta method, and applies to any sequence of random variables that is asymptotically normal. It is also readily generalized to higher dimensions where \\(\\mathbf g\\) is a vector valued function.\n\nTheorem 2.9 (Delta Method) Suppose \\((\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)\\) is a sequence of random vectors such that \\(\\sqrt n (\\mathbf{X}_n - \\mathbf t) \\overset{d}{\\to}N(\\mathbf 0, \\boldsymbol\\Sigma)\\) for some vector \\(\\mathbf t\\) in the interior of \\(\\mathcal X\\). If \\(\\mathbf g(\\mathbf{X}_n)\\) is a vector valued function that:\n\nis continuously differentiable,\ndoes not involve \\(n\\),\n\\(\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t) \\neq 0\\),2\n\nthen,\n\\[ \\sqrt n \\left[\\mathbf g(\\mathbf{X}_n) - \\mathbf g(\\mathbf t)\\right] \\overset{d}{\\to}N\\left(\\mathbf 0, \\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]\\boldsymbol\\Sigma\\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]'\\right)\\]\n\n\nExample 2.16 Return to the example where \\(X_i\\overset{iid}{\\sim}\\text{Exp}(1)\\), giving \\(\\text{E}\\left[X_i\\right] = 1\\) and \\(\\text{Var}\\left(X_i\\right) = 1\\). By the CLT, \\(\\sqrt n(\\bar X - 1)\\overset{d}{\\to}N(0,1)\\). If \\(g(t) = t^2 +3\\), what is the asymptotic distribution of \\(\\sqrt{n}[g(\\bar X) - g(1)]\\)? According to the delta method we have \\[\\begin{align*}\n&\\sqrt{n}[g(\\bar X) - g(1)]  \\overset{a}{\\sim}N(0, 1[g'(1)]^2),\\\\\n\\implies & \\sqrt{n}\\left[\\bar X^2 - 1\\right] \\overset{a}{\\sim}N(0, 4).\n\\end{align*}\\]\n\nN_sim <- 10000\nsample_size <- 10000\n\ng <- function(t){\n  t^2 + 3\n}\n\ng_of_mean <- sapply(1:N_sim, function(x) g(mean(rexp(sample_size))))\nestimates <- sqrt(sample_size)*(g_of_mean - g(1))\n\nWe can plot a histogram of our estimates and overlay the distribution \\(N(0,4)\\).\n\n\nShow code which generates figure\ntibble(x = estimates) %>% \n  ggplot(aes(x)) +\n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 100) + \n  xlab(\"Estimates of √n(g(X) - g(1)) \") +\n  theme_minimal() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), color = \"red\")\n\n\n\n\n\nFigure 2.16: Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method\n\n\n\n\n\nThe big takeaway from the delta method is that estimators which are nice functions of sample means will be asymptotically distributed according to a normal distribution. Furthermore, any nice function of such an estimator will also have a normal asymptotic distribution!\n\n\n\n\n\nFigure 2.17: A mediocre meme"
  },
  {
    "objectID": "asymptotics.html#little-o_p-big-o_p-and-taylor-expansions",
    "href": "asymptotics.html#little-o_p-big-o_p-and-taylor-expansions",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.7 Little \\(o_p\\), Big \\(O_p\\), and Taylor Expansions",
    "text": "2.7 Little \\(o_p\\), Big \\(O_p\\), and Taylor Expansions\nWe’ve talked a lot about whether or not random variables converge, and how they converge, but not the rate at which they converge. We can introduce some notation that allows us to quantify this rate.\n\nDefinition 2.6 Given a sequence of random variables \\(X_n\\), we say \\(X_n\\) is little “O.P” of \\(n^k\\), denoted \\(X_n = o_p(n^k)\\), if \\(X_n / n^k\\overset{p}{\\to}0\\).\n\nNote that \\(X_n\\overset{p}{\\to}0\\) implies that \\(X_n = o_p(1)\\). The use of “=” is a bit misleading in this definition, as \\(X_n = o_p(n^k)\\) does not establish any equality, instead referring to how \\(X_n\\) behaves asymptotically. For instance, if we have two sequences of random variables \\(X_n\\) and \\(Y_n\\) such that \\(X_n\\overset{p}{\\to}X\\) and \\(Y_n\\overset{p}{\\to}0\\), we have $ X_n + Y_n X + 0 $, but could write \\(X_n + Y_n\\) as \\(X_n + o_p(1)\\). This emphasizes the sequence \\(X_n\\), and frames \\(Y_n\\) as some negligible remainder term that tends to zero.\n\nDefinition 2.7 Given a sequence of random variables \\(X_n\\), we say \\(X_n\\) is big “O.P” of \\(n^k\\), denoted \\(X_n = O_p(n^k)\\), if for all \\(\\varepsilon > 0\\), there exists some \\(\\delta\\) and \\(N\\) such that \\(\\Pr(|X_n/n^k| \\ge \\delta) <\\varepsilon\\) for all \\(n > N\\). In other words, \\(X_n/n^k\\) is bounded in probability.\n\nWe are most interested in the case where \\(X_n = O_p(1)\\). If this is the case, then as \\(n\\to\\infty\\), we can bound the area in the tails of \\(f_{X_n}\\) by some constant \\(\\delta\\) such that the area is negligible (less than \\(\\varepsilon\\)).\n\nExample 2.17 We know that \\(\\bar X \\overset{d}{\\to}N(\\mu, \\sigma^2/n)\\) when \\(\\mathbf{X}\\) is an iid sample. We have that \\(\\bar X = O_p(1)\\). We have \\[\\Pr(|\\bar X/1| \\ge \\delta) = \\Pr(-\\bar X\\ge -\\delta \\text{ and }\\delta \\le \\bar X) = 2\\cdot\\Pr(\\bar X\\ge \\delta) = 2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right].\\] If we take the limit of this as \\(n\\to \\infty\\) we have \\[ \\lim_{n\\to \\infty}\\Pr(|X\\bar X/1| \\ge \\delta) = \\lim_{n\\to \\infty}2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right] = 0.\\] By the definition of a limit, there must exists some \\(N\\) such that \\(\\Pr(|\\bar X/1| \\ge \\delta) < \\varepsilon\\) for any \\(n > N\\), so \\(\\bar X = O_p(1)\\).\n\n\nProposition 2.5 If \\(X_n \\overset{d}{\\to}X\\), then \\(X_n = O_p(1)\\).\n\n\nCorollary 2.4 If \\(X_n = o_p(1)\\), then \\(X_n = O_p(1)\\).\n\nA common place to encounter \\(o_p\\) is when performing Taylor expansions.\n\nExample 2.18 (Taylor’s Theorem) Taylor’s theorem, as given in Rudin (1976), tells us that if \\(f:\\mathbb R\\to\\mathbb R\\) is \\(k-\\)times differentiable at a point \\(a\\in \\mathbb R\\), then there is some element \\(c\\in (a,b)\\) such that \\[\\begin{align*}\nf(b) & = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{k!}(b-a)^j + \\frac{f^{(n)}(c)}{k!}(b-a)^k.\n\\end{align*}\\] For \\(n = 2\\), we have the mean value theorem: \\[ f(b)= f(a) + f'(c)(b-a).\\] If we let \\(a\\to b\\), then \\[\\begin{align*}\n&\\lim_{a\\to b}f(b)  = \\sum_{j=0}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k.\\\\\n\\implies & f(b)  =\\lim_{a\\to b} f(a) + \\sum_{j=1}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j +  \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & f(b)  = f(b) + \\underbrace{\\sum_{j=1}^{k-1}\\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!}(b-a)^k = 0\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!(b-a)^k} = 0\\\\\n\\implies & \\frac{f^{(k)}(c)}{k!} = o(|a-b|^k)\\\\\n\\end{align*}\\] Here, \\(o\\) is the deterministic counterpart of \\(o_p\\) (we’re not working with random variables just yet). This means we can write Taylor’s theorem as \\[ f(b) = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).\\] In the event \\(f\\) is infinitely differentiable we can make this approximation arbitrarily accurate, giving rise toa function’s Taylor series.\nNow suppose \\(f_n(X)\\) is a sequence of functions of random variables, where the subscript \\(n\\) emphasizes that \\(f_n\\) is a random variable. IF we apply Taylor’s theorem to \\(f_n(X)\\) we have \\[f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)\\] for realizations of the random variable \\(a,b,c\\in\\mathcal X\\) where \\(c\\in(a,b)\\). Assuming \\(k \\ge 1\\), then \\(o_p(|a-b|^k)\\) implies \\(o_p(1)\\), so \\[f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).\\]"
  },
  {
    "objectID": "asymptotics.html#asymptotically-normal-estimators",
    "href": "asymptotics.html#asymptotically-normal-estimators",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.8 Asymptotically Normal Estimators",
    "text": "2.8 Asymptotically Normal Estimators\nWhen putting our asymptotic tools to work on an estimator of interest, we will almost always find that it converges to a normal distribution, is consistent, and that the rate of convergence is linked to \\(\\sqrt{n}\\).\n\nDefinition 2.8 An estimator \\(\\hat{\\boldsymbol{\\theta}}\\) is \\(\\sqrt{n}-\\)consistent asymptotically normal (root-n CAN), if \\[\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}- \\boldsymbol{\\theta}) \\overset{d}{\\to}N(\\mathbf 0, \\mathbf V)\\] for a PSD matrix \\(\\mathbf V\\). Equivalently, \\[ \\hat{\\boldsymbol{\\theta}}\\overset{a}{\\sim}N(\\boldsymbol{\\theta}, \\mathbf V/n).\\] We refer to \\(\\mathbf V/n\\) as the asymptotic variance of \\(\\hat{\\boldsymbol{\\theta}}\\) and write \\(\\text{Avar}\\left(\\hat{\\boldsymbol{\\theta}}\\right) = \\mathbf V /n\\).\n\n“\\(\\sqrt n-\\)” emphasizes the fact that \\(\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = O_p(1)\\), which is equivalent to \\(\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\theta}+ O_p(n^{-1/2})\\). In other words, as \\(n\\to\\infty\\) the error term associated with our estimate decreases at a rate of \\(n^{1/2}\\). A fourfold increase in observations results in half the error. We also have that \\(\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = o_p(1)\\), so \\(\\hat{\\boldsymbol{\\theta}}\\overset{p}{\\to}\\boldsymbol{\\theta}\\), hence the “consistent” in the previous definition. We also have that \\(\\hat{\\boldsymbol{\\theta}}\\) is asymptotically unbiased if it is root-n CAN, as \\(\\text{E}\\left[\\hat{\\boldsymbol{\\theta}}\\right]\\to \\boldsymbol{\\theta}\\). This will be the one of, if not the, most important property an estimator can posses.\nOur final example highlights some interesting properties of root-N CAN estimators in the context of the mean value theorem. This will be especially important in Section @ref(extremum-estimators)."
  },
  {
    "objectID": "asymptotics.html#thinking-beyond-mathbbrk",
    "href": "asymptotics.html#thinking-beyond-mathbbrk",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.9 Thinking Beyond \\(\\mathbb{R}^k\\)",
    "text": "2.9 Thinking Beyond \\(\\mathbb{R}^k\\)\n\n2.9.1 Normed Vector Spaces and Metric Spaces\nA normed vector space \\(V\\) defined over a field \\(F\\) is, as the name implies, a vector space equipped with a norm \\(\\left\\lVert\\cdot\\right\\rVert:V\\mapsto [0,\\infty)\\) satisfying:\n\n\\(\\left\\lVert v\\right\\rVert = 0 \\iff v = 0\\), where \\(0\\) is the additive identity;\n\\(\\left\\lVert av\\right\\rVert = \\left\\lvert a\\right\\rvert\\left\\lVert v\\right\\rVert\\) for all \\(a\\in F\\) and \\(v\\in V\\);\n\\(\\left\\lVert w + v\\right\\rVert \\le \\left\\lVert w\\right\\rVert + \\left\\lVert v\\right\\rVert\\) for all \\(w,v\\in V\\).\n\nA norm measures the the “distance” an element of \\(V\\) is from the origin \\(0\\). A metric space \\((X,d)\\) is some set \\(X\\) with a metric \\(d:X\\times X\\to [0,\\infty)\\) such that:\n\n\\(d(x,y) = 0 \\iff x=y\\) for \\(x,y\\in X\\);\n\\(d(y,x)=d(x,y\\) for all \\(x,y\\in X\\);\n\\(d(x,z) \\le d(x,y) + d(y,z)\\).\n\nA metric measures the distance between any two points in a metric space. Any normed vector space is a metric space if we define\n\\[d(x,y)= \\left\\lVert x-y\\right\\rVert\\] for \\(x,y\\in V\\).\n\nExample 2.19 (Euclidean Space) \\(\\mathbb{R}^k\\) is a vector space, where the norm is given as \\[ \\left\\lVert\\mathbf{x}\\right\\rVert = \\left(\\sum_{i=1}^k x_i^2\\right)^{1/2},\\] and is called the Euclidean norm. To measure the distance between two vectors in \\(\\mathbb{R}^k\\) we use a familiar formula. \\[ \\left\\lVert\\mathbf{x}-\\mathbf{y}\\right\\rVert = \\left(\\sum_{i=1}^k (x_i-y_i)^2\\right)^{1/2}.\\]\n\n\nExample 2.20 (Euclidean Space with the \\(p\\)-norm) We could also define the vector space \\(\\mathbb{R}^k\\) with the norm \\[\\left\\lVert\\mathbf{x}\\right\\rVert_p = \\left(\\sum_{i=1}^k \\left\\lvert x_i\\right\\rvert^p\\right)^{1/p}.\\] If \\(p = 1\\), then \\[\\left\\lVert\\mathbf{x}- \\mathbf{y}\\right\\rVert_1 = \\sum_{i=1}^n \\left\\lvert x_i - y_i\\right\\rvert,\\] whose name follows from how you would define distance if you were driving between two points in a city with a grid layout. If \\(p=2\\) we have the Euclidean norm. Why bother considering values of \\(p\\) other than \\(2\\)? We can think of \\(p\\) as the way we weight the component-wise “distances” from the origin \\(\\left\\lvert x_1\\right\\rvert,\\left\\lvert x_2\\right\\rvert,\\ldots \\left\\lvert x_k\\right\\rvert\\). For \\(p = 2\\), we square each of these distances (meaning the result is always positive rendering the absolute value moot), assigning more weight to components where \\(x_i\\) is relatively large. What happens as \\(p\\to\\infty?\\)? Let’s look at some plots.\n\n\nShow code which generates figure\np_norm <- function(p, x){\n  (sum(abs(x)^p))^(1/p)\n}\n\ntibble(x1 = c(1, 1, 1, 1),\n       x2 = c(0, 2, 2, 2),\n       x3 = c(0, 0, 3, 3),\n       x4 = c(0, 0, 0, -4)\n       ) %>% \n  mutate(vector = ifelse(x4 == -4, \"x = (1,2,3,-4)\", ifelse(x3 == 3, \"x = (1,2,3)\", ifelse(x2 == 2, \"x = (1,2)\", \"x = 1\"))))  %>% \n  expand_grid(p = seq(1,10, length = 100)) %>% \n  rowwise() %>% \n  mutate(norm = p_norm(p, c(x1, x2, x3, x4))) %>% \n  ggplot(aes(p, norm)) +\n  geom_line() + \n  facet_wrap(~vector, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"p\", y = \"p-norm\")\n\n\n\n\n\nFigure 2.18: p norm\n\n\n\n\nIt appears that \\[ \\lim_{p\\to\\infty}\\left\\lVert\\mathbf{x}\\right\\rVert_p= \\max_{x_i}\\left\\lvert x_i\\right\\rvert.\\] For this reason we define the limiting case of the \\(p-\\)norm as \\[ \\left\\lVert\\mathbf{x}\\right\\rVert_\\infty= \\max_{x_i}\\left\\lvert x_i\\right\\rvert.\\] A useful illustration is of the unit circle in \\((\\mathbb{R}^k,\\left\\lVert\\cdot\\right\\rVert_p)\\) for various values of \\(p\\)\n\n\n\n\n\nFigure 2.19: test\n\n\n\n\n\nWe want to think about a vector space formed by random variables where each vector is its own random variable. Before jumping straight to this case, let’s consider a vector space formed by real functions, and then worry about randomness later on.\n\nExample 2.21 (The Space of Real Continuous Bounded Functions) Define the space \\(C(X)\\) as\n\\[C(X) = \\{f:X\\to\\mathbb{R}\\mid X\\subset\\mathbb{R}, f\\text{ continuous and bounded}\\}.\\] This is a vector space over the field \\(\\mathbb{R}\\) where addition and scalar multiplication are defined as: \\[\\begin{align*}\n(f +g)(x)&=f(x)+g(x),\\\\\n(cf)(x) & = c\\times f(x).\n\\end{align*}\\]\nHow would we define a norm on a space of functions though? Firstly, by the definition of addition of vectors, the \\(0\\) elements is \\(f(x) = 0\\). So how do we measure the distance between some \\(f\\in C(X)\\) and \\(f(x) = 0\\)? In the event \\(X\\) is an interval of \\(\\mathbb{R}\\), it contains uncountably many points, so we cannot use the \\(p\\)-norm and sum \\(|f(x)|\\) over all the values of \\(x\\). We can however integrate over all such values, as one interpretation of the integral is as an uncountably infinite counterpart to a countable sum. We define the \\(p-\\)norm on \\(C(X)\\) as \\[ \\left\\lVert f\\right\\rVert_p = \\left[\\int_X \\left\\lvert f(x)\\right\\rvert^p\\ dx\\right]^{1/p}.\\] In the event we want to take \\(p\\to\\infty\\), we have \\[ \\left\\lVert f\\right\\rVert_\\infty = \\sup_{x\\in X}\\left\\lvert f(x)\\right\\rvert.\\] We know can use this norm to define the distance between two functions in \\(C(X)\\), \\[ \\left\\lVert f\\right\\rVert_\\infty = \\sup_{x\\in X}\\left\\lvert f(x) - g(x)\\right\\rvert.\\] This gives the distance between two functions as the distance between their evaluated values where this distance is its largest. We refer to \\(\\left\\lVert\\cdot\\right\\rVert_\\infty\\) on \\(C(X)\\) as the uniform norm or supremum norm.\n\n\n\n2.9.2 Convergence and Continuity\nGiven a metric space \\((X,d)\\), a sequence of elements \\(\\{x_n\\}\\subset X\\) converges to an element \\(x\\) if in \\(X\\) for all \\(\\varepsilon > 0\\), there exists some integer \\(N\\in \\mathbf{Z}^+\\) such that \\(d(x_n,x)<\\varepsilon\\) for all \\(n>N\\). If \\(\\{x_n\\}\\) converges to \\(x\\), we write \\(x_n\\to x\\) or \\(\\lim_{n\\to\\infty}x_n=x\\). It’s important to remember that convergence is defined in relation to the metric space \\(X\\). A sequence \\(\\{x_n\\}\\) may converge in one metric space but not another, so it’s important to remember what space we’re working in, and the metric the space is equipped with.\n\nExample 2.22 (Euclidean Space) A sequence of vectors \\(\\{\\mathbf{x}_n\\}\\subset \\mathbb{R}^k\\) converges to \\(\\mathbf{x}\\) if for all \\(\\varepsilon > 0\\), there exists some integer \\(N\\in \\mathbb{Z}^+\\) such that \\[\\left\\lVert x_n -x\\right\\rVert =\\left(\\sum_{i=1}^k (x_{n,i} - x_i)^2\\right)^{1/2} <\\varepsilon\\] for all \\(n>N\\). It can be shown that a sequence of vectors only converges to a limit if and only if each of its components converges to their respective limits.\n\n\nExample 2.23 (Pointwise Convergence) Suppose we have a sequence of real functions \\(\\{f_n\\}\\) where \\(f_n:X\\to \\mathbb{R}\\) for some \\(X\\subset \\mathbb{R}\\). One way to think about \\(f_n\\) converging to some limit is by considering \\(\\{f_n(x)\\}\\subset \\mathbb{R}\\) as a sequence in Euclidean space for each \\(x\\in X\\). If \\(f_n(x)\\to f(x)\\) for all \\(x\\in X\\), then we say \\(f\\) converges pointwise to \\(f\\), and write \\(f_n\\to f\\). In other words, \\(f_n\\to f\\) if for all \\(\\varepsilon > 0\\), there exists some \\(N_x\\) such that \\(\\left\\lvert f_n(x) - f(x)\\right\\rvert< \\varepsilon\\) for all \\(n> N\\), for each \\(x\\in X\\).\n\n\nExample 2.24 (Uniform Convergence) In the definition of pointwise convergence, note that \\(N_x\\) depends on \\(x\\). This means that while \\(\\left\\lvert f_n(x_1) - f(x_1)\\right\\rvert < \\varepsilon\\), it may not be the case that \\(\\left\\lvert f_n(x_2) - f(x_2)\\right\\rvert < \\varepsilon\\). We can strengthen pointwise convergence by insuring that \\(N\\) does not depend on \\(x\\), and \\(\\{f_n\\}\\) gets arbitrarily close to \\(f\\) uniformly on \\(X\\). We say \\(f\\) converges uniformly to \\(f\\), and write \\(f_n\\overset{uni}\\to f\\) if for all \\(\\varepsilon >0\\), there exists some \\(N\\) (idenpendent of \\(x\\)) such that \\(\\left\\lvert f_n(x) - f(x)\\right\\rvert< \\varepsilon\\) for all \\(n> N\\) and any \\(x\\in X\\). Uniform convergence has some great properties that pointwise convergence lacks,3 making it much more attractive, albeit a much stricter condition. A more natural formulation of uniform convergence arises by considering the vector space of bounded continuous functions \\(f:X\\to \\mathbb{R}\\), \\(C(X)\\), equipped with the supremum norm \\(\\left\\lVert\\cdot\\right\\rVert_\\infty\\). In this case, \\(f_n\\to f\\) (in \\((C(X),\\left\\lVert\\cdot\\right\\rVert_\\infty)\\)) if for all \\(\\varepsilon > 0\\) there exists some \\(N\\in \\mathbb Z^+\\) such that \\[d(f_n,f) = \\left\\lVert f_n-f\\right\\rVert = \\sup_{x\\in X}\\left\\lvert f_n-f\\right\\rvert < \\varepsilon\\] for all \\(n > N\\). By the definition of \\(\\left\\lVert\\cdot\\right\\rVert_\\infty\\), \\[\\left\\lvert f_n(x)-f(x)\\right\\rvert \\le \\sup_{x\\in X}\\left\\lvert f_n-f\\right\\rvert \\le \\varepsilon\\] for all \\(x\\in X\\) when \\(n>N\\).\n\nRecall from real analysis that a function \\(f:X\\to Y\\) defined on metric spaces \\(X\\) and \\(Y\\), each equipped with metrics \\(d_X(\\cdot,\\cdot)\\) and \\(d_Y(\\cdot,\\cdot)\\), is continuous at \\(x\\in X\\) if for all \\(\\varepsilon>0\\), there exists some \\(\\delta_x\\) such that \\[ d_Y(f(x),f(p)) < \\varepsilon\\] when \\(d_X(x,p) < \\delta_x\\). That is, for any arbitrarily small distance \\(\\varepsilon\\), we can find some point \\(p\\) which is arbitrarily close (within a distance of \\(\\delta_x\\) to be exact) to \\(x\\) such that the distance between \\(f(x)\\) and \\(f(p)\\) is less than \\(\\varepsilon\\). If \\(f\\) is continuous for all \\(p\\) in some subset \\(E\\subset X\\), then we say \\(f\\) is continuous on \\(E\\).\nOne of the more subtle parts about the definition of continuity is that \\(\\delta_x\\) depends on \\(x\\). If we want to ensure that a single value of \\(\\delta\\) can be used for all \\(x\\in E\\), then we need to strengthen the definition of continuity. We say \\(f\\) is uniformly continuous on \\(E\\) if for every \\(\\varepsilon > 0\\), there exists some \\(\\delta\\) independent of \\(x\\) such that \\[d_Y(f(x),f(p)) < \\varepsilon\\] whenever \\(d(x,p) <\\delta\\) for all \\(x,p\\in E\\). Now we can use one uniform \\(\\delta\\) on the entire set \\(x\\in E\\). Uniform continuity is much stronger then continuity, but it naturally arises in many common settings.4\nNow suppose we have a collection of a functions \\(\\mathcal F\\), each of which is uniformly continuous on \\(E\\subset X\\). This means for all \\(f\\in\\mathcal F\\) and \\(\\varepsilon > 0\\), there exists a \\(\\delta_{f}\\) independent of \\(x\\) such that \\[d_Y(f(x),f(p)) < \\varepsilon\\] when \\(d_X(x,p) < \\delta_f\\). Of course \\(\\delta_f\\) depends on the function \\(f\\in\\mathcal F\\) here. Once we start looking at multiple functions, we need to adjust \\(\\delta_f\\) accordingly. Even if \\(\\delta_f\\) is uniform across \\(x\\in E\\) for each separate \\(f\\), it isn’t guaranteed it will be uniform across \\(x\\in E\\) uniformly across \\(f\\in \\mathcal F\\). A special case we’re interested in is when \\(\\mathcal F\\) is some sequence sequence of functions indexed by \\(n\\), \\(\\mathcal F=\\{f_n\\mid n\\in\\mathbf{Z}^+\\}\\).\n\nExample 2.25 Define the sequence of functions \\(f_n:[0,1]\\to \\mathbb{R}\\) where \\(f_n(x)=x^n\\) and let our collection of functions be this sequence. We will focus our attention on \\(x = 1\\). The function \\(f_n\\) is continuous on \\([0,1]\\) for all \\(n\\). In fact, one can show that for all \\(\\varepsilon > 0\\),\n\\[\\left\\lvert f_n(x) - f_n(p)\\right\\rvert < \\varepsilon\\] whenever \\[ \\delta_{n,x} = \\frac{\\varepsilon}{n(x + 1)^{n-1}}.\\] We can go a step further and conclude that \\(f_n\\) is uniformly continuous on \\([0,1]\\), by setting \\[ \\delta_{n} = \\inf_{x\\in[0,1]}\\delta_{n,x} = \\frac{\\varepsilon}{n(1 + 1)^{n-1}} = \\frac{\\varepsilon}{n2^{n-1}}.\\] By construction \\(\\delta_{n}\\) will now work on the entire interval \\([0,1]\\) and doesn’t depend on \\(x\\). Unfortunately, \\(\\delta_n\\) still depends on the function \\(f_n\\) (hence the subscript \\(n\\)), and we cannot perform the same “trick” to get a \\(\\delta\\) common to all \\(n\\).\n\nConvergence of Stochastic Process\n\n\nFunctional Central Limit Theorem"
  },
  {
    "objectID": "asymptotics.html#further-reading",
    "href": "asymptotics.html#further-reading",
    "title": "2  Asymptotic Properties of Estimators",
    "section": "2.10 Further Reading",
    "text": "2.10 Further Reading\n\nEric Zivot’s primer on asymptotics\nWooldridge (2010), Chapter 3\nGreene (2018), Appendix D\nVan der Vaart (2000)\nWhite (1984)\n\n\n\n\n\n\n\nBillingsley, Patrick. 2008. Probability and Measure. John Wiley & Sons.\n\n\nDurrett, Rick. 2019. Probability: Theory and Examples. Vol. 49. Cambridge university press.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson Education.\n\n\nNewey, Whitney K, and Daniel McFadden. 1994. “Large Sample Estimation and Hypothesis Testing.” Handbook of Econometrics 4: 2111–2245.\n\n\nRudin, Walter. 1976. Principles of Mathematical Analysis. Vol. 3. McGraw-hill New York.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press.\n\n\nWhite, Halbert. 1984. Asymptotic Theory for Econometricians. Academic press.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. MIT press."
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "3  Hypothesis Testing",
    "section": "",
    "text": "Until now, we’ve focused on point estimation, but that’s only half the picture when it comes to statistics. The other half is inference. How do we test hypotheses about the true \\(\\boldsymbol{\\theta}_0\\), and make inferences about the underling data generating process \\(P_{\\boldsymbol{\\theta}_0}\\in \\mathcal P\\)? An exhaustive treatment of inference is due to Romano and Lehmann (2005), while Bickel and Doksum (2015) offer an equally technical, yet briefer, treatment."
  },
  {
    "objectID": "testing.html#decision-theory",
    "href": "testing.html#decision-theory",
    "title": "3  Hypothesis Testing",
    "section": "3.1 Decision Theory",
    "text": "3.1 Decision Theory\nIn Chapter 1 we defined a (point) estimator as a function from a sample space \\(\\mathcal X\\) to the parameter space \\(\\Theta\\). For some specified model \\(\\mathcal P\\), we observe a realization of the random vector \\(\\mathbf{X}\\sim P_{\\boldsymbol{\\theta}_0}\\) for \\(P_{\\boldsymbol{\\theta}_0}\\in \\mathcal P\\), and then calculate an estimate \\(\\hat{\\boldsymbol{\\theta}}\\). This process is a special case of a more general framework that unifies point estimation and hypothesis testing.\nConsider an action space \\(\\mathcal A\\). A decision process/rule \\(\\delta:\\mathcal X\\to \\mathcal A\\) prescribes an action given an observation of a random vector \\(\\mathbf{X}\\) defined on \\(\\mathcal X\\). The set of all decision rules is \\(\\mathcal D\\). If the true data generating process is \\(P_\\boldsymbol{\\theta}\\in\\mathcal P\\), the cost of taking the action \\(a\\) is given by the loss function \\(l(P_\\boldsymbol{\\theta}, a)\\) where \\(l:\\mathcal P\\times \\mathcal A\\to\\mathbb R^+\\). The loss associated with a decision rule is \\(l(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X}))\\). We cannot calculate this loss, as we do not know \\(P_\\boldsymbol{\\theta}\\). Instead we average the loss over \\(\\Theta\\) (which is the same as over all \\(P_\\theta\\) if \\(\\mathcal P\\) is identified), giving a risk function \\(R:\\mathcal P\\to \\mathbb R^+\\) defined as \\(\\text{E}\\left[l(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X}))\\right]\\).\n\nExample 3.1 (Point Estimation) In the case of Section Chapter 1, we took \\(\\mathcal A = \\Theta\\). Our space of actions where simply parameter values. We also defined a quadratic loss function which resulted in the risk function taking the form of the MSE of an estimator.\n\nWe can also define hypothesis testing using decision theory.\n\nDefinition 3.1 Let \\(\\mathcal X\\) and \\(\\mathcal P\\) be the sample space and model, respectively, and partition \\(\\mathcal P\\) into \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\).1 A test function is a decision rule defined on \\(\\mathcal A =\\{\\mathcal P_0,\\mathcal P_1\\}\\) given as \\[\\delta(\\mathbf{X}) = \\begin{cases}\\mathcal P_1 & T(\\mathbf{X}) \\in C \\\\ \\mathcal P_0 & T(\\mathbf{X})\\notin C\\end{cases}\\] for some critical region \\(C\\subseteq \\mathcal X\\) and test statistic \\(T:\\mathcal X\\to\\mathcal X\\).\n\nOf the set of models \\(\\mathcal P_0\\) and \\(\\mathcal P_1\\), one is often easier to specify. For example, suppose \\(\\mathbf{X}= (X_1,\\ldots,X_n)\\) captures the effectiveness of a drug on a series of patients \\(i=1,\\ldots,n\\), and \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\in \\mathcal P\\). If we want to test whether this drug has an effect on patients’ health, then we want to partition \\(\\mathcal P\\) into two groups: one group corresponding to the drug having no effect, and one where the drug has an effect. It’s much easier to specify the models which correspond to no effect than the models that correspond to the drug having an effect, as there are nearly infinite possibilities when it comes to the type and degree of effectiveness. The easier of the two groups to specify is traditionally denoted \\(\\mathcal P_0\\), and is often associated with some well formed (null) hypothesis. This hypothesis is often written as \\(H_0:P_\\theta\\in \\mathcal P_0\\). Our decision \\(\\delta(\\mathbf{X})\\) prescribed whether we fail to reject/reject \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\). If we reject of the hypothesis \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\), we conclude that \\(P_\\boldsymbol{\\theta}\\) belongs to the class of alternative \\(H_1:P_\\boldsymbol{\\theta}\\in \\mathcal P_1\\). We often think of \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\) as a statement we assume to be true, with the burden of proof being on \\(H_1:P_\\boldsymbol{\\theta}\\in \\mathcal P_1\\).\n\nThere exist two other popular ways of writing the decision problem associated with testing a hypothesis:\n\nAssuming \\(\\mathcal P\\) is identified and each \\(P_\\boldsymbol{\\theta}\\) is uniquely determined by a \\(\\boldsymbol{\\theta}\\in \\Theta\\), then we can partition \\(\\Theta\\) into \\(\\Theta_1\\) and \\(\\Theta_0\\), and define \\(\\delta(\\mathbf{X}) = \\begin{cases}\\Theta_1 & T(\\mathbf{X}) \\in C \\\\ \\Theta_0 & T(\\mathbf{X})\\notin C\\end{cases},\\) where \\(\\mathcal A = \\{\\Theta_0, \\Theta_1\\}\\) The hypothesis and class of alternatives are now written as \\(H_0: \\boldsymbol{\\theta}\\in\\Theta_0\\) and \\(H_1: \\boldsymbol{\\theta}\\in\\Theta_1\\), respectively.\nWe could define \\(\\mathcal A = \\{0,1\\}\\), where \\(1\\) corresponds to rejecting \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\) and \\(0\\) failing to reject \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\).\n\nVirtually all concrete examples of hypothesis test use notation similar to 1.\n\n\nExample 3.2 (One-Sided Z-Test) Suppose \\(\\mathcal P\\) is the collection of normal distributions with known variance \\(\\sigma^2\\). This model is parameterized by mean \\(\\mu\\). We want to test the following hypothesis: \\[\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\n\\end{align*}\\] In this case, the null hypothesis is often abbreviated as \\(H_0: \\mu = \\mu_0\\). Our hypothesis has partitions our parameter space \\(\\Theta = \\mathbb R\\) into \\(\\Theta_0 = (-\\infty,\\mu_0]\\) and \\(\\Theta_1 = (\\mu_0,\\infty)\\). Define a statistic \\[T(\\mathbf{X}) = \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} = \\frac{\\frac{1}{n}\\sum_{i=1}^nX_i - \\mu_0}{\\sigma/\\sqrt n},\\] and a critical region \\(C = [1.645,\\infty)\\). Our decision rule \\(\\delta\\) takes the form \\[\\delta(\\mathbf{X}) = \\begin{cases} (-\\infty,\\mu_0] & T(\\mathbf{X}) < 1.645 \\\\ (\\mu_0,\\infty) & T(\\mathbf{X}) \\ge 1.645 \\end{cases},\\] which is written succinctly as \\(\\delta(\\mathbf{X}) = 1[T(\\mathbf{X}) \\ge 1.645 ]\\) if we take \\(\\mathcal A = \\{0,1\\}\\)\n\nThe binary nature of hypothesis testing makes defining a loss function quite simple. Our decision is either correct or incorrect. We can take the loss function to be 0 if we are correct, and 1 if we are incorrect. \\[l(P_\\theta, \\delta(\\mathbf{X}))=\\begin{cases}1 & \\delta(\\mathbf{X}) = \\mathcal P_0 \\text{ and }P_\\boldsymbol{\\theta}\\in\\mathcal P_0 \\\\\n0 & \\delta(\\mathbf{X}) = \\mathcal P_1 \\text{ and }P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\\\\n1 & \\delta(\\mathbf{X}) = \\mathcal P_1 \\text{ and }P_\\boldsymbol{\\theta}\\in\\mathcal P_1\\\\\n0 & \\delta(\\mathbf{X}) = \\mathcal P_0 \\text{ and }P_\\boldsymbol{\\theta}\\in\\mathcal P_1\\end{cases}\\] The associated risk function becomes \\[\\begin{align*}\nR(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X})) & = \\text{E}\\left[l(P_\\theta, \\delta(\\mathbf{X}))\\right] = l(P_\\theta, \\mathcal P_0)\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_0) +l(P_\\theta, \\mathcal P_1)\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_1).\n\\end{align*}\\] We can simplify \\(R(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X}))\\) if we condition one either \\(P_\\theta \\in \\mathcal P_0\\), or \\(P_\\theta \\in \\mathcal P_1\\).\n\\[\\begin{align*}\nR(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X}) \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0)& =  \\underbrace{l(P_\\theta, \\mathcal P_0)}_0\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_0 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0) +\\underbrace{l(P_\\theta, \\mathcal P_1)}_1\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_1 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0) =\\Pr(\\delta(\\mathbf{X}) = \\mathcal P_1 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0)\\\\\nR(P_\\boldsymbol{\\theta}, \\delta(\\mathbf{X}) \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_1)& =  \\underbrace{l(P_\\theta, \\mathcal P_0)}_1\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_0 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_1) +\\underbrace{l(P_\\theta, \\mathcal P_1)}_0\\cdot \\Pr(\\delta(\\mathbf{X}) = \\mathcal P_1 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_1) =\\Pr(\\delta(\\mathbf{X}) = \\mathcal P_0 \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_1)\n\\end{align*}\\]\nIn both cases, the risk function is the probability of making an erroneous decision. These two types of errors are likely familiar.\n\nDefinition 3.2 Suppose we have a null hypothesis \\(H_0:P_\\boldsymbol{\\theta}\\in\\mathcal P_0\\). If \\(P_\\boldsymbol{\\theta}\\in \\mathcal P_0\\), but \\(\\delta(\\mathbf{X}) = \\mathcal P_1\\) (the null hypothesis is true but we reject it), then we have committed a type I error. On the other hand, if \\(P_\\boldsymbol{\\theta}\\in \\mathcal P_1\\), but \\(\\delta(\\mathbf{X}) = \\mathcal P_0\\) (the null hypothesis is false but we fail to reject it), then we have committed a type II error.\n\n\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\ntype I error\ncorrect decision\n\n\nFail to Reject \\(H_0\\)\ncorrect decision\ntype II error\n\n\n\nConsidering we have two types of errors, which is more important? How do we construct optimal test? Neyman and Pearson (1933) provide a solution to these problems."
  },
  {
    "objectID": "testing.html#size-and-power",
    "href": "testing.html#size-and-power",
    "title": "3  Hypothesis Testing",
    "section": "3.2 Size and Power",
    "text": "3.2 Size and Power\nIn order to assess tests, we will need to define probabilities related to type I and type II error.\n\nDefinition 3.3 The level \\(\\alpha \\in [0,1]\\) is a specified number such that \\(\\Pr(\\text{type I error})>\\alpha\\) is unacceptable. In other words, \\[\\Pr(\\delta(\\mathbf{X}) = \\mathcal P_1 \\mid P_\\boldsymbol{\\theta}\\in P_0) = \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in P_0) \\le \\alpha \\ \\ \\forall P_\\boldsymbol{\\theta}\\in \\mathcal P_0.\\]\n\nNote that \\(\\alpha\\) must hold for all \\(P_\\boldsymbol{\\theta}\\in \\mathcal P_0\\). A test may have a level of 0.01 for one \\(P_{\\boldsymbol{\\theta}} \\in \\mathcal P_0\\), but could have a level of 0.10 for \\(P_{\\boldsymbol{\\theta}'} \\in \\mathcal P_0\\). How then do we assess the “aggregate” level of a test across all \\(\\mathcal P_0\\)? We will do so by considering the worst case scenario, and associating a test with the largest level \\(\\alpha\\) possible, where the maximum is taken over all \\(P_\\boldsymbol{\\theta}\\in \\mathcal P_0\\). This will be known as the size of the test.\n\nDefinition 3.4 The size of a test defined with a statistic \\(T(\\mathbf{X})\\) and critical value \\(c\\) is \\[\\alpha(\\delta) = \\sup_{P_\\boldsymbol{\\theta}\\in \\mathcal P_0} \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in \\mathcal P_0).\\]\n\nThe size is nothing more than the maximum probability of committing a type I error permitted by a test \\(\\delta\\). If we want to avoid type I errors, we want \\(\\alpha\\) to be very small. We also need to consider type II errors. Note that the probability of a type II error is\n\\[\\Pr(\\text{type II error}) = \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0) = 1 - \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_1).\\] A small chance of committing a type II error is the same as the probability of our test correctly identifying \\(P_\\boldsymbol{\\theta}\\in \\mathcal P_1\\) being high. This ability is the power of our test.\n\nDefinition 3.5 The power of a test defined with a statistic \\(T(\\mathbf{X})\\) and critical region \\(C\\) is \\[\\beta(\\delta, P_\\theta) = \\Pr(T(\\mathbf{X}) \\in C \\mid P_\\boldsymbol{\\theta}).\\]\n\nThe power function is simply the probability of rejecting the null hypothesis for any \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\). It can be thought of as a test power detect that \\(H_1\\) is true (\\(H_0\\) is false). Note that \\(\\beta(\\delta, P_\\theta)\\) is redundant on the subset \\(\\mathcal P_0 \\subset \\mathcal P\\). For any \\(P_\\theta\\in \\mathcal P_0\\), \\[ \\beta(\\delta, P_\\theta\\mid P_\\theta\\in \\mathcal P_0) = \\Pr(T(\\mathbf{X}) \\in C \\mid   P_\\boldsymbol{\\theta}\\in \\mathcal P_0) \\le \\sup_{P_\\boldsymbol{\\theta}\\in P_0} \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in\\mathcal P_0) = \\alpha(\\delta).\\] By construction \\(\\beta(\\delta, P_\\theta) \\le \\alpha(\\delta)\\) on \\(\\mathcal P_0\\), the power on \\(\\mathcal P_0\\) doesn’t provide any new information. On the other hand it gives us another way of writing the size of a test: \\[ \\alpha(\\delta) = \\sup_{P_\\theta \\in\\mathcal P_0} \\beta(P_\\boldsymbol{\\theta}, \\delta).\\]We’re interested in the power of our test when \\(\\mathcal P\\in P_1\\), which corresponds to the probability of correctly detecting \\(\\mathcal P\\in P_1\\).\n\\[\\beta(\\delta, P_\\theta\\mid P_\\theta\\in \\mathcal P_1) = \\Pr(T(\\mathbf{X}) \\in C \\mid   P_\\boldsymbol{\\theta}\\in \\mathcal P_1) = 1 - \\Pr(\\text{type II error})\\] For this reason, you will very often see \\(\\beta\\) only defined on \\(\\mathcal P_1\\).\n\nExample 3.3 (Z-Test) Reconsider Example 3.2. We have \\[\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\\\\\nT(\\mathbf{X}) &= \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n }\\\\\n\\delta(\\mathbf{X}) &= \\begin{cases} (-\\infty,\\mu_0] & T(\\mathbf{X}) < 1.645 \\\\ (\\mu_0,\\infty) & T(\\mathbf{X}) \\ge 1.645 \\end{cases}\n\\end{align*}\\]\nNote that an equivalent test for our null hypothesis is \\[\\delta(\\mathbf{X}) = \\begin{cases} (-\\infty,\\mu_0] & \\bar X < \\mu_0 +1.645\\left(\\frac{\\sigma}{\\sqrt n}\\right) \\\\ (\\mu_0,\\infty) & \\bar X  \\ge \\mu_0 +1.645\\left(\\frac{\\sigma}{\\sqrt n}\\right) \\end{cases}\\] We can calculate the size of our test using the fact that \\(T(\\mathbf{X}) \\sim N(0,1)\\). Any such \\(\\mu\\) can be written as \\(\\mu_0 + t(\\sigma/\\sqrt n)\\) for \\(t \\le 0\\). The level of a test for some \\(\\mu \\in (-\\infty,\\mu_0]\\) is \\[\\begin{align*}\n\\Pr(\\bar X \\ge \\mu_0 +1.645(\\sigma/\\sqrt n)) &= \\Pr(\\bar X \\ge [\\mu - t(\\sigma/\\sqrt n)] +1.645(\\sigma/\\sqrt n)) & (\\mu_0 =\\mu - t(\\sigma/\\sqrt n))\\\\\n&= \\Pr(\\bar X \\ge \\mu+ (1.645-t)(\\sigma/\\sqrt n))\\\\\n& = \\Pr \\left(\\frac{\\bar X - \\mu}{(\\sigma/\\sqrt n)} \\ge 1.645 - t\\right)\\\\\n& = \\Phi(-1.645 + t)\n\\end{align*}\\] Before we take the supremum over all such probabilities, note that \\(\\mu \\le \\mu_0\\) is equivalent to \\(t\\le 0\\) where \\(\\mu = \\mu_0 + t(\\sigma/\\sqrt n)\\).2 Therefore the size of our test is \\[\\begin{align*}\n\\alpha &= \\sup_{\\mu \\le \\mu_0} \\Pr(T(\\mathbf{X})\\in C \\mid \\mu < \\mu_0)\\\\\n& = \\sup_{t \\le 0} \\Phi(-1.645 + t)\n\\end{align*}\\]\nPlotting this probability makes it clear that \\(\\alpha(\\delta) \\approx 0.05\\), and the supremum is achieved when \\(\\mu = \\mu_0\\) (\\(t=0\\)).\n\n\nShow code which generates figure\ntibble(t = -300:0/100) %>% \n  mutate(y = pnorm(-1.645 + t)) %>% \n  ggplot(aes(t,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"t =  (μ - μ0)/(σ√n)\", y = \"Pr(Reject H0 | H0 True)\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\nFigure 3.1: The size of the test is given by the red dashed line corresponding to the supremum of the probability of rejecting a true null hypothesis.”\n\n\n\n\nNow we can consider the power \\(\\beta(\\mu)\\). We’ve in fact already done nearly all the calculations required for this. Consider \\(\\mu = \\mu_0 + t(\\sigma/\\sqrt n)\\) for \\(t\\in \\mathbb{R}\\). If \\(t \\le 0\\), then \\(\\mu \\le \\mu_0\\), and the null hypothesis is true. If \\(t > 0\\), then \\(\\mu >\\mu_0\\) and the null hypothesis is false. The power \\(\\beta(\\mu)\\) is \\(\\Phi(-1.645 + t)\\), but on the domain \\(t\\in \\mathbb{R}\\).\n\n\nShow code which generates figure\ntibble(\n  t = seq(-2, 4, length = 1000), \n  group = \"Power Curve\"\n) %>% \n  mutate(y = pnorm(-1.645 + t)) %>% \n  bind_rows(data.frame(t =c(-2,0), y = c(0,0), group = \"H0 True, μ < μ0\")) %>%\n  bind_rows(data.frame(t =c(0,4), y = c(0,0), group = \"H1 True, μ > μ0\")) %>%\n  ggplot(aes(t,y, color = group)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\"t =  (μ - μ0)/(σ√n)\", y = \"Pr(Reject H0)\" , color= \"\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"black\")) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.2: Power curve of test.\n\n\n\n\nTo make this more concrete, we can perform a Monte Carlo simulation for fixed values of \\(\\mu_0\\), \\(\\mu\\), \\(\\sigma^2\\), and \\(n\\). For all simulations, let’s fix \\(\\mu_0 = 2\\), \\(n = 100\\) and \\(\\sigma^2 =1\\). First, assume \\(\\mu = \\mu_0 = 2\\), and record \\(\\delta(\\mathbf{X})\\) for 100,000 simulations. In this case we should expect to make the correct decision (fail to reject the null hypothesis) about 5% of the time, as the size of our test is \\(\\alpha = 0.05\\) which corresponds to the maximum level which occurs at \\(\\mu = \\mu_0\\).\n\nN_sim <- 1e5\nmu_0 <- 2\nn <- 100\nsigma <- 1\n\ntest_stat <- function(X){\n  (mean(X)-mu_0)/(sigma/sqrt(n))\n}\ndecision <- function(X){\n  test_stat(X) >= 1.645\n}\n\nmu <- 2\nsimulated_decisions <- sapply(1:N_sim, FUN = function(k){decision(rnorm(n, mu, sigma))}) \ntable(simulated_decisions)\n\nsimulated_decisions\nFALSE  TRUE \n94862  5138 \n\nmean(simulated_decisions)\n\n[1] 0.05138\n\n\nThis simulation calculated the power of our test given \\(\\mu\\). Let’s define the power function using this simulation, and calculate the power for \\(\\mu\\in[1.8,2.4]\\)\n\npower <- function(mu){\n  simulated_decisions <- sapply(1:N_sim, FUN = function(k){decision(rnorm(n, mu, sigma))})\n  mean(simulated_decisions)\n}\npower_curve <- sapply(seq(1.8, 2.4, length = 25), power)\n\nWe can plot this simulated power curve over the theoretical curve we calculated for a general \\(\\mu_0\\) (see Figure 3.2).3\n\n\nShow code which generates figure\ndf <- tibble(t = seq(1.8, 2.4, length = 25),\n                 y = power_curve,\n                 group = \"Simulated Power\")\n\ntibble(\n  t = seq(1.8, 2.4, length = 1000),\n  group = \"Power Curve\"\n) %>%\n  mutate(y = pnorm(-1.645 + (t-2)/(1/10) )) %>%\n  bind_rows(data.frame(t =c(1.8,2), y = c(0,0), group = \"H0 True, μ < μ0\")) %>%\n  bind_rows(data.frame(t =c(2,2.4), y = c(0,0), group = \"H1 True, μ > μ0\")) %>%\n  ggplot(aes(t,y, color = group)) +\n  geom_line() +\n  geom_point(data = df, size = 1.5) +\n  theme_minimal() +\n  labs(\"True μ\", \"Pr(Reject H0)\", color = \"\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"black\", \"blue\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.3: Simulated power function on the interval [1.8, 2.4] using 10,000 simulations for each point.\n\n\n\n\nOur simulated probabilities are virtually identical to the theoretical probabilities calculated!\n\nThis example is special for a few reason. Note that we have: \\[\\begin{align*}\n\\alpha & = \\sup_{\\mu \\le \\mu_0}\\beta(\\mu) = \\beta(\\mu_0).\n\\end{align*}\\] This equality holds because \\(\\beta\\) is monotonically increasing: \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\mu} \\beta(\\mu) &= \\frac{\\partial }{\\partial \\mu}\\Phi(-1.645 + t)  \\\\\n& = \\frac{\\partial t}{\\partial \\mu}\\varphi(-1.645 - t) & (\\varphi \\text{ standard normal pdf})\\\\\n& = \\frac{\\partial}{\\partial \\mu}\\left(\\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right)\\varphi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right) &(t = (\\mu - \\mu_0)/(\\sigma/\\sqrt n))\\\\ & =\n\\frac{1}{\\sigma/\\sqrt n}\\varphi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right) \\\\\n& > 0 &(\\varphi(\\cdot) >0 , n >0, \\sigma > 0).\n\\end{align*}\\]\nIf the probability we reject the null hypothesis grows with \\(\\mu\\in(-\\infty,\\mu_0]\\), then of course the supremum of these probabilities is the probability at the boundary \\(\\mu_0\\). Equivalently, \\[ \\alpha = \\inf_{\\mu_0 < \\mu}\\beta(\\mu) = \\inf_\\mu \\beta(\\mu \\mid \\mu_0 < \\mu) = \\inf_\\mu \\beta(\\mu \\mid H_0 \\text{ false}).\\] Many people would define power \\(\\beta(\\mu)\\) only on \\((\\mu_0,\\infty)\\) (when \\(H_0\\) is false), so in this case the size is in the infimum of the power. Finally because \\[\\beta(\\mu) = \\Phi(-1.645 + t) = \\Phi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right),\\] we have \\(\\alpha = \\beta(\\mu_0) = \\Phi(-1.645)\\). This means if we desire a size of \\(\\alpha\\), we can use the standard quantile function to calculate the critical value required – \\(c = -\\Phi^{-1}(\\alpha)\\). Just to reiterate, these nice properties hold because \\(\\beta\\) is monotonic!\n\nOur decision function \\(\\delta(\\mathbf{X})\\) only tells us whether we reject the null hypothesis or not. It doesn’t directly give us any information on how confident we should be in our decision, or by what degree we reject the null hypothesis. We can capture this by looking at the distribution of the test statistics \\(T(\\mathbf{X})\\) in relation to the critical region \\(C(\\alpha)\\) for size \\(\\alpha\\). We can define the \\(p\\)-value, denoted \\(p\\), as \\[p = \\inf\\{\\alpha \\mid T(\\mathbf{X})\\in C(\\alpha)\\}.\\] The \\(p-\\)value is the minimum level of the test such that we still reject the null hypothesis (because \\(T(\\mathbf{X})\\) is in the critical region). Suppose in the case of the one sided \\(Z-\\)test we had \\(T(\\mathbf{X}) = 2\\). We reject the null hypothesis because \\(T(\\mathbf{X}) \\ge 2\\). We would also reject the null hypothesis for any size \\(\\alpha\\) such that \\(2 \\ge -\\Phi^{-1}(\\alpha)\\). Because \\(-\\Phi^{-1}\\) is monotonic, \\[p = \\inf\\{\\alpha\\mid 2 \\ge -\\Phi^{-1}(\\alpha)\\} = \\{\\alpha\\mid 2= -\\Phi^{-1}(\\alpha)\\} = \\Phi(-2) \\approx 0.02275.\\] One important result which follows immediately from the definition of of \\(p\\): \\[T(\\mathbf{X}) \\in C\\iff \\alpha(\\delta) < p.\\] We reject \\(H_0\\) if and only if the \\(p-\\)value associated with \\(T(\\mathbf{X})\\) is less than the size of the test. One interpretation related to this is that the \\(p-\\)value tells you the degree to which you reject the null hypothesis. If \\(p\\approx0.02275\\), not only do we reject the null hypothesis for \\(\\alpha(\\delta) = 0.05\\), but we also reject it for more stringent tests with smaller sizes.\n\nSo if we can pick \\(\\alpha\\) by virtue of the critical value \\(c\\), then why don’t we simply pick \\(\\alpha\\approx 0\\). This would mean that we almost always fail to reject the null hypothesis, and in doing so we’re bound to fail to reject the null hypothesis even when it is false, increasing the probability of a type II error\n\nExample 3.4 (Power vs. Size) Again consider \\[\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\n\\end{align*}\\] where \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for a known \\(\\sigma^2\\), and \\(\\delta(\\mathbf{X}) = 1[T(\\mathbf{X}) \\ge c]\\) for some critical value \\(c\\). If we desire a test of size \\(\\alpha\\) we set our critical value as \\(c = -\\Phi^{-1}(\\alpha)\\).\n\n\nShow code which generates figure\ntibble(x = 0:1000/1000) %>% \n  mutate(y = -qnorm(x)) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Desired Size, α\", y = \"Required Critical Value, c\")\n\n\n\n\n\nFigure 3.4: The relationship between critical value and size.\n\n\n\n\nDefine \\(t = (\\mu - \\mu_0)/(\\sigma/\\sqrt n)\\) to be the standardized distance between \\(\\mu_0\\) and the true \\(\\mu\\). The power of the test is \\[\\beta(\\mu)= \\Phi(- c + t) = \\Phi(\\Phi^{-1}(\\alpha) + t),\\] which is increasing in \\(\\alpha\\): \\[\\begin{align*}\n\\frac{\\partial \\beta}{\\partial \\alpha} & = \\frac{\\partial \\Phi^{-1}(\\alpha)}{\\partial \\alpha} \\varphi(\\Phi^{-1}(\\alpha) + t) \\\\\n& = \\frac{\\varphi(\\Phi^{-1}(\\alpha) + t)}{\\varphi(\\Phi^{-1}(\\alpha))} & (\\text{inverse function theorem}) \\\\\n& > 0 & (\\varphi(\\cdot) > 0).\n\\end{align*}\\]\nAs we let \\(\\alpha \\to 0\\), we have \\(\\beta \\to 0\\).\n\n\nShow code which generates figure\nexpand_grid(\n  a = c(0.01, 0.05, 0.10, 0.25), \n  t = -2000:4000/1000\n) %>% \n  mutate(power = pnorm(-(-qnorm(a)) + t)) %>% \n  ggplot(aes(t, power, color = as.factor(a))) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"t =  (μ - μ0)/(σ√n)\", y = \"Power\", color = \"size\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.5: There is an inherent tradeoff between the power and size of a test, as small values of α result in less power.\n\n\n\n\n\nSo how do we select \\(\\alpha\\)? Do we care more about type I error or type II error? Furthermore, how do we even construct test statistics? We can begin to answer this question with the guidance of Neyman and Pearson (1933)."
  },
  {
    "objectID": "testing.html#neyman-pearson-lemma-and-ump-tests",
    "href": "testing.html#neyman-pearson-lemma-and-ump-tests",
    "title": "3  Hypothesis Testing",
    "section": "3.3 Neyman-Pearson Lemma and UMP Tests",
    "text": "3.3 Neyman-Pearson Lemma and UMP Tests\nOne of the key ideas presented proposed by Neyman and Pearson (1933) is that type II errors are more erroneous than their type I counterparts, so we should minimize the probability of committing a type II error subject to a predetermined test size \\(\\alpha\\). If someone if getting tested for a disease, a false positive (type I) is much better than a false negative (type II). On the other hand, many would argue it is worse to sentence an innocent person to jail (type I error) than let a guilty person go free, so whether this assumption holds depends on the context of our test.\nThe Neyman-Pearson lemma solves this problem by maximizing the power of a test subject to a specified \\(\\alpha\\): \\[ \\max_{\\delta\\in \\mathcal D}\\{\\beta(\\delta, P_\\boldsymbol{\\theta}) \\mid \\alpha(\\delta) < \\alpha\\},\\] fixing \\(P_\\boldsymbol{\\theta}\\). The test which solves this maximization problem may vary across \\(P_\\boldsymbol{\\theta}\\). As such, the Neyman-Pearson lemma solves our problem in the context of simplified hypotheses.\n\nDefinition 3.6 A null hypothesis \\(H_0\\) is simple if \\(\\mathcal P_0\\) is a singleton, \\(\\mathcal P_0 = \\{P_{\\boldsymbol{\\theta}_0}\\}\\). Similarly, an alternative hypothesis \\(H_1\\) is simple if \\(\\mathcal P_1 = \\{P_{\\boldsymbol{\\theta}_1}\\}\\).\n\nThe Neyman-Pearson Lemma will only apply directly to hypotheses of the form: \\[\\begin{align*}\nH_0:P_\\boldsymbol{\\theta}&= P_{\\boldsymbol{\\theta}_0}\\\\\nH_1:P_\\boldsymbol{\\theta}&=P_{\\boldsymbol{\\theta}_1}\n\\end{align*}\\] Note that for simple hypotheses \\[ \\alpha(\\delta) = \\sup_{P_\\boldsymbol{\\theta}\\in \\mathcal P_0} \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}\\in \\mathcal P_0) = \\Pr(T(\\mathbf{X})\\in C \\mid P_\\boldsymbol{\\theta}=P_{\\boldsymbol{\\theta}_0}) \\]\n\nConsider a test with simple hypotheses \\(H_0:P_\\boldsymbol{\\theta}= P_{\\boldsymbol{\\theta}_0}\\) and \\(H_1:P_\\boldsymbol{\\theta}=P_{\\boldsymbol{\\theta}_1}\\). If \\(\\delta(\\mathbf{X})\\) is test with size \\(\\alpha\\) and defined as \\[\\delta(\\mathbf{X})=\\begin{cases} P_{\\boldsymbol{\\theta}_0} & \\frac{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)} > \\eta\\\\  P_{\\boldsymbol{\\theta}_1} & \\frac{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)} < \\eta\\end{cases}\\] for \\(\\eta > 0\\), then \\(\\beta(\\delta, \\theta_1) \\ge \\beta(\\delta', \\theta_1)\\) for all \\(\\delta' \\in \\mathcal D\\) with a size less than or equal to \\(\\alpha\\). In this case we refer to \\(\\delta(\\mathbf{X})\\) as the most powerful (MP) test for our hypotheses.\n\n\nProof. Let \\(\\delta(\\mathbf{X})\\) be some arbitrary decision rule in \\(\\mathcal D\\) with size \\(\\alpha\\), and let \\(\\delta'\\in\\mathcal D\\) be some other decision rule with size less than or equal to \\(\\alpha\\). \\[\\begin{align*}\n\\delta(\\mathbf{X}) &= \\begin{cases}P_{\\boldsymbol{\\theta}_1} & \\mathbf{X}\\in C \\\\ P_{\\boldsymbol{\\theta}_0} & \\mathbf{X}\\notin C\\end{cases}\\\\\n\\delta'(\\mathbf{X})& = \\begin{cases}P_{\\boldsymbol{\\theta}_1} & \\mathbf{X}\\in C' \\\\ P_{\\boldsymbol{\\theta}_0} & \\mathbf{X}\\notin C'\\end{cases}\\\\\n\\Pr(\\delta'(\\mathbf{X}) = P_{\\boldsymbol{\\theta}_1} \\mid P_{\\boldsymbol{\\theta}} = P_{\\boldsymbol{\\theta}_0} ) &\\le \\alpha = \\Pr(\\delta(\\mathbf{X}) = P_{\\boldsymbol{\\theta}_1} \\mid P_{\\boldsymbol{\\theta}} = P_{\\boldsymbol{\\theta}_0} )\n\\end{align*}\\] Without loss of generality, we’ve taken \\(T(\\mathbf{X}) = \\mathbf{X}\\).4 The only assumption we have made about our decision rules is, \\[\\begin{align}\n& \\Pr(\\delta'(\\mathbf{X}) = P_{\\boldsymbol{\\theta}_1} \\mid P_{\\boldsymbol{\\theta}} = P_{\\boldsymbol{\\theta}_0} ) \\le \\alpha = \\Pr(\\delta(\\mathbf{X}) = P_{\\boldsymbol{\\theta}_1} \\mid P_{\\boldsymbol{\\theta}} = P_{\\boldsymbol{\\theta}_0} )\\\\\n\\implies & \\Pr(\\mathbf{X}\\in C' \\mid \\boldsymbol{\\theta}_0 ) \\le \\Pr(\\mathbf{X}\\in C \\mid \\boldsymbol{\\theta}_0 ) (\\#eq:npa).\n\\end{align}\\]\nIn order to compare \\(\\delta\\) and \\(\\delta'\\), we’ll want to write their powers in term of the critical regions of both tests. We can write \\(C\\) in terms of the disjoint union of its intersection with the disjoint sets \\(C'\\) and \\((C')^c\\), as \\(C'\\) and \\((C')^c\\) partition the sample space \\(\\mathcal X\\). We can also do the same with \\(C'\\) and the disjoint sets \\(C\\) and \\(C^c\\). \\[\\begin{align*}\nC & = (C\\cap C') \\cup (C\\cap (C')^c)\\\\\nC' & = (C'\\cap C) \\cup (C'\\cap C^c)\n\\end{align*}\\] Accounting for these being disjoint unions, we have the following conditional probabilities: \\[\\begin{align}\n\\Pr(\\mathbf{X}\\in C\\mid \\boldsymbol{\\theta}) & = \\Pr(\\mathbf{X}\\in C\\cap C' \\mid \\boldsymbol{\\theta}) + \\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta})& \\text{for }\\boldsymbol{\\theta}\\in\\{\\boldsymbol{\\theta}_0, \\boldsymbol{\\theta}_1\\} (\\#eq:npa2)\\\\\n\\Pr(\\mathbf{X}\\in C'\\mid\\boldsymbol{\\theta}) & = \\Pr(\\mathbf{X}\\in C'\\cap C \\mid \\boldsymbol{\\theta}) + \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}) & \\text{for }\\boldsymbol{\\theta}\\in\\{\\boldsymbol{\\theta}_0, \\boldsymbol{\\theta}_1\\} (\\#eq:npa3)\n\\end{align}\\] We can use these two equations to rewrite Equation @ref(eq:npa). \\[\\begin{align}\n&  \\Pr(\\mathbf{X}\\in C' \\mid \\boldsymbol{\\theta}_0 ) \\le \\Pr(\\mathbf{X}\\in C \\mid \\boldsymbol{\\theta}_0 )\\\\\n\\implies & \\Pr(\\mathbf{X}\\in C'\\cap C \\mid \\boldsymbol{\\theta}_0) + \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_0) \\le \\Pr(\\mathbf{X}\\in C\\cap C' \\mid \\boldsymbol{\\theta}_0) + \\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_0)\\\\\n\\implies & \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_0) \\le\\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_0) (\\#eq:npa4)\n\\end{align}\\] Similarly, \\[\\begin{align}\n\\beta(\\delta,\\boldsymbol{\\theta}_1) & = \\Pr(\\mathbf{X}\\in C \\mid \\theta_1) &(\\text{definition of }\\beta)  (\\#eq:npa5)\\\\\n& = \\Pr(\\mathbf{X}\\in C\\cap C' \\mid \\boldsymbol{\\theta}_1) + \\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_1) & (\\text{Equation }(4.2))\\\\\n\\beta(\\delta',\\boldsymbol{\\theta}_1) & =\\Pr(\\mathbf{X}\\in C'\\cap C \\mid \\boldsymbol{\\theta}_1) + \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_1) & (\\text{Equation }(4.3))  (\\#eq:npa6)\n\\end{align}\\]\nWe want to construct \\(\\delta(\\mathbf{X})\\) such that \\[\\begin{align*}\n&\\beta(\\delta,\\boldsymbol{\\theta}_1)  \\ge\\beta(\\delta',\\boldsymbol{\\theta}_1),\\\\\n\\implies&\\Pr(\\mathbf{X}\\in C\\cap C' \\mid \\boldsymbol{\\theta}_1) + \\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_1) \\le  \\Pr(\\mathbf{X}\\in C'\\cap C \\mid \\boldsymbol{\\theta}_1) + \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_1) &(\\text{Equation }(4.5)\\text{ and }(4.6)),\\\\\n\\implies &\\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_1)  \\ge \\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_1),\\\\\n\\implies & \\int 1[\\mathbf{X}\\in C\\cap (C')^c]\\ dF_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\ge \\int 1[\\mathbf{X}\\in C'\\cap C^c]\\ dF_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1),\\\\\n\\implies & \\int_{C\\cap (C')^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\ d\\mathbf{x}\\ge \\int_{C'\\cap C^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\ d\\mathbf{x}\n\\end{align*}\\] In other words, we need to define \\(C\\) such that \\[ \\int_{C\\cap (C')^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)\\stackrel{?}{\\ge}\\int_{C'\\cap C^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\ d\\mathbf{x}\\] using the fact that \\[\\begin{align*}\n\\int_{C'\\cap C^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}& \\le \\int_{C\\cap (C')^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}& (\\text{Equation (4.4)}).\n\\end{align*}\\] We can write the left hand side of this known inequality as \\[\\begin{align*}\n\\int_{C'\\cap C^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}= \\int_{C'\\cap C^c}f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\cdot \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)}{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}\\ d\\mathbf{x}.\n\\end{align*}\\] We want to somehow relate this to the integral of \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)\\), but in general cannot “remove” \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)\\) from the integrand without an assumption about \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)\\). Namely, if we assume \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) >\\eta^{-1}\\) on \\(C'\\cap C^c\\) for some constant \\(\\eta\\),5 we have \\[\\begin{align*}\n\\int_{C'\\cap C^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}= \\int_{C'\\cap C^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\cdot \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)}{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}\\ d\\mathbf{x}\n\\ge \\eta^{-1}\\int_{C'\\cap C^c}f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0) \\ d\\mathbf{x}.\n\\end{align*}\\] Similarly, the right hand side of Equation 4.4 in integral form can be written as \\[\\int_{C\\cap (C')^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}= \\int_{C\\cap (C')^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\cdot \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)}{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}\\ d\\mathbf{x}\\le \\eta^{-1}\\int_{C\\cap (C')^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0) \\ d\\mathbf{x},\\] assuming \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) < \\eta^{-1}\\) on \\(C\\cap (C')^c\\). Therefore, \\[\\begin{align*}\n\\int_{C\\cap (C')^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)\\ d\\mathbf{x}& = \\int_{C\\cap (C')^c}f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)  \\cdot \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)}{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}\\ d\\mathbf{x}\\\\\n& \\ge \\eta^{-1}\\int_{C\\cap (C')^c}f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}& (f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) < \\eta^{-1} \\text{ on } C\\cap (C')^c)\\\\\n& =\\eta^{-1}\\Pr(\\mathbf{X}\\in C\\cap (C')^c \\mid \\boldsymbol{\\theta}_0)\\\\\n& \\ge  \\eta^{-1}\\Pr(\\mathbf{X}\\in C'\\cap C^c \\mid \\boldsymbol{\\theta}_0) & (\\text{Equation }(4.4))\\\\\n& =\\eta^{-1}\\int_{ C'\\cap C^c}f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)\\ d\\mathbf{x}\\\\\n& \\ge \\int_{C'\\cap C^c }f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1) \\cdot \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)}{f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}\\ d\\mathbf{x}& (f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) > \\eta^{-1} \\text{ on } C'\\cap C^c)\\\\\n& = \\int_{C'\\cap C^c} f_\\mathbf{X}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)\\ d\\mathbf{x},\n\\end{align*}\\] which is the desired result for a fixed \\(\\delta'\\). If we extend \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) > \\eta^{-1}\\) to all of \\(C^c\\) and \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)/f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) < \\eta^{-1}\\) to all of \\(C\\), then this will hold for all \\(\\delta'\\in \\mathcal D\\) (with a size of at least \\(\\alpha\\), otherwise Equation @ref(eq:npa4) needn’t hold). What then is the explicit form of \\(\\delta(\\mathbf{X})\\)? It is defined using the bounds which allowed us to establish the desired inequality using properties of integrals. The critical region is, \\[\\begin{align*}\nC &= \\left\\{\\mathbf{x}\\ \\bigg|\\ \\frac{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)}{f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0)} > \\eta \\right\\}\n\\end{align*}\\] and \\(\\delta(\\mathbf{X}) = 1[\\mathbf{x}\\in C]\\).\n\nBefore discussing the intuition behind Theorem @ref(thm:NPlemma), here are some technical points:\n\nIn the event \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_1)/f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}_0) = \\eta\\), then it doesn’t matter if \\(\\delta(\\mathbf{X})\\) rejects the null hypothesis or not.6\nA more general version of the result can be found in Bickel and Doksum (2015) or Romano and Lehmann (2005), and concerns non-deterministic decision rules which the reject the null hypothesis with some probability when \\(T(\\mathbf{X}) \\in C\\) (instead of with probability one)\nThe result says nothing about the uniqueness of the MP test.\n\nTheorem @ref(thm:NPlemma) tells us that for simple hypotheses, our test statistic should be \\(T(\\mathbf{X}) = \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)}\\), which makes a fair bit of sense. If we observe \\(\\mathbf{x}\\), then \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)\\) and \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)\\) are the probabilities we observe \\(\\mathbf{x}\\) given \\(\\boldsymbol{\\theta}_1\\) and \\(\\boldsymbol{\\theta}_0\\), respectively. In the event that \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1) \\gg f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)\\), it’s so likely that \\(\\boldsymbol{\\theta}= \\boldsymbol{\\theta}_1\\), that we should reject the null hypothesis. In other words, we reject the null hypothesis is the ratio of these probabilities is high enough. This ratio actually pops up elsewhere in statistics and has its own name.\n\nDefinition 3.7 The likelihood ratio associated with densities \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)\\) and \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)\\) is defined as \\[ L(\\boldsymbol{\\theta}_1, \\boldsymbol{\\theta}_0 \\mid \\mathbf{x}) = \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}_0)}.\\]\n\nSo how large does this ratio need to be such that we reject the null hypothesis? The Neyman-Pearson Lemma seems a bit vague here, as it only says that it needs to exceed some \\(\\eta\\). The proof gives us some mathematical context on \\(\\eta\\), but also fails to explicitly define it, so can we really pick any constant? Of course not, because we assume that \\(\\delta(\\mathbf{X})\\) has size \\(\\alpha\\). The actual value \\(\\eta\\) is implicitly given when we assume the size of \\(\\delta(\\mathbf{X})\\), but we can define is explicitly. \\[\\begin{align*}\n& \\alpha = \\Pr(T(\\mathbf{X}) > \\eta \\mid P_\\boldsymbol{\\theta}= P_{\\boldsymbol{\\theta}_0})\\\\\n\\implies & \\alpha = 1 - \\Pr(T(\\mathbf{X}) \\le \\eta \\mid P_\\boldsymbol{\\theta}= P_{\\boldsymbol{\\theta}_0})\\\\\n\\implies & \\alpha = 1 - F_{T(\\mathbf{X})}(\\eta \\mid P_{\\boldsymbol{\\theta}_0})\\\\\n\\implies & \\eta = F_{T(\\mathbf{X})}^{-1}(1-\\alpha \\mid P_{\\boldsymbol{\\theta}_0})\n\\end{align*}\\]\nLet’s see the Neyman-Pearson Lemma in action.\n\nSuppose \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for a known \\(\\sigma^2\\), and \\[\\begin{align*}\nH_0:\\mu=\\mu_0,\\\\\nH_1:\\mu=\\mu_1,\n\\end{align*}\\] where \\(\\mu_1 > \\mu_0\\). Our likelihood ratio is \\[\\begin{align*}\n\\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_0)} & = \\frac{\\prod_{i=1}^nf_{X_i}(x\\mid\\mu_1)}{\\prod_{i=1}^nf_{X_i}(x\\mid\\mu_0)} \\\\\n& = \\frac{\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\\n& = \\frac{\\prod_{i=1}^n\\exp\\left[-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\prod_{i=1}^n\\exp\\left[-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\\n& = \\frac{\\exp\\left[-\\sum_{i=1}^n\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\exp\\left[-\\sum_{i=1}^n\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\\n& = \\exp\\left[\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n(x_i - \\mu_0)^2 - \\sum_{i=1}^n(x_i - \\mu_1)^2\\right)\\right]\\\\\n& = \\exp\\left[\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i^2 - x_i\\mu_0 + \\mu_0^2 - x_i^2  - x_i\\mu_1 + \\mu_1^2)\\right]\\\\\n& = \\exp\\left[\\frac{1}{2\\sigma^2}[n(\\mu_0^2 - \\mu_1^2) - 2n\\bar x(\\mu_0 - \\mu_1)]\\right].\n\\end{align*}\\] The Neyman-Pearson lemma says the critical region of our test should take the form \\[\\begin{align*}\nC & = \\left\\{\\mathbf{x}\\ \\Big| \\  \\exp\\left[\\frac{1}{2\\sigma^2}[n(\\mu_0^2 - \\mu_1^2) - 2n\\bar x(\\mu_0 - \\mu_1)]\\right] > \\eta\\right\\}\\\\\n  & = \\left\\{\\mathbf{x}\\ \\Big| \\ \\bar x > \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)}\\right\\}\n\\end{align*}\\] …okay so this looks like a monstrosity. Let’s define the constant \\(\\eta^*\\) to as \\[\\eta^* = \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)},\\] as to give us \\[ C = \\{\\mathbf{x}\\mid \\bar x > \\eta^*\\}.\\] If we want our test to have a size of \\(\\alpha\\), we let \\(\\eta^* = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\), which is the same one sided test we’ve been exploring in this section! Therefore the most powerful test for \\(H_0 : \\mu = \\mu_1\\) versus \\(H_1 : \\mu = \\mu_1\\) is:\n\\[\\begin{align*}\n\\delta(\\mathbf{X}) & = \\begin{cases}\\mu_0 & \\bar X < \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n} \\\\ \\mu_1 & \\bar X > \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\end{cases} & (T(\\mathbf{X}) = \\bar X)\\\\\n& = \\begin{cases}\\mu_0 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} < \\Phi^{-1}(1-\\alpha) \\\\ \\mu_1 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} > \\Phi^{-1}(1-\\alpha) \\end{cases}& (T(\\mathbf{X}) = (\\bar X - \\mu_0)/(\\sigma/\\sqrt n))\n\\end{align*}\\]\nThese decision rules are equivalent to the likelihood ratio test given by the Neyman-Pearson lemma. If we want to keep with the spirit of the lemma and insist on using the test statistic \\(T(\\mathbf{X}) = \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_0)}\\), we need to solve for \\(\\eta\\).\n\\[\\begin{align*}\n&\\eta^* = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\\\\n\\implies & \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)} = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\\\\n\\implies & \\eta = \\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right]\n\\end{align*}\\]\nThis gives the decision rule \\[\\begin{align*}\n\\delta(\\mathbf{X}) &= \\begin{cases}\\mu_0 & \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_0)} < \\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right] \\\\ \\mu_1 & \\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_1)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_0)} >\\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right]\\end{cases} & (T(\\mathbf{X}) = f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_1)/f_\\mathbf{X}(\\mathbf{x}\\mid\\mu_0))\n\\end{align*}\\]\nTo confirm these three test are equivalent, let’s simulate the decision rule for \\(H_0:\\mu = 0\\) versus \\(H_1: \\mu = 3\\), where \\(\\sigma^2 = 1\\), \\(n = 100\\), \\(\\alpha = 0.05\\), and null hypothesis \\(\\mu = 0\\) is true. Not only should the tests agree for each simulation, but we should also see that we reject the null hypothesis (commit a type I error) with probability \\(\\alpha\\).\n\nN_sim <- 100000\nmu0 <- 0\nmu1 <- 3\nmu <- 0\nn <- 100\nsigma <- 1\nalpha = 0.05\n\nsample_mean_test <- function(X){\n  test_stat <- function(X) {\n    mean(X)\n  }\n  test_stat(X) > mu0 + qnorm(1-alpha)*(sigma/sqrt(n))    \n}\n\nz_score_test <- function(X){\n  test_stat <- function(X) {\n    (mean(X) - mu0)/(sigma/sqrt(n))\n  }\n  test_stat(X) > qnorm(1-alpha)   \n}\n\nlikelihood_ratio_test <- function(X){\n  test_stat <- function(X) {\n    prod(dnorm(X, mu1, sigma)) / prod(dnorm(X, mu0, sigma))\n  }\n  eta <- exp(-(mu0 + qnorm(1-alpha)*(sigma/sqrt(n)) - (mu1+mu0)/2)*((n*(mu0-mu1)) /(sigma^2)))\n  test_stat(X) > eta\n}\n\nmean_decisions <- vector(\"numeric\", N_sim)\nz_decisions <- vector(\"numeric\", N_sim)\nlikelihood_decisions <- vector(\"numeric\", N_sim)\n\nfor (k in 1:N_sim) {\n  X <- rnorm(n, mu, sigma)\n  mean_decisions[k] <- sample_mean_test(X)\n  z_decisions[k] <- z_score_test(X)\n  likelihood_decisions[k] <- likelihood_ratio_test(X)\n}\n\n#If all decisions were the same, this should be 1\nmean(z_decisions == mean_decisions)\n\n[1] 1\n\nmean(z_decisions == likelihood_decisions)\n\n[1] 1\n\n#Check number of rejections of the true null hypothesis\nmean(z_decisions)\n\n[1] 0.04992\n\n\nThis example is particularly special, because we were able to write the most powerful test in a form that did not depend on \\(\\mu_1\\): \\[\\delta(\\mathbf{X}) =\\begin{cases}\\mu_0 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} > \\Phi^{-1}(1-\\alpha) \\\\ \\mu_1 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} < \\Phi^{-1}(1-\\alpha) \\end{cases}\\] The test statistic and critical region do not depend on \\(\\mu_1\\), so this test is the most powerful test for any alternative \\(\\mu_1 > \\mu_0\\). This means that this test is the most powerful test for any hypothesis of the form \\(H_0:\\mu = \\mu_0\\) versus \\(\\mu \\ge \\mu_0\\). In fact, we can even go one step further – this test is the most powerful test for any hypotheses of the form \\(H_0:\\mu\\le \\mu_0\\) versus \\(H_1:\\mu > \\mu_0\\)! Consider testing \\(H_0:\\mu = \\mu_0\\) versus \\(H_1:\\mu \\ge \\mu_0\\), and testing the modified hypothesis \\(H_0':\\mu = \\mu_0'\\) versus \\(H_1':\\mu \\ge \\mu_0'\\), where \\(\\mu_0'<\\mu_0\\). All we’ve done is slightly tweaked \\(H_0 : \\mu \\le \\mu_0\\) by lowering the value of \\(\\mu_0'\\). What happens if we attempt to test \\(H_0'\\) versus \\(H_1'\\) using the test statistic for \\(\\delta(\\mathbf{X})\\) (as given above) instead of the test statistic of its modified counterpart \\(\\delta'(\\mathbf{X})\\)? \\[\\begin{align*}\nT(\\mathbf{X})&=\\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n}\\\\\nT'(\\mathbf{X})&=\\frac{\\bar X - \\mu_0'}{\\sigma/\\sqrt n}\n\\end{align*}\\] Under \\(H_0'\\): \\[\\begin{align*}\nT'(\\mathbf{X}) & \\sim N(0,1)\\\\\nT(\\mathbf{X}) & = \\frac{\\bar X - \\mu_0'}{\\sigma/\\sqrt n} + \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} = T'(\\mathbf{X}) + \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} \\sim N\\left(\\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}, 1\\right)\n\\end{align*}\\]\nIf we calculate the power of \\(\\delta'\\), we find that \\[\\begin{align*}\nc' & = -\\Phi^{-1}(\\alpha),\\\\\nc & = -\\left[\\Phi^{-1}\\left(\\alpha\\right) - \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}\\right] = \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}- \\Phi^{-1}(\\alpha).\n\\end{align*}\\] The power of the tests are \\[\\begin{align*}\n\\beta(\\delta, \\mu) & = \\Pr\\left(T(\\mathbf{X}) > \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}- \\Phi^{-1}(\\alpha)\\ \\bigg| \\ \\mu\\right)\\\\\n& = \\Pr\\left(T(\\mathbf{X}) - \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} >  \\Phi^{-1}(\\alpha)\\ \\bigg| \\ \\mu\\right)\\\\\n& = \\Pr\\left(T'(\\mathbf{X}) > - \\Phi^{-1}(\\alpha)  \\mid \\mu\\right)\\\\\n& = \\beta(\\delta', \\mu).\n\\end{align*}\\] Both tests have the same power, and same size, so \\(\\delta\\) is the most powerful test for any \\(H_0':\\mu \\neq \\mu_0'\\) versus \\(H_1':\\mu > \\mu_0'\\) where \\(\\mu_0'<\\mu_0\\). We can confirm this with simulations:\n\nN_sim <- 100000\nmu0 <- 3\nmu0_prime <- 2.7\nn <- 100\nsigma <- 1\nalpha <- 0.05\nc <- (mu0_prime - mu0)/(sigma / sqrt(100)) - qnorm(alpha)\nc_prime <- -qnorm(alpha)\n\ndelta <- function(X){\n  (mean(X)-mu0)/(sigma/sqrt(n)) > c\n}\ndelta_prime <- function(X){\n  (mean(X)-mu0_prime)/(sigma/sqrt(n)) > c_prime\n}\n\n#Verify that alpha < alpha' by simulating at mu = mu0'\ndecisions <- sapply(1:N_sim, function(t) delta(rnorm(n, mu0_prime, sigma)))\ndecisions_prime <- sapply(1:N_sim, function(t) delta_prime(rnorm(n, mu0_prime, sigma)))\n\nmean(decisions_prime) #should be ≈ alpha'\n\n[1] 0.05092\n\nmean(decisions) #should be ≈ alpha\n\n[1] 0.05086\n\nN_sim <- 10000\n\npower <- function(mu, decision_rule){\n  simulated_decisions <- sapply(1:N_sim, FUN = function(k){decision_rule(rnorm(n, mu, sigma))})\n  mean(simulated_decisions)\n}\npower_curve <- sapply(seq(2.6, 3.3, length = 40), power, decision_rule = delta)\npower_curve_prime <- sapply(seq(2.6, 3.3, length = 40), power, decision_rule = delta_prime)\n\n\n\nShow code which generates figure\ntibble(\n  x = rep(seq(2.6, 3.3, length = 40), 2),\n  y = c(power_curve, power_curve_prime), \n  decision = c(rep(\"δ\", 40), rep(\"δ'\", 40))\n) %>% \n  ggplot(aes(x, y, color = decision)) + \n  geom_line() + \n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  labs(x = \"μ\", y = \"Simulated Power\", color = \"\") +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 3.6: Simulated power curve for the two null hypotheses\n\n\n\n\nThis will hold for all hypotheses \\(H_1':\\mu\\le \\mu_0'\\) where \\(\\mu_0'\\), as \\(\\mu_0'\\) was arbitrary. This means that if we are testing \\(H_0:\\mu \\le \\mu_0\\) versus \\(H_1:\\mu>\\mu_0\\), the most powerful test is the z-test, regardless of the specified \\(\\mu_0\\) or alternative \\(\\mu\\). Because we pick the test that was derived using \\(H_0:\\mu = \\mu_0\\), we often write \\[\\begin{align*}\nH_0&:\\mu=\\mu_0,\\\\\nH_1&:\\mu>\\mu_0.\n\\end{align*}\\]\n\nWhile this example started as an application of the Neyman-Pearson lemma, it took several interesting turns. Firstly, while the Neyman-Pearson lemma gives the most powerful test in terms of the likelihood ratio, we were able to find several equivalent tests. Secondly, the most powerful test for the simple hypothesis \\(H_0: \\mu = \\mu_0\\) versus \\(H_1:\\mu = \\mu_1\\) (where \\(\\mu_1 > \\mu_0\\)), as given by the Neyman-Pearson lemma, did not depend on \\(\\mu_1\\). This meant it was the most powerful test for \\(H_0: \\mu = \\mu_0\\) versus \\(H_1:\\mu > \\mu_0\\). It is uniform in its status as the most powerful test.\n\nDefinition 3.8 A decision rule \\(\\delta(\\mathbf{X})\\), with size \\(\\alpha\\), is the uniformly most powerful (UMP) test for \\(H_0: P_{\\boldsymbol{\\theta}}\\in\\mathcal P_0\\) versus \\(H_1: P_{\\boldsymbol{\\theta}}\\in\\mathcal P_1\\) if \\[ \\beta(\\delta,P_\\boldsymbol{\\theta}) \\ge \\beta(\\delta',P_\\boldsymbol{\\theta}) \\ \\ \\ \\text{for all }P_\\boldsymbol{\\theta}\\in\\mathcal P_1\\] for all other decision rules \\(\\delta'\\) with a size of at least \\(\\alpha\\).\n\nWe also were able to establish that the UMP test for \\(H_0: \\mu = \\mu_0\\) versus \\(H_1:\\mu > \\mu_0\\), is the UMP for any test of the form \\(H_0:\\mu \\le \\mu_0\\) versus \\(H_1:\\mu >\\mu_0\\). Despite the Neyman-Pearson lemma only holding for simple hypotheses, we were able to derive the optimal (in the sense of power) test for composite hypotheses in this case. Is this always the case, or was there something special at work? Let’s look at another example to get a better lay of the land.\n\nExample 3.5 Suppose we draw a single observation of \\(X\\) where \\(X\\sim \\text{Cauchy}(\\theta,1)\\) and want to test \\(H_0:\\theta = 0\\) versus \\(H_1:\\theta = \\theta_1\\). In this case \\[f_X(x\\mid\\theta)= \\frac{1}{\\pi(1+(x-\\theta)^2)},\\] so the Neyman-Pearson lemma tells us the most powerful test is \\[\\delta(X) = \\begin{cases}0 & \\frac{1+x^2}{1+(x-\\theta_1)^2} < \\eta\\\\ \\theta_1 & \\frac{1+x^2}{1+(x-\\theta_1)^2} > \\eta\\end{cases}.\\] To get a better sense of what the critical region for this test is, let’s plot the likelihood ratio.\n\n\nShow code which generates figure\nexpand_grid(\n  x = seq(-5, 10, length = 3000), \n  alt_par = c(1, 1.2, 1.4,1.6,1.8,2)\n) %>% \n  mutate(y = dcauchy(x, alt_par, 1)/dcauchy(x, 0, 1)) %>% \n  ggplot(aes(x,y, color = as.factor(alt_par))) + \n  geom_line() +\n  ylab(\"Likelihood Ratio\") +\n  xlab(\"Value of Single Observation\") +\n  theme_minimal() +\n  geom_hline(yintercept = 2, color = \"black\", linetype = \"dashed\") +\n  labs(color = \"Alternative θ\") +\n  theme(legend.position = \"bottom\") +\n  annotate(\"text\", x = -3, y = 2.3, label = \"η\")\n\n\n\n\n\nFigure 3.7: The liklihood ratio associated with the Cauchy distribution. The liklihood ratio test tells us to reject the null hypothesis whenever the ratio exceeds η\n\n\n\n\nOur rejection region will be a bounded interval. After some quick algebra, we find that the critical region is \\[C = \\left\\{x\\ \\Bigg| \\ \\frac{\\eta\\theta_1-\\sqrt{\\eta\\theta_1^2+2\\eta-\\eta^2-1}}{\\eta-1}< x <\\frac{\\eta\\theta_1+\\sqrt{\\eta\\theta_1^2+2\\eta-\\eta^2-1}}{\\eta-1} \\right\\}.\\] Fixing \\(\\eta\\) to be some value, say \\(\\eta = 2\\), we can plot these critical regions across values of \\(\\theta_1\\).\n\n\nShow code which generates figure\neta <- 2\nexpand_grid(par = c(1,1.2,1.4,1.6,1.8,2)) %>% \n  mutate(\n    lower = (eta*par - sqrt(eta*par^2+2*eta-eta^2-1)) / (eta - 1),\n    upper = (eta*par + sqrt(eta*par^2+2*eta-eta^2-1)) / (eta - 1),\n    val = (upper + lower)/2\n  ) %>% \n  ggplot(aes(par, val)) +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  theme_minimal() +\n  labs(x = \"Alternative θ\", y = \"Critical Region\")\n\n\n\n\n\nFigure 3.8: The critical regions associated with the liklihood ratio test for varying alternatives. None of the critial regions are subsets of others.\n\n\n\n\nThe upper and lower bounds depend on the alternate \\(\\theta_1\\) so the most powerful test for one choice of \\(\\theta_1\\) fails to be the most powerful test for another \\(\\theta_1'\\).\n\nSo what makes this different from the example with the normal distribution? Fixing \\(H_0:\\mu_0 = 0\\), \\(\\sigma^2 = 1\\), and \\(n=1\\), Let’s see if the likelihood ratio for normally distributed data has any clues.\n\n\nShow code which generates figure\nexpand_grid(\n  x = seq(0, 3, length = 3000), \n  alt_par = c(1, 1.2, 1.4,1.6,1.8,2)\n) %>% \n  mutate(y = dnorm(x, alt_par, 1)/dnorm(x, 0, 1)) %>% \n  ggplot(aes(x,y, color = as.factor(alt_par))) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Value of Single Observation\", y = \"Likelihood Ratio\", color = \"Alternative μ\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.9: The liklihood ratio associated with the standard normal distribution\n\n\n\n\nNow let’s plot the critical region \\[ C = \\left\\{x \\mid x > \\mu_1/2 + \\ln \\eta / \\mu_1\\right\\}\\] for varying alternatives \\(\\mu_1\\), fixing \\(\\eta = 2\\) (which is equivalent to fixing the size \\(\\alpha\\)).\n\n\nShow code which generates figure\nexpand_grid(par = c(1,1.2,1.4,1.6,1.8,2)) %>% \n  mutate(\n    lower = par/2 - (log(2))/(par),\n    upper = Inf,\n    val = (upper + lower)/2\n  ) %>% \n  ggplot(aes(par, val)) +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  theme_minimal() +\n  labs(x = \"Alternative μ\", y = \"Critical Region\")\n\n\n\n\n\nFigure 3.10: The critical regions associated with the liklihood ratio test for varying alternatives. As we increase the alternative μ, each critical region is a subset of the prior one.\n\n\n\n\nAs \\(\\mu_1\\) increases, each critical region is a subset of the prior. In other words, for \\(\\mu_1' > \\mu_1\\), \\[ x > \\frac{\\mu_1'}{2} + \\frac{\\ln \\eta}{\\mu_1'} \\implies x > \\frac{\\mu_1}{2} + \\frac{\\ln \\eta}{\\mu_1}.\\] If we reject the null hypothesis for \\(\\mu_1'\\), we will reject it for \\(\\mu_1\\). If we want to pick the critical region that gives us the most power out of these options (subject to the fixed size \\(\\alpha\\)), we should pick the largest one, as it maximizes our chance of rejecting the null hypothesis, and is is a superset of the other choices of critical region. It is essential that the critical regions nest in one another like this as. Such sets are often called a monotonic sequence of sets, which is interesting, as the corresponding likelihood ratio is monotonically increasing for any \\(\\theta_1>\\theta_0\\). This monotonicity turns out to be the key to extending the Neyman-Pearson lemma to composite hypotheses.\n\nDefinition 3.9 A density \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\theta)\\) has a monotone likelihood ratio (MLR) with respect to a statistic \\(T\\) if \\(f(\\mathbf{x}\\mid \\theta_1)/f(\\mathbf{x}\\mid \\theta_0)\\) can be written as \\(f_{\\mathbf{X}}(T(\\mathbf{x}) \\mid \\theta_1)/f_{\\mathbf{X}}(T(\\mathbf{x}) \\mid \\theta_0)\\), and \\(f_{\\mathbf{X}}(T(\\mathbf{x}) \\mid \\theta_1)/f_{\\mathbf{X}}(T(\\mathbf{x}) \\mid \\theta_0)\\) is a monotonically increasing function in \\(T(\\mathbf{x})\\) for \\(\\theta_1 > \\theta_0\\).\n\nIn Example @ref(exm:npnorm), the normal distribution has an increasing MLR in the statistic \\(T(\\mathbf{X}) = \\bar X\\).\n\nTheorem 3.1 (Karlin-Rubin Theorem) Suppose \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\theta)\\) has an increasing MLR in the statistic \\(T\\). The decision rule \\(\\delta(\\mathbf{X})\\) defined as \\[ \\delta(\\mathbf{X}) = \\begin{cases} \\mathcal P_1 & T(\\mathbf{X}) > \\eta \\\\\n\\mathcal P_0  & T(\\mathbf{X}) < \\eta\n\\end{cases}\\] is the UMP test for \\(H_0: \\theta \\le \\theta_0\\) versus \\(H_1:\\theta > \\theta_0\\). Additionally, the power function \\(\\beta(\\delta, \\theta)\\) is monotonically increasing. Analogously, \\[ \\delta(\\mathbf{X}) = \\begin{cases} \\mathcal P_1 & T(\\mathbf{X}) < \\eta \\\\\n\\mathcal P_0  & T(\\mathbf{X}) > \\eta\n\\end{cases}\\] is the UMP test for \\(H_0: \\theta \\ge \\theta_0\\) versus \\(H_1:\\theta < \\theta_0\\), and the power function is monotonically decreasing.\n\n\nProof. See DeGroot and Schervish (2012).\n\n\nExample 3.6 (Testing Variance) Suppose we want to test \\(H_0:\\sigma^2 \\ge \\sigma_0^2\\) versus \\(H_1:\\sigma^2 < \\sigma_0^2\\) where \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for a known \\(\\mu\\). For \\(\\sigma_1^2 < \\sigma_0^2\\), the likelihood ratio of the joint distribution of our data is \\[\\begin{align*}\n\\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_1^2)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_0^2)} & = \\frac{\\prod_{i=1}^nf_{X_i}(x\\mid\\sigma_1^2)}{\\prod_{i=1}^nf_{X_i}(x\\mid\\sigma_0^2)} \\\\\n& = \\frac{\\left(\\frac{1}{\\sigma_1\\sqrt{2\\pi}}\\right)^n}{\\left(\\frac{1}{\\sigma_0\\sqrt{2\\pi}}\\right)^n}\\frac{\\exp\\left[-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_1^2}\\right]}{\\exp\\left[-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_0^2}\\right]}\\\\ & = \\left(\\frac{\\sigma_0}{\\sigma_1}\\right)^{n}\n\\exp \\left[\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_0^2}-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_1^2}  \\right] \\\\\n& = \\left(\\frac{\\sigma_1^2}{\\sigma_0^2}\\right)^{-n/2}\n\\exp \\left[-\\frac{1}{2(\\sigma_1^2 - \\sigma_0^2)}\\sum_{i=1}^n(x_i-\\mu)^2 \\right]  \\\\\n& = \\left(\\frac{\\sigma_1^2}{\\sigma_0^2}\\right)^{-n/2}\n\\exp \\left[-\\frac{T(\\mathbf{x})}{2(\\sigma_1^2 - \\sigma_0^2)} \\right]\n\\end{align*}\\] If we define \\(T(\\mathbf{X}) = \\sum_{i=1}^n(X_i-\\mu)^2\\), then the likelihood ratio is monotonically decreasing for \\(\\sigma_1^2 < \\sigma_0^2\\) (meaning \\(\\sigma_1^2 - \\sigma_0^2 < 0\\)).\n\\[ \\frac{\\partial}{\\partial T(\\mathbf{x})}\\left[\\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_1^2)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_0^2)}\\right] = -\\frac{1}{2(\\sigma_1^2 - \\sigma_0^2)}\\cdot \\left[\\frac{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_1^2)}{f_\\mathbf{X}(\\mathbf{x}\\mid\\sigma_0^2)}\\right] > 0.\\] This means the UMP test takes the form \\[\\delta(\\mathbf{X}) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n(X_i-\\mu)^2 < \\eta \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n(X_i-\\mu)^2 > \\eta\n\\end{cases}.\\] If we define \\(\\eta^* = 1/\\sigma_0^2\\), then \\[\\delta(\\mathbf{X}) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 < \\eta^* \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 > \\eta^*\n\\end{cases}.\\] Writing our test like this is far more useful, because \\(T(\\mathbf{X})\\) is now the sum of \\(n\\) random variables which are distributed according to a standard normal distribution, i.e \\(T(\\mathbf{X}) \\sim \\chi^2_n\\). This allows us to easily calculate \\(\\eta^*\\) given a desired size \\(\\alpha\\) using the quantile function for \\(T(\\mathbf{X}) \\sim \\chi^2_n\\). \\[\\delta(\\mathbf{X}) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 < (\\chi^2_n)^{-1}(\\alpha) \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 > (\\chi^2_n)^{-1}(\\alpha)\n\\end{cases}\\]\n\nN_sim <- 10000\nmu <- 0\nn <- 100\nsigma0 <- 1\nalpha <- 0.05\nc <- qchisq(alpha, n)\n\ndelta <- function(X){\n  sum(((X - mu)/sigma0)^2) < c\n}\n\npower <- function(sigma, decision_rule){\n  simulated_decisions <- sapply(1:N_sim, FUN = function(k){decision_rule(rnorm(n, mu, sigma))})\n  mean(simulated_decisions)\n}\npower_curve <- sapply(seq(0.65, 1.2, length = 40), power, decision_rule = delta)\n\n\n\nShow code which generates figure\ntibble(\n  x = seq(0.65, 1.2, length = 40), \n  y = power_curve\n) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Alternate σ\", y = \"Power\")\n\n\n\n\n\nFigure 3.11: Power curve associated with the liklihood ratio for variance\n\n\n\n\n\nThe Karlin-Rubin Theorem is very powerful, but is limited. It only works for scalar parameters \\(\\theta_0\\). The second we consider situations with a vector of parameters \\(\\boldsymbol{\\theta}_0\\), or a situation where are model is semiparametric or nonparametric, UMP tests usually do not exist. The theorem also does not directly apply to two-sided tests. There is some disagreement among sources about whether UMP tests even exist for two-sided tests (DeGroot and Schervish (2012) and Casella and Berger (2021) argue that they do not exist, while Romano and Lehmann (2005) say otherwise).\n\nExample 3.7 (Two-Sided Z Test) Suppose we want to test \\(H_0:\\mu = \\mu_0\\) versus \\(H_1:\\mu\\neq \\mu_0\\) where \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for a known \\(\\sigma^2\\). To test this for some significance level \\(\\alpha\\), we usually use a two-sided form of the \\(z-\\)test: \\[\\begin{align*}\n\\delta(\\mathbf{X}) & = \\begin{cases}\\mu = \\mu_0 & \\Phi^{-1}(\\alpha/2)< \\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}<\\Phi^{-1}(1-\\alpha/2)\\\\\n\\mu \\neq \\mu_0 & \\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\le \\Phi^{-1}(1-\\alpha/2) \\text{ or }\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\ge\\Phi^{-1}(1-\\alpha/2)\\end{cases}  \n= \\begin{cases}\\mu = \\mu_0 & \\left\\lvert\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\right\\rvert<\\Phi^{-1}(1-\\alpha/2)\\\\\n\\mu \\neq \\mu_0 & \\left\\lvert\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\right\\rvert\\ge \\Phi^{-1}(1-\\alpha/2)\\end{cases}\n\\end{align*}\\] Consider the “right-handed” and “left-handed” alternatives \\(\\delta_R(\\mathbf{X})\\) and \\(\\delta_L(\\mathbf{X})\\), respectively. Let’s graph the power functions of these three tests for \\(\\alpha = 0.05\\).\n\n\nShow code which generates figure\ntibble(x = (-500:500)/100) %>% \n  mutate(\n    'Right Handed' = pnorm(-1.645 + x), \n    'Left Handed' = pnorm(-1.645 - x),\n    'Two Sided' = pnorm(-1.956 + abs(x))\n  ) %>% \n  gather(key = \"test\", value = \"power\", -x) %>% \n  ggplot(aes(x, power, color = test)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"(μ - μ0)/(σ√n)\", y = \"Power\", color = \"\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.12: Power for three versions of the Z-test\n\n\n\n\nWe see that \\(\\beta(\\delta_R) > \\beta(\\delta)\\) when \\(\\mu > \\mu_0\\) and \\(\\beta(\\delta_L) > \\beta(\\delta)\\) when \\(\\mu < \\mu_0\\), so clearly \\(\\delta\\) is not a UMP. Despite this Romano and Lehmann (2005) would argue this test is UMP, because it actually has significance level \\(\\alpha/2\\). For two sided tests in this setting, Romano and Lehmann (2005) define a size \\(\\alpha\\) test to be one where \\[ \\alpha(\\delta \\mid \\mu < \\mu_0) =  \\alpha(\\delta \\mid \\mu > \\mu_0) = \\alpha.\\]\n\n\nRemark. The approach to testing proposed by Neyman and Pearson (1933) was/is not without controversy. Ronald Fisher, one of the central figures in the formalization of statistics, took issue with approach of Neyman and Pearson. A debate raged between the scientists as to the proper way of testing statistical hypotheses. The full details of this dispute is summarized by Lehmann (1993) and Lehmann (2011)."
  },
  {
    "objectID": "testing.html#asymptotics",
    "href": "testing.html#asymptotics",
    "title": "3  Hypothesis Testing",
    "section": "3.4 Asymptotics",
    "text": "3.4 Asymptotics\nUntil now, all of our examples have assumed \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). In this case, we were able to find the exact distribution of test statistics used to test hypotheses about \\(\\mu\\) and \\(\\sigma\\). This will usually not be possible, and we will need to use the tools developed in Section Chapter 2 to determine the asymptotic distribution of test statistics. To highlight this, we will consider a situation where we can derive the distribution of a test statistic, but as \\(n\\to\\infty\\), it converges in distribution such that there is virtually no harm done from using the asymptotic distribution instead.\n\nExample 3.8 (t-Test) For one final time, assume \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\), but drop the assumption that \\(\\sigma^2\\) is known. Assuming that we knew \\(\\sigma^2\\) made little to no sense. If we don’t know \\(\\mu\\), then how would we possibly know \\(\\sigma^2\\) (which is calculated using \\(\\mu\\))?! If we want to test \\(H_0:\\mu = \\mu_0\\) versus \\(H_1: \\mu \\neq \\mu_0\\), which decision rule do we pick? We can no longer calculate the statistic \\(Z = \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n}\\), as we do not know \\(\\sigma\\). What if we simply replaced \\(\\sigma\\) with the (unbiased) sample standard deviation? \\[T(\\mathbf{X}) = \\frac{\\bar X - \\mu_0}{s/\\sqrt n}\\] By replacing \\(\\sigma\\) with \\(s\\), we now have \\(T(\\mathbf{X})\\not\\sim N(0,1)\\), so we can no longer define the critical region using \\(\\Phi^{-1}\\). Instead we have \\(T(\\mathbf{X})\\sim t_{n-1}\\),7 so we can still test \\(H_0\\) versus \\(H_1\\). This new test is the classic (student’s) \\(t-\\)test, and we usually write the test statistic as \\(t = T(\\mathbf{X})\\). \\[\\delta(\\mathbf{X}) = \\begin{cases} \\mu \\neq \\mu_0& \\left\\lvert\\frac{\\bar X - \\mu_0}{s/\\sqrt n}\\right\\rvert \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\ \\mu = \\mu_0 & \\left\\lvert\\frac{\\bar X - \\mu_0}{s/\\sqrt n}\\right\\rvert < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases} \\]\nBut does this difference in distribution really matter? In Section @ref(asymptotic-properties-of-estimators), one example highlighted that if \\(Y \\sim t_{n}\\), then \\(Y\\overset{d}{\\to}N(0,1)\\). This means that \\(t\\overset{d}{\\to}Z\\), and the \\(t-\\)test is asymptotically equivalent to the \\(z-\\)test. To illustrate this, suppose \\(X_i \\sim N(4,1)\\), \\(H_0:\\mu = 2\\), and \\(H_1:\\mu \\neq 2\\). For \\(\\alpha = 0.01\\), we can compare the results of the two tests as \\(n\\) increases.\n\nN_sim <- 10000\nmu_0 <- 2\nmu <- 4\nsigma <- 1\nalpha <- 0.01\n\n#Define t-test\nt_test <- function(X){\n  n <- length(X)\n  T_stat <- (mean(X) - mu_0)/(sd(X)/sqrt(n))\n  abs(T_stat) > qt(1 - alpha/2, df = n - 1)\n}\n\n#Define an \"incorrect\" z-test using the sample standard deviation\nz_test <- function(X){\n  n <- length(X)\n  Z <- (mean(X) - mu_0)/(sd(X)/sqrt(n))\n  abs(Z) > qnorm(1 - alpha/2)\n}\n\n#Define a function which compares the two decision rules for a sample size n over 10000 simulations\ncompare_tests <- function(n){\n  output <- vector(\"numeric\", N_sim)\n  for (k in 1:N_sim) {\n    X <- rnorm(n, mu, sigma)\n    #Do the tests agree?\n    output[k] <- t_test(X) == z_test(X)\n  }\n  #Return the % of simulations where the tests agreed\n  mean(output)\n}\n\n#Compare the tests for various choices of sample size\nresults <- sapply(2:15, compare_tests)\n\n\n\nShow code which generates figure\ntibble(\n  x = 2:15, \n  y = results\n) %>% \n  ggplot(aes(x,y)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Frequency of Tests Agreeing over 10000 Simulations\")\n\n\n\n\n\nFigure 3.13: As the sample size increases, the frequency at which the Z-test and t-test agree increase\n\n\n\n\nEven for modest values of \\(n\\), the tests return the same results for 10,000 simulations.\n\nWhile this example illustrates how we can leverage asymptotics when testing hypotheses, we weren’t “required” by any means to take advantage of the fact that \\(t\\overset{d}{\\to}N(0,1)\\), because we knew that \\(t\\sim t_{n-1}\\). Unfortunately, \\(t\\sim t_{n-1}\\) will only hold if \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). Once we drop this assumption we must rely on the fact that \\(t \\overset{d}{\\to}N(0,1)\\)\n\nExample 3.9 (Non-normal Data) Suppose \\(X_i\\overset{iid}{\\sim}F_X\\), and we want to test \\(H_0:\\mu = \\mu_0\\) versus \\(H_1: \\mu \\neq \\mu_0\\). The (irregular) model \\(\\mathcal P\\) is a collection of sets, each comprised of all the distributions with a common expected value \\(\\mu\\). \\[P_\\mu = \\left\\{f_\\mathbf{X}(\\mathbf{x})\\ \\Bigg|\\ f_\\mathbf{X}(\\mathbf{x}) = \\prod_{i=1}^n f_{X_i}(x) \\text{ and } f_{X_i} = f_{X_j}\\ \\forall i,j \\text{ and }\\text{E}\\left[X_i\\right]=\\mu\\ \\forall i\\right\\}\\]In this case, the CLT tells us that \\(\\sqrt{n}(\\bar X - \\mu_0) \\overset{d}{\\to}N(0,1)\\), so \\[ t = \\frac{(\\bar X - \\mu_0)}{S/\\sqrt{n}} = \\frac{\\sqrt{n}(\\bar X - \\mu_0)}{S}  \\overset{d}{\\to}\\frac{N(0,\\sigma^2)}{S} = t_{n-1}.\\] Even though we do not know the actual distribution of \\(T\\), we know the asymptotic distribution, and can calculate critical regions when \\(n\\) is sufficiently large. To demonstrate this, let’s simulate \\(\\Pr(T\\in C\\mid \\mu=\\mu_0)\\) for \\[\\delta(\\mathbf{X}) = \\begin{cases} \\mu \\neq \\mu_0& \\left\\lvert\\frac{\\bar X - \\mu_0}{s/\\sqrt n}\\right\\rvert \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\ \\mu = \\mu_0 & \\left\\lvert\\frac{\\bar X - \\mu_0}{s/\\sqrt n}\\right\\rvert < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases}\\] when \\(X_i \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)\\) (such that \\(\\text{E}\\left[X_i\\right]=\\mu\\)). As \\(n\\to\\infty\\), we should see \\(\\Pr(T\\in C\\mid \\mu=\\mu_0)\\to \\alpha\\), indicating that \\(t_{n-1}\\) does indeed provide a sufficient approximation to calculate critical values. We will test \\(H_0:\\mu=2\\) versus \\(H_0:\\mu=8\\) where \\(\\alpha = 0.05\\).\n\nN_sim <- 10000\nmu_0 <- 2\nmu <- 8\nalpha <- 0.05\n\nt_test <- function(X){\n  n <- length(X)\n  T_stat <- (mean(X) - mu_0)/(sd(X)/sqrt(n))\n  abs(T_stat) > qt(1 - alpha/2, df = n - 1)\n}\n\nsimulate_alpha <- function(n){\n  mean(sapply(1:N_sim, FUN = function(t){t_test(rexp(n , 1/mu_0))}))\n}\nalpha_n <- sapply((1:100)*20, simulate_alpha)\n\n\n\nShow code which generates figure\ntibble(\n  x = (1:100)*20, \n  y = alpha_n\n) %>% \n  ggplot(aes(x,y)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Simulated α\") +\n  geom_hline(yintercept = 0.05, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\nFigure 3.14: Despite our data being drawn from an exponential distribution, the test statistic is asympotically distributed according to the t-distribution, so the simulated size approaches the theoretical size of 0.05\n\n\n\n\nWe also have that \\(t\\overset{d}{\\to}N(0,1)\\) when \\(X_i\\overset{iid}{\\sim}F_X\\). We have \\(S\\to\\sigma\\), so by Slutsky’s theorem \\[ t = \\underbrace{\\sqrt{n}(\\bar X - \\mu_0)}_{\\overset{d}{\\to}N(0,\\sigma^2)} / \\underbrace{S}_{\\overset{p}{\\to}\\sigma} \\overset{d}{\\to}N(0,1).\\] If we wanted to, we could still calculate critical values using the standard normal distribution, even with non-normal data, but those calculated with \\(t_{n-1}\\) would be more slightly more accurate (with this advantage quickly diminishing as \\(n\\to\\infty\\)).\n\nIn practice, the overwhelming majority of our tests will rely on asymptotic distributions of test statistics. What does this mean when considering the power of a test? The Neyman-Pearson lemma and Karlin-Rubin theorem assumed a fixed sample size. In this sense, we were consider finite sample properties of tests, just like how we considered finite sample properties of estimators in @ref(finite-sample-properties-of-estimators). This was a bit shortsighted considering the effect sample size has on power.\n\nExample 3.10 (Power and Sample Size) Consider testing \\(H_0:\\mu \\le \\mu_0\\) versus \\(H_1:\\mu > \\mu_0\\) using the \\(z-\\)test. where \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\). The power of this test is \\[\\beta(\\mu) = \\Phi\\left(-1.645 + \\frac{\\mu-\\mu_0}{\\sigma / \\sqrt{n}} \\right).\\] Power is increasing in \\(n\\) as \\[ \\frac{\\partial \\beta}{\\partial n} = \\frac{\\mu-\\mu_0}{2\\sigma\\sqrt n}\\varphi\\left(-1.645 + \\frac{\\mu-\\mu_0}{\\sigma / \\sqrt{n}} \\right) > 0.\\]\nIf we plot \\(\\beta(\\mu)\\) for various sample sizes, it appears that that \\(\\beta(\\mu \\mid \\mu > \\mu_0) \\to 1\\), and \\(\\beta(\\mu \\mid \\mu \\le \\mu_0) \\to 0\\). For example if \\(\\mu_0 = 2\\) and \\(\\sigma = 1\\), we have:\n\n\nShow code which generates figure\nmu_0 <- 2\nsigma <- 1\nexpand_grid(\n  mu = seq(1.8, 2.2, length = 1000), \n  n = c(5,10,25,100,1000, 10000)\n) %>% \n  mutate(power = pnorm(-1.645 + (mu - mu_0)/(sigma/sqrt(n)))) %>% \n  ggplot(aes(mu, power, color = as.factor(n))) +\n  geom_line() +\n  theme_minimal() +\n  geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 0), color = \"black\") +\n  geom_segment(aes(x = 2, y = 1, xend = 2.2, yend = 1), color = \"black\") +\n  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = \"black\", linetype = \"dashed\") +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Alternate μ\", y = \"Power\", color = \"Sample Size\") +\n  guides(colour = guide_legend(nrow = 1))\n\n\n\n\n\nFigure 3.15: As the sample size increases, the power curve approaches a step function that is 0 where the null hypothesis is true, and 1 where is is false\n\n\n\n\n\nThis example highlights that as \\(n\\to\\infty\\), tests become optimal in the sense that the probability of committing any error (whether it be type I or type II) approaches zero.\n\nDefinition 3.10 Suppose \\(\\delta(\\mathbf{X})\\) is a decision rule for the hypotheses \\(H_0:P_{\\boldsymbol{\\theta}}\\in\\mathcal P_0\\) versus \\(H_1:P_{\\boldsymbol{\\theta}}\\in\\mathcal P_0\\). The decision rule has asymptotic size \\(\\alpha\\) if \\[ \\lim_{n\\to\\infty} \\beta(\\delta, P_{\\boldsymbol{\\theta}} \\mid P_{\\boldsymbol{\\theta}}\\in\\mathcal P_0) = \\alpha.\\] The decision rule has consistent if \\[ \\lim_{n\\to\\infty} \\beta(\\delta, P_{\\boldsymbol{\\theta}} \\mid P_{\\boldsymbol{\\theta}}\\in\\mathcal P_1) = 1.\\]\n\nThese two definitions are somewhat informal, and barely scratch the surface of asymptotic properties of hypothesis testing. For a detailed treatment, see Romano and Lehmann (2005)."
  },
  {
    "objectID": "testing.html#confidence-intervals",
    "href": "testing.html#confidence-intervals",
    "title": "3  Hypothesis Testing",
    "section": "3.5 Confidence Intervals",
    "text": "3.5 Confidence Intervals\nAn alternate way to think about hypothesis testing is in terms of confidence intervals. An inherent flaw in point estimation is that is necessarily provides a single estimate. We may want to provide a set of plausible estimates in an effort to address the inherent uncertainty that arises from point estimation.\n\nDefinition 3.11 A statistic \\(\\underline\\theta(\\mathbf{X})\\) is a level \\((1-\\alpha)\\) lower confidence bound for \\(\\theta\\) if \\(\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta] \\ge 1 - \\alpha\\) for all \\(\\theta \\in \\Theta\\). A statistic \\(\\bar\\theta(\\mathbf{X})\\) is a level \\((1-\\alpha)\\) upper confidence bound for \\(\\theta\\) if \\(\\Pr[\\bar\\theta(\\mathbf{X}) \\ge \\theta] \\ge 1 - \\alpha\\) for all \\(\\theta \\in \\Theta\\). Together these statistics form an interval \\([\\underline\\theta(\\mathbf{X}),\\bar\\theta(\\mathbf{X})]\\) known as a level level \\((1-\\alpha)\\) confidence interval for \\(\\theta\\) when \\[ \\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})] \\ge 1-\\alpha\\] The confidence coefficient for a confidence interval is the largest possible confidence level given as \\[ \\inf_{\\theta \\in \\Theta}\\{\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})]\\}.\\] A confidence interval with confidence coefficient \\(1-\\alpha\\) is called a \\(100(1-\\alpha)\\)% confidence interval.\n\nThe level of a confidence interval is not unique. If \\(\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})] \\ge 1-\\alpha\\), then \\(\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})] \\ge 1-\\alpha'\\) for all \\(\\alpha'>\\alpha\\), so a confidence interval of level \\((1-\\alpha)\\) will be a confidence interval of level \\((1-\\alpha')\\) for all \\(\\alpha'>\\alpha\\). The confidence coefficient eliminates this ambiguity by taking the maximum level of confidence \\((1-\\alpha)\\).\nIt’s important to emphasize that the probability \\(\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})]\\) does not correspond to the probability that \\(\\theta\\) is contained in \\([\\underline\\theta,\\bar\\theta]\\). The parameter \\(\\theta\\) is a constant, and not a random variable. It is either in the interval or not. Instead \\(\\Pr[\\underline\\theta(\\mathbf{X}) \\le \\theta \\le \\bar\\theta(\\mathbf{X})]\\) is related to the calculation of the statistics \\(\\underline\\theta\\) and \\(\\bar\\theta\\), both of which are random variables. If we construct a 95% confidence interval for \\(\\theta\\), then there is a 95% chance that the interval \\([\\underline\\theta,\\bar\\theta]\\) contains \\(\\theta\\), where probability is taken over all possible samples \\(\\mathbf{X}\\).\n\nExample 3.11 Suppose we want to construct a confidence interval for \\(\\mu\\), where \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for an unknown \\(\\sigma\\). In order to construct some confidence interval \\([\\underline\\mu(\\mathbf{X}),\\bar \\mu(\\mathbf{X})]\\), we need to define \\(\\underline\\mu\\) and \\(\\bar \\mu\\) such that we know their respective probability distributions, thereby allowing us to calculate \\(\\Pr[\\underline\\mu(\\mathbf{X}) \\le \\mu \\le \\bar\\mu(\\mathbf{X})]\\). One statistic for which we know the distribution is \\(t = \\frac{\\bar X - \\mu}{s/\\sqrt n}\\), as \\(t \\sim t_{n-1}\\). The distribution \\(t_{n-1}\\) is symmetric, so we can use the quantile function \\(t_{n-1}^{-1}\\) to determine a confidence interval for \\(t\\). \\[\\Pr[-t_{n-1}^{-1}(1-\\alpha/2)\\le t\\le t_{n-1}^{-1}(1-\\alpha/2)] = 1-\\alpha \\] This is a key step in the direction of finding a confidence interval for \\(\\mu\\), as \\(t\\) is a function of \\(\\mu\\). \\[\\begin{align*}\n&\\Pr[-t_{n-1}^{-1}(1-\\alpha/2)\\le t\\le t_{n-1}^{-1}(1-\\alpha/2)] = 1-\\alpha \\\\\n\\implies &\\Pr\\left[-t_{n-1}^{-1}(1-\\alpha/2)\\le \\frac{\\bar X - \\mu}{s/\\sqrt n}\\le t_{n-1}^{-1}(1-\\alpha/2)\\right] = 1-\\alpha \\\\\n\\implies &\\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] = 1-\\alpha\n\\end{align*}\\] Therefore we have a \\((1-\\alpha)\\) level confidence interval for \\(\\mu\\) in the form of \\[ \\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} , \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right].\\] If we let \\(\\alpha = 0.05\\), \\(\\mu = 0\\), \\(\\sigma = 1\\), and \\(n = 10\\), we can generate confidence intervals for a large number of simulated samples. About 95% of the constructed intervals will contain the true mean \\(\\mu = 0\\).\n\nN_sim <- 10000\nn <- 10\nsigma <- 1\nmu <- 0\nalpha <- 0.05\n\nlower <- vector(\"numeric\", N_sim)\nupper <- vector(\"numeric\", N_sim)\ncontain <- vector(\"numeric\", N_sim)\nfor (k in 1:N_sim) {\n  X <- rnorm(n, mu, sigma)\n  lower[k] <- mean(X) - qt(0.975, n-1)*(sd(X)/sqrt(n))\n  upper[k] <- mean(X) + qt(0.975, n-1)*(sd(X)/sqrt(n))\n  contain[k] <- (lower[k] < 0)*(upper[k] > 0)\n}\n\nmean(contain)\n\n[1] 0.9515\n\n\nLet’s plot 100 of our constructed 95% confidence intervals (drawn at random).\n\n\nShow code which generates figure\ntibble(\n  lower, \n  upper, \n  contain, \n  x = 0\n) %>% \n  slice_sample(n = 100) %>% \n  mutate(\n    n = row_number(),\n    contain = as.logical(contain)\n  ) %>% \n  ggplot(aes(n, color = as.factor(contain))) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  labs(color = \"Interval Contains   μ = 0:\", x = \"Sample\", y = \"Parameter Space\") +\n  scale_color_manual(values = c(\"red\", \"green\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.16: 100 of the 10,000 simulated confidence intervals drawn at random. Approximately 5% of the 100 do not contain the true parameter value\n\n\n\n\n\nWe can also construct approximate confidence intervals using asymptotic theory. If we modified the previous example such that \\(X_i\\) had some arbitrary distribution \\(F_X\\) with mean \\(\\mu\\), then \\(t\\overset{a}{\\sim}t_{n-1}\\), so \\[ \\lim_{n\\to\\infty}\\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] = 1- \\alpha. \\] In other words, for a sufficiently large \\(n\\), we will have \\[ \\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] \\approx 1- \\alpha.\\]\nThe previous example also suggests that we may be able to use confidence intervals to test hypotheses. We derived an interval \\([\\underline\\mu(\\mathbf{X}),\\bar \\mu(\\mathbf{X})]\\) such that \\(\\Pr(0 \\notin [\\underline\\mu(\\mathbf{X}),\\bar \\mu(\\mathbf{X})]) = \\alpha\\). If we wanted to test \\(H_0:\\mu=0\\) versus \\(H_1:\\mu\\neq0\\) with a significance level of \\(\\alpha\\), then perhaps we define a decision rule \\(\\delta\\) such that we fail to reject \\(H_0\\) if and only if \\(\\bar X\\in[\\underline\\mu(\\mathbf{X}),\\bar \\mu(\\mathbf{X})]\\). Not only does this work, but it highlights how confidence intervals and hypothesis tests are two sides of the same coin.\n\nTheorem 3.2 Suppose \\(\\delta(\\mathbf{X})\\) is a decision rule with level \\(\\alpha\\) for the hypothesis \\(H_0:\\boldsymbol{\\theta}= \\boldsymbol{\\theta}_0\\). If \\(A(\\boldsymbol{\\theta}_0)=\\{\\mathbf{x}\\mid \\delta(\\mathbf{x}) = 0\\}\\) is the set of observations for which we fail to reject \\(H_0\\), then the set \\[S(\\mathbf{x}) = \\{\\boldsymbol{\\theta}\\mid \\mathbf{x}\\in A(\\boldsymbol{\\theta})\\} \\] is a family of confidence intervals for \\(\\boldsymbol{\\theta}\\) with level \\(1-\\alpha\\).\n\n\nProof. We’ve defined \\(S(\\mathbf{x})\\) such that \\(\\boldsymbol{\\theta}\\in S(\\mathbf{x})\\) holds if and only if \\(\\mathbf{x}\\in A(\\boldsymbol{\\theta})\\), so \\[ \\Pr[\\boldsymbol{\\theta}\\in S(\\mathbf{X})] = \\Pr[\\mathbf{X}\\in A(\\boldsymbol{\\theta})] .\\] Since \\(\\delta\\) is a level \\(\\alpha\\) test, \\(\\Pr[\\mathbf{X}\\in A(\\boldsymbol{\\theta})] \\ge 1-\\alpha\\), so \\[\\Pr[\\boldsymbol{\\theta}\\in S(\\mathbf{X})] \\ge 1-\\alpha.\\]"
  },
  {
    "objectID": "testing.html#wald-test-and-t-test",
    "href": "testing.html#wald-test-and-t-test",
    "title": "3  Hypothesis Testing",
    "section": "3.6 Wald Test and t-Test",
    "text": "3.6 Wald Test and t-Test\nWhile we’ve talked a fair amount about the properties tests can have, and considered very specific examples of tests, we have only defined one test in general – the likelihood ratio test given by Theorem @ref(thm:NPlemma). This test has appealing properties in finite samples, especially if @ref(thm:KR) holds. Unfortunately, the distribution of the test statistic in finite samples depends on the model which generates the data, so the likelihood ratio is not very robust. Instead, we will consider a large-sample alternative.\nSuppose we are testing \\(H_0: P_{\\boldsymbol{\\theta}} \\in \\mathcal P_0\\) versus \\(H_1: P_{\\boldsymbol{\\theta}} \\in \\mathcal P_1\\) where \\[\\begin{align*}\n\\mathcal P_0 = \\{P_{\\boldsymbol{\\theta}}\\in\\mathcal P \\mid \\mathbf h(\\boldsymbol{\\theta}) = \\mathbf 0\\},\\\\\n\\mathcal P_1 = \\{P_{\\boldsymbol{\\theta}}\\in\\mathcal P \\mid \\mathbf h(\\boldsymbol{\\theta}) \\neq \\mathbf 0\\},\n\\end{align*}\\] for some function \\(\\mathbf h:\\boldsymbol\\Theta \\to \\mathbb R^q\\). In other words, we are testing \\(H_0:\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf 0\\) versus \\(H_1: \\mathbf h(\\boldsymbol{\\theta}) \\neq \\mathbf 0\\). See Wolak (1989) for the case where \\(\\mathbf h(\\boldsymbol{\\theta}) \\ge \\mathbf{0}\\) or \\(\\mathbf h(\\boldsymbol{\\theta}) \\le \\mathbf{0}\\). The function \\(\\mathbf h:\\boldsymbol\\Theta \\to \\mathbb R^q\\) describes the \\(q\\) relationships we are testing between the \\(k=\\dim(\\boldsymbol\\Theta)\\) components of \\(\\boldsymbol{\\theta}= (\\theta_1,\\ldots,\\theta_k)\\). For example, if \\(\\boldsymbol\\Theta = \\mathbb R^4\\) and our null hypothesis was comprised of the following equations: \\[\\begin{align*}\n\\theta_1 &= 2\\theta_3\\\\\n\\theta_2 &= \\theta_1\\ln \\theta_4\\\\\n\\theta_3 &= 3\n\\end{align*}\\] then \\(\\mathbf h: \\mathbb R^4 \\to \\mathbb R^3\\) is \\[ \\mathbf h(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\theta_1 - 2\\theta_3\\\\ \\theta_2 -\\theta_1\\ln \\theta_4\\\\\\theta_3-3\\end{bmatrix}.\\] There are two special cases of hypotheses we should consider.\n\nExample 3.12 (Linear Hypotheses) In the event our null hypothesis postulates that the components of \\(\\boldsymbol{\\theta}\\) are linear combinations of each other, we can write \\(\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf H\\boldsymbol{\\theta}\\) for some \\(q\\times k\\) matrix of constants. \\[\\begin{cases}\\sum_{i=1}^k c_{1i}\\theta_i = 0\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\\\\ \\sum_{i=1}^k c_{qi}\\theta_i = 0\\end{cases} \\implies \\mathbf H = \\begin{bmatrix} c_{11} & \\cdots &c_{1k} \\\\ \\vdots & \\ddots & \\vdots \\\\ c_{q1} & \\cdots &c_{qk} \\end{bmatrix}\\] Our null hypothesis is \\(\\mathbf H\\boldsymbol{\\theta}= \\mathbf 0\\).\n\n\nExample 3.13 In many settings, the default null hypothesis considered is \\(H_0:\\boldsymbol{\\theta}= \\mathbf 0\\). In this case, \\(\\mathbf h(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}\\). If we are only interested in one parameter \\(\\theta_j\\), then \\(\\mathbf h(\\boldsymbol{\\theta}) = \\theta_j\\).\n\nIn order to determine the asymptotic distribution of the test statistics prescribed by each test, we need to adopt a distributional assumption about our estimator.\n\nWe have an estimator \\(\\hat {\\boldsymbol{\\theta}}\\) satisfying \\(\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) \\overset{d}{\\to}N(\\mathbf{0}, \\mathbf V)\\) for a PSD matrix \\(\\mathbf V\\). That is, \\(\\hat {\\boldsymbol{\\theta}}\\) is root-\\(n\\) CAN and \\(\\hat {\\boldsymbol{\\theta}}\\overset{a}{\\sim}N(\\boldsymbol{\\theta}, \\mathbf V/n)\\) where \\(\\text{Avar}\\left(\\hat{\\boldsymbol{\\theta}}\\right)= \\mathbf V/n\\)\n\nWe are making no assumptions about the distribution of \\(\\mathbf{X}\\), the asymptotic distribution of \\(\\mathbf{X}\\), or the distribution of \\(\\hat{\\boldsymbol{\\theta}}\\). We are only making an assumption about the asymptotic distribution of some \\(\\hat{\\boldsymbol{\\theta}}\\). Because of the LLN and CLT, this assumption turns out to be fairly weak, as nearly all common estimators are root-\\(n\\) CAN.\nThe first test we consider will be the most familiar, and will not involve the likelihood function in our general setting. Suppose \\(\\theta\\) and \\(h(\\theta)=\\theta_0\\) are scalars. If we want to test $h()=0 $, we may want to decide whether or not to reject the null hypothesis based on the discrepancy between \\(h(\\theta)\\) and \\(h(\\hat\\theta)\\). If the true parameter value is \\(\\theta_0\\), then we would reject the null hypothesis if \\(\\hat\\theta- \\theta_0 \\gg 0\\) or \\(\\hat\\theta- \\theta_0 \\ll 0\\). We can consider these cases simultaneously by rejecting the null hypothesis if \\((\\hat\\theta-\\theta_0)^2 \\gg 0\\). This is a bit ambiguous though, because it isn’t clear what distance \\(\\hat\\theta- \\theta_0\\) is surprising enough to merit rejecting the null hypothesis. Fortunately, we’ve assumed that \\(\\text{Avar}\\left(\\hat{\\theta}\\right)= V/n\\) so we can standardize the distance using the standard deviation of \\(\\hat\\theta\\). This standard deviation has a special name.\n\nDefinition 3.12 The standard error of an estimator \\(\\hat{\\boldsymbol{\\theta}}\\) is its standard deviation. \\[\\text{se}\\left(\\hat{\\boldsymbol{\\theta}}\\right)=\\text{diag}\\left[\\text{Var}\\left(\\hat{\\boldsymbol{\\theta}}\\right)\\right]^{1/2} \\]\n\nUsing the standard error we can measure the non-negative distance as \\[ \\left(\\frac{\\hat\\theta-\\theta_0}{\\text{se}\\left(\\hat{\\theta}\\right)}\\right)^2 = \\frac{(\\hat\\theta-\\theta_0)^2}{\\text{Var}\\left(\\hat\\theta\\right)}.\\] This still doesn’t do though, because we do not know \\(\\text{Var}\\left(\\hat\\theta\\right)\\). Perhaps instead we can use \\(\\text{Avar}\\left(\\hat\\theta\\right)\\), giving \\[ \\frac{(\\hat\\theta-\\theta_0)^2}{\\text{Avar}\\left(\\hat\\theta\\right)} = \\frac{(\\hat\\theta-\\theta_0)^2}{V/n} .\\] Unfortunately, if \\(\\text{Avar}\\left(\\hat\\theta\\right)\\) is a function of \\(\\theta\\) (which is more often than not the case), then we must estimate \\(\\text{Avar}\\left(\\hat\\theta\\right)\\), giving the statistic \\[ \\frac{(\\hat\\theta-\\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat\\theta)} = \\frac{(\\hat\\theta-\\theta_0)^2}{\\hat V/n}.\\] This should look very familiar. For example, if we have \\(X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) and want to test \\(H_0:\\mu\\neq \\mu_0\\), a situation where \\(\\sqrt n(\\bar X-\\mu_0) \\overset{d}{\\to}N(0,\\sigma^2)\\) and \\(\\text{Avar}\\left((\\right)\\bar X) = \\sigma^2/n\\), we have \\[ \\frac{(\\hat\\theta-\\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat\\theta)} = \\frac{(\\bar X-\\mu_0)^2}{\\widehat{(\\sigma^2/n)}} = \\frac{(\\bar X-\\mu_0)^2}{S^2/n}.\\] This is just the squared test statistic for the \\(t-\\)test! \\[ \\left[\\frac{(\\bar X-\\mu_0)^2}{S^2/n}\\right]^{1/2} = \\frac{\\bar X -\\mu_0}{S/\\sqrt n}.\\] In the event we know \\(\\sigma^2\\), we don’t even need to estimate \\(\\text{Avar}\\left(\\bar X\\right)\\), and this statistic becomes the squared version of test statistic from the \\(z-\\)test. In general this test statistic simply reports how many estimated standard deviations \\(\\theta\\) is from \\(\\theta_0\\), squared. This statistic is due to Wald (1943), and we will now define it in higher dimensions for possibly nonlinear hypotheses.\n\nDefinition 3.13 Given the hypotheses \\(H_0:\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf 0\\) versus \\(H_1: \\mathbf h(\\boldsymbol{\\theta}) \\neq \\mathbf 0\\), the Wald statistic is defined as \\[ W(\\mathbf{X}) = \\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\widehat{\\text{Avar}}(\\hat{\\boldsymbol{\\theta}})\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol{\\theta}}).\\]\n\nWhile this looks fairly complex, the following example shows that it does indeed simplify to statistic we used to build intuition.\n\nExample 3.14 Suppose \\(\\dim(\\boldsymbol\\Theta)=1\\), and \\(h(\\theta) = \\theta - \\theta_0\\). The null hypothesis is \\(H_0: \\theta - \\theta_0 = 0\\), which is \\(H_0: \\theta = \\theta_0\\). We have \\[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}) = \\frac{\\partial }{\\partial \\theta}[\\theta - \\theta_0]_{\\theta=\\hat\\theta} = 1.\\] Because we have a single parameter, transposes are trivial: \\[\\begin{align*}\n\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})' & = [1]' = 1 = \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}).\\\\\n\\mathbf h(\\hat{\\boldsymbol{\\theta}})' &= [\\hat\\theta - \\theta_0]'=\\hat\\theta - \\theta_0 = \\mathbf h(\\hat{\\boldsymbol{\\theta}}).\n\\end{align*}\\] Our statistic is \\[\\begin{align*}\nW &= \\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\widehat{\\text{Avar}}(\\hat{\\boldsymbol{\\theta}})\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol{\\theta}})\\\\\n& = (\\hat\\theta - \\theta_0)[1\\cdot\\widehat{\\text{Avar}}(\\hat{\\theta})\\cdot 1 ]^{-1}(\\hat\\theta - \\theta_0)\\\\\n& = \\frac{(\\hat\\theta - \\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat{\\theta})}\n\\end{align*}\\]\n\nTo use the Wald statistic in a test, we need to be able to construct critical regions given a choice of size \\(\\alpha\\). When \\(n\\) is sufficiently large, this requires knowing the asymptotic distribution of \\(W\\). It’s possible to make an informed guess about this distribution if we think about the Wald statistic as a squared \\(t-\\)statistic. We established that \\[ t = \\frac{(\\bar X-\\mu_0)^2}{S^2/n} \\overset{d}{\\to}N(0,1).\\] If we apply the continuous mapping theorem to \\(t^2 = W\\), we have \\[ W = t^2 \\overset{d}{\\to}[N(0,1)]^2 = \\chi^2_1.\\] Not only is \\(W\\) asymptotically distributed according to a chi-square distribution in this simple setting, but it is in the general setting. The only thing we need to account for when showing this in higher dimensions is that we are testing \\(q\\) one dimensional hypotheses simultaneously when \\(\\mathbf h:\\boldsymbol \\Theta \\to \\mathbb R^q\\).\n\nIf \\(\\mathbf{x}\\sim N(\\mathbf{0},\\mathbf I)\\) where \\(\\mathbf{x}= (x_1,\\ldots,x_n)\\), then \\(\\mathbf{x}'\\mathbf{x}\\sim \\chi_n^2\\).\n\n\nProof. We have \\(\\mathbf{x}'\\mathbf{x}= \\sum_{i=1}^n x_i^2\\) where \\(x_i\\sim N(0,1)\\), so \\(\\mathbf{x}'\\mathbf{x}\\) is the sum of \\(n\\) random variables with a standard normal distribution. This means \\(\\mathbf{x}'\\mathbf{x}\\sim \\chi_n^2\\).\n\n\nTheorem 3.3 Suppose:\n\nAssumption @ref(exr:assone) holds;\n\\(\\mathbf h:\\boldsymbol \\Theta \\to \\mathbb R^q\\) is continuously differentiable on \\(\\boldsymbol\\Theta\\subset\\mathbb R^k\\) and \\(\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}\\) is invertible;\n\\(\\hat {\\mathbf V}\\) is a consistent estimator for \\(\\mathbf V\\);\n\\(\\boldsymbol{\\theta}\\) is in the interior of \\(\\boldsymbol\\Theta\\) .\n\nThen \\(W \\overset{d}{\\to}\\chi_q^2\\) under \\(H_0\\). As a result, the Wald test with size \\(\\alpha\\) takes the form \\[\\delta(\\mathbf{X}) = \\begin{cases} \\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}& W < (\\chi_q^2)^{-1}(1-\\alpha)\\\\\n\\mathbf h(\\boldsymbol{\\theta}) \\neq \\mathbf{0}& W \\ge (\\chi_q^2)^{-1}(1-\\alpha)\\end{cases}.\\]\n\n\nProof. We are given that \\(\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) \\overset{d}{\\to}N(\\mathbf{0}, \\mathbf V)\\). Under assumptions 2 and 4, we can apply the delta method as follows: \\[ \\sqrt{n}(\\mathbf h(\\hat{\\boldsymbol{\\theta}}) - \\mathbf h(\\boldsymbol{\\theta})) \\overset{d}{\\to}N\\left(\\mathbf{0}, \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right).\\] We are assuming \\(H_0\\) holds, so \\(\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\), giving \\[\\begin{align*}\n&\\sqrt{n}(\\mathbf h(\\hat{\\boldsymbol{\\theta}}) - \\mathbf h(\\boldsymbol{\\theta})) \\overset{d}{\\to}N\\left(\\mathbf{0}, \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right)\\\\\n\\implies & \\sqrt{n}\\mathbf h(\\hat{\\boldsymbol{\\theta}}) \\overset{d}{\\to}N\\left(\\mathbf{0}, \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right)\\\\\n\\implies & \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1/2}\\sqrt{n}\\mathbf h(\\hat{\\boldsymbol{\\theta}}) \\overset{d}{\\to}N(\\mathbf{0}, \\mathbf I)\n\\end{align*}\\] Using the previous lemma we have \\[\\begin{align*}\n&\\sqrt{n}\\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1/2}\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1/2}\\sqrt{n}\\mathbf h(\\hat{\\boldsymbol{\\theta}})\\overset{d}{\\to}\\chi_q^2\\\\\n\\implies &\nn\\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol{\\theta}})\\overset{d}{\\to}\\chi_q^2\\\\\n\\implies & \\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})(\\mathbf V/n)\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol{\\theta}})\\overset{d}{\\to}\\chi_q^2\n\\end{align*}\\] If we replace \\(\\mathbf V\\) with our estimator \\(\\hat {\\mathbf V}\\) which satisfies \\(\\hat {\\mathbf V}\\overset{p}{\\to}\\mathbf V\\), then we can apply Slutky’s theorem and conclude \\[\\mathbf h(\\hat{\\boldsymbol{\\theta}})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})(\\hat {\\mathbf V}/n)\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol{\\theta}})\\overset{d}{\\to}\\chi_q^2\\]\n\nThe intuition behind the Wald test is identical to that of the \\(z-\\)test, the \\(t\\)-test, and many other familiar tests – we know the distribution of \\(W(\\mathbf{X})\\) under \\(H_0\\), so if we observe a value of \\(W(\\mathbf{x})\\) that is very unlikely then we should reject \\(H_0\\).\n\nExample 3.15 We can write a simple function which implements the Wald test.\n\nWald_test <- function(alpha, h, X, theta_hat, V_hat){\n  #Calculate test stat\n  n <- nrow(X)\n  h_prime <- jacobian(h, theta_hat)\n  W <- t(h(theta_hat)) %*% solve(h_prime %*% (V_hat/n) %*% t(h_prime)) %*% h(theta_hat)\n  \n  #determine if we reject\n  dof <- length(h(theta_hat))\n  c <- qchisq(1 - alpha, dof)\n  decision <- (W >= c)\n  \n  #Output information\n  return_list <- c(\"Statistic\" = W, \n                   \"Critical Value\" = c, \n                   \"Reject Null Hypothesis\" = decision)\n  return(return_list)\n}\n\nIn order to calculate \\(\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\), we made use of the jacobian() function from the numDeriv package instead of requiring an additional argument where we supply the calculated Jacobian. Let’s put our function to use. Suppose that \\(\\mathbf{X}\\overset{iid}{\\sim}N(\\boldsymbol\\mu, \\boldsymbol\\Sigma)\\) where \\[\\begin{align*}\n\\boldsymbol\\mu &= \\begin{bmatrix}2&2&2\\end{bmatrix}',\\\\\n\\boldsymbol\\Sigma&= \\begin{bmatrix}1&0.4&0.2\\\\0.4& 2& 0.1\\\\0.2&0.1&1 \\end{bmatrix}.\n\\end{align*}\\] The sample mean satisfies \\(\\sqrt{n}(\\bar {\\mathbf{X}} - \\boldsymbol\\mu)\\overset{d}{\\to}N(\\mathbf{0}, \\mathbf V)\\) trivially, as \\(\\sqrt{n}(\\bar {\\mathbf{X}} - \\boldsymbol\\mu)\\sim N(\\mathbf{0}, \\boldsymbol\\Sigma)\\). We can use the Wald test to test the hypothesis that \\(\\boldsymbol\\mu = (2,2,2)\\) with \\(\\alpha = 0.05\\) and \\(n = 100\\). In this case we have \\[ \\mathbf h(\\boldsymbol\\mu) = \\begin{bmatrix} \\mu_1 - 2\\\\ \\mu_2 -2\\\\\\mu_3-2\\end{bmatrix}.\\] We do need a consistent estimate of \\(\\mathbf V = \\boldsymbol \\Sigma\\), but the most intuitive candidate will do. The sample covariance, calculated using cov(), of our observed data is a consistent estimator of \\(\\boldsymbol \\Sigma\\) just as the sample variance is a consistent estimator of the actual variance.\n\nN_sim <- 10000\nmu0 <- c(2,2,2)\nn <- 100\nSigma <- matrix(c(1,.4,.2,.4,2,.1,.2,.1,1), nrow = 3, ncol = 3)\nalpha <- 0.05\n\n#Define hypothesis\nh <- function(t){\n  c(t[1], t[2], t[3]) - mu0\n}\n\n#Create vectors to store results\ndecisions <- vector(\"numeric\", N_sim)\ntest_stats <- vector(\"numeric\", N_sim) \n\nfor (k in 1:N_sim) {\n  #Draw random sample where H0 is true,\n  X <- rmvnorm(n, mu0, Sigma)\n  #Estimate mean and covariance\n  theta_hat <- colMeans(X)\n  V_hat <- cov(X)\n  #Perform test and store results\n  test_results <- Wald_test(alpha, h, X, theta_hat, V_hat)\n  decisions[k] <- test_results[3]\n  test_stats[k] <- test_results[1]\n}\n\n#Should be ≈0.05\nmean(decisions)\n\n[1] 0.0587\n\n\nIf we plot the test statistics from our 10,000 simulations, we can illustrate that \\(W\\overset{a}{\\sim}\\chi_3^2\\).\n\n\nShow code which generates figure\ntibble(x = test_stats) %>% \n  ggplot(aes(x)) +\n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 50) +\n  xlab(\"Simulated Wald Test Statistics\") +\n  theme_minimal() +\n  stat_function(fun = dchisq, args = list(df = 3), color = \"red\") +\n  xlim(0,15)\n\n\n\n\n\nFigure 3.17: The hisotrgram of simulated wald test statistics (calculated using a sample size of 100) coincides with the asymptotic distribution given in Theorem 3.2\n\n\n\n\n\nIf we are only testing \\(\\theta_j = \\theta_0\\) for one component of \\(\\boldsymbol{\\theta}\\), then we can use a general version of the \\(t-\\)test, which is a modified Wald test.\n\nDefinition 3.14 Given the hypotheses \\(H_0:\\theta_j = \\theta_{0}\\) versus \\(H_1: \\theta_j \\neq \\theta_{0}\\), the \\(t-\\)statistic is defined as \\[ t = \\frac{\\hat\\theta_j-\\theta_0}{\\widehat{\\text{se}}(\\hat\\theta_j)} = [W(\\mathbf{X})]^{1/2}.\\] Assuming @ref(exr:assone) holds, the \\(t-\\)test with size \\(\\alpha\\) takes the form \\[\\delta(\\mathbf{X}) = \\begin{cases} \\theta_j = \\theta_0 & \\left\\lvert t\\right\\rvert \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\\n\\theta_j \\neq \\theta_0 & \\left\\lvert t\\right\\rvert < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases}.\\]\n\nConsidering the \\(t-\\)test in the broader context of the Wald test highlights an important distinction – there is a difference between the one hypothesis \\(\\boldsymbol{\\theta}= \\boldsymbol{\\theta}_0\\), and the separate hypotheses \\(\\theta_j = \\theta_{0,j}\\) for \\(j=1,\\ldots,k\\). The prior requires that \\(\\theta_j = \\theta_{0,j}\\) for all \\(j\\) simultaneously, while the latter hypotheses are completely independent. We can highlight this by comparing the \\(t-\\)test and the Wald test.\n\nExample 3.16 (t-test versus Wald test) We will start by writing a function which performs the \\(t-\\)test just like we did with the Wald test.\n\nt_test <- function(alpha, theta0, X, theta_hat, se_hat){\n  #calculate test stat\n  n <- nrow(X)\n  t <- (theta_hat - theta0)/se_hat\n  \n  #determine if we reject\n  dof <- n - 1\n  c <- qt(1 - alpha/2, dof)\n  decision <- (abs(t) >= c)\n  decision\n  #Output information\n  return_list <- c(\"Statistic\" = t, \n                   \"Critical Value\" = c, \n                   \"Reject Null Hypothesis\" = decision\n                   )\n  return(return_list)\n}\n\nAs defined, the function is able to perform multiple \\(t-\\)tests simultaneously. If we supply a matrix for X along with vectors for theta0, theta_hat, and se_hat, thent will be a vector and decision will record whether the components of t exceed the critical value.\nTo compare t_test() and Wald_test(), suppose \\(\\mathbf{X}\\overset{iid}{\\sim}N(\\boldsymbol\\mu, \\boldsymbol\\Sigma)\\) for \\[\\begin{align*}\n\\boldsymbol\\mu &= \\begin{bmatrix}0&0.5\\end{bmatrix}',\\\\\n\\boldsymbol\\Sigma&= \\begin{bmatrix}1&0\\\\0&2 \\end{bmatrix},\n\\end{align*}\\] and \\(n=2\\). We will test \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) with Wald_test() and the separate hypotheses \\(H_0:\\mu_1 = 0\\) (which is true) and \\(H_0:\\mu_2 = 0\\), all with \\(\\alpha = 0.05\\). We should reject \\(\\mu_1 = 0\\) with an approximate probability \\(\\alpha\\).\n\nN_sim <- 10000\nmu0 <- c(0,0)\nmu <- c(0,0.5)\nSigma <- matrix(c(1,0,0,2), nrow = 2, ncol = 2)\nalpha <- 0.05\nn <- 100\nh <- function(t){\n  c(t[1], t[2]) - mu0\n}\n\nt_decision_1 <- vector(\"numeric\", N_sim)\nt_decision_2 <- vector(\"numeric\", N_sim)\nwald_decision <- vector(\"numeric\", N_sim)\nfor (j in 1:N_sim) {\n  X <- rmvnorm(n, mu, Sigma)\n  se_hat <- sqrt(diag(var(X)/n))\n  t_results <-  t_test(alpha, mu0, X, colMeans(X), se_hat)\n  wald_results <- Wald_test(alpha, h, X, colMeans(X), cov(X))\n  \n  t_decision_1[j] <- t_results[4]\n  t_decision_2[j] <- t_results[5]\n  wald_decision[j] <- wald_results[3]\n}\n\nc(mean(t_decision_1), mean(t_decision_2), mean(wald_decision))\n\n[1] 0.0523 0.9413 0.8953\n\n\nWe end up rejecting the null hypothesis \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) more than the null hypothesis \\(H_0:\\mu_2 = 0\\), as the Wald test also must incorporate evidence about \\(\\mu_1\\) in the form of \\(\\bar X_1\\). A large value of \\(\\bar X_2\\) (which is sufficient to reject \\(H_0:\\mu_2 = 0\\) with the \\(t-\\)test), is only one part of the story for the Wald test, as it also must consider \\(\\bar X_1\\). That being said, \\(\\bar X_2\\) is usually so large that the Wald test will still reject \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) even if \\(\\bar X_1\\) is not large enough for the \\(t-\\)test to reject \\(H_0:\\mu_1= 0\\). To see this, we can breakdown the simulated probabilities that we reject \\(H_0:\\mu_1= 0\\) and/or \\(H_0:\\mu_2 = 0\\).\n\n\nShow code which generates figure\ntest <- matrix(c(\n  mean(t_decision_1*t_decision_2),\n  mean(t_decision_1*(1-t_decision_2)),\n  mean(t_decision_2*(1-t_decision_1)),\n  mean((1-t_decision_1)*(1-t_decision_2))\n), nrow = 2)\n\ncolnames(test) <- c(\"Reject μ2 = 0\", \"Fail to Reject μ2 = 0\")\nrownames(test) <- c(\"Reject μ1 = 0\", \"Fail to Reject μ1 = 0\")\nprint(test)\n\n\n                      Reject μ2 = 0 Fail to Reject μ2 = 0\nReject μ1 = 0                0.0496                0.8917\nFail to Reject μ1 = 0        0.0027                0.0560\n\n\nWe fail to reject \\(H_0:\\mu_1= 0\\) and \\(H_0:\\mu_2 = 0\\) (simultaneously) with an approximate probability of \\(0.047\\), far less than that calculated using the Wald test. Why does this happen? For our example, we reject \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) if \\(W \\ge (\\chi_2^2)^{-1}(1-0.5) \\approx 6\\). We reject \\(H_0:\\mu_1 = 0\\) if \\(|t_1| \\ge 1.98\\) and \\(H_0:\\mu_2 = 0\\) if \\(|t_2| \\ge 1.98\\). In terms of \\(\\bar X\\), we reject \\(H_0:\\mu_1 = 0\\) if \\[\\left\\lvert\\bar X_1\\right\\rvert \\ge 0 + 1.98\\cdot\\widehat{\\text{se}}(\\bar X_1) \\approx 1.98\\cdot\\text{se}(\\bar X_1) = 1.98\\cdot(1/\\sqrt{ 100}) = 0.198,\\] and we reject \\(H_0:\\mu_2 = 0\\) if \\[\\left\\lvert\\bar X_2\\right\\rvert \\ge 0 + 1.98\\cdot\\widehat{\\text{se}}(\\bar X_2) \\approx 1.98\\cdot\\text{se}(\\bar X_1) = 1.98\\cdot(2/\\sqrt{ 100}) \\approx 0.28.\\] Because the Wald statistic is a function of \\(\\bar X_1\\) and \\(\\bar X_2\\), we can plot the values of \\(W\\) over values of \\(\\{\\bar X_1,\\bar X_2\\}\\) and compare this value to the rejection regions of the \\(t-\\)tests.\n\n\nShow code which generates figure\nexpand_grid(\n  x = (-40:40)/100,\n  y = (-40:40)/100\n) %>% \n  rowwise() %>% \n  mutate(\n    W = Wald_test(alpha, h, X, c(x,y), Sigma)[1],\n    reject = (W > 5.991465)\n  ) %>% \n  ggplot(aes(x,y, fill = W)) +\n    geom_tile() +\n    scale_fill_gradient2(low = \"red\", mid = \"white\", high = \"green\", midpoint = 5.991465) +\n    labs(x = \"Estimate of μ1\", y = \"Estimate of μ2\", fill = \"Wald Statistic\") +\n    theme_minimal() +\n    geom_vline(xintercept = c(-.198,.198), linetype = \"dashed\", size = 0.1) +\n    geom_hline(yintercept = c(-(sqrt(2)/10)*1.98,(sqrt(2)/10)*1.98), linetype = \"dashed\", size = 0.1) +\n    theme(legend.position = \"bottom\") +\n    annotate(\"text\", x = 0, y = 0, label = \"Don't Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = -.3, y = 0, label = \"Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = .3, y = 0, label = \"Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = 0, y = .35, label = \"Don't Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = 0, y = -.35, label = \"Don't Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = -.3, y = -.35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = .3, y = .35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = -.3, y = .35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n    annotate(\"text\", x = .3, y = -.35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5)\n\n\n\n\n\nFigure 3.18: The values of the Wald test statistic in relation to the decisions seperate t-tests make regarding the null hypotheses when considering each in one dimmension\n\n\n\n\nThis illustrates that it’s possible to reject \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) using the Wald test while not rejecting \\(H_0:\\mu_1 = 0\\) or not rejecting \\(H_1:\\mu_1 = 0\\). On the other hand, if we reject \\(H_0:\\mu_1 = 0\\) and \\(H_1:\\mu_1 = 0\\) using separate \\(t-\\)tests, we are guaranteed to reject \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) using the Wald test. This plot omits an important consideration – the true distribution of \\(\\{\\bar X_1,\\bar X_2\\}\\) which will determine how frequently our estimates fall in the various rejection regions.\n\n\nShow code which generates figure\ndf <- expand_grid(\n  x = (-100:100)/100,\n  y = (-40:100)/100\n  ) %>%\n  rowwise() %>%\n  mutate(\n    W = Wald_test(alpha, h, X, c(x,y), Sigma)[1],\n    reject = ifelse(W > 5.991465, \"Wald Test Rejects μ = 0\", \"Wald Test Does Not Reject μ = 0\"),\n    density = dmvnorm(c(x,y), mu, sqrt(Sigma)/10)\n  )\n\ndf %>% \n  ggplot(aes(x,y, fill = density)) +\n  geom_tile() + \n  labs(x = \"Estimate of μ1\", y = \"Estimate of μ2\", fill = \"Density\") +\n  theme_minimal() +\n  geom_vline(xintercept = c(-.198,.198), linetype = \"dashed\", size = 0.2, color = \"red\") +\n  geom_hline(yintercept = c(-(sqrt(2)/10)*1.98,(sqrt(2)/10)*1.98), linetype = \"dashed\", size = 0.2, color = \"red\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  facet_wrap(~reject)\n\n\n\n\n\nFigure 3.19: The joint density of the sample means\n\n\n\n\nThe majority of the density is concentrated in a region where the Wald test rejects \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\), the \\(t-\\)test does not reject \\(H_0:\\mu_1 = 0\\), and the \\(t-\\)test rejects \\(H_0:\\mu_2 = 0\\). This is why our Wald test rejected \\(H_0:\\boldsymbol\\mu = \\mathbf{0}\\) so often, despite \\(\\mu_1 = 0\\) being true. Finally, note that we’ve assumed \\(\\text{Cov}\\left(X_1,X_2\\right) = 0\\). In the event that \\(\\text{Cov}\\left(X_1,X_2\\right) \\neq 0\\), things become even more interesting, as the Wald test considers this covariance, whereas the \\(t-\\)tests do not. If you play around with this example by changing \\(\\boldsymbol\\Sigma =\\text{Var}\\left(\\mathbf{X}\\right)\\), the elliptical rejection region of the Wald test will rotate/shrink/expand, thereby affecting how its results relate to those of the separate \\(t-\\)tests.\n\n\n\n\n\n\n\nBickel, Peter J, and Kjell A Doksum. 2015. Mathematical Statistics: Basic Ideas and Selected Topics, Volume i. 2nd ed. CRC Press.\n\n\nCasella, George, and Roger L Berger. 2021. Statistical Inference. Cengage Learning.\n\n\nDeGroot, Morris H, and Mark J Schervish. 2012. Probability and Statistics. Pearson Education.\n\n\nLehmann. 1993. “The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?” Journal of the American Statistical Association 88 (424): 1242–49.\n\n\n———. 2011. Fisher, Neyman, and the Creation of Classical Statistics. Springer Science & Business Media.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of the Most Efficient Tests of Statistical Hypotheses.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 231 (694-706): 289–337.\n\n\nRomano, Joseph P, and EL Lehmann. 2005. Testing Statistical Hypotheses. Vol. 3. Springer.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large.” Transactions of the American Mathematical Society 54 (3): 426–82.\n\n\nWolak, Frank A. 1989. “Local and Global Testing of Linear and Nonlinear Inequality Constraints in Nonlinear Econometric Models.” Econometric Theory 5 (1): 1–35."
  },
  {
    "objectID": "exp_fam.html",
    "href": "exp_fam.html",
    "title": "4  Exponential Families",
    "section": "",
    "text": "This section is not essential to understanding econometrics, but it will provide some neat context later on. Many problems in statistics become much simpler if are data is generated from a special type of distribution. We already saw this in Chapter 3 when talking about UMP tests and the MLR property. It turns out there is a general class of probability distributions which not only satisfy the MLR property, but also satisfy many other convenient properties which will make our lives easier later on. This section will introduce this class of distributions which are known as the exponential family of distributions. Right from the get go, a rather large disclaimer is in order. The only time we will be able to reap the benefits of distributions in the exponential family is when we assume that our model \\(\\mathcal P\\) is a parametric model which is also regular (each element \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) corresponds to a unique distribution \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta})\\)).1 Many common models in econometrics will not fall into this category as it requires specifying a distribution of some unobservable quantity. This is why exponential families are usually emphasized more in the setting of statistics, particularly when covering standard/classical topics."
  },
  {
    "objectID": "exp_fam.html#motivation-sufficiency",
    "href": "exp_fam.html#motivation-sufficiency",
    "title": "4  Exponential Families",
    "section": "4.1 Motivation – Sufficiency",
    "text": "4.1 Motivation – Sufficiency\nIn a sense, a statistic \\(T:\\mathcal X \\to\\mathcal T\\) (where \\(\\mathcal T\\) is almost always \\(\\mathbb R^k\\)) is a way of boiling down information about a sample \\(\\mathbf{x}\\) to a single value \\(T(\\mathbf{x})\\), where \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\) for some regular parametric model \\(\\mathcal P\\). We then use the information “captured” in the statistic \\(T(\\mathbf{x})\\) to estimate \\(\\boldsymbol{\\theta}\\). Once we have the value \\(T(\\mathbf{x})\\), does the particular value of \\(\\mathbf{x}\\) matter? Is it possible that we can just ignore \\(\\mathbf{x}\\) if \\(T(\\mathbf{x})\\) encompasses all the relevant information provided by our sample? Here are two examples that show that this may, or may not, be the case.\n\nExample 4.1 Suppose \\(X_i \\overset{iid}{\\sim}\\text{Ber}(p)\\). If we want to estimate \\(p\\), the natural estimator is \\(\\hat p(\\mathbf{X}) = \\sum_{i=1}^n X_i / n\\) when we observe a sample \\(\\mathbf{X}= (X_1,\\ldots, X_n)\\). Let’s just focus on the part of this function that depends on \\(\\mathbf{X}\\), that being the statistic \\(T(\\mathbf{X}) = \\sum_{i=1}^n X_i\\) which records the number of successes over \\(n\\) Bernoulli trials. We can think of our estimation process as follows: we observe \\(\\mathbf{x}\\), we calculate \\(T(\\mathbf{x})\\), we calculate our estimate \\(\\hat p\\) using only the value \\(T(\\mathbf{x})\\). Do we lose anything by only using \\(T(x)\\)? If this were the case, then what would that look like mathematically? For some insight, let’s look at the distribution of our sample \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid p)\\). If \\(f_X(x_i\\mid p)\\) is the distribution associated with \\(\\text{Ber}(p)\\), then \\[f_{\\mathbf{X}}(\\mathbf{x}\\mid p) = \\prod_{i=1}^nf_X(x_i\\mid p) = \\prod_{i=1}^np^{x_1}(1-p)^{1-x_i} = p^{\\sum_{i=1}^n x_i}(1-p)^{n-\\sum_{i=1}^n x_i} = p^{T(\\mathbf{x})}(1-p)^{n-T(\\mathbf{x})}.\\] Substituting \\(T(\\mathbf{x})\\) into this distribution illuminates a striking feature – \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid p)\\) depends on \\(\\mathbf{x}\\) only through the statistic \\(T(\\mathbf{x})\\). In other words, if we are estimating \\(\\hat p\\) (or rather estimating \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid p)\\) vicariously through \\(\\hat p\\)), then we only need to know the value of our statistic \\(T(\\mathbf{x})\\). The statistic contains all the relevant information needed for estimation. Another way to think about this is with an experiment. Suppose an unfair coin lands on heads with probability \\(p\\), and \\(\\mathbf{X}= (X_1,\\ldots X_n)\\) records the number of heads observed (\\(X_i = 1\\) means the \\(i\\)th flip is heads). If you are going to estimate \\(p\\) with \\(\\hat p(\\mathbf{X}) = \\sum_{i=1}^n X_i / n\\), does it matter the order in which the coins landed on heads? If \\(n = 2\\), is there a difference between observing \\(\\mathbf{x}= (1,0)\\) and \\(\\mathbf{x}' = (0,1)\\)? No – the only information you really care about is the fact that the coin landed on heads once, i.e \\(T(\\mathbf{x}) = T(\\mathbf{x}') = 1\\). In an effort to beat a dead horse, let \\(n = 10\\), and \\(T(\\mathbf{x}) = 7\\) (the coin lands on heads 7 times). There are \\(3,628,800\\) possible permutations where we get \\(7\\) heads, all of which will give the same estimate of \\(\\hat p(\\mathbf{x}) = 0.7\\).\n\nn <- 10\nx <- c(rep(1, 7), rep(0, n - 7))\nsamples <- permn(x)\n\n#write estimator s.t we specify which sample we want to use from the permutations\np_hat <- function(i){\n  x <- samples[[i]]\n  sum(x)/n\n}\nestimates <- sapply(1:length(samples), p_hat)\n\n#What % of our estimates are 0.7?\nmean(estimates == 0.7)\n\n[1] 1\n\n\n\n\nExample 4.2 Suppose \\(X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\) for a known \\(\\sigma^2\\). We could estimate using the median \\(\\hat \\mu(\\mathbf{X})\\), \\[\\hat \\mu(\\mathbf{X}) = \\begin{cases}X_{(n+1)/2} & n\\text{ odd}\\\\ \\frac{X_{n/2} + X_{n/2 + 1}}{2} & n\\text{ even} \\end{cases}.\\] In this case, is \\(\\hat \\mu(\\mathbf{X})\\) calculated via a statistic which encapsulates all the relevant information about \\(\\mathbf{x}\\)? Heuristically, it seems like this is not the case. The only real statistic that \\(\\hat\\mu(\\mathbf{X})\\) is a function of is itself, so let \\(T(\\mathbf{X}) = \\hat\\mu(\\mathbf{X})\\). Suppose we observe \\(\\mathbf{x}= (-100, 0, 0.1)\\) and \\(\\mathbf{x}' = (-0.1,0, 100)\\). For both of these observations we have \\(T(\\mathbf{X}) = \\hat \\mu(\\mathbf{x}) = 0\\), but discarding \\(\\mathbf{x}'\\) and \\(\\mathbf{x}\\) doesn’t seem like the best idea here, because they have drastically different sample means, numbers that seem especially relevant here. Admittedly, this argument lacks the rigor of the previous one, but it hopefully illustrates that not all estimators can be calculated with some magic statistic that perfectly captures an observation \\(\\mathbf{x}\\).\n\nTo formalize this property of optimal data reduction, we will consider the distribution \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta})\\) like we did in the first example. The definition is due to Fisher (1922).\n\nDefinition 4.1 Let \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\) for some \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\), where \\(\\mathcal P\\) is a regular parametric model. A statistic \\(T(\\mathbf{X})\\) is sufficient for \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) (or for \\(\\boldsymbol{\\theta}\\)), if \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid T(\\mathbf{x}), \\boldsymbol{\\theta})\\) is not a function of \\(\\boldsymbol{\\theta}\\).\n\nSufficiency can seem a little abstract at first, but there are a few different ways to think about it that may make it a bit clearer. One way was already highlighted by the Bernoulli trials example. Sufficiency means that if we have \\(T(\\mathbf{x}) = T(\\mathbf{x}')\\) for any two observed samples \\(\\mathbf{x},\\mathbf{x}' \\in \\mathcal X\\), then \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\) provide us the same amount of information about \\(\\boldsymbol{\\theta}\\). Another way of thinking about sufficiency is via a thought experiment involving two statisticians. Suppose Statistician A and Statistician B want to estimate \\(\\boldsymbol{\\theta}\\). Statistician A has access to some random sample \\(\\mathbf{x}\\), while Statistician B only knows \\(T(\\mathbf{x})\\). If \\(T\\) is a sufficient statistic, then Statistician B is at no disadvantage because he can generate his own random sample! He may not know \\(\\boldsymbol{\\theta}\\), but he knows \\(T(\\mathbf{x})\\), and \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid T(\\mathbf{x}), \\boldsymbol{\\theta})\\) does not depend on \\(\\boldsymbol{\\theta}\\), so he can just simulate a random sample from \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid T(\\mathbf{x}))\\).\nUsing the definition of sufficiency to verify a statistic has the property can be a bit cumbersome, so we usually do so using a famous theorem.\n\nTheorem 4.1 (Fisher–Neyman factorization theorem) Let \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\) for some \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\), where \\(\\mathcal P\\) is a regular parametric model. The statistic \\(T(\\mathbf{X})\\) is sufficient if and only if there exist non-negative functions \\(g:\\mathcal T\\times \\Theta\\to\\mathbb R\\) and \\(h:\\mathcal X\\to \\mathbb R\\) such that \\[f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}) = g(T(\\mathbf{x}) \\mid \\boldsymbol{\\theta})h(\\mathbf{x}).\\]\n\n\nProof. space\n\\((\\Longrightarrow)\\) Suppose \\(T\\) is a sufficient statistic. The statistic \\(T\\) is a function of \\(\\mathbf{x}\\), so the joint density of \\((\\mathbf{X}, T(\\mathbf{X})\\) is simply the density of \\(\\mathbf{X}\\). \\[f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}) = f_{\\mathbf{X}, T(\\mathbf{X})}(\\mathbf{x}, T(\\mathbf{x})\\mid \\boldsymbol{\\theta})\\] BY properties of conditional variables, \\[ f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta}) = f_{\\mathbf{X}, T(\\mathbf{X})}(\\mathbf{x}, T(\\mathbf{x})\\mid \\boldsymbol{\\theta}) = \\underbrace{f_{X\\mid T(\\mathbf{X})}(\\mathbf{x}\\mid T(\\mathbf{x}), \\boldsymbol{\\theta})}_{h(\\mathbf{x})}\\underbrace{f_{T(\\mathbf{X})}(T(\\mathbf{x})\\mid\\boldsymbol{\\theta})}_{g(T(\\mathbf{x}) \\mid \\boldsymbol{\\theta})}.\\] We know that \\(f_{X\\mid T(\\mathbf{X})}(\\mathbf{x}\\mid T(\\mathbf{x}), \\boldsymbol{\\theta})\\) is a suitable candidate for \\(h(\\mathbf{x})\\), because \\(T\\) is sufficient, so \\(f_{X\\mid T(\\mathbf{X})}(\\mathbf{x}\\mid T(\\mathbf{x}), \\boldsymbol{\\theta})\\) is not a function of \\(\\boldsymbol{\\theta}\\).\n\\((\\Longleftarrow)\\) Suppose \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}) = g(T(\\mathbf{x}) \\mid \\boldsymbol{\\theta})h(\\mathbf{x})\\), and fix \\(T(\\mathbf{x}) = t\\). By the definition of conditional expectation, \\[\\begin{align*}\nf_{\\mathbf{X}\\mid T(\\mathbf{X})}(\\mathbf{x}\\mid t, \\boldsymbol{\\theta}) &= \\frac{f_{X, T(\\mathbf{X})}(\\mathbf{x},t\\mid\\boldsymbol{\\theta})}{f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta})}\\\\\n& = \\frac{f_{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta})} &(T(\\mathbf{x})\\text{ function of }\\mathbf{x})\\\\\n& = \\frac{g(t \\mid \\boldsymbol{\\theta})h(\\mathbf{x})}{f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta})} &(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta}) = g(t \\mid \\boldsymbol{\\theta})h(\\mathbf{x}))\\\\\n& = \\frac{g(t \\mid \\boldsymbol{\\theta})h(\\mathbf{x})}{f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta})}\n\\end{align*}\\] We can write the denominator in terms of \\(f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta})\\) by integrating \\(f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\) over all \\(\\mathbf{x}\\in \\mathcal X\\) such that \\(T(\\mathbf{x}) = t\\) for some fixed \\(t\\): \\[f_{T(\\mathbf{x})}(t\\mid \\boldsymbol{\\theta}) = \\int_{\\{\\mathbf{x}\\mid T(\\mathbf{x}) = t\\}} f_\\mathbf{X}(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\ d\\mathbf{x}= \\int_{\\{\\mathbf{x}\\mid T(\\mathbf{x}) = t\\}} g(t\\mid \\boldsymbol{\\theta})h(\\mathbf{x})\\ d\\mathbf{x}=  g(t\\mid \\boldsymbol{\\theta})\\int_{\\{\\mathbf{x}\\mid T(\\mathbf{x}) = t\\}}h(\\mathbf{x})\\ d\\mathbf{x}\\] Therefore, \\[ f_{\\mathbf{X}\\mid T(\\mathbf{X})}(\\mathbf{x}\\mid t, \\boldsymbol{\\theta}) = \\frac{g(t \\mid \\boldsymbol{\\theta})h(\\mathbf{x})}{g(t\\mid \\boldsymbol{\\theta})\\int_{\\{\\mathbf{x}\\mid T(\\mathbf{x}) = t\\}}h(\\mathbf{x})\\ d\\mathbf{x}} = \\frac{h(\\mathbf{x})}{\\int_{\\{\\mathbf{x}\\mid T(\\mathbf{x}) = t\\}}h(\\mathbf{x})\\ d\\mathbf{x}},\\] which is not a function of \\(\\boldsymbol{\\theta}\\). This makes \\(T(\\mathbf{x})\\) a sufficient statistic.\n\n\nExample 4.3 Suppose \\(X_i \\overset{iid}{\\sim}N(\\mu, \\sigma^2)\\) for a known \\(\\sigma^2\\). After some calculation, we can conclude \\[\\begin{align*}\nf_\\mathbf{X}(\\mathbf{x}\\mid \\mu)& = \\prod_{i=1}^n f_X(x_i \\mid \\mu )\\\\\n& =  \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp \\left[-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]\\\\\n& = (2\\pi\\sigma^2)^{-n/2}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\bar x)^2\\right] \\exp\\left[-\\frac{n}{2\\sigma^2}(\\mu - \\bar x)^2\\right]\\\\\n& =  \\underbrace{(2\\pi\\sigma^2)^{-n/2}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i - \\bar x)^2\\right]}_{h(\\mathbf{x})} \\underbrace{\\exp\\left[-\\frac{n}{2\\sigma^2}(\\mu -  T(\\mathbf{x}))^2\\right]}_{g(T(\\mathbf{x})\\mid \\mu)} & (T(\\mathbf{x}) = \\bar x),\n\\end{align*}\\] so \\(T(\\mathbf{X}) = \\bar X\\) is a sufficient statistic for \\(\\mu\\)."
  },
  {
    "objectID": "exp_fam.html#exponential-families",
    "href": "exp_fam.html#exponential-families",
    "title": "4  Exponential Families",
    "section": "4.2 Exponential Families",
    "text": "4.2 Exponential Families\nWhether \\(T(\\mathbf{X})\\) is sufficient for \\(P_\\boldsymbol{\\theta}\\) depends entirely on the distribution \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid \\boldsymbol{\\theta})\\). Is it possible to describe the entire class of distributions which admit a sufficient statistic? Do they all take a similar form? As proved independently by Pitman (1936), Koopman (1936), and Darmois (1935), the answer is yes (sort-of)! We will broaden our view a bit by considering a vector of statistics \\(\\mathbf T(\\mathbf{X})\\).\n\nTheorem 4.2 (Pitman–Koopman–Darmois theorem) Suppose \\(X_i \\overset{iid}{\\sim}P_\\boldsymbol{\\theta}\\) where the support of \\(f_{\\mathbf{X}}(x_i\\mid\\boldsymbol{\\theta})\\) does not depend on \\(\\boldsymbol{\\theta}\\). There exists a sufficient statistic \\(\\mathbf T:\\mathcal X\\to \\mathbb R^k\\) such that \\(k\\) is fixed for all sample sizes \\(n\\) if and only if \\(f_{\\mathbf{X}}\\) can be written as \\[f_{\\mathbf{X}}(\\mathbf{x}\\mid\\boldsymbol{\\theta})= h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})]\\] for functions \\(h:\\mathcal X \\to \\mathbb R\\), \\(\\boldsymbol \\eta:\\boldsymbol \\Theta \\to \\mathbb R^k\\), \\(A:\\mathcal X \\to \\mathbb R\\).\n\nProving the sufficiency of this condition is a direct application of Theorem Theorem 4.1, as we can just let \\(g(\\mathbf T(\\mathbf{x}) \\mid \\boldsymbol{\\theta}) = \\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})]\\). Proving this is a necessary condition is a bit more complicated and relies on the assumptions that the dimension of \\(\\mathbf T\\) is fixed, and that the support of \\(f_{\\mathbf{X}}(x_i\\mid\\boldsymbol{\\theta})\\) is independent of \\(\\boldsymbol{\\theta}\\). The second condition should seem familiar, as it plays a crucial role in proving the Cramér–Rao lower bound holds (see Chapter 1). The distributions given by the Pitman–Koopman–Darmois theorem merit their own definition.\n\nDefinition 4.2 A regular parametric model \\(\\mathcal P\\) is an exponential family if \\[f_{\\mathbf{X}}(\\mathbf{x}\\mid\\boldsymbol{\\theta})= h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})].\\] We refer to \\(\\mathbf T(\\mathbf{x})\\) as the sufficient statistic, \\(\\boldsymbol\\eta(\\boldsymbol{\\theta})\\) as the natural parameter, and \\(A(\\boldsymbol{\\eta})\\) as the cumulant function. In the event \\(\\boldsymbol\\eta(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}\\), the exponential family is in canonical form. If \\(\\boldsymbol\\eta(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}\\) and \\(\\mathbf T(\\mathbf{x}) = \\mathbf{x}\\), we say our model is a natural exponential family.\n\nSometimes, people will opt to write the cumulant function in terms of the parameter \\(\\boldsymbol{\\theta}\\), which is completely fine.\n\nExample 4.4 If \\(X \\overset{iid}{\\sim}\\text{Ber}(p)\\) where \\(n=1\\), then \\(T(\\mathbf{X}) = \\sum_{i=1}^n X_i = X\\) is a sufficient statistic for \\(p\\), so \\(f_{X}(x\\mid p)\\) is an exponential family by the Pitman–Koopman–Darmois theorem. \\[\\begin{align*}\nf_{X}(x\\mid p) & = p^x(1-p)^{1-x}\\\\\n& = \\exp[\\log(p^x(1-p)^{1-x})]\\\\\n& = \\exp[x\\log(p) + (1-x)\\log(1-p))]\\\\\n& = \\exp[x(\\log(p) - \\log(1-p)) + \\log(1-p)]\\\\\n& = 1\\cdot \\exp\\left[x\\log\\left(\\frac{p}{1-p}\\right) + \\log(1-p)\\right]\\\\\nh(x) & = 1\\\\\nT(x) & = x\\\\\n\\eta(p) & = \\log\\left(\\frac{p}{1-p}\\right)\\\\\nA(\\eta)& = -\\log(1-p)\\\\\n& =  \\log\\left(1 + \\frac{p}{1-p}\\right)\\\\\n& = \\log\\left[1 + \\exp\\left[\\log\\left(\\frac{p}{1-p}\\right)\\right]\\right]\\\\\n& = \\log[1 + \\exp(\\eta)]\n\\end{align*}\\]\n\n\nExample 4.5 For \\(X\\sim N(\\mu,\\sigma^2)\\) where both \\(\\mu\\) and \\(\\sigma^2\\) are unknown,\n\\[\\begin{align*}\nf_X(x\\mid \\mu,\\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right]\\\\\n& = \\frac{\\sigma^{-1}}{\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2\\sigma^2}(x^2-2x\\mu +\\mu^2)\\right]\\\\\n& = \\frac{\\exp[-\\log(\\sigma)]}{\\sqrt{2\\pi}}\\exp\\left[\\frac{\\mu}{\\sigma^2} - \\frac{1}{2\\sigma^2}x^2 - \\frac{1}{2\\sigma^2}\\mu^2\\right]\\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left[\\frac{\\mu}{\\sigma^2}x - \\frac{1}{2\\sigma^2}x^2 - \\frac{1}{2\\sigma^2}\\mu^2-\\log(\\sigma)\\right]\\\\\nh(x) & = \\frac{1}{\\sqrt{2\\pi}}\\\\\n\\boldsymbol \\eta(\\mu, \\sigma^2) & = [\\mu/\\sigma^2, -1/2\\sigma^2]'\\\\\n\\mathbf T(x) & = [x, x^2]' \\\\\nA(\\boldsymbol{\\eta}) &= \\frac{\\mu^2}{2\\sigma^2} + \\log \\sigma\\\\\n& = -\\frac{\\eta_1^2}{4\\eta_2} - \\frac{\\log(-2\\eta_2)}{2}\n\\end{align*}\\]\n\nAlmost all the distributions we rely on happen to be exponential families. A select collection of these, along with their associated sample and parameter space, are:\n\n\n\n\n\n\n\n\n\nDistribution/Model\n\\(\\mathcal X\\)\n\\(\\boldsymbol{\\theta}\\)\n\\(\\boldsymbol \\Theta\\)\n\n\n\n\nBernoulli Distribution\n\\(\\{0,1\\}\\)\n\\(p\\)\n\\([0,1]\\)\n\n\nBinomial Distribution (\\(n\\) known)\n\\(\\{0,1,\\ldots,n\\}\\)\n\\(p\\)\n\\([0,1]\\)\n\n\nNegative Binomial Distribution (failures \\(r\\) known)\n\\(\\mathbb N\\)\n\\(p\\)\n\\([0,1]\\)\n\n\nGeometric Distribution\n\\(\\mathbb N\\backslash\\{0\\}\\)\n\\(p\\)\n\\([0,1]\\)\n\n\nExponential Distribution\n\\([0,\\infty)\\)\n\\(\\lambda\\)\n\\(\\mathbb R^+\\)\n\n\nPoisson Distribution\n\\(\\mathbb N\\)\n\\(\\lambda\\)\n\\(\\mathbb R^+\\)\n\n\nNormal Distribution\n\\(\\mathbb R\\)\n\\((\\mu,\\sigma^2)\\)\n\\(\\mathbb R\\times \\mathbb R^+\\)\n\n\nChi-squared Distribution\n\\([0,\\infty)\\)\n\\(k\\)\n\\(\\mathbb N\\)\n\n\nGamma Distribution\n\\(\\mathbb R^+\\)\n\\((\\alpha,\\beta)\\)\n\\(\\mathbb R^+\\times \\mathbb R^+\\)\n\n\nBeta Distribution\n\\([0,1]\\)\n\\((\\alpha,\\beta)\\)\n\\(\\mathbb R^+\\times \\mathbb R^+\\)\n\n\nMultivariate Normal Distribution\n\\(\\mathbb R^k\\)\n\\((\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\)\n\\(\\mathbb R^k\\times (\\mathbb R^+)^k \\times \\mathbb R^{k\\times(k-1)}\\)\n\n\nMultinomial Distribution (\\(n\\) known)\n\\(\\{0,1,\\ldots,n\\}^k\\)\n\\(\\mathbf p\\), where \\(\\sum_{j=1}^k p_j = 1\\)\n\\(k-\\)simplex over \\([0,1]\\)\n\n\n\nTechnically, listing some of these are redundant. The chi-squared distribution and exponential distribution both special cases of the gamma distribution. The Bernoulli distribution is a binomial distribution where \\(n=1\\). The The natural parameters, sufficient statistic, and cumulant function of each of these distributions are:\n\n\n\n\n\n\n\n\n\n\n\nDistribution/Model\n\\(\\boldsymbol \\eta(\\boldsymbol{\\theta})\\)\n\\(h(\\mathbf{x})\\)\n\\(\\mathbf T(\\mathbf{x})\\)\n\\(A(\\boldsymbol \\eta)\\)\n\n\n\n\n\nBernoulli Distribution\n\\(\\log[1/(1-p)]\\)\n\\(1\\)\n\\(x\\)\n\\(\\log[1 + \\exp(\\eta)]\\)\n\n\n\nBinomial Distribution (\\(n\\) known)\n\\(\\log[1/(1-p)]\\)\n\\(\\binom{n}{x}\\)\n\\(x\\)\n\\(n\\log[1 + \\exp(\\eta)]\\)\n\n\n\nNegative Binomial Distribution (failures \\(r\\) known)\n\\(\\log p\\)\n\\(\\binom{x+r-1}{x}\\)\n\\(x\\)\n\\(-r\\log[1-\\exp(\\eta)]\\)\n\n\n\nGeometric Distribution\n\\(\\log(1-p)\\)\n\\(1\\)\n\\(x\\)\n\\(\\eta - \\log[1-\\exp(\\eta)]\\)\n\n\n\nExponential Distribution\n\\(-\\lambda\\)\n\\(1\\)\n\\(x\\)\n\\(-\\log(-\\eta)\\)\n\n\n\nPoisson Distribution\n\\(\\log\\lambda\\)\n\\(1/x!\\)\n\\(x\\)\n\\(\\exp(\\eta)\\)\n\n\n\nNormal Distribution\n\\([\\mu/\\sigma^2, -1/2\\sigma^2]'\\)\n\\(1/\\sqrt{2\\pi}\\)\n\\([x,x^2]'\\)\n\\(-\\eta_1^2/4\\eta_2 - \\log(-2\\eta_2)/2\\)\n\n\n\nChi-squared Distribution\n\\(k/2-1\\)\n\\(\\exp(-x/2)\\)\n\\(\\log x\\)\n\\(\\log[\\Gamma(\\eta+1)] + (\\eta+1)\\log2\\)\n\n\n\nGamma Distribution\n\\([\\alpha - 1, -\\beta]'\\)\n\\(1\\)\n\\([\\log x, x]'\\)\n\\(\\log[\\Gamma(\\eta_1+1)] - (\\eta_1+1)\\log(-\\eta_2)\\)\n\n\n\nBeta Distribution\n\\([\\alpha - 1,\\beta - 1]\\)\n\\(1/[x(1-x)]\\)\n\\([\\log x, \\log(1-x)]'\\)\n\\(\\log[\\Gamma(\\eta_1 + 1)] + \\log[\\Gamma(\\eta_2 + 1)] - \\log[\\Gamma(\\eta_1+\\eta_2+1)]\\)\n\n\n\nMultivariate Normal Distribution\n\\([\\boldsymbol \\Sigma^{-1}\\boldsymbol \\mu, -\\boldsymbol\\Sigma^{-1}/2]'\\)\n\\((2\\pi)^{-k/2}\\)\n\\([\\mathbf{x},\\mathbf{x}\\mathbf{x}']\\)\n\\(-\\frac{1}{4}\\boldsymbol{\\eta}_1'\\boldsymbol{\\eta}_2^{-1}\\boldsymbol{\\eta}_1 - \\frac{1}{2}\\log|-2\\boldsymbol{\\eta}_2|\\)\n\n\n\nMultinomial Distribution (\\(n\\) known)\n\\([\\log(p_1/p_k),\\ldots, \\log(p_{k-1}/p_k),0]'\\)\n\\(\\mathbf{x}\\)\n\\(\\frac{n!}{\\prod_{i=1}^kx_i!}\\)\n\\(n\\log[1+\\sum_{i=1}^{k-1}\\exp(\\eta_i)]\\)\n\n\n\n\nIt’s worth noting there are some really important distributions that are not exponential families, namely the student’s \\(t-\\)distribution, the uniform distribution, and the \\(F-\\)distribution. Interestingly, the \\(F-\\)distribution is asymptotically equivalent to a \\(\\chi^2\\) distribution, and the \\(t-\\)distribution is asymptotically equivalent to the standard normal distribution, so as \\(n\\to\\infty\\), these “become” exponential families."
  },
  {
    "objectID": "exp_fam.html#properties",
    "href": "exp_fam.html#properties",
    "title": "4  Exponential Families",
    "section": "4.3 Properties",
    "text": "4.3 Properties\nExponential families have a myriad of properties that make them easy to work with. First, let’s look into the cumulant function \\(A(\\boldsymbol{\\eta})\\). The role of \\(A(\\boldsymbol{\\eta})\\) is to normalize the density \\(f_{\\mathbf{X}}(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\) when we express it as an exponential family. Without getting into the proof behind the Pitman–Koopman–Darmois theorem, it would seem that the function \\(h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\) would suffice for \\(\\mathbf T\\) to be a sufficient statistic for \\(\\boldsymbol{\\theta}\\), because we could just let \\(g(\\mathbf T(\\mathbf{x})\\mid \\boldsymbol{\\theta}) = \\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\) and apply the Fisher–Neyman factorization theorem. The issue with this is that \\(h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\) may not be a valid density function which integrates to \\(1\\) over \\(\\mathcal X\\). To ensure it is a valid density, we need to find some normalizing scalar \\(A\\) which satisfies:\n\\[ \\int_{\\mathcal X}\\frac{1}{A}\\left[h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\right]\\ d\\mathbf{x}= 1 \\] We could also take the scalar to be \\(\\exp \\kappa\\), giving \\[ \\int_{\\mathcal X}h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})-A]\\ d\\mathbf{x}= 1.\\]\nIf we solve for \\(A\\), we have\n\\[\\begin{align*}\n\\implies & \\int_{\\mathcal X}h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})-A]\\ d\\mathbf{x}= 1\\\\\n\\implies & \\exp(-A)\\int_{\\mathcal X}h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}= 1\\\\\n\\implies & A =  \\log\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)\n\\end{align*}\\] This constant is a function of \\(\\boldsymbol{\\eta}\\) only, as the dependence on \\(\\mathbf{x}\\) is eliminated when integrating, so our constant really should be \\(A(\\boldsymbol{\\eta})\\). This is the cumulant function. Besides it role in normalizing the reparameterized density, the cumulant function is inherently related to the moments of \\(\\mathbf T(\\mathbf{x})\\).\n\nDefinition 4.3 Suppose \\(\\mathbf{X}\\sim F_{\\mathbf{X}}\\). The moment-generating function (MGF), denoted as \\(M_{\\mathbf{X}}(\\mathbf t)\\), is defined as \\[ M_{\\mathbf{X}}(\\mathbf t) = \\text{E}\\left[\\exp(\\mathbf t'\\mathbf{X})\\right].\\] The cumulant-generating function (CMF), denoted as \\(K_{\\mathbf{X}}(\\mathbf t)\\), is defined as \\[K_{\\mathbf{X}}(\\mathbf t) = \\log(\\text{E}\\left[\\exp(\\mathbf t'\\mathbf{X})\\right]) = \\log M_{\\mathbf{X}}(\\mathbf t).\\]\n\nThe cumulant-generating function is an alternative to the more common moment-generating function. Both aim to provide a more convenient way to work with random variables than working directly with the density \\(f_{\\mathbf{X}}\\) or distribution \\(F_{\\mathbf{X}}\\), both of which often require integration.2 The defining property of moment-generating functions and cumulant-generating functions is that we can calculate quantities like expected value and variance via differentiation. This is a win, because differentiation much more straightforward than integration (in theory and in practice). The following lemma solidifies this fact.\n\nLemma 4.1 Let \\(M_{\\mathbf{X}}(\\mathbf t)\\) and \\(K_{\\mathbf{X}}(\\mathbf t)\\) be the MGF and CMF, respectively, of a random vector \\(\\mathbf{X}\\). For any integer \\(r = r_1 + \\cdots + r_n\\), we have \\[\\begin{align*}\n\\frac{\\partial^r M_{\\mathbf{X}}}{\\partial t_1^{r_1}\\cdots \\partial t_n^{r_n}}(\\mathbf{0}) & = \\text{E}\\left[X_1^{r_1}\\cdots X_n^{r_n}\\right],\\\\\n\\frac{\\partial K_{\\mathbf{X}}}{\\partial \\mathbf t}(\\mathbf{0}) & = \\text{E}\\left[\\mathbf{X}\\right],\\\\\n\\frac{\\partial^2 K_{\\mathbf{X}}}{\\partial \\mathbf t \\partial \\mathbf t'}(\\mathbf{0}) & = \\text{Var}\\left(\\mathbf{X}\\right)\n\\end{align*}\\] The various derivatives of \\(K_{\\mathbf{X}}(\\mathbf t)\\) are known as cumulants of \\(X\\), and happen to coincide with expectation and variance (both specific moments of \\(X\\)) for the derivatives shown above.\n\nProving this is a neat application of Taylor series, and you may have seen it in an undergrad probability course. When applying this to exponential families, we can relate the cumulant function \\(A(\\boldsymbol{\\eta})\\) to the expectation and variance of \\(\\mathbf{X}\\), hence its name.\n\nProposition 4.1 Suppose \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\), where \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) for an exponential family \\(\\mathcal P\\). The MGF and KGF of \\(\\mathbf T(\\mathbf{X})\\) are given as: \\[\\begin{align*}\nM_{\\mathbf T(\\mathbf{X})}(\\mathbf t) & = \\exp[A(\\boldsymbol{\\eta}+ \\mathbf t) - A(\\boldsymbol{\\eta})],\\\\\nK_{\\mathbf T(\\mathbf{X})}(\\mathbf t) & = A(\\boldsymbol{\\eta}+ \\mathbf t) - A(\\boldsymbol{\\eta}).\n\\end{align*}\\] Consequently, we have \\[\\begin{align*}\n\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] & = \\frac{\\partial A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}} = \\nabla_\\boldsymbol{\\eta}A(\\boldsymbol{\\eta}),\\\\\n\\text{Var}\\left(\\mathbf T(\\mathbf{X})\\right) & = \\frac{\\partial ^2A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}\\partial \\boldsymbol{\\eta}'}.\n\\end{align*}\\]\n\n\nProof. \\[\\begin{align*}\nM_{\\mathbf T(\\mathbf{X})}(\\mathbf t) & = \\text{E}\\left[\\exp(\\mathbf t'\\mathbf T(\\mathbf{X}))\\right] = \\int_{\\mathcal X} \\exp(\\mathbf t'\\mathbf T(\\mathbf{X}))h(\\mathbf{x})\\exp[\\boldsymbol \\eta(\\boldsymbol{\\theta})\\cdot\\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})]\\ d\\mathbf{x}\\\\\n& =  \\exp[-A(\\boldsymbol{\\eta})]\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\boldsymbol \\eta(\\boldsymbol{\\theta}) + \\mathbf t)'\\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\\\\n& = \\exp[-A(\\boldsymbol{\\eta})]\\exp\\left[\\log\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\boldsymbol \\eta(\\boldsymbol{\\theta}) + \\mathbf t)'\\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)\\right]\\\\\n& = \\exp[-A(\\boldsymbol{\\eta})]\\exp[A(\\boldsymbol{\\eta}+\\mathbf t)]\\\\\n& = \\exp[A(\\boldsymbol{\\eta}+\\mathbf t) - A(\\boldsymbol{\\eta})]\\\\\nK_{\\mathbf T(\\mathbf{X})}(\\mathbf t) & = \\log M_{\\mathbf{X}}(\\mathbf t)\\\\\n& = A(\\boldsymbol{\\eta}+\\mathbf t) - A(\\boldsymbol{\\eta})\n\\end{align*}\\] We can not differentiate \\(K_{\\mathbf T(\\mathbf{X})}(\\mathbf t)\\) to calculate the expectation and variance. \\[\\begin{align*}\n\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] & = \\frac{\\partial K_{\\mathbf{X}}}{\\partial \\mathbf t}(\\mathbf{0}) = \\left[\\frac{\\partial}{\\partial \\mathbf t}[A(\\boldsymbol{\\eta}+\\mathbf t) - A(\\boldsymbol{\\eta})]\\right]_{\\mathbf t = \\mathbf{0}} = \\left[\\frac{\\partial}{\\partial \\mathbf t}[\\boldsymbol{\\eta}+ \\mathbf t]\\frac{\\partial A}{\\partial \\boldsymbol{\\eta}}\\right]_{\\mathbf t = \\mathbf{0}} = \\frac{\\partial A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}}\\\\\n\\text{Var}\\left(\\mathbf T(\\mathbf{X})\\right) & = \\frac{\\partial^2 K_{\\mathbf{X}}}{\\partial \\mathbf t \\partial \\mathbf t'}(\\mathbf{0})  =\\left[\\frac{\\partial}{\\partial \\mathbf t'}\\left[\\frac{\\partial K_{\\mathbf{X}}}{\\partial \\mathbf t}\\right]\\right]_{\\mathbf t = \\mathbf{0}} =  \\left[\\frac{\\partial}{\\partial \\mathbf t '}\\left[\\frac{\\partial}{\\partial \\mathbf t}[A(\\boldsymbol{\\eta}+\\mathbf t) - A(\\boldsymbol{\\eta})]\\right]\\right]_{\\mathbf t = \\mathbf{0}} = \\frac{\\partial ^2A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}\\partial \\boldsymbol{\\eta}'}.\n\\end{align*}\\]\n\n\nCorollary 4.1 Suppose \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\), where \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) for a natural exponential family \\(\\mathcal P\\). Then \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{X}\\right] & = \\frac{\\partial A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}} = \\nabla_\\boldsymbol{\\eta}A(\\boldsymbol{\\eta}),\\\\\n\\text{Var}\\left(\\mathbf{X}\\right) & = \\frac{\\partial ^2A(\\boldsymbol{\\eta})}{\\partial \\boldsymbol{\\eta}\\partial \\boldsymbol{\\eta}'}.\n\\end{align*}\\]\n\n\nProof. If \\(\\mathcal P\\) is a natural exponential family, then \\(T(\\mathbf{X}) = \\mathbf{X}\\).\n\n\nExample 4.6 If \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(A(\\boldsymbol{\\eta}) = -\\eta_1^2/4\\eta_2 - \\log(-2\\eta_2)/2\\) for \\(\\boldsymbol{\\eta}= [\\mu/\\sigma^2, -1/2\\sigma^2]'\\). The sufficient statistic is \\([x,x^2]'\\). We have: \\[\\begin{align*}\n\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] & = \\frac{\\partial}{\\partial \\boldsymbol{\\eta}}[-\\eta_1^2/4\\eta_2 - \\log(-2\\eta_2)/2]\\\\\n& = \\begin{bmatrix} - \\frac{\\eta_1}{2\\eta_2} & \\frac{\\eta_1^2}{4\\eta_2^2} - \\frac{1}{2\\eta_2} \\end{bmatrix} \\\\\n& = \\begin{bmatrix} - \\frac{\\mu/\\sigma^2}{2(-1/2\\sigma^2)} & \\frac{(\\mu/\\sigma^2)^2}{4(-1/2\\sigma^2)^2} - \\frac{1}{2(-1/2\\sigma^2)} \\end{bmatrix}\\\\\n& = \\begin{bmatrix} \\mu & \\mu^2 - \\sigma^2 \\end{bmatrix}\n\\end{align*}\\]\n\nExponential families also exhibit convexity in two respects.\n\nProposition 4.2 Suppose \\(\\mathbf{X}\\sim P_\\boldsymbol{\\theta}\\), where \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\) for an exponential family \\(\\mathcal P\\). The natural parameter space, defined as \\[ \\mathcal N = \\left\\{\\boldsymbol{\\eta}\\ \\bigg|\\ \\int_{\\mathcal X}\\exp[\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}<\\infty \\right\\},\\] is a convex set. In addition, the cumulant function \\(A(\\boldsymbol{\\eta})\\) is convex on the set \\(\\mathcal N\\)\n\n\nProof. To show the convexity of \\(\\mathcal N\\), we must show that \\(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2 \\in \\mathcal N\\) for any \\(\\alpha \\in [0,1]\\). This means we must verify that the following integral is finite: \\[ \\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}<\\infty .\\] This happens to be an application of Hölder’s Inequality. \\[\\begin{align*}\n\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}& =\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)^1 \\\\ & = \\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)^{\\alpha + (1-\\alpha)}  \\\\& = \\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1)\\cdot \\mathbf T(\\mathbf{x})]\\exp[((1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)^ {\\alpha + (1-\\alpha)}\\\\\n& \\le \\underbrace{\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)^{\\alpha}}_{\\boldsymbol{\\eta}_2 \\in \\mathcal N \\implies < \\infty}\\underbrace{\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[((1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)^{1-\\alpha}}_{\\boldsymbol{\\eta}_2 \\in \\mathcal N \\implies < \\infty}\n\\end{align*}\\] The integral is finite, so \\(\\mathcal N\\) is convex. If we take the logarithm of both sides of this inequality, we find that \\(A(\\boldsymbol{\\eta})\\) is a convex function, recalling that \\(A(\\boldsymbol{\\eta})\\) can be written as the log of the integral of \\(\\exp[\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x})]\\) over \\(\\mathcal X\\). \\[\\begin{align*}\n&\\log\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)   \\le \\alpha \\log\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[(\\alpha\\boldsymbol{\\eta}_1)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right) + (1- \\alpha) \\log\\left(\\int_{\\mathcal X}h(\\mathbf{x})\\exp[((1-\\alpha)\\boldsymbol{\\eta}_2)\\cdot \\mathbf T(\\mathbf{x})]\\ d\\mathbf{x}\\right)\\\\\n\\implies & A[(\\alpha\\boldsymbol{\\eta}_1 + (1-\\alpha)\\boldsymbol{\\eta}_2)] \\le \\alpha A(\\boldsymbol{\\eta}_1) + (1-\\alpha)A(\\boldsymbol{\\eta}_2)\n\\end{align*}\\] This makes \\(A\\) convex.\n\nFinally, we can show that in one dimension, exponential families exhibit the MLR property when \\(\\eta\\) is an increasing function. Consequently, we can always apply the Karlin-Rubin theorem from Chapter 3 in this case.\n\nTheorem 4.3 (Exponential Families and MLR) When \\(\\dim(\\boldsymbol{\\theta}) = 1\\) and \\(\\eta(\\theta)\\) is non-decreasing, exponential families exhibit the MLR property in that sufficient statistic \\(T(x)\\).\n\n\nProof. When \\(f_X(\\mathbf{x}\\mid \\theta) = h(x)\\exp\\left[\\eta(\\theta)T(\\mathbf{x}) - A(\\theta)\\right]\\), then the likelihood ratio is \\[ \\frac{f_X(\\mathbf{x}\\mid \\theta_1)}{f_X(\\mathbf{x}\\mid \\theta_0)} = \\exp\\left[(\\eta(\\theta_1)-\\eta(\\theta_0)T(x)) - (A(\\theta_1) - A(\\theta_0))\\right].\\] The derivative of this ratio with respect to the statistic \\(T(x)\\) is \\[ [\\eta(\\theta_1)-\\eta(\\theta_0)]\\cdot \\frac{f_X(\\mathbf{x}\\mid \\theta_1)}{f_X(\\mathbf{x}\\mid \\theta_0)},\\] where \\([\\eta(\\theta_1)-\\eta(\\theta_0)] > 0\\) because \\(\\eta\\) is non-decreasing, and \\(f_X(\\mathbf{x}\\mid \\theta_1)/f_X(\\mathbf{x}\\mid \\theta_0) > 0\\) because it is the ratio of two probability densities. The derivative is therefore positive, and the likelihood ratio is monotonically increasing in \\(T(x)\\).\n\nThis theorem is particularly useful in the context of hypothesis testing. If our test statistic is a sufficient statistic, then by Theorem 4.2 it is distributed according to an exponential family, exhibits the MLR property, and we can use Theorem 3.1 to construct a UMP test."
  },
  {
    "objectID": "exp_fam.html#entropy-and-the-maximum-entropy-principle",
    "href": "exp_fam.html#entropy-and-the-maximum-entropy-principle",
    "title": "4  Exponential Families",
    "section": "4.4 Entropy and the Maximum Entropy Principle",
    "text": "4.4 Entropy and the Maximum Entropy Principle\nSufficiency is not the only means of arriving at the exponential family. A second derivation deals with some basic concepts from information theory. Loosely speaking, information theory studies how information is stored and communicated. The discipline exists at the intersection of probability, computer science, electrical engineering, physics, and statistical mechanics. The foundations of information theory were outline in Shannon (1948), an article which happens to be the fourth most cited paper ever (according to Google Scholar).\nA crucial aspect of the transmission of information is uncertainty. If we have a probability space \\((\\mathcal X,\\mathcal F, P)\\) and some random variable \\(X\\), how do we measure how “surprising” an event \\(x\\in \\mathcal X\\) is? The greater \\(\\Pr(X = x)\\), the less surprising the outcome \\(x\\) is. We want to define some measure \\(\\text{Surprise}(x)\\) such that:\n\n\\(\\text{Surprise}(x) \\to 1\\) as \\(\\Pr(X = x)\\to 0\\) and \\(\\text{Surprise}(x) \\to 0\\) as \\(\\Pr(X = x)\\to \\infty\\).\n\\(\\text{Surprise}(x)\\) is monotonic in \\(\\Pr(X= x)\\).\n\nThese properties are satisfied by the function \\(\\log(1/\\Pr(X= x))\\).\n\nDefinition 4.4 The information content of an outcome \\(x\\in \\mathcal X\\) is \\[ I_X(x) = \\log_b\\left(\\frac{1}{\\Pr(X=x)}\\right) = -\\log_b[\\Pr(X= x)]\\] for a base \\(b\\). If \\(b = 2\\) then the unit \\(I_X(x)\\) is given in bits. If \\(b\\) is the natural exponent, the unit is nat.\n\n\nExample 4.7 Suppose \\(X \\sim \\text{Bernoulli}(p)\\), where \\(\\mathcal X = \\{0,1\\}\\) and \\(\\Pr(X = 1) = p\\). The information content for \\(x = 1\\) (a “success”) is \\[I_X(1) = \\log_2(1/p).\\]\n\n\nShow code which generates figure\ntibble(\n  p = (0:1000)/1000,\n  I = -log2(p)\n) %>% \n  ggplot(aes(p, I)) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Pr(x = 1)\",\n       y = \"Information Content of x = 1\")\n\n\n\n\n\nFigure 4.1: Information for x = 1 for various values of the parameter p\n\n\n\n\n\nIf we average the information content over the sample space \\(\\mathcal X\\), we get the entropy of a random variable.\n\nDefinition 4.5 The entropy of a random variable \\(X\\) is \\[H(X) = \\text{E}\\left[I_X(x)\\right] = -\\int_\\mathcal X  \\log_b[f(x)]\\ dF_X = -\\int_\\mathcal X f(x) \\log_b[f(x)]\\ dx.\\]\n\nEntropy captures the average amount of information inherent in a random variable’s outcomes.\n\nExample 4.8 Again, suppose \\(X \\sim \\text{Bernoulli}(p)\\). The entropy of \\(X\\) is \\[\\begin{align*}\nH(X) = - \\sum_{x\\in \\{0,1\\}} \\Pr(x) \\log_2[\\Pr(x)] = - (1-p)\\log_2(1-p) - p \\log_2(p).\n\\end{align*}\\]\n\n\nShow code which generates figure\ntibble(\n  p = (0:1000)/1000,\n  I = -(1-p)*log2(1-p) - p*log2(p)\n) %>% \n  ggplot(aes(p, I)) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"p = Pr(x = 1)\",\n       y = \"Entropy of X\")\n\n\nWarning: Removed 2 row(s) containing missing values (geom_path).\n\n\n\n\n\nFigure 4.2: The entropy of a bernoulli random variable as a function of the parameter p\n\n\n\n\nThe greater the entropy, the greater the uncertainty associated with a random variables outcomes. In the event \\(p\\in\\{0,1\\}\\), then we’re certain that \\(x = 1\\) or \\(x=0\\), and there is no uncertainty. If \\(p=0.5\\), it’s equally likely that \\(x=1\\) as it is that \\(x=0\\), so things are less certain.\n\nWe can also measure the relative entropy between two distributions. Henceforth we’ll stick to the natural logarithm.\n\nDefinition 4.6 Suppose \\(X\\) and \\(Y\\) are random variables with distributions \\(f_X\\) and \\(F_Y\\), respectively. The Kullback–Leibler (KL) divergence/relative entropy of \\(f_X\\) and \\(F_Y\\), denoted \\(D_{KL}(f_X \\mid\\mid F_Y)\\) is defined as \\[ D_{KL}(f_X \\mid\\mid F_Y) = \\int_{\\mathcal X} f_X(t)\\cdot \\log \\frac{f_X(t)}{f_Y(t)}\\ dt.\\]\n\nKL divergence measures how “close” \\(f_X\\) is to \\(F_Y\\). Despite measuring “distance”, it is not a valid metric because it is not symmetric \\(( D_{KL}(f_X \\mid\\mid F_Y) \\neq D_{KL}(F_Y \\mid\\mid f_X) )\\) and does not satisfy the triangle inequality.\n\nExample 4.9 Suppose the role of an unfair six-sided die corresponds to a random variable \\(X\\) whose density is \\(f_X(t) = x/21\\) for \\(t=1,\\ldots,6\\). If we want to model the role of the die, wrongfully assuming it is fair, we would pick \\(f_Y(x) = 1/6\\) for \\(t=1,\\ldots,6\\).\n\n\nShow code which generates figure\nexpand_grid(\n  gr = c(\"Y\", \"X\"), \n  t = 1:6\n) %>% \n  mutate(f = ifelse(gr == \"Y\", 1/6,  t/21)) %>% \n  ggplot(aes(t, f)) +\n  geom_segment(aes(x = t, xend = t, y= 0, yend = f)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  facet_wrap(~gr, ncol =1) +\n  labs(x = \"Value of Die Roll\", y = \"Probability Density\")\n\n\n\n\n\nFigure 4.3: Respective densities of X and Y\n\n\n\n\nIn this case \\(D_{KL}(f_X \\mid\\mid F_Y)\\) measures the expected excess surprise from modeling the die roll with the random variable \\(Y\\) instead of \\(X\\).\n\\[\\begin{align*}\nD_{KL}(f_X \\mid\\mid F_Y) & = \\sum_{t \\in \\mathcal X} f_X(t)\\cdot \\log \\frac{f_X(t)}{f_Y(t)}\\\\\n& = \\sum_{t=1}^6 \\frac{t}{21}\\cdot \\log\\left(\\frac{t/21}{1/6}\\right)\\\\\n& = \\frac{1}{21}\\sum_{t=1}^6 t\\cdot \\log\\left(2t/7\\right)\\\\\n& \\approx 0.1293825\n\\end{align*}\\]\n\n\nTheorem 4.4 (Gibb’s Inequality) \\(D_{KL}(f_X \\mid\\mid F_Y) \\ge 0\\) if and only if \\(f_X \\neq F_Y\\).\n\n\nProof. The function \\(-\\log\\) is convex, so by Jensen’s inequality \\[\\begin{align*}\nD_{KL}(f_X \\mid\\mid F_Y) & = \\int_{\\mathcal X} f_X(t)\\cdot \\log \\frac{f_X(t)}{f_Y(t)}\\ dt\\\\\n& = \\int_{\\mathcal X} f_X(t)\\cdot -\\log \\frac{f_Y(t)}{f_X(t)}\\ dt\\\\\n& \\ge -\\log\\left[\\int_{\\mathcal X} f_X(t)\\frac{f_Y(t)}{f_X(t)}\\ dt\\right]\\\\\n& = -\\log\\left[\\int_{\\mathcal X} f_Y(t)\\ dt\\right]\\\\\n& = -\\log 1\\\\\n& = 0\n\\end{align*}\\]  space \n\n\nExample 4.10 (Entropy of Uniform Distribution) Suppose \\(X \\sim \\text{Uni}(a,b)\\). The entropy of \\(f_X\\) is \\[ H(X) = -\\int_a^b\\frac{1}{b-a}\\log\\left(\\frac{1}{b-a}\\right)\\ dt = \\log(b-a).\\] It turns out, that the uniform distribution has the maximum entropy of all distributions contained on the interval \\([a,b]\\). Intuitively, if the probability \\(X = x\\) is uniform over \\(x\\in\\mathcal X\\), then there is no certainty about our outcomes. If you were asked to guess a realized value of \\(X\\) beforehand, you would have zero confidence in your guess, because all outcomes are equally likely. Formally, we want to solve the problem \\[\\max_{f} H(x)\\text{ such that }\\int_{a}^bf(t)\\ dt= 1.\\] The Lagrangian associated with this problem is \\[\\begin{align*}\n\\mathcal L(f) &= -H(t) - \\lambda \\left(\\int_{a}^b f(t)\\ dt - 1\\right)\\\\\n& = \\int_a^b f(t) \\log f(t)\\ dt - \\lambda \\left(\\int_{a}^b f(t)\\ dt - 1 \\right)\\\\\n& = \\int_a^b f(t) \\log f(t) - \\lambda f(t) \\ dt - \\lambda\n\\end{align*}\\] Okay, but how do we optimize a function with respect to another function? We’re not picking some value to minimize \\(\\mathcal L\\), we’re picking some function \\(f_X\\) (which happens to be a valid density on the support \\([a,b]\\)). Optimization problems like these are solved using the calculus of variations (see Clarke (2013)).3 If \\(\\mathcal F\\) is the set of all real functions \\(f:\\mathbb R\\to \\mathbb R\\), then \\(\\mathcal L:\\mathcal F\\to \\mathbb R\\). Mappings which take functions to numbers are known as functionals. The functional derivative of \\(\\mathcal L\\) with respect to \\(f\\) is given as \\[\\frac{\\delta \\mathcal L}{\\delta f} = \\lim_{\\varepsilon \\to 0} \\frac{\\mathcal L(f + \\varepsilon g) - \\mathcal L(f)}{\\varepsilon} \\] for some arbitrary function \\(g\\in \\mathcal F\\).4 We can calculate functional derivatives directly appealing to the definition, but that’s a pain in the butt. Instead we’ll use the Euler-Lagrange equation which gives the derivative in the case where \\(\\mathcal L\\) can be expressed as an integral: \\[\\mathcal L(f) = \\int J(t,f(t), f'(t))\\ dt \\implies \\frac{\\delta \\mathcal L}{\\delta f} = \\frac{\\partial J}{\\partial f} - \\frac{d}{dt}\\frac{\\partial J}{\\partial f'}.\\] If we apply this to the Lagrangian we have the following first order conditions: \\[\\begin{align*}\n    \\log f(t) & = -1 - \\lambda \\\\\n    \\int_{a}^b f(t)\\ dt & = 1\n\\end{align*}\\] We can solve for \\(f(t) = \\exp(-1-\\lambda)\\), which is constant, so our distribution \\(f\\) is constant over \\([a,b]\\), making it the uniform distribution. Explicitly, we have \\[\\begin{align*}\n&\\int_{a}^b f(t)\\ dt  = 1 \\\\\n\\implies & \\exp(-1-\\lambda) \\int_{a}^b \\ dt  = 1\\\\\n\\implies & \\exp(-1-\\lambda) = \\frac{1}{b-a}\\\\\n\\implies & f(t)  = \\frac{1}{b-a}.\n\\end{align*}\\] Therefore \\(f_X(t) = 1/(b-a)\\) maximizes entropy.\n\nSo what is appealing about maximizing entropy? In a sense, a distribution with maximal entropy comes with minimal assumptions. This concept is known as the “principle of maximum entropy” as is due to Jaynes (1957). If we want to model a natural phenomenon with a probability distribution \\(F_X\\), the class of which define a regular model \\(\\mathcal P\\), and we only know that \\(\\mathcal X = [a,b]\\), then we should assume \\(X\\sim \\text{Uni}(a,b)\\) according to the principle of maximum entropy. What if we have additional information? For instance, we may have data that allows us to estimate \\(\\text{E}\\left[X\\right]\\) or \\(\\text{Var}\\left(X\\right)\\), something that can be done without specifying a regular model \\(\\mathcal P\\).\nFormally, consider defining a model \\(\\mathcal P\\) where \\(\\mathbf{X}\\sim P_{\\boldsymbol{\\theta}}\\) such that \\(\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] = \\boldsymbol{\\theta}\\) for some function \\(\\mathbf g(\\mathbf{X}) = [T_1(\\mathbf{X}), \\ldots, T_k(\\mathbf{X})]\\). The function \\(\\mathbf T\\) corresponds to all the distributional assumptions we are willing to make about \\(\\mathbf{X}\\), and these assumptions come in the form of moment conditions. Where do these assumptions come from? If we observe \\(n\\) realizations of \\((\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)\\) then we can consistently estimate \\(\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right]\\), so for a sufficiently large \\(n\\) we will be able to approximate \\(\\boldsymbol{\\theta}\\). We could define the model as \\[\\begin{align*}\n\\mathcal P &= \\{ P_{\\boldsymbol{\\theta}} \\},\\\\\nP_{\\boldsymbol{\\theta}} &= \\{F_\\mathbf{X}\\mid \\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] = \\boldsymbol{\\theta}\\},\n\\end{align*}\\] where each model value \\(P_{\\boldsymbol{\\theta}}\\) is an infinite collection of distributions satisfying our moment conditions. This model is parametric but is not regular, as \\(P_{\\boldsymbol{\\theta}}\\) is not a singleton for all \\(P_{\\boldsymbol{\\theta}} \\in \\mathcal P\\). If we insisted on a regular model, we need to go beyond moment conditions and actually assume the functional form of \\(F_X\\). The principle of maximum entropy gives us a criterion to appeal to here. We will define \\(\\mathcal P\\) such that each \\(P_{\\boldsymbol{\\theta}} \\in \\mathcal P\\) is a single distribution given by \\[\\begin{align*}\n& \\max_{f} H(\\mathbf t)\\text{ such that }\\int_{\\mathcal X}f(\\mathbf t)\\ d\\mathbf t= 1\\text{ and }\\text{E}\\left[\\mathbf T(\\mathbf{X})\\right] = \\boldsymbol{\\theta}\\\\\n\\implies & \\max_{f} H(\\mathbf t)\\text{ such that }\\int_{\\mathcal X}f(\\mathbf t)\\ d\\mathbf t= 1\\text{ and }\\int_{\\mathcal X}\\mathbf T(\\mathbf t)f(\\mathbf t) \\ d\\mathbf t= \\boldsymbol{\\theta}\n\\end{align*}\\] The Lagrangian associated with this problem is \\[\\begin{align*}\n\\mathcal L(f) &= - H(f) - \\lambda \\left(\\int_{\\mathcal X}f(\\mathbf t)\\ d\\mathbf t-1\\right) - \\boldsymbol \\eta \\left(\\int_{\\mathcal X}\\mathbf T(\\mathbf t)f(\\mathbf t)\\ d\\mathbf t-\\boldsymbol{\\theta}\\right)\\\\\n& = \\int_{\\mathcal X}f(\\mathbf t) \\log f(\\mathbf t)\\ d\\mathbf t - \\lambda \\left(\\int_{\\mathcal X}f(\\mathbf t)\\ d\\mathbf t-1\\right) - \\sum_{j=1}^k\\eta_j\\left(\\int_{\\mathcal X} T_j(\\mathbf t)f(\\mathbf t)\\ d\\mathbf t-c_j\\right)\\\\\n& = \\int_{\\mathcal X}\\left[f(\\mathbf t) \\log f(\\mathbf t)- \\lambda(\\mathbf t)- \\sum_{j=1}^k\\eta_jT_j(\\mathbf t) \\ d\\mathbf t \\right]- \\lambda- \\boldsymbol \\eta \\cdot \\boldsymbol{\\theta}\n\\end{align*}\\] where the multiplier \\(\\lambda\\) corresponds to the first constraint (\\(f\\) is a valid density), and the multipliers \\(\\boldsymbol{\\eta}\\) correspond to the second constraint (the moment conditions are satisfied). The corresponding first order conditions are: \\[\\begin{align*}\n&\\frac{\\delta \\mathcal L}{\\delta f}  = \\log f(\\mathbf{x}) + 1 - \\lambda - \\boldsymbol \\eta \\cdot \\mathbf T(\\mathbf{x}) = 0\\\\\n&\\int_{\\mathcal X}f(\\mathbf t)\\ d\\mathbf t  = 1\\\\\n&\\int_{\\mathcal X}\\mathbf T(\\mathbf t)f(\\mathbf t) \\ d\\mathbf t = \\boldsymbol{\\theta}\n\\end{align*}\\] Solving the first equation for \\(f(\\mathbf{x})\\) gives \\[ f(\\mathbf{x}) = \\exp(\\lambda - 1)\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x})).\\] Substituting this into the second condition gives: \\[\\begin{align*}\n&\\int_{\\mathcal X}\\exp(\\lambda- 1)\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}))\\ d\\mathbf{x}= 1\\\\\n\\implies & \\exp(\\lambda - 1)\\int_{\\mathcal X}\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}))\\ d\\mathbf{x}= 1\\\\\n\\implies & \\int_{\\mathcal X}\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}))\\ d\\mathbf{x}= \\exp(1-\\lambda)\n\\end{align*}\\] We integrate over \\(\\mathbf{x}\\), but \\(\\exp(1-\\lambda)\\) is still a function of \\(\\boldsymbol{\\eta}\\). Define \\(A(\\boldsymbol{\\eta})\\) as \\[ A(\\boldsymbol{\\eta}) =  \\log\\left[\\int_{\\mathcal X}\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}))\\ d\\mathbf{x}\\right]\\] such that \\(\\exp(\\lambda - 1) = \\exp (-A(\\boldsymbol{\\eta}))\\). \\[\\begin{align*}\nf(\\mathbf{x}) &= \\exp(\\lambda - 1)\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x})) \\\\ &= \\exp(-A(\\boldsymbol{\\eta}))\\exp(\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}))\\\\ & = \\exp\\left[\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})\\right].\n\\end{align*}\\] It turns out that \\(f(\\mathbf{x})\\) is an exponential family where \\(h(\\mathbf{x}) = 1\\). The reason \\(h(\\mathbf{x})\\) is normalized in this instance has to do with a change of probability measure, but in general exponential families are those with maximum entropy.\n\nTheorem 4.5 (Exponential Families Maximize Entropy) For all probability densities \\(g(\\mathbf{x})\\) satisfying \\(\\text{E}\\left[\\mathbf T(\\mathbf{x})\\right] = \\boldsymbol{\\theta}\\), \\[ H(f) \\ge H(g)\\] where \\(f(\\mathbf{x})= \\exp\\left[\\boldsymbol{\\eta}\\cdot \\mathbf T(\\mathbf{x}) - A(\\boldsymbol{\\eta})\\right]\\) is define as above.\n\n\nExample 4.11 (Numerical Optimization) In a perfect world we could confirm the fact that exponential families maximize entropy by telling our computer “solve this constrained optimization problem” and confirming the result is an exponential family. Unfortunately, this is only feasible for discrete random variable. For continuous random variables, the solution to the maximum-entropy problem is a continuous function, so it’s not clear how to solve the problem numerically. Fortunately, we can approximate the optimization problem arbitrarily well via “discretization”, just like how we can approximate integrals with finite Riemann sums. Suppose our sample space \\(\\mathcal X\\) is an interval of \\(\\mathbb{R}\\). Instead of calculating the entropy by integrating over all of \\(\\mathcal X\\), we can approximate it by calculating the sum of the entropy at a set of discrete points in \\(\\mathbf{X}\\). If these points are \\(\\{x_i\\}_{i=1}^n\\), and the \\(p_i=f(x_i)\\) for a density function \\(f\\), then we have \\[ H(t) = -\\int_{\\mathcal X}f(t)\\log f(t)\\ dt \\approx - \\sum_{i=1}^n p_i\\log p_i \\cdot\\underbrace{(x_{i-1}-x_i)}_{\\Delta x_i}.\\] Similarly, our approximate constraints are \\[\\begin{align*}\n\\int_{\\mathcal X}f(t)\\ dt &\\approx \\sum_{i=1}^n p_i\\cdot\\Delta x_i = 1,\\\\\n\\int_{\\mathcal X}\\mathbf T(\\mathbf t)f(\\mathbf t) \\ d\\mathbf t &\\approx \\sum_{i=1}^n\\mathbf T(x_i)p_i\\cdot\\Delta x_i = \\boldsymbol{\\theta}.\n\\end{align*}\\] Our discretized problem is \\[ \\max_{\\mathbf p} - \\sum_{i=1}^n p_i\\log p_i \\cdot\\Delta x_i \\text{ such that }\\sum_{i=1}^n p_i\\cdot\\Delta x_i = 1 \\text{ and } \\sum_{i=1}^n\\mathbf T(x_i)p_i\\cdot\\Delta x_i = \\boldsymbol{\\theta},\\] where the vector \\(\\mathbf{x}\\) is the finite set of points which we approximate the sample space \\(\\mathcal X\\) with, and \\(\\mathbf p = f(\\mathbf{x})\\) is the probability assigned to each of these points. For a concrete example, consider the problem of maximizing the entropy of a distribution over the interval \\([0,1]\\) with no other constraints. We’ve already shown that the resulting distribution is the uniform distribution using the calculus of variations, but let’s arrive at the same conclusion by solving the discretized version of the problem. We’ll divide the interval \\([0,1]\\) using 100 equally spaced points \\(\\{0.01,0.02,\\ldots,0.99,1\\}\\) (\\(\\Delta x_i = 1/100\\) for all \\(i\\)). We will find the vector \\(\\mathbf p\\in\\mathbb{R}^{100}\\) which solves \\[ \\max_{\\mathbf p} - \\sum_{i=1}^n \\frac{p_i\\log p_i}{100} \\text{ such that }\\sum_{i=1}^n \\frac{p_i}{100} = 1.\\] We could solve this problem using R’s optim(), but instead we’ll use the CVXR package due to Fu, Narasimhan, and Boyd (2017) based on the work of Grant, Boyd, and Ye (2006). This package is made specifically for convex optimization problems (a category which our problem falls into), and is very user-friendly.\n\n#set the dimension of the problem\nn <- 100\ndelta_x <- 1/n\n\n#define variable, objective, constraints, and problem\np <- Variable(n)\n#make sure to use CVXR's entr() function\nobjective <- Maximize(sum(entr(p)*delta_x))\nconstraints <- list(sum(p*delta_x) == 1)\nproblem <- Problem(objective, constraints)\n\n#solve problem\nresult <- solve(problem)\n\nIf we plot our solution, we see that it corresponds perfectly to the uniform distribution.\n\n\nShow code which generates figure\ntibble(\n x = seq(0, 1, length = n),\n p = result$getValue(p)\n) %>%\n  ggplot(aes(x, p)) + \n  geom_function(fun = dunif, aes(color = \"Uniform Distribution\")) + \n  geom_point(size = 0.5, aes(color = \"Numerical Solution\")) +\n  ylim(0,2) + \n  labs(y = \"Density\") +\n  scale_color_manual(\"\", values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4.4: Analytic and numerical solution to maximum entropy problem.\n\n\n\n\n\nWe’ll demonstrate Theorem 4.5 with two more examples where we’ll derive exponential families by maximizing entropy. In each case we’ll confirm our work by solving the corresponding discretized optimization problem numerically.\n\nExample 4.12 (Exponential Distribution) Suppose we want to model a random variable \\(X\\) with a sample space \\(\\mathcal X =[0,\\infty)\\) according to the principle of maximum entropy such that \\(\\text{E}\\left[X\\right]= \\theta\\). The Lagrangian is \\[ \\mathcal L(f) = \\int_0^\\infty f(t)\\log f(t)\\ dt - \\lambda\\left(\\int_0^\\infty f(t) \\ dt - 1\\right) - \\eta\\left(\\int_0^\\infty t\\cdot f(t) \\ dt - \\theta\\right),\\]\nwhich gives first order conditions:\n\\[\\begin{align*}\n&\\frac{\\delta \\mathcal L}{\\delta f} = \\log f(x) + 1 - \\lambda - \\eta x = 0\\\\\n&\\int_0^\\infty f(t) \\ dt = 1\\\\\n&\\int_0^\\infty t\\cdot f(t) = \\theta\n\\end{align*}\\] Solving the first equation for \\(f(x)\\) gives \\(f(x) = \\exp(1-\\lambda)\\exp(\\eta x)\\). If we plug this into the second equation (the first constraint) we have: \\[\\begin{align*}\n&  \\int_0^\\infty \\exp(\\lambda - 1)\\exp(\\eta x) = 1\\\\\n\\implies & \\exp(\\lambda - 1)\\int_0^\\infty \\exp(\\eta x) = 1\\\\\n\\implies & \\exp(\\lambda - 1)\\left[\\frac{1}{\\eta}\\exp(\\eta x)\\right]_0^\\infty = 1\\\\\n\\implies & \\exp(\\lambda - 1)(-1/\\eta) = 1 & (-1/\\eta < 0)\n\\end{align*}\\] If \\(-1/\\eta \\ge 0\\), then the improper integral will not converge. Let’s repeat this step with the second integral: \\[\\begin{align*}\n& \\int_0^\\infty x\\exp(\\lambda - 1)\\exp(\\eta x) = \\theta\\\\\n\\implies & \\exp(\\lambda - 1)\\int_0^\\infty x\\exp(\\eta x) = \\theta\\\\\n\\implies & \\exp(\\lambda - 1)\\left[\\frac{x\\exp(\\eta x)}{\\eta} - \\frac{\\exp(\\eta x)}{\\eta}\\right]_0^\\infty = 1 & (\\text{integration by parts})\\\\\n\\implies & \\exp(\\lambda - 1)(1/\\eta^2) = \\theta\n\\end{align*}\\] If we divide the two constraints by each other, we have \\(-\\eta = 1/\\theta\\): \\[\\begin{align*}\n& \\frac{\\exp(\\lambda - 1)(-1/\\eta)}{\\exp(\\lambda - 1)(1/\\eta^2)} = \\frac{1}{\\theta}\\\\\n\\implies & -\\eta = 1/\\theta\\\\\n\\implies & -1/\\eta = \\theta\n\\end{align*}\\] Therefore, \\[\\begin{align*}\n&\\exp(\\lambda - 1)(-1/\\eta) = 1\\\\\n\\implies &\\exp(\\lambda - 1)\\theta = 1\\\\\n\\implies & \\exp(\\lambda - 1) = 1/\\theta\n\\end{align*}\\] so \\[f(x) = \\exp(1-\\lambda)\\exp(\\eta x) = \\frac{1}{\\theta}\\exp(-x/\\theta).\\] This is the exponential distribution which is parameterized by \\(\\theta\\), where \\(\\theta\\) comes from the constraint \\(\\text{E}\\left[X\\right] = \\theta\\).\nTo discretize the problem, we’ll approximate the sample space \\(\\mathcal X =(0,\\infty)\\) with the \\(n=250\\) points \\(\\{0.04, 0.08, \\ldots, 10\\}\\) (\\(\\Delta x_i = 1/100\\) for \\(i=1,\\ldots,250\\)). The exponential distribution has negligible density on the interval \\((10,\\infty)\\), so our approximation of \\([0,\\infty)\\) is still valid despite the points being a subset of \\([0,10]\\). The approximated problem is\n\\[\\begin{align*}\n& \\max_{\\mathbf p} - \\sum_{i=1}^n p_i\\log p_i \\cdot \\frac{1}{250},\\\\\n&\\text{such that} \\sum_{i=1}^n p_i\\cdot  \\frac{1}{250} = 1, \\\\\n&\\text{and} \\sum_{i=1}^n p_i x_i  \\frac{1}{250} = \\theta .\\\\\n\\end{align*}\\]\nWe’ll take \\(\\theta = 1\\) for this problem.\n\n#set the dimension of the problem\ntheta <- 1\nn <- 250\nx <- seq(0.04, 10, length = n)\ndelta_x <- x[2] - x[1]\n\n#define variable, objective, constraints, and problem\np <- Variable(n)\nobjective <- Maximize(sum(entr(p)*delta_x))\nconstraints <- list(sum(p*delta_x) == 1, \n                    sum(p*x*delta_x) == theta)\nproblem <- Problem(objective, constraints)\n\n#solve problem\nresult <- solve(problem)\n\nIf we plot our solution, we find that it is in almost perfect alignment with the exponential distribution.\n\n\nShow code which generates figure\ntibble(\n x = x,\n p = result$getValue(p)\n) %>%\n  ggplot(aes(x, p)) + \n  geom_function(fun = dexp, aes(color = \"Exponential Distribution, θ = 1\")) + \n  geom_point(size = 0.1, aes(color = \"Numerical Solution\")) + \n  labs(y = \"Density\") +\n  scale_color_manual(\"\", values = c(\"red\", \"black\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4.5: Analytic and numerical solution to maximum entropy problem.\n\n\n\n\n\n\nExample 4.13 (Normal Distribution) Suppose we want to model a random variable \\(X\\) with a sample space \\(\\mathcal X =\\mathbb R\\) according to the principle of maximum entropy such that \\(\\text{E}\\left[X\\right]= \\mu\\), and \\(\\text{Var}\\left(X\\right) = \\sigma^2\\). We can combine these into a single constraint \\(\\text{E}\\left[(x-\\mu)^2\\right]=\\sigma^2\\). The Lagrangian is \\[ \\mathcal L(f) = \\int_{-\\infty}^\\infty f(t)\\log f(t)\\ dt - \\lambda\\left(\\int_{-\\infty}^\\infty f(t) \\ dt - 1\\right) - \\eta\\left(\\int_{-\\infty}^\\infty (t-\\mu)^2\\cdot f(t) \\ dt - \\sigma^2\\right),\\] which gives the first order conditions: \\[\\begin{align*}\n&\\frac{\\delta \\mathcal L}{\\delta f} = \\log f(x) + 1 - \\lambda - \\eta (x-\\mu)^2  = 0\\\\\n&\\int_{-\\infty}^\\infty f(t) \\ dt  = 1\\\\\n&\\int_{-\\infty}^\\infty  (t-\\mu)^2\\cdot f(t) \\ dt = \\sigma^2\n\\end{align*}\\] Solving the first equation gives \\[ f(x) = \\exp(\\lambda - 1)\\exp(\\eta(x-\\mu)^2),\\] which we can substitute into the first constraint. \\[\\begin{align*}\n& \\int_{-\\infty}^\\infty \\exp(\\lambda - 1)\\exp(\\eta(t-\\mu)^2) \\ dt = 1\\\\\n\\implies & \\exp(\\lambda - 1)\\int_{-\\infty}^\\infty \\exp(\\eta(t-\\mu)^2) \\ dt = 1\\\\\n\\implies & \\exp(\\lambda - 1)(-\\pi/\\eta)^{1/2} = 1 & \\left(\\int_{-\\infty}^\\infty \\exp(a(t+b)^2)\\ dt = \\sqrt{\\pi/a}\\right)\\\\\n\\implies & \\exp(\\lambda - 1) = (-\\eta/\\pi)^{1/2}\n\\end{align*}\\] The key step was recognizing the integral of the Gaussian function. Now we can turn to the second constraint. \\[\\begin{align*}\n& \\int_{-\\infty}^\\infty  (t-\\mu)^2\\cdot f(t) \\ dt = \\sigma^2\\\\\n\\implies & \\int_{-\\infty}^\\infty  (t-\\mu)^2\\cdot \\exp(\\lambda - 1)\\exp(\\eta(x-\\mu)^2) \\ dt = \\sigma^2\\\\\n\\implies & (-\\eta/\\pi)^{1/2}\\int_{-\\infty}^\\infty  (t-\\mu)^2\\cdot \\exp(\\eta(x-\\mu)^2) \\ dt = \\sigma^2 & (\\exp(\\lambda - 1) = (-\\eta/\\pi)^{1/2})\\\\\n\\implies & (-\\eta/\\pi)^{1/2}\\cdot \\frac{1}{2}(-\\pi/\\eta^3)^{1/2} = \\sigma^2\\\\\n\\implies & \\eta = -\\frac{1}{2\\sigma^2}\n\\end{align*}\\] The integral of \\((t-\\mu)^2\\cdot \\exp(\\eta(x-\\mu)^2)\\) follows from a generalization of the integral of the Gaussian function. Therefore, \\[\\begin{align*}\nf(x) &= \\exp(\\lambda - 1)\\exp(\\eta(x-\\mu)^2)\\\\\n& = (-\\eta/\\pi)^{1/2}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right)\\\\\n& = (-(-1/2\\sigma^2)/\\pi)^{1/2}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right)\\\\\n& = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[-\\frac{(x-\\mu^2)}{2\\sigma^2}\\right]\n\\end{align*}\\]\nThe normal distribution has negligible density outside the interval \\([-4,4]\\subset\\mathcal X=\\mathbb{R}\\), so we can approximate it with \\(n=250\\) equally spaced points on \\([-4,4]\\) (\\(\\Delta\\)). The discretized problem is\n\\[\\begin{align*}\n& \\max_{\\mathbf p} - \\sum_{i=1}^n p_i\\log p_i \\cdot \\frac{8}{250},\\\\\n&\\text{such that} \\sum_{i=1}^n p_i\\cdot  \\frac{8}{250} = \\theta_1, \\\\\n&\\text{and} \\sum_{i=1}^n p_i x_i^2 \\cdot \\frac{8}{250} = \\theta_2 .\\\\\n\\end{align*}\\]\nWe’ll let \\(\\boldsymbol{\\theta}= (0,1)\\), which should give the standard normal distribution.\n\n#set the dimension of the problem\ntheta <- c(0,1)\nn <- 250\nx <- seq(-4 + 8/n, 4, length = n)\ndelta_x <- x[2] - x[1]\n\n#define variable, objective, constraints, and problem\np <- Variable(n)\nobjective <- Maximize(sum(entr(p)*delta_x))\nconstraints <- list(sum(p*delta_x) == 1, \n                    sum(p*x*delta_x) == theta[1],\n                    sum(p*x^2*delta_x) == theta[2])\nproblem <- Problem(objective, constraints)\n\n#solve problem\nresult <- solve(problem)\n\nOnce again, we have a solution that looks nearly identitical to the distribution we derived analytically.\n\n\nShow code which generates figure\ntibble(\n x = x,\n p = result$getValue(p)\n) %>%\n  ggplot(aes(x, p)) + \n  geom_function(fun = dnorm, aes(color = \"Standard Normal Distribution\")) + \n  geom_point(size = 0.1, aes(color = \"Numerical Solution\")) + \n  labs(y = \"Density\") +\n  scale_color_manual(\"\", values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 4.6: Analytic and numerical solution to maximum entropy problem."
  },
  {
    "objectID": "exp_fam.html#further-reading",
    "href": "exp_fam.html#further-reading",
    "title": "4  Exponential Families",
    "section": "4.5 Further reading",
    "text": "4.5 Further reading\nExponential Families: Chapter 18 of DasGupta (2011), Section 1.6 of Bickel and Doksum (2015), Section 1.5 of Lehmann and Casella (1998), these notes\nInformation Theory: Chapter 6 of Murphy (2022), Section 1.6 Bishop and Nasrabadi (2006)\nMaximum Entropy Principle: Here, here, here, and here. This course page\nAll of the Above: Jaynes (2003)\n\n\n\n\n\n\nBickel, Peter J, and Kjell A Doksum. 2015. Mathematical Statistics: Basic Ideas and Selected Topics, Volume i. 2nd ed. CRC Press.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nClarke, Francis. 2013. Functional Analysis, Calculus of Variations and Optimal Control. Vol. 264. Springer.\n\n\nDarmois, Georges. 1935. “Sur Les Lois de Probabilitéa Estimation Exhaustive.” CR Acad. Sci. Paris 260 (1265): 85.\n\n\nDasGupta, Anirban. 2011. Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics. Springer.\n\n\nFisher, Ronald A. 1922. “On the Mathematical Foundations of Theoretical Statistics.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 222 (594-604): 309–68.\n\n\nFu, Anqi, Balasubramanian Narasimhan, and Stephen Boyd. 2017. “CVXR: An r Package for Disciplined Convex Optimization.” arXiv Preprint arXiv:1711.07582.\n\n\nGrant, Michael, Stephen Boyd, and Yinyu Ye. 2006. “Disciplined Convex Programming.” In Global Optimization, 155–210. Springer.\n\n\nJaynes, Edwin T. 1957. “Information Theory and Statistical Mechanics.” Physical Review 106 (4): 620.\n\n\n———. 2003. Probability Theory: The Logic of Science. Cambridge university press.\n\n\nKoopman, Bernard Osgood. 1936. “On Distributions Admitting a Sufficient Statistic.” Transactions of the American Mathematical Society 39 (3): 399–409.\n\n\nLehmann, Erich L, and George Casella. 1998. Theory of Point Estimation. 2nd ed. Springer.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press. probml.ai.\n\n\nPitman, Edwin James George. 1936. “Sufficient Statistics and Intrinsic Accuracy.” In Mathematical Proceedings of the Cambridge Philosophical Society, 32:567–79. 4. Cambridge University Press.\n\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27 (3): 379–423."
  },
  {
    "objectID": "ols.html",
    "href": "ols.html",
    "title": "5  The Classical Linear Model",
    "section": "",
    "text": "Now that we’re equipped with all the tools necessary to consider our first econometric model – the classic linear model."
  },
  {
    "objectID": "ols.html#identification-estimation-and-inference",
    "href": "ols.html#identification-estimation-and-inference",
    "title": "5  The Classical Linear Model",
    "section": "5.1 Identification, Estimation, and Inference",
    "text": "5.1 Identification, Estimation, and Inference\nBefore we cover our first, and arguably most important, model in econometrics, it’s worth reiterating how the problems of identification, estimation, and inference are related. It’s also important to consider whether these problems occur in the context of a model/population, or a sample drawn from the model/population. These slides provide a nice diagram which provides some insight.\n\n\n\n\n\nFigure 5.1: The relationship between models, parameters, and observed data.\n\n\n\n\nWhen we posit a model \\(\\mathcal P\\) (which is just a collection of probability distributions), the first thing we need to do is parameterize our model. Angrist and Pischke (2008) refer to this as a “population first” approach in which we “define the objects [parameters] of interest before we can use data to study [estimate/make inferences] about them.” We then must address the question of identification – for a parameterization \\(\\boldsymbol{\\theta}\\mapsto P_\\boldsymbol{\\theta}\\), does \\(\\boldsymbol{\\theta}\\) uniquely determine \\(P_\\boldsymbol{\\theta}\\)? If \\(\\boldsymbol{\\theta}= \\boldsymbol{\\theta}'\\), then is it necessarily the case that \\(P_\\boldsymbol{\\theta}= P_{\\boldsymbol{\\theta}'}\\)? For this to be the case, we will usually need to amend our initial model by adding one or more assumptions. Once this is done, we can tackle the problem of estimation/inference given a sample drawn from the population knowing that once we’ve made a decision (whether that be an estimate or rejecting a null hypothesis) regarding the parameter space \\(\\boldsymbol\\Theta\\), that it will be equivalent to a decision about the model \\(\\mathcal P\\) via identification.\nAnother way to think about identification is in terms of some “perfect” estimate of \\(\\boldsymbol{\\theta}\\). Imagine that you had an infinite amount of data such that it was guaranteed that \\(\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\theta}\\). If \\(\\boldsymbol{\\theta}\\) is not identified, then our perfect estimate \\(\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\theta}\\) could correspond to multiple \\(P_\\boldsymbol{\\theta}\\in \\mathcal P\\), so it is impossible to know the which model value \\(P_\\boldsymbol{\\theta}\\) generated the infinite amount of data which gave us our estimate. This speaks to how fundamental the problem of identification is. We usually like to focus on all the nice properties an estimator has, but even if that estimator checks all the boxes, it is meaningless if our parameters/model isn’t identified.\nThe term “identification” can sometimes be the cause of confusion because it appears in a wide array of contexts, and definitions of identification sometimes only apply to a specific model.1 For an excellent survey of identification in econometrics see Lewbel (2019)."
  },
  {
    "objectID": "ols.html#conditional-expectation-and-linear-projection",
    "href": "ols.html#conditional-expectation-and-linear-projection",
    "title": "5  The Classical Linear Model",
    "section": "5.2 Conditional Expectation and Linear Projection",
    "text": "5.2 Conditional Expectation and Linear Projection\nWe will begin with an example owing to Galton (1886).\n\nExample 5.1 Suppose we are interested in how the height of two parents is related to their child’s height. Let \\(X\\) be a random variable associated with the average height of two parents, and \\(Y\\) be a random variable associated with the height of a child. Furthermore, assume the joint distribution of \\((X,Y)\\) is: \\[\\begin{align*}\n(X,Y) &\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),\\\\\n\\boldsymbol \\mu & = [68, 68]',\\\\\n\\boldsymbol \\Sigma & = \\begin{bmatrix}8 & 4\\\\ 4 & 6\\end{bmatrix}.\n\\end{align*}\\] As a consequence, \\(X \\sim N(68, 8)\\) and \\(Y ~ N(68,6)\\) have the same marginal density of \\(N(68, 8)\\). In other words, the average height of individuals is the same across generations. The variance of \\(X\\) and \\(Y\\) are given as \\(\\sigma_X^2=\\boldsymbol{\\Sigma}_{11}\\) and \\(\\sigma_Y^2 = \\boldsymbol{\\Sigma}_{22}\\).\n\n\nShow code which generates figure\nmu <- c(68, 68)\nSigma <- matrix(c(8, 4, 4, 6), ncol = 2)\n\np1 <- tibble(x = c(seq(57, 80, length = 1000), seq(57, 80, length = 1000)),\n           key = c(rep(\"Parents\", 1000), rep(\"Child\", 1000))\n  ) %>% \n  mutate(y = dnorm(x, 68, sqrt(8))) %>% \n  ggplot(aes(x, y)) +\n  geom_line() +\n  facet_wrap(~key, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Height (in)\",\n       y = \"density\")\n\ndf <- expand_grid(\n  x = c(seq(57, 80, length = 1000), 60, 65, 70, 75), \n  y = seq(57, 80, length = 1000)\n)\ndf$p <- dmvnorm(df, mu, Sigma)\np2 <- ggplot(df, aes(x, y, z = p)) +\n  geom_contour(bins = 20, color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Parents' (Average) Height (in)\", y = \"Child's Height (in)\")\n\nggarrange(p1, p2, ncol = 1)\n\n\n\n\n\nFigure 5.2: The marginal desnity of childs’ and parents’ height along with their joint density\n\n\n\n\nIf we want to predict a child’s height using parents’ height \\(X = x\\), we can inspect the conditional expectation \\(\\text{E}\\left[Y\\mid X = x\\right]\\). This expectation is given as\n\\[\\begin{align*}\n\\text{E}\\left[Y\\mid X = x\\right] & = \\int_{\\mathcal Y} y\\cdot f_{Y\\mid x}(y\\mid x)\\ dy\\\\ & = \\int_{\\mathcal Y} y\\cdot \\frac{f_{Y,X}(y \\mid x, \\boldsymbol \\mu, \\boldsymbol \\Sigma)}{f_{X}(x \\mid \\mu_1, \\sigma_{X})}\\ dy\\\\\n& = \\int_{\\mathcal Y}y\\cdot \\frac{\\exp\\left(-\\frac{1}{2}([x,y]' - \\boldsymbol \\mu)'\\boldsymbol\\Sigma^{-1}([x,y]' - \\boldsymbol \\mu)\\right)/\\sqrt{(2\\pi)^k\\det(\\boldsymbol\\Sigma)}}{\\exp[-(x-\\mu_X)^2/2\\sigma^2]/\\sqrt{2\\pi\\sigma_X^2}}\\ dF_Y\\\\\n&\\vdots\\\\\n& = \\mu_Y + \\frac{\\sigma_{Y}}{\\sigma_{X}}\\rho(x - \\mu_X)\n\\end{align*}\\] If we substitute in our parameters, and the calculated correlation coefficient of \\[ \\rho = \\frac{\\text{Cov}\\left(X,Y\\right)}{\\sigma_{X}\\sigma_{Y}} = \\frac{1}{4\\sqrt 3} \\approx 0.577,\\] we have \\[ \\text{E}\\left[Y\\mid X = x\\right] \\approx 68 + \\frac{\\sqrt 6}{\\sqrt 8}\\cdot\\frac{1}{4\\sqrt 3}(x - 68) = 34 + \\frac{1}{2}x.\\]\n\n\nShow code which generates figure\np1 <- df %>% \n  filter(x %in% c(60, 65, 70, 75) )%>% \n  mutate(p_y = p/dnorm(x, 68, sqrt(8))) %>% \n  ggplot(aes(y, p_y, color = as.factor(x))) + \n  geom_line() +\n  theme_minimal() +\n  labs(color = \"Parents's Height\",\n       x = \"Childs's Height\",\n       y = \"Conditional Density\") +\n  theme(legend.position = \"bottom\")\n\nrho <- (Sigma[1,2])/(sqrt(Sigma[1,1]) * sqrt(Sigma[2,2]))\ns1 <- sqrt(Sigma[1,1])\ns2 <- sqrt(Sigma[2,2])\ndf2 <- data.frame(x = seq(57, 80, length = 1000)) %>% \n  mutate(E_y = mu[2] + rho*(s2/s1)*(x - mu[1]))\n\n\np2 <- df2 %>% ggplot(aes(x,E_y)) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Parents' (Average) Height (in)\",\n      y = \"Conditional Expectation of Child's Height\")\n\nggarrange(p1, p2, ncol = 1)\n\n\n\n\n\nFigure 5.3: The density of a child’s height conditioning on their parent’s height\n\n\n\n\nThe key observation is that the function \\(\\text{E}\\left[Y \\mid X = x\\right]\\) is linear in \\(x\\)! If we overlay the line associated with \\(\\text{E}\\left[Y\\mid X =x\\right]\\) on the joint density of \\((X,Y)\\) we end up with a figure emulating one in Galton (1886).\n\n\nShow code which generates figure\ndf <- expand_grid(\n  x = c(seq(57, 80, length = 1000), 60, 65, 70, 75), \n  y = seq(57, 80, length = 1000)\n)\ndf$p <- dmvnorm(df, mu, Sigma)\n\nggplot(df, aes(x, y, z = p)) +\n  geom_hline(yintercept = 68, linetype = \"dashed\", size = 0.4) +\n  geom_vline(xintercept = 68, linetype = \"dashed\", size = 0.4) +\n  geom_contour(bins = 20, color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Parents' (Average) Height (in)\", y = \"Child's Height (in)\") +\n  geom_abline(intercept = 34, slope = 1/2, color = \"red\") +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\") +\n  annotate(\"text\", label = \"E[Y|X = x]\",x = 75.5, y = 34 + 76/2 - 1, size = 4, colour = \"red\") +\n  annotate(\"text\", label = \"X = x\",x = 75, y = 74, size = 5, colour = \"blue\")\n\n\n\n\n\nFigure 5.4: The conditional expectation of a child’s height given their parent’s height in red, over the joint density of child and parent height. The red lines position in relation to the blue 45° line illustrates regression to the mean\n\n\n\n\nOn average, children with tall parents tend to be shorter than their parents. Conversely, children with short parents tend to be taller than their parents. In other words, as shown by the lines superimposed on \\(f_{X,Y}\\), \\(\\text{E}\\left[Y\\mid X = x\\right] < x\\) when \\(x > \\text{E}\\left[X\\right]\\) and \\(\\text{E}\\left[Y\\mid X = x\\right] > x\\) when \\(x < \\text{E}\\left[X\\right]\\).\n\nThis phenomenon is known as regression to the mean, and it lends its name to the practice of relating \\(Y\\) to \\(X\\). The ideas from Galton (1886) were extended by one of Galton’s students in Pearson and Lee (1903), who actually uses the term “regression line” in reference to the function \\(\\text{E}\\left[Y\\mid X\\right]\\).\n\nRemark. It behooves one to acknowledge that Francis Galton’s motivation for studying height among parents and children stemmed from his interest in genetics and Darwinism. He was an early proponent of eugenics, and even coined the term “eugenics”. He referred to regression to the mean as “regression to mediocrity”, and believed this should be avoided by selective reproduction. Many of Galton’s beliefs are classic examples of scientific racism.\n\nLet’s abstract from the example of height. Suppose we have \\((Y, \\mathbf X)\\sim F_{Y,\\mathbf{X}}\\) for some dependent variable \\(Y\\) (with sample space \\(\\mathcal Y\\)) and a vector of independent variables/explanatory variables/covariates/regressors \\(\\mathbf{X}\\) (with sample sapce \\(\\mathcal X\\)). If we want to explore the relationship between \\(Y\\) and \\(\\mathbf{X}\\), one measure of interest is the conditional expectation of \\(Y\\) given \\(\\mathbf{X}\\). If we know \\(\\mathbf{X}\\) takes on the value \\(\\mathbf{x}\\), then on average, what is the value of \\(Y\\)? \\[ \\text{E}\\left[Y\\mid \\mathbf{X}= \\mathbf{x}\\right] = \\int_{\\mathcal Y}y \\ dF_{Y\\mid \\mathbf{x}} = \\int_{\\mathcal Y}y\\cdot f_{Y\\mid \\mathbf{x}}(y\\mid \\mathbf{x}) \\ dy = \\int_{\\mathcal Y}y\\cdot\\frac{f_{Y, \\mathbf{X}}(y,\\mathbf{x})}{f_{\\mathbf{X}}(\\mathbf{x})}\\ dy\\] This conditional expectation maps values from the sample space of \\(\\mathbf{X}\\) to the sample space for \\(Y\\). In this sense, \\(\\text{E}\\left[Y\\mid \\mathbf{X}= \\mathbf{x}\\right]\\) is a function mapping \\(\\mathcal X \\mapsto \\mathcal Y\\). \\(\\text{E}\\left[Y\\mid \\mathbf{X}= \\mathbf{x}\\right]\\) is not a function of \\(y\\), as it is calculated via integrating over all values of \\(y\\in\\mathcal Y\\). Following Angrist and Pischke (2008) and Hansen (2022), we name this function.\n\nDefinition 5.1 If \\((Y, \\mathbf X)\\sim F_{Y,\\mathbf{X}}\\), then the conditional expectation function (CEF) \\(\\hat Y:\\mathcal X \\to \\mathcal Y\\) is defined as \\[\\hat Y(X) = \\text{E}\\left[Y\\mid \\mathbf{X}\\right]. \\] The CEF is an expectation, so observations of \\(Y\\) are bound to deviate from it. We will define this deviation as the CEF error \\(\\varepsilon_{c} = Y - \\hat Y(X)\\).\n\nIn the height example, \\(\\varepsilon_{c}\\) captured how much a child’s height differed from the trend given by \\(\\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\).\n\nProposition 5.1 (Properties of CEF Error) The CEF error is:\n\nMean independent of \\(\\mathbf{X}\\), \\(\\text{E}\\left[\\varepsilon_{c}\\mid\\mathbf{X}\\right] = 0\\);\nUncorrelated with \\(\\mathbf{X}\\), \\(\\text{E}\\left[\\varepsilon_{c}\\mathbf{X}\\right] = \\mathbf{0}\\);\nUncorrelated with any function \\(h(\\mathbf{X})\\), \\(\\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X})\\right] = 0\\)\n\n\n\nProof. \\[\\begin{align*}\n\\text{E}\\left[\\varepsilon_{c}\\mid\\mathbf{X}\\right] &= \\text{E}\\left[Y - \\hat Y(X)\\mid \\mathbf{X}\\right]\\\\\n& =  \\text{E}\\left[Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right] \\mid \\mathbf{X}\\right] \\\\\n& = \\text{E}\\left[Y\\mid \\mathbf{X}\\right] - \\text{E}\\left[\\text{E}\\left[Y\\mid \\mathbf{X}\\right] \\mid \\mathbf{X}\\right] & (\\text{E}\\left[\\cdot | \\mathbf{X}\\right]\\text{ linear})\\\\\n& = \\text{E}\\left[Y\\mid \\mathbf{X}\\right] - \\text{E}\\left[Y\\mid \\mathbf{X}\\right] & (\\text{Law of Iterated Expectations})\\\\\n& = 0.\n\\end{align*}\\] \\(\\mathbf{X}\\) and \\(\\varepsilon_{c}\\) being uncorrelated is a consequence of mean independence. For some \\(h(\\mathbf{X})\\), we have \\[\\begin{align*}\n\\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X})\\right] & = \\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X}) \\text{E}\\left[\\varepsilon_{c}\\mid\\mathbf{X}\\right]\\right] & (\\text{Law of Iterated Expectations})\\\\\n& = \\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X})\\cdot 0\\right] & (\\text{E}\\left[\\varepsilon_{c}\\mid\\mathbf{X}\\right] = 0)\\\\\n& = \\text{E}\\left[0\\right]\\\\\n& = 0.\n\\end{align*}\\] space\n\nWe are not assuming that \\(\\text{E}\\left[\\varepsilon_{c}\\mid\\mathbf{X}\\right] = 0\\). This equality holds by the definition of the CEF.\nSo why restrict our attention to the CEF? Perhaps there are other functions \\(g:\\mathcal X\\to\\mathcal Y\\) which is better at predicted \\(Y\\) than \\(\\hat Y(\\mathbf{X})\\). It turns out that \\(\\hat Y(\\mathbf{X})\\) is the function which minimizes the MSE which arises from predicted \\(Y\\).\n\nProposition 5.2 (CEF Minimizes MSE) For some arbitrary \\(g:\\mathcal X\\to\\mathcal Y\\), \\[ \\hat Y(\\mathbf{X}) = \\text{E}\\left[Y\\mid \\mathbf{X}\\right] = \\mathop{\\mathrm{argmin}}_{g}\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right].\\]\n\n\nProof. \\[\\begin{align*}\n\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right] & = \\text{E}\\left[(Y-g(\\mathbf{X}) + 0)^2\\right]\\\\\n& = \\text{E}\\left[(Y-g(\\mathbf{X}) + (\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - \\text{E}\\left[Y\\mid \\mathbf{X}\\right]))^2\\right] & (0 = \\text{E}\\left[Y\\mid \\mathbf{X}\\right] - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])\\\\\n& = \\text{E}\\left[(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right]) + (\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X})))^2\\right]\\\\\n& = \\text{E}\\left[(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2 + 2(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X})) + (\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right]\\\\\n& = \\text{E}\\left[(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2\\right] + 2\\text{E}\\left[(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X})) \\right] + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - h(\\mathbf{X}))^2\\right]\\\\\n& = E{(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2} + 2\\text{E}\\left[\\varepsilon_{c}(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X})) \\right] + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right] & (\\varepsilon_{c} = Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])\n\\end{align*}\\] If we define \\(h(\\mathbf{X}) = \\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X})\\), we can use the fact that \\(\\varepsilon_{c}\\) is uncorrelated with any function of \\(\\mathbf{X}\\). \\[\\begin{align*}\n\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right] & = E{(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2} + 2\\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X})\\right] + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right]\\\\\n\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right] & = E{(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2} + 2\\cdot 0 + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right] & (\\text{E}\\left[\\varepsilon_{c} h(\\mathbf{X})\\right] = 0)\\\\\n\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right] & = E{(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2} + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right]\n\\end{align*}\\] Only the second term includes the variable we are minimizing over, so \\[ \\mathop{\\mathrm{argmin}}_{h}\\text{E}\\left[(Y-g(\\mathbf{X}))^2\\right] =\\mathop{\\mathrm{argmin}}_{h}\\left\\{E{(Y - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2} + \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right]\\right\\} = \\mathop{\\mathrm{argmin}}_h \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right],\\] where \\(\\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right] \\ge 0\\) because we are squaring a quantity. If we take \\(g(\\mathbf{X}) = \\hat Y(X) = \\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\), we have \\[ \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - g(\\mathbf{X}))^2\\right] = \\text{E}\\left[(\\text{E}\\left[Y\\mid \\mathbf{X}\\right] - \\text{E}\\left[Y\\mid \\mathbf{X}\\right])^2\\right] = \\text{E}\\left[0\\right]=0.\\] Therefore the CEF does minimize the MSE in question.\n\nThis results makes the CEF the optimal candidate for predicting \\(Y\\) given \\(\\mathbf{X}= \\mathbf{x}\\) in a decision theoretic sense (taking the loss function to be quadratic), but in practice we don’t actually know the precise form of the CEF. When \\((X,Y) \\sim N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\), we saw the CEF is linear, but this needn’t be the case.\n\nExample 5.2 Define the following density on the sample space \\(\\mathcal X\\times \\mathcal Y = [0,2]\\times[2,4]\\):\n\\[f_{X,Y}(x,y)=\\frac{1}{8}(6-x-y)\\] The marginal density of \\(X\\) is \\[ f_X(x) = \\int_{\\mathcal Y}f_{X,Y}(x,y)\\ dy = \\int_2^4\\frac{1}{8}(6-x-y)\\ dy = \\frac{1}{8}(6-2x),\\] and the conditional density of \\(Y\\mid X = x\\) is \\[ f_{Y\\mid x}(y\\mid x) = \\frac{f_{X,Y}(x,y)}{f_{X}(x)} = \\frac{\\frac{1}{8}(6-x-y)}{\\frac{1}{8}(6-2x)} = \\frac{6-x-y}{6-2x}\\] Using this to calculate the expectation \\(\\text{E}\\left[Y \\mid X = x\\right]\\) gives \\[\\text{E}\\left[Y \\mid X = x\\right] = \\int_{\\mathcal Y} y\\cdot f_{Y\\mid x}(y\\mid x)\\ dy= \\int_2^4 y\\frac{6-x-y}{6-2x}\\ dy = \\frac{26-9x}{9-3x}.\\]\n\n\nShow code which generates figure\ntibble(x = seq(0, 2, length = 1000)) %>% \n  mutate(y = (-9*x + 26)/(-3*x + 9)) %>% \n  ggplot(aes(x, y)) + \n  geom_line() +\n  labs(x = \"x\", y = \"Conditional Expectation of Y given x\") +\n  theme_minimal()\n\n\n\n\n\nFigure 5.5: A nonlinear CEF function\n\n\n\n\n\nIn order to give the CEF some form, we will approximate it with a linear function (which may hold for certain \\(f_{\\mathbf{X},Y}\\)): \\[\\text{E}\\left[Y\\mid \\mathbf{X}\\right] =  \\mathbf{X}\\boldsymbol{\\beta}.\\] Henceforth, we will assume that \\(\\mathbf{X}\\) includes a column of 1’s such that \\(\\mathbf{X}\\boldsymbol{\\beta}\\) includes an intercept term. We will take the coefficient \\(\\boldsymbol{\\beta}\\) to be that which gives the best linear prediction of \\(Y\\) given \\(\\mathbf{X}\\). \\[ \\boldsymbol{\\beta}= \\mathop{\\mathrm{argmin}}_{\\mathbf{b}}\\text{E}\\left[(Y-\\mathbf{X}\\mathbf b)^2\\right]\\] The error associated with this projection is the linear projection error, \\(\\varepsilon_{\\ell} = Y-\\mathbf{X}\\mathbf b\\). The linear projection error is not the same as the conditional expectation error. It is only the case that \\(\\varepsilon_{\\ell} = \\varepsilon_c\\) if the CEF is truly linear.\nTaking the definition of \\(\\boldsymbol{\\beta}\\) to be a parameterization, we can define our first model. We’ll follow the naming convention of Hansen (2022).\n\nDefinition 5.2 The linear projection (CEF) model is defined as \\(\\mathcal P_\\text{LP} = \\{P_\\boldsymbol{\\beta}\\mid \\boldsymbol{\\beta}\\in \\mathbb R^{k+1}\\}\\), where \\[\\begin{align*}\nP_\\boldsymbol{\\beta}&= \\{F_{\\mathbf{X},Y} \\mid \\text{E}\\left[Y\\mid \\mathbf{X}\\right]= \\mathbf{X}\\boldsymbol{\\beta}\\},\\\\\n\\boldsymbol{\\beta}&= \\mathop{\\mathrm{argmin}}_{\\mathbf{b}}\\text{E}\\left[(Y-\\mathbf{X}\\mathbf b)^2\\right],\\\\\n\\mathbf{X}& = (1, X_2, \\ldots, X_K).\n\\end{align*}\\]\n\nThis model is not regular, as each element \\(P_\\boldsymbol{\\beta}\\) is itself a collection of distributions. As the following example highlights, it won’t be possible to identify the underlying \\(F_{\\mathbf{X},Y}\\), only \\(\\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\). Consequently, each element of \\(\\mathcal P_\\text{LP}\\) is a collection of distributions with a common \\(\\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\).\n\nExample 5.3 (Exercise in Identification) Suppose \\((X,Y)\\sim N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\). In this case, the CEF is actually linear and given as \\[\\text{E}\\left[Y\\mid X\\right] = \\mu_Y + \\frac{\\sigma_{Y}}{\\sigma_{X}}\\rho(x - \\mu_X).\\] We can define many normal distributions with a common CEF. For example, if we have \\(\\rho = \\mu_x = \\mu_y = \\rho' = \\mu_x' = \\mu_y' = 1\\), \\(\\sigma_X = 1\\), \\(\\sigma_Y = 2\\), \\(\\sigma_X' = 2\\), and \\(\\sigma_Y' = 4\\), then \\[\\mu_Y + \\frac{\\sigma_{Y}}{\\sigma_{X}}\\rho(x - \\mu_X) = \\mu_Y' + \\frac{\\sigma_{Y}'}{\\sigma_{X}'}\\rho'(x - \\mu_X') = \\frac{1}{2} +x.\\] This means that \\(\\boldsymbol{\\beta}= (1/2,1)\\) for both distributions. This problem is reminiscent of systems of equations in that we have so many more variables than equations, that there are infinite possibilities (remember this). This is also just the tip of the iceberg when considering how many elements are included in \\(P_{(1/2,1)}\\). It isn’t just all the Gaussian distributions where the CEF is \\(\\frac{1}{2} +x\\). It isn’t all the distributions with a linear CEF which is \\(\\frac{1}{2} +x\\). It is all the distributions for which the best linear approximation of the CEF is \\(\\frac{1}{2} +x\\) (which happens to include the previous groups). Is this an issue? Well not really. We aren’t concerned with the joint distribution \\(F_{\\mathbf{X},Y}\\), as the only thing with any bearing on prediction here is \\(\\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\) (we made no assumptions about \\(F_{\\mathbf{X},Y}\\) when proving that the CEF minimizes MSE). In the event we did want to identify \\(F_{\\mathbf{X},Y}\\), we would need to impose additional assumptions on \\(\\mathcal P_\\text{LP}\\). Consider the following assumptions:\n\n\\((X,Y)\\sim N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\)\n\\(\\mu_X = 0\\)\n\\(\\sigma_X = \\sigma_Y = 1\\)\n\nIn this case, \\[\\text{E}\\left[Y\\mid X\\right] = \\mu_Y + \\rho x = \\mu_Y + \\frac{\\text{Cov}\\left(X,Y\\right)}{\\underbrace{\\sigma_X \\sigma_Y}_{1\\cdot 1}}x = \\underbrace{\\mu_Y}_{\\beta_1} +\\underbrace{\\text{Cov}\\left(X,Y\\right)}_{\\beta_2}x,\\] Assuming \\((X,Y)\\sim N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\), \\(\\mu_X = 0\\), and \\(\\sigma_X = \\sigma_Y = 1\\), it must be the case that\n\\[\\begin{align*}\n(X,Y) &\\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma),\\\\\n\\boldsymbol \\mu & = [0, \\beta_1]',\\\\\n\\boldsymbol \\Sigma & = \\begin{bmatrix}1 & \\beta_2\\\\ \\beta_2 & 1\\end{bmatrix}.\n\\end{align*}\\]\n\nNow we can turn to the question of identifying \\(\\boldsymbol{\\beta}\\).\n\nProposition 5.3 If \\(\\text{E}\\left[\\mathbf{X}\\mathbf{X}'\\right]\\) is invertible, then the parameter \\(\\boldsymbol{\\beta}\\) is identified in the linear projection (CEF) model.\n\n\nProof. We must show that \\(\\boldsymbol{\\beta}= \\boldsymbol{\\beta}'\\), then is it necessarily the case that \\(P_\\boldsymbol{\\beta}= P_{\\boldsymbol{\\beta}'}\\). By the definition of the linear projection (CEF) model, it suffices to show that \\(\\mathop{\\mathrm{argmin}}_{\\mathbf{b}}\\text{E}\\left[(Y-\\mathbf{X}\\mathbf b)^2\\right]\\) has a solution, and that the solution is unique. \\[\\begin{align*}\n\\boldsymbol{\\beta}&= \\mathop{\\mathrm{argmin}}_{\\mathbf{b}}\\text{E}\\left[(Y-\\mathbf{X}\\mathbf b)^2\\right]\\\\\n& = \\mathop{\\mathrm{argmin}}_{\\mathbf{b}}\\text{E}\\left[Y^2 + 2\\mathbf b(\\mathbf{X}'Y) + (\\mathbf{X}\\mathbf b)'(\\mathbf{X}\\mathbf b)\\right]\\\\\n& = \\mathop{\\mathrm{argmin}}_{\\mathbf{b}} \\left\\{\\text{E}\\left[Y^2\\right] + 2\\mathbf b\\text{E}\\left[\\mathbf{X}'Y\\right] + 2\\mathbf b'\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\mathbf b)\\right\\}\n\\end{align*}\\] The first order condition associated with this problem is \\[ 2\\text{E}\\left[\\mathbf{X}'Y\\right] + 2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] \\boldsymbol{\\beta}= \\mathbf{0}.\\] This is equivalent to \\[\\text{E}\\left[\\mathbf{X}'Y\\right] = \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] \\boldsymbol{\\beta}.\\] We now use the assumption that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible to solve for a unique solution for \\(\\boldsymbol{\\beta}\\): \\[ \\boldsymbol{\\beta}= \\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right].\\] Therefore, \\(\\boldsymbol{\\beta}\\) is identified.\n\nTo illustrate how the assumption that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible leads to identification, consider what happens when it does not hold. In \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is not invertible, than \\(\\mathbf{X}'\\mathbf{X}\\) does not have full rank and has infinite solutions. Suppose we have: \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{X}'Y\\right] & = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\\\\\n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] & = \\begin{bmatrix} 1 & 1\\\\ 0 & 0 \\end{bmatrix}.\n\\end{align*}\\] In this case \\[\\begin{align*}\n&\\text{E}\\left[\\mathbf{X}'Y\\right] = \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] \\boldsymbol{\\beta}\\\\\n\\implies &\\begin{bmatrix} 1 & 1\\\\ 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\\\\n\\implies & \\beta_1 + \\beta_2 = 1.\n\\end{align*}\\] This final equation has an infinite number of solutions. If two of those solutions happen to be \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\beta}'\\), then \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} = P_{\\boldsymbol{\\beta}'}\\) despite \\(\\boldsymbol{\\beta}\\neq\\boldsymbol{\\beta}'\\)."
  },
  {
    "objectID": "ols.html#structural-models-and-the-linear-model",
    "href": "ols.html#structural-models-and-the-linear-model",
    "title": "5  The Classical Linear Model",
    "section": "5.3 Structural Models and The Linear Model",
    "text": "5.3 Structural Models and The Linear Model\nThe CEF approach to regression aims to describe a characteristic of the joint density \\(f_{Y,\\mathbf{X}}\\). It captures an association between variables, but not a causal link. Econometricians are often concerned with causal links to inform economic policy, something which differentiates econometrics from statistics. This is why the approach to linear regression seen in standard econometrics sources such as Greene (2018), Wooldridge (2010), Hayashi (2011), Wooldridge (2015), and Stock and Watson (2003) take a structural approach to linear regression. Before giving a heuristic definition of “structural”, let’s consider an example due to Reiss and Wolak (2007) and inspired Cobb and Douglas (1928).\n\nExample 5.4 Assume a firms output \\(Y\\) is related to labor input \\(L\\) and capital input \\(K\\) according to \\[Q = AL^{\\beta}K^{\\alpha}.\\] The total factor of productivity is \\(A\\), while \\(L\\) and \\(K\\) are the elasticity of output with respect to labor and capital, respectively. The production function can be written as \\[ \\log Q = \\log A + \\beta \\log L + \\alpha \\log K.\\] Now consider the linear regression: \\[ \\log Q = \\log A + \\beta \\log L + \\alpha \\log K + \\varepsilon\\] where \\(\\varepsilon\\) is an error addressing the fact that the linear relationship may not hold perfectly. In this case are \\((\\log A, \\alpha, \\beta)\\) associated with the best linear projection of \\(\\log Q\\) onto \\(\\log L\\) and \\(\\alpha \\log K\\), or do they correspond to the factor of productivity, and elasticities of output? If the latter is the case, then what does \\(\\varepsilon\\) represent in the context of the deterministic theoretical relationship \\(Q = AL^{\\beta}K^{\\alpha}\\)? It will turn out that for the coefficients of the best linear projection to coincide with the economic interpretation from \\(Q = AL^{\\beta}K^{\\alpha}\\), we will need to impose assumptions about \\(\\varepsilon\\), a step that is one of the defining features of econometrics.\n\nThe key difference between this example and the height example from Galton (1886)’s is that we are now trying to root our linear regression in structure provided by economic theory/intuition, as to enable us to make economic conclusions. Philip Haile distinguishes these approaches in an excellent set of slides. He would classify Galton (1886)’s work as descriptive as it “estimates relationships between observables”. This is opposed to a structural approach which “estimates features of a data generating process (i.e, a model) that are (assumed to be) invariant to the policy changes or other counterfactuals of interest.” This difference is also linked to the error in the linear regression, \\(\\varepsilon\\). As put by Reiss and Wolak (2007):\n\nWhere did the error term in the empirical model come from? The answer to this question is critical because it affects whether… the parameters of the Cobb–Douglas production function [are identified], as opposed to the parameters of the best linear predictor of the logarithm of output given the logarithms of the two inputs [being identified]. In other words, it is the combination of an economic assumption (production is truly Cobb–Douglas) and statistical assumptions (\\(\\varepsilon\\) satisfies certain moment conditions) that distinguishes a structural model from a descriptive one.\n\nIn an effort to beat a dead horse, a final definition of a structural model is due to Goldberger (1972), who simply puts “By structural equation models, I refer to stochastic models in which each equation represents a causal link, rather than a mere empirical association.” None of this is to say that descriptive model is not useful. Just like descriptive statistics give insight into data, a descriptive model (such as the linear projection model) is an excellent way to investigate data, and findings may inform the development of a structural model.\nLet’s now reintroduce linear regression from a structural perspective. We will do so with no assumptions about our model, and amend our definition as we determine which assumptions are required for identification and desirable properties of estimators. The goal of this approach is to emphasize that the assumptions associated with an econometric model aren’t set in stone from the onset. You begin with little to no assumptions, and then determine which assumptions are necessary as you analyze the model and accompanying estimators.\nWe have a vector of \\(K\\) regressors \\(\\mathbf{X}= [X_1,\\ldots,X_K]\\) (assuming \\(X_1 = 1\\) to allow for an intercept), structural parameters \\(\\boldsymbol{\\beta}= [\\beta_1,\\ldots,\\beta_n]'\\) , and some structural error term \\(\\varepsilon\\). The density underlying the model is the joint density between regressors and the error \\(f_{\\mathbf{X},\\varepsilon}\\). The independent variable \\(Y\\) is given as \\[ Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\varepsilon.\\] The major difference between this and the linear projection model is the underlying density for the latter is \\(f_{\\mathbf{X},Y}\\) where \\(\\boldsymbol{\\beta}\\) and \\(\\varepsilon\\) are defined using this density. Now we’re determining \\(Y\\) via some structural parameter \\(\\boldsymbol{\\beta}\\) and the density \\(f_{\\mathbf{X},\\varepsilon}\\). There are many situations in which the realizations of \\(\\varepsilon\\) may not be identically, or independently, distributed, so we need to consider the joint density of \\(\\boldsymbol{\\varepsilon}= (\\varepsilon_1, \\ldots, \\varepsilon_n)\\) where our sample is size \\(n\\). This joint density is \\(f_{\\boldsymbol{\\varepsilon}}\\). The underlying density from which we draw regressors and errors is not \\(f_{\\mathbf{X},\\boldsymbol{\\varepsilon}}\\), as a realization from this distribution would be comprised of \\(K\\) regressors and \\(n\\) errors. What we need is the joint density of \\(\\boldsymbol{\\varepsilon}\\) and \\(n\\) observations of \\(\\mathbf{X}\\), so we need to consider the following random matrix: \\[\\mathbb{X}= \\begin{bmatrix}\\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_i \\\\ \\vdots\\\\ \\mathbf{X}_n\\end{bmatrix}\\] A sample of \\(n\\) observations of \\(K\\) regressors \\(\\mathbf{X}\\) and errors \\(\\boldsymbol{\\varepsilon}\\) is a single realization drawn from the density \\(f_{\\mathbb{X},\\boldsymbol{\\varepsilon}}\\).\n\\[ \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}= \\begin{bmatrix}  \\mathbf{X}_1\\boldsymbol{\\beta}+ \\varepsilon_1 \\\\ \\vdots \\\\ \\mathbf{X}_i\\boldsymbol{\\beta}+ \\varepsilon_i \\\\ \\vdots \\\\  \\mathbf{X}_n\\boldsymbol{\\beta}+ \\varepsilon_n  \\end{bmatrix} = \\begin{bmatrix}  \\beta_0 + \\beta_1X_{21}  + \\cdots +\\beta_KX_{K1}+ \\varepsilon_1 \\\\ \\vdots \\\\ \\beta_0 + \\beta_1X_{2i}  + \\cdots +\\beta_KX_{Ki}+ \\varepsilon_i  \\\\ \\vdots \\\\  \\beta_0 + \\beta_1X_{2n}  + \\cdots +\\beta_KX_{Kn}+ \\varepsilon_n  \\end{bmatrix}\\] We could also write \\(\\mathbb{X}\\) as \\(K\\) column vectors of length \\(n\\), each corresponding to the \\(n\\) observations of each regressor. \\[\\mathbb{X}= \\begin{bmatrix}\\mathbf{X}_1 & \\cdots & \\mathbf{X}_j & \\cdots& \\mathbf{X}_K\\end{bmatrix}.\\] To distinguish between \\(\\mathbf{X}_i\\) (one observation of \\(K\\) regressors) and \\(\\mathbf{X}_j\\) (\\(n\\) observations of one regressor), we will use the indices \\(i\\) and \\(j\\), respectively.2 We will assume that our data is the result of a random sample of observations of regressors \\(\\mathbf{X}_i\\): \\[ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i}.\\] The random sample assumption is essential as it will allow us to apply the LLN and CLT. Finally, we introduce a parameter which dictates the variance of the error \\(\\boldsymbol{\\varepsilon}\\). This will be the PSD matrix \\(\\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right)\\). Now we can define the linear model in the absence of assumptions.\n\nDefinition 5.3 The linear model is defined as \\(\\mathcal P_\\text{LM} = \\{P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K},\\ \\boldsymbol{\\Sigma}\\in \\mathbb R^{n\\times n} \\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} &= \\{F_{\\mathbb{X},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon},\\ \\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right),\\ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i} \\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n],\\\\\n\\boldsymbol{\\varepsilon}& = [\\varepsilon_1, \\ldots, \\varepsilon_n]\\\\\n\\end{align*}\\]\n\nWhen discussing a model \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}}\\in\\mathcal P_\\text{LM}\\), we are implicitly assuming that the specification of the model is correct, and regressors are IID. If the model were not linear than \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}}\\notin\\mathcal P_\\text{LM}\\), which is not our focus at the moment, but is a legitimate concern.\nIn the context of a structural linear model, \\(\\boldsymbol{\\varepsilon}\\) is not simply an approximation error. In introduces a stochastic element to a deterministic economic model. Reiss and Wolak (2007) enumerate four ways that this randomness can be introduced. We will explore these in the context of the Cobb-Douglas production model where \\(\\log Q_i = \\log A + \\beta \\log L_i + \\alpha \\log K_i + \\varepsilon\\) for an observation from firm \\(i\\).\n\nWe may be uncertain about the economic environment at hand.\nAgent uncertainty about the economic environment;\nOptimization errors on the part of economic agents;\nMeasurement errors in observed variables.\n\n\nExample 5.5 Suppose an agent is deciding between purchasing two cars (\\(j=1,2\\)) has a linear utility function \\(u_{ij} = \\mathbf{X}_{ij}\\boldsymbol{\\beta}\\). The vector \\(\\mathbf{x}_{ij}\\) are attributes of car \\(j\\) (size, mpg, make, model, etc.). We do observe their choice of vehicle \\(y_i\\), but cannot observe their utility from the respective vehicles. Assuming agents maximize their utility, then their choice can be defined as \\[y_i = \\begin{cases}\\text{car }1& u_{i1} \\ge u_{i2}\\\\ \\text{car }2& u_{i2} > u_{i1} \\end{cases}.\\] How would we incorporate \\(\\varepsilon\\) into our model? In the linear model the error directly affects the dependent variable, but in this case the (presumable) dependent variable \\(y_i\\) is an indicator. It doesn’t make sense to add a stochastic element to it, as we observe a customer’s choice with no uncertainty.\n\nPeople are inherently heterogeneous in the utility they receive from any product. One agent may live in a city with access to public transit and would not gain much utility from a car, while another agent may live in a rural area and rely on a car to commute to work and run errands. The error term \\(\\varepsilon_i\\) could correct for these differences. It also could be the case that the error is specific to a consumer and a particular vehicle \\(j\\). Maybe consumer \\(i\\)’s is particularly loyal to the manufacturer of car \\(j\\) and they receive more utility as a result. This could be captured with an error \\(\\varepsilon_{ij}\\).\nAn agent may be not have the opportunity to test drive each car before purchasing, so their is some uncertainty as to how much utility they would receive from purchasing it. This uncertainty could be incorporated via \\(\\varepsilon_{ij}\\).\nAn agent may not be perfectly rational and could make a mistake while attempting to maximize their utility. They could purchase car \\(j=2\\) despite the fact that \\(u_{i1} \\ge u_{i2}\\). To correct for this miscalculation, we could include an error \\(\\varepsilon_{ij}\\) such that \\(u_{i1} + \\varepsilon_{i1} \\le u_{i2} + \\varepsilon_{i2}\\)\nWe may not be able to perfectly measure all the variables in the model. If one of the attributes in the vector \\(\\mathbf{x}_{ij}\\) is price, but we only observe MSRP, then we aren’t accounting for the fact that some customers may have purchased their car for a lower price (it could be used, or on sale). This measurement error can be accounted for with \\(\\varepsilon_{ij}\\)\n\nIt’s important to notice how the error term is indexed in each example. Sometimes the error arises because of the agent \\(i\\) (\\(\\varepsilon_i\\)), or the agent and the specific car (\\(\\varepsilon_{ij}\\)). There could also be ways to incorporate an error that is specific to each car, but not agents (\\(\\varepsilon_j\\)). Later on in Section @ref(binary-choice) we will talk about how to estimate models like one.\n\nThe precise interpretation of \\(\\boldsymbol{\\varepsilon}\\) is key if we want to justify the statistical assumptions about \\(\\boldsymbol{\\varepsilon}\\) which Reiss and Wolak (2007) cite as a key player in identification.\n\nRemark. Another classical assumption of linear regression that we have explicitly violated is that \\(\\mathbb{X}\\) is a matrix of constants. In certain settings researchers are able to determine the values before collecting data. For instance, in a laboratory setting you may have enough control over the (observed/sampled) regressors as to be able to record the value of \\(\\mathbf{Y}\\) at predetermined realizations of \\(\\mathbb{X}\\). This is rarely the case in social sciences, the realm in which econometrics exists. For this reason, we treat \\(\\mathbb{X}\\) as random, and the case of fixed regressors as a special case. In practice, this means we need to condition on \\(\\mathbb{X}\\) when considering expectations and variances of quantities related to \\(\\mathbb{X}\\).\n\nIs it still the case that \\(\\boldsymbol{\\beta}\\) is identified when \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible? It turns out that we will need an additional assumption that we got “for free” with the CEF model via Proposition @ref(prp:ceferr), that being that \\(\\boldsymbol{\\varepsilon}\\) and \\(\\mathbf{X}\\) are uncorrelated.\n\nDefinition 5.4 The covariates \\(\\mathbf{X}\\) are weakly exogenous if \\(\\text{E}\\left[X_i\\varepsilon_i\\right] = 0\\). In matrix form this is \\[\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}.\\] Equivalently,3 \\(\\mathbf{X}\\) is weakly exogenous if: \\[\\begin{align*}\n\\text{E}\\left[\\boldsymbol{\\varepsilon}\\right] &= \\mathbf{0}\\\\\n\\text{Cov}\\left(\\mathbf{X},\\varepsilon_i\\right)&=0 & (i=1,\\ldots,n).\n\\end{align*}\\] If this assumption fails, we say \\(\\mathbf{X}\\) is endogenous.\n\nYou may hear this assumption associated with the word “orthogonal”, or known as an orthogonality condition. This is the precise type of assumption that Reiss and Wolak (2007) referred to when talking about the role \\(\\varepsilon\\) plays in structural models. If \\(\\mathbf{X}_1\\) is the column of 1s associated with the intercept \\(\\beta_1\\), then \\(\\text{E}\\left[\\mathbf{X}_1\\boldsymbol{\\varepsilon}\\right] = 0\\).\n\nExample 5.6 (Endogeneity) A classic example in econometrics due to labor economists is estimating the impact that education has on earnings. An early paper to consider this was Griliches and Mason (1972), Card (1995) is perhaps the most famous attempt at estimating this effect (Card (1999) and Card (2001) reviews similar studies and survey approaches to this problem). Economic intuition tells us that the more schooling someone receives, the higher their earnings/salary will be. Professions that are associated with high salaries often require (or are associated with) graduates degrees: doctors need an MD, lawyers need a JD, and business executives often have MBAs. On the opposite side of the spectrum, many white collar jobs require a college diploma, so only having a high school diploma limits a prospective employee’s ability to qualify for certain jobs which traditionally have higher pay. This observation leads us to posit the deterministic relationship: \\[\\log(income_i) = \\beta_1 + \\beta_2\\cdot educ_i,\\] where \\(income_i\\) is an agent \\(i\\)’s annual income and \\(educ_i\\) is years of post-secondary education (we will operate under the assumption that every agent has a high school diploma). There are, of course, other factors impacting earnings (work experience, profession, location of residence, etc.) that are readily observable, but for the purpose of the example we will ignore those. There are of course exceptions to this deterministic relationship. Bill Gates and Mark Zuckerberg both only have high school diplomas,4 but have higher incomes than virtually everyone in the world. To account for this, we introduce the stochastic element \\(\\varepsilon_i\\) to our model. \\[\\log(income_i) = \\beta_1 + \\beta_2\\cdot educ_i + \\varepsilon_i\\] In this case, \\(\\varepsilon_i\\) corresponds to all the other unobservable determinants of earnings. A major unobservable determinant is innate ability. Bill Gates and Mark Zuckerberg make a great deal of money because of their ambition, business acumen, and ability to innovate despite not having a college degree. It’s not really possible to measure something abstract like someone’s ambition, so the best we can do is incorporate it with \\(\\varepsilon_i\\). Is it the case that \\(\\text{E}\\left[\\mathbb{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\) in this case? Most likely not. In all likelihood \\(\\text{E}\\left[educ_i\\cdot\\varepsilon_i\\right]\\neq 0\\), because people who are ambitious and have an innate ability to innovate tend to pursue higher education to further their abilities. If this hypothesis is true, then \\(educ_i\\) is endogenous.\n\nWe also can give a name to the assumption that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible.\n\nDefinition 5.5 The covariates \\(\\mathbf{X}\\) exhibit (perfect) multicollinearity if \\[\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K,\\] which is equivalent to \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) failing to be invertible.\n\nIn the event that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is not invertible, then there exists some linear dependence between the set of covariates \\((1,X_1,\\ldots,X_k)\\), i.e one regressor is a linear function of another. These two assumptions insure that \\(\\boldsymbol{\\beta}\\) is identified for \\(\\mathcal P_\\text{LM}\\).\n\nTheorem 5.1 (Identification of the Linear Model) If \\(\\mathbf{X}\\) is weakly exogenous and does not exhibit multicollinearity, then \\((\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\\) are identified for the linear model \\(\\mathcal P_\\text{LM},\\) and \\(\\beta\\) given as \\[\\boldsymbol{\\beta}= \\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right].\\]\n\n\nProof. Weak exogeneity gives\n\\[\\begin{align*}\n&\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\mathbf{X}'(Y-\\mathbf{X}\\boldsymbol{\\beta})\\right] = \\mathbf{0}& (\\boldsymbol{\\varepsilon}= (Y-\\mathbf{X}\\boldsymbol{\\beta}))\\\\\n\\implies & \\text{E}\\left[\\mathbf{X}'Y\\right]-\\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]= \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\mathbf{X}'Y\\right] = \\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right].\n\\end{align*}\\] We have also assumed that \\(\\mathbf{X}\\) does not exhibit multicollinearity, so \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible. This means \\(\\text{E}\\left[\\mathbf{X}'Y\\right] = \\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) has a unique solution in the form of \\(\\boldsymbol{\\beta}= \\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right].\\) If \\(\\boldsymbol{\\beta}\\) is unique, then \\(\\boldsymbol{\\Sigma}\\) is unique and written as \\[ \\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right) = \\text{Var}\\left(Y - \\mathbf{X}\\boldsymbol{\\beta}\\mid \\mathbb{X}\\right) = \\text{Var}\\left(Y - \\mathbf{X}\\left[\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right]\\right]\\mid \\mathbb{X}\\right).\\] Therefore, if \\((\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})\\neq(\\boldsymbol{\\beta}',\\boldsymbol{\\Sigma}')\\), then \\(\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\neq \\mathbf{X}\\boldsymbol{\\beta}' + \\boldsymbol{\\varepsilon}\\) and \\(\\boldsymbol{\\Sigma}\\neq \\boldsymbol{\\Sigma}\\), so \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}}\\neq P_{\\boldsymbol{\\beta}',\\boldsymbol{\\Sigma}'}\\), meaning our parameters are identified.\n\nThe parameter \\(\\boldsymbol{\\beta}\\) takes the same analytic form as that of the linear projection (CEF) model, but it’s important to remember that they arise from different approaches. We arrived at this form using the relationship between \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\varepsilon}\\) in a structural model, not from defining \\(\\boldsymbol{\\beta}\\) to be the solution to an optimization problem.\n\nExample 5.7 (Multicollinearity) Suppose \\(Y = 1 + 5 X_1 + 2 X_2 + \\varepsilon\\) where \\(X_1 = 3X_2\\) (suppressing the indices \\(i\\) which are not relevant at the moment). This model corresponds to the parameters \\(\\boldsymbol{\\beta}= (1,5,2)\\) We can rewrite our model as \\[ Y = 1 + 5 X_1 + 2 X_2 + \\varepsilon = 1 + 5(3X_2) + 2 X_2 + \\varepsilon = 1 + 0X_1 + 17X_2 + \\varepsilon,\\] so the model also corresponds to parameters \\(\\boldsymbol{\\beta}'=(1,0,17)\\), and our model is not identified.\n\n\nExample 5.8 (Non-Zero Mean Errors) Suppose \\(Y = 1 + 5 X_1 + 2 X_2 + \\varepsilon\\) where \\(\\text{E}\\left[\\varepsilon\\right] = 3\\) and \\(\\text{Var}\\left(\\varepsilon\\mid \\mathbf{X}\\right) = \\sigma^2\\). In this case \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]\\neq 0\\), so we shouldn’t expect that \\(\\boldsymbol{\\beta}\\) is identified. In particular, we won’t be able to identify \\(\\beta_0\\). We can write \\(\\varepsilon = 3 + \\nu\\) for \\(\\text{Var}\\left(\\nu\\mid \\mathbf{X}\\right) = \\sigma^2\\), giving \\[ Y = 1 + 5 X_1 + 2 X_2 + (3 + \\nu) = Y = 4 + 5 X_1 + 2 X_2 + \\nu.\\] So the model can be written with parameters \\(([1,5,2]', \\sigma^2)\\) or with parameters \\(([4,5,2]', \\sigma^2)\\). Therefore the model, in particular \\(\\beta_0\\), is not identified.\n\nWe will consider what happens when \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]=0\\), how this situation arises, and what can be done about it in Section @ref(endogeniety-i-iv-and-2sls). For now, let’s update our model with our identifying assumptions\n\nDefinition 5.6 The (identified) linear model is defined as \\(\\mathcal P_\\text{LM} = \\{P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\boldsymbol{\\Sigma}\\in\\mathbb R^n\\times\\mathbb R^n\\}\\),5 where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} &= \\{F_{\\mathbb{X},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}, \\ \\ \\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right),\\ \\ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i}, \\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K,\\ \\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n].\n\\end{align*}\\]"
  },
  {
    "objectID": "ols.html#ordinary-least-squares",
    "href": "ols.html#ordinary-least-squares",
    "title": "5  The Classical Linear Model",
    "section": "5.4 Ordinary Least Squares",
    "text": "5.4 Ordinary Least Squares\nNow that we have identified our model, we can finally estimate \\(\\boldsymbol{\\beta}\\) using our favorite estimator – ordinary least squares! There are a handful of ways to derive the ordinary least squares estimator, but for now we will focus on two constructions.\nWe want to estimate \\(\\boldsymbol{\\beta}\\) using observations of \\((\\mathbf{Y}, \\mathbb{X})\\), which is the same as saying \\(n\\) observations of \\((Y, \\mathbf{X})\\). By definition, we do not observe realizations of \\(\\boldsymbol{\\varepsilon}\\). We know that \\(\\boldsymbol{\\beta}=\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right]\\), so perhaps we can simply estimate \\(\\boldsymbol{\\beta}\\) using the sample analog of \\(\\left(\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbb{X}'\\mathbf{Y}\\right]\\). This approach is sometimes referred to as the analogy principle (see Goldberger (1991)), and will come up again. Denote the sample moments as \\(\\widehat{\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]}\\) and \\(\\widehat{\\text{E}\\left[\\mathbf{X}'Y\\right]}\\). If we have a sample of size \\(n\\), then \\[\\begin{align*}\n\\widehat{\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]} & = \\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}\\\\\n\\widehat{\\text{E}\\left[\\mathbf{X}'Y\\right]} & = \\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_iY_i\n\\end{align*}\\] Therefore, our estimator is \\[\\hat {\\boldsymbol{\\beta}}(\\mathbb{X}, \\mathbf{Y}) = \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_iY_i\\right).\\]\nWe can also write this in the form of matrices. First we need to expand \\(\\mathbf{X}'\\mathbf{X}\\): \\[\\begin{align*}\n\\mathbf{X}'\\mathbf{X}&= \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_K\\end{bmatrix}\\begin{bmatrix} \\mathbf{X}_1 & \\cdots & \\mathbf{X}_K\\end{bmatrix} = \\begin{bmatrix}\\mathbf{X}_1\\cdot\\mathbf{X}_1 & \\mathbf{X}_1\\cdot\\mathbf{X}_2 & \\cdots & \\mathbf{X}_1\\mathbf{X}_K\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\mathbf{X}_K\\cdot\\mathbf{X}_1 & \\mathbf{X}_K\\cdot\\mathbf{X}_2 & \\cdots & \\mathbf{X}_K\\cdot \\mathbf{X}_k\\end{bmatrix} = \\begin{bmatrix}\\sum_{i=1}^n X_{1,i}^2 & \\sum_{i=1}^n X_{1,i}X_{2,i} & \\cdots & \\sum_{i=1}^n X_{1,i}X_{K,i}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\sum_{i=1}^n X_{K,i}X_{1,i} & \\sum_{i=1}^n X_{K,i}X_{2,i} & \\cdots & \\sum_{i=1}^n X_{K,i}^2\\end{bmatrix}\\\\\n\\end{align*}\\] The expectation is taken element-wise where\n\\[ \\text{E}\\left[\\sum_{i=1}^n X_{j,i}X_{\\ell,i}\\right] = \\sum_{i=1}^n \\text{E}\\left[X_{j,i}X_{\\ell,i}\\right] = n \\text{E}\\left[X_{j,i}X_{\\ell,i}\\right],\\] so applying this to each entry and factoring out the common scalar \\(n\\) gives:\n\\[ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] = n\\begin{bmatrix}\\text{E}\\left[X_1^2\\right] & \\text{E}\\left[X_1X_2\\right] & \\cdots & \\text{E}\\left[X_1X_K\\right]\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\text{E}\\left[X_KX_1\\right] & \\text{E}\\left[X_KX_2\\right] & \\cdots & \\text{E}\\left[X_K^2\\right]\\end{bmatrix}.\\]\nThe sample analog (as a function of random variables, not realizations) is \\[\\begin{align*}\n\\widehat{\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]} &= n\\begin{bmatrix}n^{-1}\\sum_{i=1}^n X_{1,i}^2 & n^{-1}\\sum_{i=1}^n X_{1,i}X_{2,i} & \\cdots & n^{-1}\\sum_{i=1}^n X_{1,i}X_{K,i}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ n^{-1}\\sum_{i=1}^n X_{K,i}X_{1,i} & n^{-1}\\sum_{i=1}^n X_{K,i}X_{2,i} & \\cdots & n^{-1}\\sum_{i=1}^n X_{K,i}^2\\end{bmatrix}\\\\&=\\begin{bmatrix}\\sum_{i=1}^n X_{1,i}^2 & \\sum_{i=1}^n X_{1,i}X_{2,i} & \\cdots & \\sum_{i=1}^n X_{1,i}X_{K,i}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\sum_{i=1}^n X_{K,i}X_{1,i} & \\sum_{i=1}^n X_{K,i}X_{2,i} & \\cdots & \\sum_{i=1}^n X_{K,i}^2\\end{bmatrix}\\\\\n& = \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_K\\end{bmatrix}\\begin{bmatrix} \\mathbf{X}_1 & \\cdots & \\mathbf{X}_K\\end{bmatrix}\\\\\n& = \\mathbb{X}'\\mathbb{X}\n\\end{align*}\\] We can perform the analogous inspection on \\(\\text{E}\\left[\\mathbf{X}'Y\\right]\\) and conclude that \\(\\widehat{\\text{E}\\left[\\mathbf{X}'Y\\right]} = \\mathbb{X}\\mathbf{Y}\\). Therefore, in matrix form, our estimator is \\[\\hat {\\boldsymbol{\\beta}} =(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\mathbf{Y}\\]\n\nExample 5.9 (The Simple Linear Model) In the event \\(K = 2\\), we have \\(Y = \\beta_1 + \\beta_2 X + \\varepsilon\\) for a single non trivial regressor \\(X\\). The random vector of regressors is \\(\\mathbf{X}= [\\mathbf 1, X]\\). Let’s calculate the population parameters \\(\\boldsymbol{\\beta}\\) in this setting. \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] & = \\begin{bmatrix}1 & \\text{E}\\left[X\\right]\\\\\\text{E}\\left[X\\right] & \\text{E}\\left[X^2\\right] \\end{bmatrix}\\\\\n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1} & =\\frac{1}{\\underbrace{\\text{E}\\left[X^2\\right] - \\text{E}\\left[X\\right]^2}_{\\text{Var}\\left(X\\right)}} \\begin{bmatrix}\\text{E}\\left[X^2\\right] & -\\text{E}\\left[X\\right]\\\\-\\text{E}\\left[X\\right] & 1 \\end{bmatrix} = \\begin{bmatrix}\\text{E}\\left[X^2\\right]/\\text{Var}\\left(X\\right) & -\\text{E}\\left[X\\right]/\\text{Var}\\left(X\\right)\\\\-\\text{E}\\left[X\\right]/\\text{Var}\\left(X\\right) & 1/\\text{Var}\\left(X\\right) \\end{bmatrix}\\\\\n\\text{E}\\left[\\mathbf{X}'Y\\right] & = \\begin{bmatrix} \\text{E}\\left[Y\\right] \\\\ \\text{E}\\left[XY\\right] \\end{bmatrix}\\\\\n\\boldsymbol{\\beta}& = \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right] = \\frac{1}{\\text{Var}\\left(X\\right)} \\begin{bmatrix} \\text{E}\\left[X^2\\right]\\text{E}\\left[Y\\right] -\\text{E}\\left[X\\right]\\text{E}\\left[XY\\right] & -\\text{E}\\left[X\\right]\\text{E}\\left[Y\\right] + \\text{E}\\left[XY\\right] \\end{bmatrix}\\\\\n\\beta_2 &= \\frac{\\text{E}\\left[XY\\right] -\\text{E}\\left[X\\right]\\text{E}\\left[Y\\right]}{\\text{Var}\\left(X\\right)} \\\\ &= \\frac{\\text{Cov}\\left(X,Y\\right)}{\\text{Var}\\left(X\\right)}\\\\\n\\beta_1 & = \\frac{\\text{E}\\left[X^2\\right]\\text{E}\\left[Y\\right] -\\text{E}\\left[X\\right]\\text{E}\\left[XY\\right] }{\\text{Var}\\left(X\\right)} \\\\\n      & = \\frac{(\\text{E}\\left[X^2\\right] - \\text{E}\\left[X\\right]^2 + \\text{E}\\left[X\\right]^2)\\text{E}\\left[Y\\right] -\\text{E}\\left[X\\right]\\text{E}\\left[XY\\right] }{\\text{Var}\\left(X\\right)}\\\\\n       & = \\frac{(\\text{Var}\\left(X\\right)+ \\text{E}\\left[X\\right]^2)\\text{E}\\left[Y\\right] -\\text{E}\\left[X\\right]\\text{E}\\left[XY\\right] }{\\text{Var}\\left(X\\right)}\\\\\n       & = \\text{E}\\left[Y\\right] - \\text{E}\\left[X\\right]\\cdot \\frac{\\text{Cov}\\left(X,Y\\right)}{\\text{Var}\\left(X\\right)}\\\\\n       & = \\text{E}\\left[Y\\right] - \\beta_2\\text{E}\\left[X\\right]\n\\end{align*}\\] The estimator calculated using the analogous moments is the familiar OLS estimator for the simple linear model: \\[\\begin{align*}\n\\hat\\beta_2 & = \\frac{\\widehat{\\text{Cov}}(X,Y) }{\\widehat{\\text{Var}}(X)} = \\frac{(1/n)\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{(1/n)\\sum_{i=1}^n(X_i - \\bar X)^2} = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n(X_i - \\bar X)^2}\\\\\n\\hat\\beta_1 & = \\bar Y - \\hat\\beta_2 \\bar X\n\\end{align*}\\]\n\nAn alternate way of arriving at this estimator is possible by solving the least squares problem that we encountered with the linear projection model.\n\\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}} &= \\mathop{\\mathrm{argmin}}_{\\mathbf b} \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\mathbf b)^2\\\\\n  & = \\mathop{\\mathrm{argmin}}_{\\mathbf b} \\left\\{(\\mathbf{Y}- \\mathbb{X}\\mathbf b)'(\\mathbf{Y}- \\mathbb{X}\\mathbf b)\\right\\} \\\\\n  & = \\mathop{\\mathrm{argmin}}_{\\mathbf b} \\left\\{ \\mathbf{Y}' \\mathbf{Y}- 2\\mathbf{Y}\\mathbb{X}\\mathbf b +\\mathbf b' \\mathbb{X}' \\mathbb{X}\\mathbf b \\right\\}\n\\end{align*}\\] The first order condition is \\[\\begin{align*}\n&-2\\mathbb{X}'\\mathbf{Y}+ 2\\mathbb{X}'\\mathbb{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\\\\\n\\implies &\\hat{\\boldsymbol{\\beta}} = (\\mathbb{X}'\\mathbb{X})^{-1}(\\mathbb{X}\\mathbf{Y})\n\\end{align*}\\] This is the same estimator we derived with the analogy principle. In order to reference estimates given by our estimator, we’ll need to introduce notation for realizations of \\((\\mathbb{X}, \\mathbf{Y}, \\boldsymbol{\\varepsilon})\\), which makes notation even more complicated. The following table presents how we will write realizations of random quantities, along with recapping the notation for \\(\\mathcal P_{LM}\\).\n\n\n\n\n\n\n\n\n\n\nRandom Quantity\nType\nDimension\nDefinition\nRealization/Observation\n\n\n\n\n\\(\\mathbf{X}\\)\nvector\n\\(1\\times K\\)\ndependent variables\n\\(\\mathbf{x}\\)\n\n\n\\(Y\\)\nvariable\n\\(1\\times 1\\)\nindependent variable\n\\(y\\)\n\n\n\\(\\boldsymbol{\\varepsilon}\\)\nvector\n\\(n\\times 1\\)\nvector of errors\n\\(\\mathbf{e}\\)\n\n\n\\(\\mathbf{Y}\\)\nvector\n\\(n\\times 1\\)\nvector of independent variables\n\\(\\mathbf{y}\\)\n\n\n\\(\\mathbb{X}\\)\nmatrix\n\\(n\\times K\\)\nmatrix of dependent variables\n\\(\\mathbf X\\)\n\n\n\\(\\mathbf{X}_i\\)\nvector\n\\(1\\times K\\)\n\\(i\\)th row of \\(\\mathbb{X}\\)\n\\(\\mathbf{x}_i\\)\n\n\n\\(\\mathbf{X}_j\\)\nvector\n\\(n \\times 1\\)\n\\(j\\)th row of \\(\\mathbb{X}\\)\n\\(\\mathbf{x}_j\\)\n\n\n\nThis notation is by no means standard, an notation unfortunately varies widely across sources. The only piece of notation which conflicts is the random vector of regressors \\(\\mathbf{X}\\) and the realization of \\(\\mathbb{X}= \\mathbf{X}\\). Whenever it is unclear which is being referenced, I will try to be specific.\n\nDefinition 5.7 The ordinary least squares (OLS) estimator is defined as \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_\\text{OLS}(\\mathbb{X},\\mathbf{Y}) &= (\\mathbb{X}'\\mathbb{X})^{-1}(\\mathbb{X}\\mathbf{Y})\n= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_iY_i\\right)\n\\end{align*}\\] An realization of this estimator (an estimate) is \\[\\begin{align*}\n\\hat{\\mathbf b}_\\text{OLS} = \\hat{\\boldsymbol{\\beta}}_\\text{OLS}(\\mathbf{X},\\mathbf{y}) &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}\\mathbf{y})\n= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i'\\mathbf{x}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_iy_i\\right)\n\\end{align*}\\] and will exist when the inverse \\((\\mathbf{X}'\\mathbf{X})^{-1}\\) exists.\n\n\nExample 5.10 We can easily write a function which calculates an OLS estimate given a random sample.\n\nOLS <- function(y, X){\n  K <- ncol(X)\n  if(det(t(X) %*% X) == 0) {stop(\"rank(X'X) < K\")}\n  \n  hat_beta <- solve(t(X) %*% X) %*% t(X) %*% y\n  rownames(hat_beta) <- paste(\"β\", 1:K, \" estimate\", sep = \"\")\n  return(hat_beta)\n}\n\nLet’s randomly generate a sample to test our function. Suppose we have a sample of size \\(n=1000\\) from a linear model where \\(X_1 \\overset{iid}{\\sim}\\text{Uni}(0,10)\\), \\(\\varepsilon \\overset{iid}{\\sim}\\text{Uni}(-5,5)\\), \\(X_1 \\perp \\varepsilon\\), and \\(Y = 2 + 4X_1 + \\varepsilon\\). Because \\(\\varepsilon\\) and \\(X\\) are independent, we’ve specified their respective marginal densities instead of joint density.\n\nbeta <- c(2,4)\nn <- 25\nx1 <- runif(n, 0, 10)\ne <- runif(n, -5, 5)\nX <- cbind(1, x1)\ny <- X %*% beta + e\n\nBefore we estimate our model, we should think about whether our model satisfies the assumptions that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible and \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\). The first assumption holds because we only have one non-trivial independent variable (the trivial one is constant 1 which gives the intercept), and it is not a constant (so it cannot be a linear function of the constant 1). We have \\(\\text{E}\\left[\\varepsilon\\right] = 0\\), so by independence we have\n\\[\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\text{E}\\left[X_1\\varepsilon\\right] = \\text{E}\\left[X_1\\right]\\text{E}\\left[\\varepsilon\\right] =\\text{E}\\left[X_1\\right]\\cdot0 = 0\\] We can actually use the LLN to consistently estimate \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) and \\(\\text{E}\\left[X_1'\\varepsilon\\right]\\), and see if our estimates satisfy our assumptions. For a sufficiently large \\(n\\), we should see that \\[\\begin{align*}\n\\text{rank}\\left(\\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i'\\mathbf{x}_i\\right) &\\approx K\\\\\n\\left(\\frac{1}{n}\\sum_{i=1}^n x_{1i}'e_i\\right) &\\approx 0\n\\end{align*}\\] The sample size \\(n=25\\) may be a bit too modest, so let’s generate a new sample of size \\(n'=100,000\\).\n\nn_prime <- 100000\nx1_prime <- runif(n_prime, 0, 10)\ne_prime <- runif(n_prime, -5, 5)\nX_prime <- cbind(rep(1, n_prime), x1_prime)\n\n# Sample rank of X'X\nrankMatrix((t(X_prime) %*% X_prime)/n_prime)[1]\n\n[1] 2\n\n# Sample mean of X'ε\nmean(x1_prime*e_prime)\n\n[1] -0.06257445\n\n\nIt appears our assumptions are met, so we can go ahead with estimation. It is important to recognize that in practice we don’t observe the realizations \\(\\boldsymbol{\\varepsilon}\\), so calculating the sample analog of \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]\\) is not possible with real data, but it is a good gut check when conducting simulations.\n\nbeta_hat <- OLS(y,X)\nbeta_hat\n\n                 [,1]\nβ1 estimate 0.3890429\nβ2 estimate 4.1678826\n\n\n\n\nShow code which generates figure\ntibble(x = X[,2], \n       y = y\n) %>% \n  ggplot(aes(x,y)) +\n  geom_point(aes(color = \"(Observed) Sample\")) +\n  geom_abline(aes(intercept = beta[1], slope = beta[2], color = \"(Unobserved) Population Model\")) +\n  geom_abline(aes(intercept = beta_hat[1], slope = beta_hat[2], color = \"Estimated Model\")) +\n  theme_minimal() +\n  labs(colour=\"\") +\n  scale_colour_manual(values=c(\"black\", \"blue\", \"red\")) +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 5.6: Our observed data (in black) is drawn from the unobserved population model (blue). We use this data to estimate the model, and the line associated with this estimate is shown in red.\n\n\n\n\n\n\nExample 5.11 (Linear Projection Model, OLS) OLS can be used in the context of \\(\\mathcal P_\\text{LM}\\) to estimate the best linear projection between two random variables \\((Y,\\mathbf{X})\\). OLS was the method used by Pearson and Lee (1903) to estimate the relationship between height and genetics. We can replicate this work with an data set based on the original data collected by Pearson and Lee (1903).\n\nheight_df <- read_csv(\"data/height_data.csv\")\nX <- as.matrix(height_df$Father)\ny <- height_df$Son\nOLS(y,X)\n\n                [,1]\nβ1 estimate 1.013914\n\n\n\n\nExample 5.12 (OLS Estimate Does not Exist) It is possible that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible while realized value \\(\\mathbb{X}'\\mathbb{X}= \\mathbf{X}'\\mathbf{X}\\) is not invertible. For example, if our model is \\(Y = \\beta_1 X_1 + \\epsilon\\) where \\(X \\sim \\text{Binom}(4, 0.5)\\),6 we have \\[\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] = \\text{E}\\left[X_1^2\\right] = 5.\\] If we observe an independent sample of size \\(n=2\\) generated from this model, we may observe something like \\(\\mathbf{x}_1 = [2,2]\\). In this case \\[\\mathbf{x}'\\mathbf{x}= \\begin{bmatrix} 4 &4 \\\\ 4 & 4\\end{bmatrix},\\] which is certainly not invertible. Furthermore, the probability we draw this sample is \\[\\Pr(\\mathbf{x}= [2,2]) = [\\Pr(X = 2)]^2 = (0.375)^2 = 0.140625,\\] so the chances this happen are not trivial. However, \\(n\\) is usually much greater than \\(2\\), and as \\(n\\to\\infty\\) the probability that \\(\\mathbf{X}'\\mathbf{X}\\) is not invertible will go to zero. This is a direct consequence of the LLN:\n\\[\\mathbf{X}'\\mathbf{X}= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i'\\mathbf{x}_i\\right) \\overset{p}{\\to}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] \\]\n\n\nRemark. Whether it is easier to write our terms related to \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) in terms of matrices or sums of vectors will depend on the result we are building to or trying to prove. This can be a bit confusing, so here is a list of various equalities (many of which imply others), that we will use: \\[\\begin{align*}\n\\mathbb{X}'\\mathbb{X}& = \\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\\\\n\\mathbb{X}'\\mathbf{Y}& = \\sum_{i=1}^n \\mathbf{X}_i'Y_i\\\\\n\\mathbb{X}'\\boldsymbol{\\varepsilon}& = \\sum_{i=1}^n \\mathbf{X}_i'\\varepsilon_i\\\\\n\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}& = \\sum_{i=1}^n \\varepsilon_i^2\n\\end{align*}\\] An important result which follows from the first equality is \\(\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right] = n \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) in the event that \\(\\mathbf{X}_i\\) are independent."
  },
  {
    "objectID": "ols.html#properties-of-ols",
    "href": "ols.html#properties-of-ols",
    "title": "5  The Classical Linear Model",
    "section": "5.5 Properties of OLS",
    "text": "5.5 Properties of OLS\nAs likely anticipated, the OLS estimator has a number of desirable properties under certain assumptions, some of which we will make in addition to weak exogeneity and lack of multicollinearity. The first property we establish is consistency.\n\nProposition 5.4 (Consistency) Suppose \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\) and \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), then \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\).\n\n\nProof. We have \\(\\boldsymbol{\\beta}= \\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right)^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right]\\), where \\(\\boldsymbol{\\beta}\\) is guaranteed to exist and be unique using our assumptions. As \\(n\\to\\infty\\), \\(\\mathbf{X}'\\mathbf{X}\\) will be invertible with probability one, so \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\) will exist (with probability one). We can write our estimator as \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_\\text{OLS} &= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'Y_i\\right)\\\\\n&= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'(\\mathbf{X}_i\\boldsymbol{\\beta}+ \\varepsilon_i)\\right) & (Y_i = \\mathbf{X}_i\\boldsymbol{\\beta}+ \\varepsilon_i)\\\\\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\boldsymbol{\\beta}\\right) + \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)\\\\\n& = \\boldsymbol{\\beta}\\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)}_{\\mathbf I} + \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)\\\\\n& = \\boldsymbol{\\beta}+ \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)\n\\end{align*}\\] We can apply the LLN along with the continuous mapping theorem (applied to the inverse term) and Slutky’s theorem (applied to the product of convergent sequences) to conclude, \\[ \\hat{\\boldsymbol{\\beta}}_\\text{OLS} =  \\boldsymbol{\\beta}+ \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}}_{\\overset{p}{\\to}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]}\\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)}_{\\overset{p}{\\to}\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]} \\overset{p}{\\to}\\boldsymbol{\\beta}+ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\underbrace{\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]}_\\mathbf{0}= \\boldsymbol{\\beta}.\\] Therefore \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\), where the limit \\(\\boldsymbol{\\beta}\\) is unique by identification.\n\n\nExample 5.13 Return to the model \\(X_1 \\overset{iid}{\\sim}\\text{Uni}(0,10)\\), \\(\\varepsilon \\overset{iid}{\\sim}\\text{Uni}(-5,5)\\), \\(X_1 \\perp \\varepsilon\\) (implying \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\)), and \\(Y = 2 + 4X_1 + \\varepsilon\\). If we estimate this model for samples of increasing size, we should see that our estimates converge to the true values.\n\nbeta <- c(2,4)\n\nbeta_hat1 <- vector(\"numeric\", 9999)\nbeta_hat2 <- vector(\"numeric\", 9999)\nfor(n in 2:10000){\n  x1 <- runif(n, 0, 10)\n  e <- runif(n, -5, 5)\n  X <- cbind(1, x1)\n  y <- X %*% beta + e\n  beta_hat <- OLS(y,X)\n  beta_hat1[n-1] <- beta_hat[1]\n  beta_hat2[n-1] <- beta_hat[2]\n}\n\nIt does appear that as \\(n\\to\\infty\\) we have \\(\\hat{\\beta}_\\text{1,OLS} \\overset{p}{\\to}2\\) and \\(\\hat{\\beta}_\\text{2,OLS} \\overset{p}{\\to}4\\)\n\n\nShow code which generates figure\ntibble(\n  x = rep(2:10000, 2), \n  y = c(beta_hat1, beta_hat2), \n  group = c(rep(\"Estimated β1\", 9999), rep(\"Estimated β2\", 9999))\n) %>% \n  ggplot(aes(x,y)) +\n  geom_line(size = 0.2) +\n  labs(x =\"Sample Size, n\", y = \"Estimated Value\") +\n  facet_wrap(~group) +\n  ylim(0,5) +\n  theme_minimal()\n\n\n\n\n\nFigure 5.7: As the sample size increases, our estimates approach their true values.\n\n\n\n\n\nNow let’s consider whether if (and under what conditions) \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\) unbiased.\n\\[\\begin{align*}\n\\text{E}\\left[\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\right] & = \\text{E}\\left[\\text{E}\\left[\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\right]\\mid \\mathbb{X}\\right] & (\\text{iterated expectations})\\\\\n& = \\text{E}\\left[\\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{Y}\\right]\\mid \\mathbb{X}\\right]\\\\\n& = \\text{E}\\left[\\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'(\\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon})\\right]\\mid \\mathbb{X}\\right] & (\\mathbf{Y}& = \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon})\\\\\n& = \\text{E}\\left[\\text{E}\\left[((\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{X})\\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon})\\right]\\mid \\mathbb{X}\\right]\\\\\n& = \\text{E}\\left[\\text{E}\\left[\\boldsymbol{\\beta}\\right]\\mid \\mathbb{X}\\right] +\\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right]\\\\\n& = \\boldsymbol{\\beta}+  \\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right] & (\\boldsymbol{\\beta}\\text{ is a constant})\\\\\n& \\neq \\boldsymbol{\\beta}\n\\end{align*}\\] Under our current assumption, OLS is has a bias of \\(\\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right]\\). While we are operating under the assumption that \\(\\text{E}\\left[ \\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), this does not imply that \\(\\boldsymbol{\\varepsilon}\\perp\\mathbb{X}\\) (which would mean \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\text{E}\\left[\\boldsymbol{\\varepsilon}\\right]=\\mathbf{0}\\)) For this to happen, we need to impose our third assumption on the linear model.\n\nDefinition 5.8 The random regressors \\(\\mathbf{X}\\) are exogenous if\n\\[\\begin{align*}\n\\text{E}\\left[\\varepsilon_i\\mid \\mathbf{X}\\right] & = 0 &(i=1,\\ldots,n)\n\\end{align*}\\] which is written compactly as \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\).\n\nBy properties of independence and conditional expectation, exogeneity implies weak exogeneity, hence its name alluding to it being a stronger assumption. Technically speaking, we aren’t adding a third assumption, as much as we are strengthening one of our current assumptions.\n\nProposition 5.5 (OLS is Unbiased) Suppose \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\) and \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\), then \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is an unbiased estimator for \\(\\boldsymbol{\\beta}\\).\n\n\nProof. \\[\\text{E}\\left[\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\right] = \\boldsymbol{\\beta}+  \\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right] =\\boldsymbol{\\beta}+  \\text{E}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{0}\\right] = \\boldsymbol{\\beta}\\] Therefore, \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\) is an unbiased estimator for \\(\\boldsymbol{\\beta}\\).\n\n\nDefinition 5.9 The (unbiased) linear model is defined as \\(\\mathcal P_\\text{LM} = \\{P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\boldsymbol{\\Sigma}\\in\\mathbb R^n\\times\\mathbb R^n\\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}} &= \\{F_{\\mathbb{X},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}, \\ \\ \\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right),\\ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i}, \\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K,\\ \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n].\n\\end{align*}\\]\n\n\nExample 5.14 If we go back to our simulated estimates where \\(X_1 \\overset{iid}{\\sim}\\text{Uni}(0,10)\\), \\(\\varepsilon \\overset{iid}{\\sim}\\text{Uni}(-5,5)\\), \\(X_1 \\perp \\varepsilon\\), and \\(Y = 2 + 4X_1 + \\varepsilon\\), we should see that the sample mean of our simulated estimates are approximately equal to the true values \\(\\boldsymbol{\\beta}= (2,4)\\)\n\ncolMeans(cbind(beta_hat1, beta_hat2))\n\nbeta_hat1 beta_hat2 \n 1.999171  4.000034 \n\n\n\nThe assumption of this stronger form of exogeneity also gives several nice properties beyond unbiasedness.\n\nProposition 5.6 (Consequences of Exogeneity) Suppose \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\), and \\(\\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right)\\). Then\n\n\\(\\text{E}\\left[\\mathbf{Y}\\mid \\mathbb{X}\\right] = \\mathbb{X}\\boldsymbol{\\beta}\\);\n\\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\) (even if \\(\\beta_1 = 0\\))\n\\(\\boldsymbol{\\Sigma}= \\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right] = \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\right)\\)\n\n\n\nProof. space\n\n\\(\\text{E}\\left[\\mathbf{Y}\\mid \\mathbb{X}\\right] = \\text{E}\\left[\\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\text{E}\\left[\\mathbb{X}\\boldsymbol{\\beta}\\mid \\mathbb{X}\\right] + \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]= \\mathbb{X}\\boldsymbol{\\beta}+ \\mathbf{0}= \\mathbb{X}\\boldsymbol{\\beta}\\)\n\\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\right] = \\text{E}\\left[\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right] = \\text{E}\\left[\\mathbf{0}\\right] = \\mathbf{0}\\)\nThe first portion is a consequence of the definition of variance. \\[\\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right) = \\text{E}\\left[[\\boldsymbol{\\varepsilon}- \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]][\\boldsymbol{\\varepsilon}- \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]]'\\mid \\mathbb{X}\\right] = \\text{E}\\left[[\\boldsymbol{\\varepsilon}- \\mathbf{0}][\\boldsymbol{\\varepsilon}- \\mathbf{0}]'\\mid \\mathbb{X}\\right] = \\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]\\] The second portion follows from the law of total variance. \\[ \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\right) = \\text{E}\\left[\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right)\\right] + \\text{Var}\\left(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right) = \\text{E}\\left[\\boldsymbol{\\Sigma}\\right] + \\text{Var}\\left(0\\right) = \\boldsymbol{\\Sigma}\\]\n\nspace\n\nFinally let’s consider the variance of our OLS estimator. Due to the stochastic nature of \\(\\mathbb{X}\\), our interest is actually in the conditional variance of \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\) given \\(\\mathbb{X}\\). Until now, we haven’t paid much attention to the parameter \\(\\boldsymbol{\\Sigma}\\), but it will come into play here.\n\\[\\begin{align*}\n\\text{Var}\\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\mid \\mathbb{X}\\right) & = \\text{E}\\left[ \\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} - \\text{E}\\left[ \\hat{\\boldsymbol{\\beta}}_\\text{OLS} \\right]\\right) \\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} - \\text{E}\\left[\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\right]\\right)'\\mid \\mathbb{X}\\right]\\\\\n& = \\text{E}\\left[ \\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} - \\boldsymbol{\\beta}\\right) \\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS} - \\boldsymbol{\\beta}\\right)'\\mid \\mathbb{X}\\right] & (\\hat{\\boldsymbol{\\beta}}_\\text{OLS} \\text{ unbiased})\\\\\n& = \\text{E}\\left[ \\left[(\\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right] \\left[(\\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right]'\\mid \\mathbb{X}\\right] & (\\hat{\\boldsymbol{\\beta}}_\\text{OLS} = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol{\\varepsilon})\\\\\n& = \\text{E}\\left[ \\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}\\right] \\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}\\right]'\\mid \\mathbb{X}\\right]\\\\\n& = \\text{E}\\left[ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mid \\mathbb{X}\\right]\\\\\n& =  (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\\\\n& =  (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\Sigma}\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1} & (\\boldsymbol{\\Sigma}= \\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right])\n\\end{align*}\\]\n\nProposition 5.7 (OLS Variance I) Suppose \\(P_{\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\), and \\(\\boldsymbol{\\Sigma}= \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right)\\). Then \\[ \\text{Var}\\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\mid \\mathbb{X}\\right) = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\Sigma}\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\]\n\n\nSuppose the our model is given as \\[\\begin{align*}\nY & = 2 + 4X_1 + \\varepsilon\\\\\nX_i &\\overset{iid}{\\sim}\\text{Uni}(0,10)\\\\\n\\boldsymbol{\\varepsilon}&\\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Sigma}_{ii} & = \\begin{cases}1 & i \\text{ even}\\\\ 2 & i \\text{ odd}\\end{cases}\\\\\n\\boldsymbol{\\Sigma}_{i\\ell} & = \\left\\lvert i-\\ell\\right\\rvert^{-1}\n\\end{align*}\\] Let’s perform a simulation to verify the formula for \\(\\text{Var}\\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\mid \\mathbb{X}\\right)\\) The variance of the error is defined such that if our observation has an even index, \\(\\text{Var}\\left(\\varepsilon_i\\right) = 1\\), otherwise \\(\\text{Var}\\left(\\varepsilon_i\\right) = 2\\). Errors are also correlated across observations. We have \\(\\text{Cov}\\left(\\varepsilon_i,\\varepsilon_\\ell\\right) = \\left\\lvert i-\\ell\\right\\rvert^{-1}\\). The closer two observations are index-wise, the stronger their correlation. We will simulate estimates for sample sizes of \\(n=100\\).\n\nn <- 100\nSigma <- matrix(NA, nrow = n, ncol = n)\nfor (i in 1:n) {\n  for (l in 1:n) {\n    #if i == l we have a diagonal entry \n    if(i == l){\n      #assign variance 1 or 2 based on modular arithmetic \n      Sigma[i,l] <- (2^((i %% 2) == 1))*(1^((i %% 2) == 0))\n    } else {\n      #otherwise asign covariance to be inverse of distance between entries\n      Sigma[i,l] <- 1/abs(i-l)\n    }\n  }\n}\nprint(Sigma[1:7,1:7])\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 2.0000000 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000 0.1666667\n[2,] 1.0000000 1.0000000 1.0000000 0.5000000 0.3333333 0.2500000 0.2000000\n[3,] 0.5000000 1.0000000 2.0000000 1.0000000 0.5000000 0.3333333 0.2500000\n[4,] 0.3333333 0.5000000 1.0000000 1.0000000 1.0000000 0.5000000 0.3333333\n[5,] 0.2500000 0.3333333 0.5000000 1.0000000 2.0000000 1.0000000 0.5000000\n[6,] 0.2000000 0.2500000 0.3333333 0.5000000 1.0000000 1.0000000 1.0000000\n[7,] 0.1666667 0.2000000 0.2500000 0.3333333 0.5000000 1.0000000 2.0000000\n\n\nWe are calculating the variance conditional on \\(\\mathbb{X}\\), so this means we will use the same realization \\(\\mathbb{X}= \\mathbf{X}\\) for each simulation. Before we simulate things, let’s draw our fixed realization of \\(\\mathbf{X}\\) and calculate the true variance of our OLS estimator using the formula \\((\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}\\boldsymbol{\\Sigma}\\mathbf{X}' (\\mathbf{X}'\\mathbf{X})^{-1}\\) where \\(\\mathbf{X}\\) is our fixed regressors.\n\nx1 <- runif(n, 0, 10)\nX <- cbind(1, x1)\nbeta <- c(2,4)\n\n# Actual variance\nsolve(t(X) %*% X) %*% t(X) %*% Sigma %*% X %*% solve(t(X) %*% X)\n\n                          x1\n    0.135371873 -0.007006977\nx1 -0.007006977  0.001335754\n\n\nNow let’s simulate 1000 estimates, where we only draw new realizations \\(\\mathbf e\\) for each simulation and leave \\(\\mathbf{X}\\) fixed.\n\nN_sim <- 1000\nstore <- matrix(NA, ncol = 2, nrow = N_sim)\nfor (k in 1:N_sim) {\n  e <- t(rmvnorm(1, rep(0, n), Sigma))\n  y <- X %*% beta + e\n  store[k,] <- OLS(y,X)\n}\n\n# Variance across simulations\ncov(store)\n\n             [,1]         [,2]\n[1,]  0.132725660 -0.006357347\n[2,] -0.006357347  0.001291283\n\n\nOur simulated variance is quite close to the true conditional variance!\n\nWe can simplify the formulas for the OLS estimator’s variance greatly if we impose one final assumption on our model.7\n\nDefinition 5.10 We the errors of a model are homoskedastic if \\[\\begin{align*}\n\\text{Var}\\left(\\varepsilon_i \\mid \\mathbf{X}\\right) &= \\sigma^2 & (i=1,\\ldots,n)\n\\end{align*}\\] (where \\(\\mathbf{X}\\) is the random vector of regressors), otherwise they are heteroskedastic. If \\[\\text{Cov}\\left(\\varepsilon_i,\\varepsilon_\\ell\\right) = 0\\] for all \\(i,\\ell = 1,\\ldots,n\\) where \\(i\\neq \\ell\\) we say errors are nonautocorrelated, otherwise they are autocorrelated/serially correlated. If errors are both homoskedastic and nonautocorrelated, then we have spherical errors and can write \\[\\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right) = \\sigma^2\\mathbf I.\\]\n\nWith the addition of this assumption, we have the classical linear model that you are likely familiar with.\n\nDefinition 5.11 The (Gauss-Markov/classical) linear model is defined as \\(\\mathcal P_\\text{LM} = \\{P_{\\boldsymbol{\\beta},\\sigma^2} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\sigma^2\\in\\mathbb R\\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\sigma^2} &= \\{F_{\\mathbb{X},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}, \\ \\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbf{X}\\right]=\\sigma^2\\mathbf I, \\ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i}, \\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K,\\ \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n].\n\\end{align*}\\]\n\nWhen people talk about “the linear (regression) model”, this is usually the model they are discussing. The collective assumptions are sometimes known as the “Gauss-Markov assumptions”, as they are the sufficient conditions for the Gauss-Markov theorem (which will be presented shortly) to hold.\n\nCorollary 5.1 (OLS Variance II) Suppose \\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\), and \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid\\mathbb{X}\\right] = \\sigma^2\\mathbf I\\). Then \\[\\begin{align*}\n\\text{Var}\\left(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\mid \\mathbf{X}\\right) &= \\sigma^2(\\mathbb{X}'\\mathbb{X})^{-1} = \\sigma^2 \\left(\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}\\right)^{-1}\n\\end{align*}\\]\n\n\nProof. \\((\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1} = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\sigma^2\\mathbf I\\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1} = \\sigma^2\\underbrace{[(\\mathbb{X}'\\mathbb{X})^{-1}(\\mathbb{X}'\\mathbb{X})]}_{\\mathbf I}(\\mathbb{X}'\\mathbb{X})^{-1} = \\sigma^2(\\mathbb{X}'\\mathbb{X})^{-1}\\)\n\n\nExample 5.15 (Comparative Statics and Variance) Suppose the linear model satisfies the Gauss-Markov assumptions and \\(K = 2\\). This simple setting allows us to gain a great deal of insight into the variance of the OLS estimator. In this case \\(\\mathbf{X}\\) has two columns: a column one 1s, and \\(\\mathbf{x}\\). \\[\\begin{align*}\n\\mathbb{X}& = [\\mathbf 1, \\mathbf{X}_1]\\\\\n\\mathbb{X}'\\mathbb{X}& = \\begin{bmatrix}n & \\sum_{i=1}^nX_i\\\\ \\sum_{i=1}^nX_i &  \\sum_{i=1}^nX_i^2 \\end{bmatrix}\\\\\n\\sigma^2(\\mathbb{X}'\\mathbb{X})^{-1} &= \\frac{\\sigma^2}{n\\sum_{i=1}^n X_i^2 - \\left(\\sum_{i=1}^n X_i\\right)^2} \\begin{bmatrix} \\sum_{i=1}^n X_i^2  & -\\sum_{i=1}^n X_i \\\\  -\\sum_{i=1}^n X_i & n \\end{bmatrix} \\\\\n\\\\ & = \\frac{\\sigma^2}{n[n(\\bar{X})^2] - (n\\bar{X^2})} \\begin{bmatrix} \\sum_{i=1}^n X_i^2  & -\\sum_{i=1}^n X_i \\\\  -\\sum_{i=1}^n X_i & n \\end{bmatrix}\\\\\n& = \\frac{\\sigma^2}{n^2(\\bar{X}^2 - \\bar{X^2})} \\begin{bmatrix} \\sum_{i=1}^n X_i^2  & -\\sum_{i=1}^n X_i \\\\  -\\sum_{i=1}^n X_i & n \\end{bmatrix}\\\\\n& = \\frac{\\sigma^2}{(n^2 - n)\\widehat{\\text{Var}}(X)} \\begin{bmatrix} \\sum_{i=1}^n X_i^2  & -\\sum_{i=1}^n X_i \\\\  -\\sum_{i=1}^n X_i & n \\end{bmatrix}\\\\  & = \\begin{bmatrix} \\frac{\\sigma^2\\sum_{i=1}^n X_i^2}{(n^2 - n)\\widehat{\\text{Var}}(X)}  & -\\frac{\\sigma^2\\sum_{i=1}^n X_i}{(n^2 - n)\\widehat{\\text{Var}}(X)} \\\\  -\\frac{\\sigma^2\\sum_{i=1}^n X_i}{(n^2 - n)\\widehat{\\text{Var}}(X)} & \\frac{\\sigma^2n}{(n^2 - n)\\widehat{\\text{Var}}(X)} \\end{bmatrix}\\\\\n\\text{Var}\\left(\\hat{\\beta}_{1,OLS}\\mid \\mathbb{X}\\right) & = \\frac{\\sigma^2\\sum_{i=1}^n X_i^2}{(n^2 - n)\\widehat{\\text{Var}}(X)}\\\\\n\\text{Var}\\left(\\hat{\\beta}_{2,OLS}\\mid \\mathbb{X}\\right) & =\\frac{\\sigma^2n}{(n^2 - n)\\widehat{\\text{Var}}(X)}\n\\end{align*}\\] What happens to these variances as we change the variance of the error \\(\\sigma^2\\), and the values of \\(x_i\\) change? Instead of finding the signs of various taking partial derivatives, let’s graph some examples. First let’s see what happens when we hold \\(\\mathbf{X}\\) constant but increase \\(\\sigma^2\\).\n\n\nShow code which generates figure\ndf1 <- data.frame(x = -4:5, e = rnorm(10, 0, 1)) %>% \n  mutate(y = 2*x + e, group = \"Low σ^2\") \n\ndf2 <- data.frame(x = -4:5, e = rnorm(10, 0, 5)) %>% \n  mutate(y = 2*x + e, group = \"High σ^2\") \n\nbind_rows(df1, df2) %>% \n  ggplot(aes(x,y)) +\n  geom_smooth(method = \"lm\", col = \"blue\", size = 0.5) +\n  geom_point() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\nFigure 5.8: The larger the variance of the error term, the larger the standard errors associated with our OLS estimates.\n\n\n\n\nEach graph contains the estimates linear model, with variance illustrated by the gray envelope around the lines. The width of this envelope at the red line \\(x=0\\) corresponds to the variance \\(\\hat{\\beta}_{1,OLS}\\), while the degree to which the width of the envelope varies along the \\(x\\)-axis corresponds to the variance of \\(\\hat{\\beta}_{1,OLS}\\). We can see that the variance of both estimators decreases when \\(\\sigma^2\\) decreases. As the uncertainty about the stochastic element of the model \\(\\varepsilon_i\\) decreases, we become more confident in our estimates. Now consider what happens when we change the variance of \\(x\\).\n\n\nShow code which generates figure\ndf1 <- tibble(\n  x = runif(10, 4,6), \n  e = rnorm(10, 0, 1)\n  ) %>% \n  mutate(\n    y = 2*x + e,\n    group = \"Low Variance of X\"\n  ) \n\ndf2 <- tibble(\n  x = runif(10, -5,15),\n  e = rnorm(10, 0, 1)\n  ) %>% \n  mutate(\n    y = 2*x + e, \n    group = \"High Variance of X\"\n  ) \n\nbind_rows(df1, df2) %>% \n  ggplot(aes(x,y)) +\n  geom_smooth(method = \"lm\", col = \"blue\", size = 0.5, fullrange=TRUE) +\n  geom_point() +\n  facet_wrap(~group) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal()\n\n\n\n\n\nFigure 5.9: The variance of the OLS estimator decreases as the variance of independent variables increases\n\n\n\n\nThe more variance we have in our regressors, the less variance our estimator exhibits. Essentially, the variance in observations provides more information about the relationship between the dependent and independent variables, so we get better estimates. Finally consider how the variance changes as the location of our data change relative to the y-axis\n\n\nShow code which generates figure\ndf1 <- tibble(\n  x = runif(10, -1,1), \n  e = rnorm(10, 0, 1)\n  ) %>% \n  mutate(\n    y = 2*x + e, \n    group = \"X Near Origin\"\n  ) \n\ndf2 <- data.frame(\n  x = runif(10, 10,12), \n  e = rnorm(10, 0, 1)\n  ) %>% \n  mutate(\n    y = 2*x + e, \n    group = \"X Far from Origin\"\n  ) \n\nbind_rows(df1, df2) %>% \n  ggplot(aes(x,y)) +\n  geom_smooth(method = \"lm\", col = \"blue\", size = 0.5, fullrange=TRUE) +\n  geom_point() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\nFigure 5.10: The proximity of the regressors to origin affects the variance of the intercept estimator\n\n\n\n\nThe closer our observations are to the y-axis the better out estimates of the intercept are.\n\nIf we forget the intercept term for a moment, then we can think \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) roughly as the amount of variance in our regressors. The variance in regressors amounts to information about \\(\\boldsymbol{\\beta}\\). The more variance/information we have about \\(\\mathbf{X}\\), the better our estimates will be."
  },
  {
    "objectID": "ols.html#gauss-markov-theorem",
    "href": "ols.html#gauss-markov-theorem",
    "title": "5  The Classical Linear Model",
    "section": "5.6 Gauss-Markov Theorem",
    "text": "5.6 Gauss-Markov Theorem\nHow do we know that there aren’t any other estimators that may be better than OLS? Recall from Section @ref(finite-sample-properties-of-estimators) we discussed the concept of a MVUE – an unbiased estimator which is more efficient (has lower variance) than all other unbiased estimators. Finding a MVUE is difficult without additional assumptions about the unbiased estimators. With one such assumption, we do have that OLS is a MVUE among all unbiased estimators satisfying this assumption. This result is known as the Gauss-Markov theorem, and tells us that the OLS estimator has the minimum variance among all linear unbiased estimators. For the remainder of our discussion of the Gauss-Markov theorem, we will assume that our linear model satisfies: \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\) , \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\right]=\\mathbf{0}\\),and \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid\\mathbb{X}\\right] = \\sigma^2\\mathbf I\\).8\nIn the context of the linear model, a linear estimator \\(\\hat{\\boldsymbol{\\beta}}\\) will take the form \\(\\hat{\\boldsymbol{\\beta}} = \\mathbf C\\mathbf{Y}+\\mathbf{D}\\) for some matrix \\(\\mathbf C\\) (which may be a function of \\(\\mathbb{X}\\)). In the case of \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\), \\(\\mathbf C = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\). We will denote a general linear unbiased estimator for \\(\\boldsymbol{\\beta}\\) as \\(\\tilde{\\boldsymbol{\\beta}}\\). For now, let’s condition on the random matrix \\(\\mathbb{X}\\). To restrict our attention to unbiased linear estimators, \\(\\tilde{\\boldsymbol{\\beta}}\\) must satisfy: \\[\\begin{align*}\n&\\text{E}\\left[\\tilde{\\boldsymbol{\\beta}} \\mid \\mathbb{X}\\right] = \\boldsymbol{\\beta}\\\\\n\\implies & \\text{E}\\left[\\mathbf C\\mathbf{Y}\\mid \\mathbb{X}\\right] = \\boldsymbol{\\beta}\\\\\n\\implies & \\text{E}\\left[\\mathbf C\\mathbb{X}\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\boldsymbol{\\beta}& (\\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon})\\\\\n\\implies & \\mathbf C\\mathbb{X}\\underbrace{\\text{E}\\left[\\boldsymbol{\\beta}\\mid \\mathbb{X}\\right]}_{\\boldsymbol{\\beta}} + \\mathbf C\\underbrace{\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]}_{\\mathbf{0}} = \\boldsymbol{\\beta}\\\\\n\\implies &  \\mathbf C\\mathbb{X}\\boldsymbol{\\beta}= \\boldsymbol{\\beta}\\\\\n\\implies & \\mathbf C\\mathbb{X}= \\mathbf I\n\\end{align*}\\] The variance of \\(\\tilde{\\boldsymbol{\\beta}}\\) can be calculated using the same exact steps we took to calculate the variance of \\(\\hat{\\boldsymbol{\\beta}}_\\text{OLS}\\): \\[\\begin{align*}\n\\text{Var}\\left(\\tilde{\\boldsymbol{\\beta}}\\mid \\mathbb{X}\\right) & = \\text{E}\\left[ \\left(\\tilde{\\boldsymbol{\\beta}} - \\text{E}\\left[ \\tilde{\\boldsymbol{\\beta}} \\right]\\right) \\left(\\tilde{\\boldsymbol{\\beta}} - \\text{E}\\left[\\tilde{\\boldsymbol{\\beta}}\\right]\\right)'\\mid \\mathbb{X}\\right]\\\\\n& = \\text{E}\\left[ \\left(\\tilde{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}\\right) \\left(\\tilde{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}\\right)'\\mid \\mathbb{X}\\right] & (\\tilde{\\boldsymbol{\\beta}} \\text{ unbiased})\\\\\n& = \\text{E}\\left[ \\left[(\\mathbf C\\mathbb{X}\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right] \\left[(\\mathbf C\\mathbb{X}\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right]'\\mid \\mathbb{X}\\right] & (\\tilde{\\boldsymbol{\\beta}} = \\mathbf C\\mathbb{X}\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon})\\\\\n& = \\text{E}\\left[ \\left[(\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right] \\left[(\\boldsymbol{\\beta}+ \\mathbf C\\boldsymbol{\\varepsilon}) - \\boldsymbol{\\beta}\\right]'\\mid \\mathbb{X}\\right]  & (\\mathbf{C}\\mathbf{X}= \\mathbf I)\\\\\n& = \\text{E}\\left[ \\left[\\mathbf C\\boldsymbol{\\varepsilon}\\right] \\left[\\mathbf C\\boldsymbol{\\varepsilon}\\right]'\\mid \\mathbb{X}\\right] \\\\\n& = \\text{E}\\left[\\mathbf C\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mathbf C'\\mid \\mathbb{X}\\right]\\\\\n& = \\mathbf C\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]\\mathbf C'\\\\\n&  = \\sigma^2\\mathbf C\\mathbf C' & (\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\right] = \\sigma^2\\mathbf I)\n\\end{align*}\\]\nOur goal is to show that: \\[\\hat{\\boldsymbol\\beta}_\\text{OLS} = \\mathop{\\mathrm{argmin}}_{\\tilde{\\boldsymbol{\\beta}}} \\text{Var}\\left(\\tilde{\\boldsymbol{\\beta}} \\mid \\mathbb{X}\\right).\\] Write \\(\\mathbf{C}= (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}\\) for some non-zero matrix \\(\\mathbf{D}\\). The requirement that \\(\\mathbf{C}\\mathbb{X}= \\mathbf I\\) implies that: \\[ [(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}]\\mathbb{X}= \\mathbf I \\implies\\underbrace{(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{X}}_{\\mathbf I} + \\mathbf{D}\\mathbf{X}= \\mathbf I \\implies \\mathbf{D}\\mathbb{X}= \\mathbf{0}\\] Note that \\[ \\tilde{\\boldsymbol{\\beta}} = \\mathbf{C}\\mathbf{Y}= [(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}]\\mathbf{y}= (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\mathbf{Y}+ \\mathbf{D}\\mathbf{y}= \\hat{\\boldsymbol\\beta}_\\text{OLS} + \\mathbf{D}\\mathbf{Y},\\] so \\(\\tilde{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol\\beta}_\\text{OLS} \\) when \\(\\mathbf{D}= \\mathbf{0}\\). Our optimization problem becomes \\[\\begin{align*}\n\\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\text{Var}\\left(\\tilde{\\boldsymbol{\\beta}} \\mid \\mathbb{X}\\right) & = \\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\sigma^2\\mathbf C\\mathbf C'\\\\\n& =  \\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\sigma^2[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}][(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}]'\\\\\n& =  \\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\sigma^2[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}+ \\mathbf{D}][\\mathbb{X}'(\\mathbb{X}'\\mathbb{X})^{-1} + \\mathbf{D}']\\\\\n& = \\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\sigma^2[\\underbrace{(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\mathbb{X}'}_{\\mathbf I}(\\mathbb{X}'\\mathbb{X})^{-1} + (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{D}' + \\underbrace{\\mathbf{D}\\mathbb{X}}_\\mathbf{0}(\\mathbb{X}'\\mathbb{X})^{-1} + \\mathbf{D}\\mathbf{D}']\\\\\n& = \\mathop{\\mathrm{argmin}}_{\\mathbf{D}} \\sigma^2[(\\mathbb{X}'\\mathbb{X})^{-1} + (\\mathbb{X}'\\mathbb{X})^{-1}\\underbrace{(\\mathbb{X}\\mathbf{D})}_\\mathbf{0}' +  \\mathbf{D}\\mathbf{D}'] & (\\mathbb{X}'\\mathbf{D}' = (\\mathbf{X}\\mathbf{D})')\\\\\n& = \\sigma^2(\\mathbb{X}'\\mathbb{X})^{-1} + \\sigma\\mathbf{D}\\mathbf{D}'.\n\\end{align*}\\] This variance is minimized when \\(\\mathbf{D}= \\mathbf{0}\\),9 so \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is the most efficient unbiased linear estimator for any fixed \\(\\mathbb{X}=\\mathbf{X}\\). This holds for all realizations \\(\\mathbb{X}=\\mathbf{X}\\), so it will hold unconditionally as well.\n\nTheorem 5.2 (Gauss-Markov Theorem) Suppose \\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{LM}\\) where \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\), and \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid\\mathbb{X}\\right] = \\sigma^2\\mathbf I\\). Then \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is the best linear unbiased estimator (BLUE)/minimum variance linear unbiased estimator (MVLUE).\n\nThe Gauss-Markov theorem is one of the major justifications for estimating linear models with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). With estimation thoroughly treated, we can now consider making inferences about \\((\\boldsymbol{\\beta},\\sigma^2)\\) for \\(P_{\\boldsymbol{\\beta},\\sigma^2}\\in\\mathcal P_\\text{LM}\\)."
  },
  {
    "objectID": "ols.html#asymptotic-distribution-of-the-ols-estimator",
    "href": "ols.html#asymptotic-distribution-of-the-ols-estimator",
    "title": "5  The Classical Linear Model",
    "section": "5.7 Asymptotic Distribution of the OLS Estimator",
    "text": "5.7 Asymptotic Distribution of the OLS Estimator\nWe know that our estimator is consistent and the BLUE, but how does its distribution behave? It turns out that \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is root-n CAN under weaker assumptions than those required for the Gauss-Markov theorem. Before showing this in earnest, let’s look at a special case of the linear model.\n\nExample 5.16 (Gaussian Linear Model) Suppose \\(P_\\text{LM}\\) satisfies the Gauss-Markov assumptions, in addition to the assumption that \\(\\boldsymbol{\\varepsilon}\\mid\\mathbb{X}\\sim N(\\mathbf{0},\\sigma^2\\mathbf I)\\) (which is equivalent to \\(\\varepsilon_i\\overset{iid}{\\sim}N(0,\\sigma^2)\\) because we have assumed spherical errors). This model is sometimes referred to as the Gaussian linear model. A common way of writing this model is \\(\\mathbf{Y}\\mid\\mathbb{X}\\sim N(\\mathbb{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf I)\\), which emphasizes the fact that \\(\\text{E}\\left[\\mathbf{Y}\\mid\\mathbb{X}\\right] = \\mathbb{X}\\boldsymbol{\\beta}\\). It’s quite easy to derive the distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) for this model. We won’t even need to approximate the distribution via asymptotics! Using the properties of the multivariate distribution, we have: \\[\\begin{align*}\n&\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}= [\\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol{\\varepsilon}] - \\boldsymbol{\\beta}\\\\\n\\implies & \\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}= (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol{\\varepsilon}\\\\\n\\implies & \\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}\\sim N(\\mathbf{0}, (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\sigma^2\\mathbf I[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}]' ) & (\\mathbf A \\boldsymbol{\\varepsilon}\\sim N(\\mathbf{0}, \\mathbf A \\sigma^2 \\mathbf I \\mathbf A'))\\\\\n\\implies & \\hat{\\boldsymbol\\beta}_\\text{OLS} \\sim N(\\boldsymbol{\\beta}, \\sigma^2\\underbrace{(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\mathbb{X}'}_{\\mathbf I}(\\mathbb{X}'\\mathbb{X})^{-1})\\\\\n\\implies & \\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbb{X}\\sim N(\\boldsymbol{\\beta}, \\sigma^2(\\mathbb{X}'\\mathbb{X})^{-1})\n\\end{align*}\\]\nTo verify this, we can simulate 50,000 estimates for the model Gaussian linear model where \\(\\beta = [2,4]'\\), and \\(\\sigma^2 = 1\\). We’ll pick a modest sample size of \\(n=5\\) to emphasize that this is the precise distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\), not just the asymptotic distribution. Because this distribution is conditional on \\(\\mathbb{X}\\), we’ll fix the realization \\(\\mathbb{X}=\\mathbf{X}\\) over the simulations.\n\nn <- 5\nsigma <- 1\nx1 <- runif(n, 0, 10)\nX <- cbind(1, x1)\nbeta <- c(2,4)\n\nN_sim <- 50000\nstore <- matrix(NA, ncol = 2, nrow = N_sim)\nfor (k in 1:N_sim) {\n  e <- e <- rnorm(n, 0, 1)\n  y <- X %*% beta + e\n  store[k,] <- OLS(y,X)\n}\n\n\n\nShow code which generates figure\ndf1 <- tibble(\n  x = c(store[,1], store[,2]),\n  group = c(rep(\"β1 Estimate\", 50000), rep(\"β2 Estimate\", 50000))\n  )\n\ndf2 <- tibble(\n  x = c(seq(min(store[,1]), max(store[,1]), length = 1000), seq(min(store[,2]) ,max(store[,2]), length = 1000)),\n  group = c(rep(\"β1 Estimate\", 1000), rep(\"β2 Estimate\", 1000))\n  ) %>% \n  mutate(y = ifelse(group == \"β1 Estimate\", dnorm(x, beta[1], sqrt(solve(t(X)%*%X)[1,1])), dnorm(x, beta[2], sqrt(solve(t(X)%*%X)[2,2]))))\n\np1 <- ggplot() +\n  geom_histogram(data = df1, aes(x, y = ..density..), color = \"black\", fill = \"white\", bins = 50) +\n  geom_line(data = df2, aes(x,y), color = \"red\") + \n  facet_wrap(~group, scales = \"free\") +\n  theme_minimal()\n  \n\ndf1 <- expand_grid(x = seq(min(store[,1]) ,max(store[,1]), length = 1000),\n                  y = seq(min(store[,2]) ,max(store[,2]), length = 1000),\n                  )\ndf1$p <- dmvnorm(df1, beta, solve(t(X)%*%X))\ndf2 <- tibble(\n  x = store[,1], \n  y = store[,2]\n)\n\np2 <- ggplot() +\n  geom_point(data = df2, aes(x,y), size = 0.001)+\n  geom_contour(data = df1, aes(x,y, z= p), bins = 14, color = \"red\", size = 1) +\n  theme_minimal() +\n  xlab(\"β1 Estimate\") +\n  ylab(\"β2 Estimate\") + \n  xlim(0,4) +\n  ylim(3.6,4.4)\n\nggarrange(p1, p2, ncol = 1)\n\n\n\n\n\nFigure 5.11: The simulated marginal density of our estimators and their simulated joint density (along with the true underlying distributions shown in red)\n\n\n\n\n\nIf we abandon the assumption that \\(\\boldsymbol{\\varepsilon}\\mid\\mathbf{X}\\sim N(\\mathbf{0},\\sigma^2\\mathbf I)\\) we are back to the standard (Gauss-Markov) linear model. All the assumptions about or model take the form of moment conditions, and not specific distributions, so we will not be able to calculate the exact distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) in general. Fortunately we can use our asymptotic toolkit to find the limiting distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\).\nThe overwhelming majority of the time, estimators will be root-n consistent, so the best starting point of finding the asymptotic distribution of an estimator is by first calculating \\(\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta})\\). In the case of the OLS estimator: \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}) & = n^{1/2}[\\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol{\\varepsilon}] - \\boldsymbol{\\beta}\\\\\n& = \\sqrt{n}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol{\\varepsilon}\\\\\n& = \\sqrt{n}\\left(\\frac{\\mathbb{X}'\\mathbb{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbb{X}'\\boldsymbol{\\varepsilon}}{n}\\right)\\\\\n& = \\left(\\frac{\\mathbb{X}'\\mathbb{X}}{n}\\right)^{-1}\\left(\\frac{\\mathbb{X}'\\boldsymbol{\\varepsilon}}{\\sqrt{n}}\\right)\\\\\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)\n\\end{align*}\\] Whether you want to show the result using the matrix form of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) or the form which is sums of vector is a matter of preference. Regardless, the first term will converge to its population counterpart \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\). The second term is a bit more interesting. We have \\[\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right) = \\sqrt n \\left(\\frac{1}{ n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i - \\mathbf{0}\\right) =  \\sqrt n \\left(\\frac{1}{ n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i - \\text{E}\\left[\\mathbf{X}_i'\\varepsilon_i\\right]\\right),\\] but this is the precise expression which the CLT applies to, so we have: \\[\\begin{align*}\n\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right) &\\overset{d}{\\to}N(\\text{E}\\left[\\mathbf{X}_i'\\varepsilon_i\\right], \\text{Var}\\left(\\textstyle \\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)/n)\\\\\n& \\overset{d}{\\to}N(\\mathbf{0}, \\textstyle \\sum_{i=1}^n\\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]/n)\\\\\n& \\overset{d}{\\to}N(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right])\n\\end{align*}\\] because \\(\\mathbf{X}_i'\\varepsilon_i\\) is an iid sample. Using this fact along with Slutsky’s theorem and the LLN gives us the distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}) & =\\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}}_{\\overset{p}{\\to}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}}\\underbrace{\\left(\\frac{1}{\\sqrt n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)}_{\\overset{d}{\\to}N(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right])}\\\\\n& \\overset{d}{\\to}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}N(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right])\\\\\n& = N(\\mathbf{0}, \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right][\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}]')\\\\\n& = N(\\mathbf{0}, \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1})\\\\\n& = N(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1})\n\\end{align*}\\]\nIf we express the variance in terms of the random matrix \\(\\mathbb{X}\\) instead of the random vector of covariates using the equality \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] = \\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]/n\\), we have\n\\[\\sqrt{n}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}) \\overset{d}{\\to}N(\\mathbf{0}, \\sigma^2(\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]/n)^{-1})= N(\\mathbf{0}, \\sigma^2n\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]^{-1}) \\]\n\nSuppose \\(P_{\\boldsymbol{\\beta},\\sigma^2}\\in \\mathcal P_\\text{LM}\\) where \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\right] = \\sigma^2\\mathbf I\\). Then \\[ \\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta},\\sigma^2\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]^{-1}\\right) = N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\right). \\]\n\nWe have stated the asymptotic distribution without conditioning on \\(\\mathbb{X}\\), so \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\) will be in terms of the expectation of \\(\\mathbb{X}\\) opposed to some fixed \\(\\mathbb{X}\\). We only appealed to the assumption \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\right] = \\sigma^2\\mathbf I\\) to simplify the asymptotic variance, so in the event \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\right] \\neq \\sigma^2\\mathbf I\\), our estimator will still be root-n CAN, albeit with a different asymptotic variance (we will show this in Section @ref(generalized-least-squares)). Depending on the level of technical rigor the assumptions which give this result may differ. I followed the derivation provided by Wooldridge (2010), but others will delineate regularity conditions on \\(\\mathbb{X}\\) so it is “well-behaved”, or impose assumptions about the behavior of errors as a martingale. These assumptions tend to be rather weak and will hold in many practical applications. We also could extend this result to data which are not independent using the Lindeberg-Feller CLT.\n\nExample 5.17 Consider the case where \\(\\beta = 2\\), \\(\\varepsilon_i \\overset{iid}{\\sim}\\text{Uni}(-1,1)\\), \\(X \\sim \\text{Uni}(-5,5)\\), \\(\\varepsilon\\perp X\\), and \\(Y= 2X + \\varepsilon\\). By properties of the uniform distribution, \\[\\begin{align*}\n\\sigma^2 &= \\frac{1}{3},\\\\\n\\text{E}\\left[X^2\\right] & = \\frac{25}{3}.\n\\end{align*}\\] If simulate realizations of \\(\\hat\\beta\\) for a sufficiently large \\(n\\), we should expect it to approximately follow a normal distribution with mean \\(2\\) and variance: \\[\\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1} =  \\frac{1/3}{n} \\text{E}\\left[X^2\\right]^{-1} = \\frac{1}{3n}(25/3)^{-1} = \\frac{1}{25n}.\\] Let’s perform 10,000 simulations for sample sizes \\(n\\in\\{3,5,8,10\\}\\).\n\nbeta <- 2\nN_sim <- 10000\nstore <- data.frame(estimate = NA, n = NA)\nfor (n in c(3,5,8,10)) {\n  for (k in 1:N_sim) {\n    e <- runif(n,-1,1)\n    X <- matrix(runif(n, -5, 5), ncol = 1) \n    y <- X * beta + e\n    store <- rbind(store, c(OLS(y,X),n))\n  }\n}\n\nAs \\(n\\) increases we should see the bias of our estimator shrink, and the simulated variance approach \\(1/25n\\).\n\n\n\n\n\nn\nBias\nSimulated Variance\nLimiting Variance\n\n\n\n\n3\n-0.0015484\n0.0228532\n0.0133333\n\n\n5\n-0.0000008\n0.0102583\n0.0080000\n\n\n8\n0.0002803\n0.0056143\n0.0050000\n\n\n10\n0.0001717\n0.0045340\n0.0040000\n\n\n\n\n\nMore importantly (because we knew how to calculate the bias and asymptotic variance of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) prior to deriving its limiting distribution), if we use our estimates to make a Q-Q plot, we see that as \\(n\\) increases our estimates fit a normal distribution increasingly well.\n\n\nShow code which generates figure\nstore %>% \n  filter(!is.na(n)) %>% \n  ggplot(aes(sample = estimate)) +\n  facet_wrap(~n) +\n  stat_qq_line(color = \"red\") +\n  stat_qq(size = 0.2) +\n  theme_minimal() +\n  ylim(0,3)\n\n\n\n\n\n\n\n\n\nEven for modest sample sizes such as \\(n=10\\), it’s clear that our estimates are approximately normally distributed."
  },
  {
    "objectID": "ols.html#estimating-textavarlefthatboldsymbolbeta_textols-right",
    "href": "ols.html#estimating-textavarlefthatboldsymbolbeta_textols-right",
    "title": "5  The Classical Linear Model",
    "section": "5.8 Estimating \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\)",
    "text": "5.8 Estimating \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\)\nWe’ve spent so much time considering the estimation of \\(\\boldsymbol{\\beta}\\), and completely ignored the other parameter of our model – \\(\\boldsymbol{\\Sigma}\\). In the case of the Gauss-Markov assumptions, \\(\\Sigma = \\sigma^2\\mathbf I\\), so estimating \\(\\boldsymbol{\\Sigma}\\) simplifies to estimating \\(\\sigma^2\\).\nA natural suggestion for the estimator would be\n\\[\\frac{1}{n-1}\\sum_{i=1}^n(e_i - \\underbrace{\\text{E}\\left[e_i\\right]}_0)^2 =   \\frac{1}{n-1}\\sum_{i=1}^ne_i^2 =\\mathbf{e}'\\mathbf{e}\\] for realizations \\(\\mathbf{e}\\) of the random variable \\(\\boldsymbol{\\varepsilon}\\). This was our approach to calculating the standard error associated with the mean when we didn’t know the population variance, but it is a nonstarter in this case because we don’t observe \\(\\boldsymbol{\\varepsilon}\\). So right from the start, we need to think of a way to estimate \\(\\mathbf{e}\\). The immediate candidate are the observed errors associated with the estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). This estimator for \\(\\mathbf{e}\\) can be defined as\n\\[\\begin{align*}\n\\hat{\\mathbf{e}} & = \\mathbf{Y}- \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} ,\\\\\n& = \\mathbf{Y}- \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf{X}'\\mathbf{Y},\\\\\n& = (\\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}')\\mathbf{Y}.\n\\end{align*}\\]\n\nDefinition 5.12 The (least squares) residuals associated with the estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) are defined as \\[ \\hat{\\mathbf{e}}(\\mathbf{Y},\\mathbb{X}) = \\mathbf{Y}- \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} = \\mathbb M\\mathbf{Y},\\] where \\(\\mathbb M = \\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\). The estimated residuals associated with observations \\((\\mathbf{y}, \\mathbf{X})\\) is \\[ \\hat{\\mathbf{e}}(\\mathbf{y},\\mathbf{X}) =\\mathbf{y}- \\mathbf{X}\\hat{\\mathbf{b}}_\\text{OLS}= \\mathbf M\\mathbf{y}.\\]\n\nTo estimate \\(\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right) = \\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]\\), let’s appeal to the analogy principle and inspect its sample counterpart, only do so using the residuals \\(\\hat{\\mathbf{e}}\\). To do this, we’ll need a few quick results, the proofs of which are applications of linear algebra and can be found in Greene (2018).\n\nLemma 5.1 (Properties of Residuals) Let the estimator \\(\\hat{\\mathbf{e}}\\) be the least squared residuals. Then:\n\n\\(\\mathbb M\\) is symmetric (\\(\\mathbb M'=\\mathbb M\\)) and idempotent (\\(\\mathbb M^2=\\mathbb M\\)). Together these imply that \\(\\mathbb M'\\mathbb M= \\mathbb M\\).\n\\(\\text{tr}(\\boldsymbol{\\varepsilon}'\\mathbb M\\boldsymbol{\\varepsilon}) =\\text{tr}(\\mathbb M\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon})\\) where \\(\\text{tr}(\\mathbf A)= \\sum_{i=1}^n \\text{diag}(\\mathbf A)\\)\n\n\nThe matrix \\(\\mathbb M\\) satisfies \\(\\mathbb M\\mathbb{X}=\\mathbf{0}\\): \\[\\mathbb M\\mathbb{X}=(\\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}')\\mathbb{X}=\\mathbb{X}-  \\mathbb{X}\\underbrace{(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{X}}_{\\mathbf I} = \\mathbf{0}.\\]\nThe sample analog of \\(\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\), using residuals as estimates for \\(\\mathbf{e}\\), is:\n\\[\\begin{align*}\n\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}} &= \\mathbf{Y}\\mathbb M'\\mathbb M\\mathbf{Y}\\\\\n& = [\\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} + \\boldsymbol{\\varepsilon}]\\mathbb M[\\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} + \\boldsymbol{\\varepsilon}] & (\\mathbf{Y}= \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} + \\boldsymbol{\\varepsilon},\\ \\mathbb M'\\mathbb M =\\mathbb M)\\\\\n& = \\boldsymbol{\\varepsilon}' \\mathbb M\\boldsymbol{\\varepsilon}& (\\mathbb M\\mathbb{X}=\\mathbf{0})\\\\\n\\end{align*}\\]\nThe expectation of this estimator is \\[\\begin{align*}\n\\text{E}\\left[\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\right] &= \\text{E}\\left[\\text{E}\\left[\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}} \\mid \\mathbb{X}\\right]\\right] \\\\\n& = \\text{E}\\left[\\text{E}\\left[\\boldsymbol{\\varepsilon}' \\mathbb M\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\right]\\\\\n& = \\text{E}\\left[\\text{E}\\left[\\text{tr}(\\boldsymbol{\\varepsilon}' \\mathbb M\\boldsymbol{\\varepsilon}) \\mid \\mathbb{X}\\right]\\right] & (\\boldsymbol{\\varepsilon}' \\mathbb M\\boldsymbol{\\varepsilon}\\text{ is a scalar})\\\\\n& = \\text{E}\\left[\\text{E}\\left[\\text{tr}( \\mathbb M\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}) \\mid \\mathbb{X}\\right]\\right] & (\\text{tr}(\\boldsymbol{\\varepsilon}'\\mathbb M\\boldsymbol{\\varepsilon}) =\\text{tr}(\\mathbb M\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}))\\\\\n& = \\text{E}\\left[\\mathbb M(\\text{tr}\\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right])\\right] & (\\mathbb M\\text{ is a function of }\\mathbb{X})\\\\\n& = \\text{tr}(\\mathbb M \\sigma^2 \\mathbf I) \\\\\n& = \\sigma^2 \\text{tr}(\\mathbb M) \\\\\n& = \\sigma^2 \\text{tr}(\\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}')\\\\\n& = \\sigma^2 \\text{tr}(\\mathbf I) - \\text{tr}((\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\mathbb{X}')\\\\\n& = \\sigma^2(n-K)\n\\end{align*}\\]\nMuch like the estimator \\(n^{-1}\\sum_{i=1}^n(X_i - \\bar X)^2\\) for some \\(\\text{Var}\\left(X\\right)\\), our estimator for the variance of our residuals is biased. If we correct for this bias, we have \\[ S^2 = \\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n-K}.\\] This correction follows from the same intuition behind Bessel’s correction. Bessel’s correction accounted for the estimation of population variance have two steps: first we estimate \\(\\bar X\\) because we do not know \\(\\mu = \\text{E}\\left[X\\right]\\), and then we use this intermediate estimate to calculate the sample variance. We’re doing precisely the same thing when estimating the variance of our errors. It requires an intermediate step where we estimate \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\), and then we use our estimated value to calculate \\(\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}/(n-K)\\). The estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is a \\(K-\\)vector, so we need to correct for each dimension.\n\nProposition 5.8 (Estimation of OLS Variance) Define the estimator \\[S^2 =  \\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n-K}\\] in the context of the classic linear model. Then:\n\n\\(S^2\\) is an unbiased for \\(\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbf{X}\\right) = \\sigma^2\\).\n\\(S^2\\) is a consistent estimator \\(\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbf{X}\\right) = \\sigma^2\\).\nThe estimator \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = S^2(\\mathbb{X}'\\mathbb{X})^{-1}\\) is a consistent estimator for \\({\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = \\sigma^2\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]^{-1}\\)\n\n\n\nProof. space\n\nThis follows from our derivation of the estimator: \\[\\text{E}\\left[S^2\\right] = \\text{E}\\left[\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\right]/(n-K) = [\\sigma^2/(n-K)]/(n-K) = \\sigma^2.\\]\nWe have: \\[\\begin{align*}\nS^2 & = \\text{E}\\left[\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\right]/(n-K)\\\\\n& = \\frac{1}{n-K}\\text{E}\\left[\\boldsymbol{\\varepsilon}'\\mathbb M\\boldsymbol{\\varepsilon}\\right]\\\\\n& = \\frac{1}{n-k}[\\boldsymbol{\\varepsilon}'(\\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}')\\boldsymbol{\\varepsilon}]\\\\\n& = \\frac{1}{n-k}[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}- \\boldsymbol{\\varepsilon}' \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}]\\\\\n& = \\frac{n}{n-k}\\left[\\frac{\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}}{n} - \\frac{\\boldsymbol{\\varepsilon}' \\mathbb{X}}{n}\\frac{(\\mathbb{X}'\\mathbb{X})^{-1}}{n}\\frac{\\mathbb{X}'\\boldsymbol{\\varepsilon}}{n}\\right]\\\\\n& = \\underbrace{\\frac{n}{n-k}}_{\\to 1}\\Bigg[\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i^2}_{\\overset{p}{\\to}\\sigma^2} - \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\varepsilon_i\\mathbf{X}_i\\right)}_{\\overset{p}{\\to}\\mathbf{0}}\\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}}_{\\overset{p}{\\to}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}}\\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)}_{\\overset{p}{\\to}\\mathbf{0}} \\Bigg] & (\\text{LLN})\\\\\n& \\overset{p}{\\to}1(\\sigma^2 - \\mathbf{0}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\mathbf{0}) & (\\text{Slutsky's theorem})\\\\\n& = \\sigma^2\n\\end{align*}\\]\nWe can now use the fact that \\(S^2\\overset{p}{\\to}\\sigma^2\\) along with Slutsky’s theorem:\n\n\\[ \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = \\underbrace{S^2}_{\\overset{p}{\\to}\\sigma^2}[\\underbrace{(\\mathbb{X}'\\mathbb{X})}_{\\overset{p}{\\to}n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]}]^{-1} \\overset{p}{\\to}\\sigma^2[n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]]^{-1}=\\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}={\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ). \\]"
  },
  {
    "objectID": "ols.html#basic-model-selection-and-inference",
    "href": "ols.html#basic-model-selection-and-inference",
    "title": "5  The Classical Linear Model",
    "section": "5.9 Basic Model Selection and Inference",
    "text": "5.9 Basic Model Selection and Inference\nWe’ve been operating under the assumption that we know the true model \\(\\mathcal P_\\text{LM}\\), but in reality knowing this is impossible. In fact, a common aphorism in statistics is that “all models are wrong”, because the world is too complex to systematically describe any phenomenon. This is especially true of the social sciences. Fortunately, the full aphorism is “all models are wrong…but some are useful.” Even if models are approximations of reality, they offer insights into the world. So how do we pick the right one?\nIn the context of \\(\\mathcal P_\\text{LM}\\), this question amounts to considering the random vector of regressors \\(\\mathbf{X}\\). Even if our model is founded in rigorous economic theory, it still may be unclear which independent variables are pertinent. In Example @ref(exm:car), we considered a model where an agent \\(i\\) got utility \\(u_{ij}\\) from purchasing a car \\(j\\), assuming that their utility function took the form \\(u_{ij}=\\mathbf{X}_{ij}\\boldsymbol{\\beta}+ \\varepsilon_i\\) for a vector of vehicle and consumer attributes \\(\\mathbf{X}_{ij}\\) where \\(\\varepsilon_i\\) corresponds to heterogeneity. It is up to us to determine which variables to include in \\(\\mathbf{X}_{ij}\\). Attributes likes vehicle price, consumer location, whether a car is new or used, and model year of the car likely affect a consumers utility. But what about things like car color, technical specifications like a vehicles torque? There is no cut and dry answer to this, hence the black hole that is literature regrading model select. For now, we will take a basic approach to model selection rooted in methods introduced in @ref(hypothesis-testing).\nConsider two models \\(\\mathcal P_\\text{LM}\\) and \\(\\mathcal P_\\text{LM}'\\):\n\\[\\begin{align*}\n\\mathcal P_\\text{LM}&: Y = \\beta_0 + \\beta_1 X_1 + \\varepsilon\\\\\n\\mathcal P_\\text{LM}'&: Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\n\\end{align*}\\]\nThese models are referred to as nested models, because the parameter space corresponding to \\(\\mathcal P_\\text{LM}\\) is a subset of the parameter space corresponding to \\(\\mathcal P_\\text{LM}'\\). If we are tasked with choosing between these two models, we can estimate \\(\\mathcal P_\\text{LM}'\\) and test the hypothesis \\(H_0:\\beta_2 = 0\\). If we find sufficient evidence to reject this null hypothesis, than \\(\\beta_2\\) is likely nontrivial and should be included in the model, prompting us to favor \\(\\mathcal P_\\text{LM}'\\).\nIn general, suppose you want to test \\(H_0:\\beta_j = \\beta_{0}\\). We established that \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is root-n CAN so we can use the \\(t-\\)test discussed in ?sec-tsting to test this hypothesis. The statistic would be \\[ t = \\frac{\\hat\\beta_{\\text{OLS},j} - \\beta_0}{\\widehat{\\text{se}}(\\hat\\beta_{\\text{OLS},j})}.\\] This statistic relies on a consistent estimator \\(\\widehat{\\text{se}}(\\hat\\beta_{\\text{OLS},j})\\), but this is given immediately by our consistent estimator \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = S^2(\\mathbb{X}'\\mathbb{X})^{-1}\\).\n\\[\\widehat{\\text{se}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = \\left[\\text{diag}(S^2(\\mathbb{X}'\\mathbb{X})^{-1})\\right]^{1/2}.\\] In the context of model selection, our default hypothesis is \\(H_0:\\beta_j = \\beta_0\\) – is the addition of \\(\\beta_j\\) in our specification nontrivial? This is why if you run a regression using almost any statistical software, it will automatically report the results associated with the hypotheses \\(H_0:\\beta_j = 0\\) for each separate \\(\\beta_j\\).\n\nExample 5.18 (Coding Exercise) Now that we know how to estimate and draw inferences about \\(\\boldsymbol{\\beta}\\), let’s return to our OLS() function which we first defined in Example @ref(exm:funref). Along with calculating \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\), let’s calculate \\(\\widehat{\\text{se}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\), the \\(t\\)-statistic associated with testing \\(K\\) null hypotheses \\(\\beta_j = 0\\) (separately) at a significance level of \\(\\alpha = 0.05\\), a 95% confidence interval for \\(\\boldsymbol{\\beta}\\), and the associated \\(p-\\)value.\n\nOLS <- function(y, X){\n  #determine dimensions, confirm estimate exists, perform OLS\n  n <- length(y)\n  K <- ncol(X)\n  if(det(t(X) %*% X) == 0) {stop(\"rank(X'X) < K\")}\n  hat_beta <- solve(t(X) %*% X) %*% t(X) %*% y\n  \n  #use OLS estimates to calculate residuals and estimate SEs\n  res <- (y-X %*% hat_beta)\n  S2 <- ((t(res) %*% res)/(n - K)) %>% as.numeric() \n  var_hat <- (S2) * solve( t(X) %*% X )\n  se_hat <- sqrt(diag(var_hat))\n  \n  #t-stat, confidence intervals, p values\n  t <- hat_beta/se_hat\n  lower_CI <- hat_beta - qnorm(0.975)*se_hat\n  upper_CI <- hat_beta + qnorm(0.975)*se_hat\n  p_val <- 2*(1 - pt(t, n-K))\n  \n  #combine everything into one table to return\n  output <- cbind(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)\n  rownames(output) <- paste(\"β\", 1:K, sep = \"\")\n  colnames(output) <- c(\"Estimate\", \"Std.Error\", \"t-Stat\", \"Lower 95% CI\", \"Upper 95% CI\", \"p-Value\")\n  return(output)\n}\n\nWe’ll estimate the model given by \\(\\boldsymbol{\\beta}= [2,5,4,3,6]'\\), \\(\\mathbf{X}\\sim N(\\mathbf{0}, \\mathbf I)\\), \\(n = 15\\), and \\(\\varepsilon\\overset{iid}{\\sim}\\text{Uni}(-1,1)\\). Before we use our OLS() function, let’s see what R’s base function lm() (which stands for “linear model”) give us.\n\nbeta <- c(2,5,4,3,6)\nn <- 15\nX <- rmvnorm(n, mean = rep(0,4), diag(1,4))\nX <- cbind(1, X)\ne <- runif(n,-1,1)\ny <- X %*% beta + e\n\n#base R function, -1 to omit intercept which we added a column for\nknitr::kable(summary(lm(y ~ X - 1))$coefficients)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n\n\n\n\nX1\n2.068600\n0.2034276\n10.16873\n1.4e-06\n\n\nX2\n4.815065\n0.2177389\n22.11394\n0.0e+00\n\n\nX3\n4.092429\n0.2163911\n18.91219\n0.0e+00\n\n\nX4\n2.672951\n0.2393738\n11.16643\n6.0e-07\n\n\nX5\n6.393333\n0.2220177\n28.79651\n0.0e+00\n\n\n\n\n\nNow for our function.\n\nknitr::kable(OLS(y,X))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd.Error\nt-Stat\nLower 95% CI\nUpper 95% CI\np-Value\n\n\n\n\nβ1\n2.068600\n0.2034276\n10.16873\n1.669889\n2.467311\n1.4e-06\n\n\nβ2\n4.815065\n0.2177389\n22.11394\n4.388305\n5.241826\n0.0e+00\n\n\nβ3\n4.092429\n0.2163911\n18.91219\n3.668311\n4.516548\n0.0e+00\n\n\nβ4\n2.672951\n0.2393738\n11.16643\n2.203787\n3.142115\n6.0e-07\n\n\nβ5\n6.393333\n0.2220177\n28.79651\n5.958187\n6.828480\n0.0e+00\n\n\n\n\n\nThe outputs are identical, so our function works perfectly!\n\nIf we want to test hypotheses jointly, we need to use the Wald test instead of the \\(t\\)-test. For some hypothesis \\(H_0:\\mathbf h(\\boldsymbol{\\beta}) = \\mathbf{0}\\) given by \\(\\mathbf h:\\mathbb R^K\\to\\mathbb R^q\\), our statistic is \\[W = \\mathbf h(\\hat{\\boldsymbol\\beta}_\\text{OLS} )'  \\left[\\frac{\\partial \\mathbf h}{\\partial\\boldsymbol{\\beta}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\frac{\\partial \\mathbf h}{\\partial\\boldsymbol{\\beta}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol\\beta}_\\text{OLS} ),\\] where \\(W \\overset{a}{\\sim}\\chi_q^2\\) under \\(H_0\\).\n\nExample 5.19 (F-Test) Consider the Gaussian linear model where all Gauss-Markov assumptions are met and \\(\\boldsymbol{\\varepsilon}\\sim N(\\mathbf{0}, \\sigma^2\\mathbf I)\\), along with the linear hypothesis that \\(\\mathbf H\\boldsymbol{\\beta}= \\boldsymbol{\\beta}_0\\) for a \\(q\\times K\\) matrix \\(\\mathbf H\\). In this case the Wald statistic is \\[\\begin{align*}\nW & = [\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'\\left[\\text{Var}\\left(\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbf{X}\\right)\\right]^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]\\\\\n  & = [\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[\\mathbf H\\text{Var}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbb{X}\\right)\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]\\\\\n  & = [\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[\\sigma^2\\mathbf H(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0] & \\left(\\text{Var}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbb{X}\\right) = \\sigma^2 (\\mathbb{X}'\\mathbb{X})^{-1}\\right)\\\\\n  & = \\frac{[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[\\mathbf H(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]}{\\sigma^2}\n\\end{align*}\\] We don’t know \\(\\sigma^2\\) so we cannot use this test statistic. Instead we define a new statistic \\(F\\) which uses the estimator \\(S^2\\). \\[\\begin{align*}\nF & = \\frac{W}{q}\\frac{\\sigma^2}{S^2}\\\\\n  & = \\frac{[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[\\mathbf H(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]}{\\sigma^2}\\cdot \\frac{1}{q}\\cdot \\frac{\\sigma^2}{S^2} \\cdot \\frac{n-K}{n-K}\\\\\n  & = \\frac{[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[\\sigma^2\\mathbf H(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]/q}{[(n-K)S^2/\\sigma^2]/(n-K)}\n\\end{align*}\\]\nUnder the assumptions that \\(\\boldsymbol{\\varepsilon}\\) is normally distributed, it can be shown that the denominator is distributed according to \\(\\frac{1}{n-K}\\chi_{n-K}^2\\). This along with the numerator being distributed according the \\(\\frac{1}{q}\\chi_q^2\\) means \\(F \\sim F_{q,n-K}\\). If we simplify the statistic \\(F\\) we have \\[ F = \\frac{[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]'[S^2\\mathbf H(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbf H']^{-1}[\\mathbf H\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0]}{q} \\sim F_{q,n-K},\\] and can test \\(\\mathbf H\\boldsymbol{\\beta}= \\boldsymbol{\\beta}_0\\) with the \\(F-\\)test.\n\nThe \\(F-\\)test is just a special case of the Wald test where we can derive an exact distribution of our test statistic. Most statistical softwares will present the \\(F-\\)stat associated with \\(H_0:\\boldsymbol{\\beta}= \\mathbf{0}\\) when you run a linear regression. This test corresponds to the hypothesis that our specification is completely wrong and none of the independent variables appear (jointly) relevant. Unfortunately, this test is not as robust as the Wald test, as it requires normal errors.\nThe next example is due to Greene (2018) and shows how a Wald test can be used in the context of model selection.\n\nExample 5.20 (Investement Model) Let \\(I_t\\) denote the investment in a fixed economy at time \\(t\\). One linear model for investment specifies: \\[ \\log I_t = \\beta_1 + \\beta_2 i_t + \\beta_3 \\Delta p_t + \\beta_4\\log Y_t + \\beta_5t + \\varepsilon_t\\] where \\(i_t\\) is the nominal interest rate, \\(\\Delta p_t\\) is the inflation rate, and \\(Y_t\\) is real output. We’ve also included a time trend \\(t\\) as an independent variable. Instead of the nominal interest rate, agents may care about the real (adjusted for inflation) interest rate \\(i_t - \\Delta p_t\\). So perhaps investment is only affected by inflation insofar that inflation determines the real interest rate.10 If this is the case our model is \\[ \\log I_t = \\beta_1 + \\beta_2 (i_t - \\Delta p_t) + \\beta_4\\log Y_t + \\beta_5t + \\varepsilon_t.\\] We can test if this second specification is favorable by estimating the first model, and then testing the hypothesis \\(H_0 : \\beta_2 + \\beta_3 = 0\\). Let’s perform a simulation where the null hypothesis is true, and perform to corresponding Wald test. For the sake of ease, we will assume everything is uniformly distributed instead of simulating values such that they are realistic in the economic context of the model.\n\nn <- 1000\nbeta <- c(1,0.5,-0.5, 2, 0.1)\nK <- 5 \n  \ni <- runif(n, 0, 10)\ndel_p <- runif(n, 0, 10)\ny <- runif(n, 0, 10)\nt <- 1:1000\ne <- runif(n,-1, 1)\n\nX <- cbind(1, i, del_p, log(y), t)\nI <- X %*% beta + e\n\nhat_beta <- OLS(I,X)[,1]\nres <- (y-X %*% hat_beta)\nS2 <- ((t(res) %*% res)/(n - K)) %>% as.numeric() \nvar_hat <- (S2) * solve( t(X) %*% X )\n\nH <- matrix(c(0,1,1,0,0), nrow = 1)\nalpha <- 0.95\nW <- t(H %*% hat_beta) %*% solve(H %*% var_hat %*% t(H)) %*% (H %*% hat_beta)\nC <- qchisq(0.95, 1)\nprint(c(W, C))\n\n[1] 2.187603e-05 3.841459e+00\n\n\nThe value of the Wald test stat is not even close to exceeding the critical value, so we fail to reject the null hypothesis and conclude that \\(\\beta_2 + \\beta_3 = 0\\)."
  },
  {
    "objectID": "ols.html#partialmarginal-effects-linear-projection-revisited",
    "href": "ols.html#partialmarginal-effects-linear-projection-revisited",
    "title": "5  The Classical Linear Model",
    "section": "5.10 Partial/Marginal Effects, Linear Projection Revisited",
    "text": "5.10 Partial/Marginal Effects, Linear Projection Revisited\nWhen interpreting the parameters \\(\\boldsymbol{\\beta}\\) in \\(\\mathcal P_\\text{LM}\\), it’s very common to think in terms of derivatives. We will define these derivatives according to Wooldridge (2010).\n\nDefinition 5.13 Suppose \\(Y\\) and \\(\\mathbf{X}= (X_1,\\ldots, X_K)\\) are a random variable and vector, respectively. The partial/marginal effect of \\(X_j\\) on \\(\\text{E}\\left[Y\\mid\\mathbf{X}\\right]\\) (sometimes called the partial effect of \\(X_j\\) on \\(Y\\)), is \\[ \\frac{\\partial \\text{E}\\left[Y\\mid\\mathbf{X}\\right]}{\\partial X_j}.\\] In the event that \\(X_j\\) is discrete, partial effects are given as the difference between \\(\\text{E}\\left[Y\\mid\\mathbf{X}\\right]\\) evaluated at two discrete values of \\(X_j\\).\n\nIf we have a linear model \\(Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\), we’re almost conditioned to conclude the marginal effect of \\(X_j\\) on \\(\\text{E}\\left[Y\\mid\\mathbf{X}\\right]\\) is \\(\\beta_j\\), but in general this is not true.\n\nExample 5.21 This example is due to this post. Suppose \\(Y=X\\beta + \\varepsilon\\) where \\(X\\sim N(0,1)\\) and \\(\\varepsilon = X^2 - 1\\). To insure that \\(\\beta\\) is identified, we need to verify that \\(\\text{E}\\left[\\varepsilon\\right] = 0\\) (we do not have an intercept) and \\(\\text{E}\\left[X\\varepsilon\\right] =0\\) (the multicollinearity assumption is trivially met). Note that \\(X^2\\sim \\chi_1^2\\), so \\(\\text{E}\\left[X^2\\right] = 1\\). We also have \\(\\text{E}\\left[X^3\\right] = 0\\), as the normal distribution is not skewed (skewness being defined as the third moment centered about the mean).\n\\[\\begin{align*}\n\\text{E}\\left[\\varepsilon\\right]&= \\text{E}\\left[X^2 - 1\\right]= \\text{E}\\left[X^2\\right] - 1 = 1 - 1 = 0\\\\\n\\text{E}\\left[X\\varepsilon\\right]&= \\text{E}\\left[X^3 - X\\right] = \\text{E}\\left[X^3\\right] - \\text{E}\\left[X\\right] = 0 - 0 = 0\n\\end{align*}\\] Our model is identified. Furthermore \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) will present consistent estimates of \\(\\beta\\). For the sake of illustration, let \\(\\boldsymbol{\\beta}= 2\\).\n\nbeta <- 2 \nN_sim <- 10000\n\nestimates <- vector(\"numeric\", N_sim-1)\nfor (n in 2:N_sim) {\n  X <- as.matrix(rnorm(n, 0, 1))\n  e <- X^2 - 1\n  y <- beta*X + e\n  estimates[n-1] <- OLS(y,X)[1]\n}\n\n\n\nShow code which generates figure\ntibble(\n  x = 2:N_sim, \n  y = estimates\n) %>% \n  ggplot(aes(x,y)) +\n  geom_line(size = 0.2) +\n  theme_minimal() +\n  labs(x = \"sample size\",\n       y = \"OLS estimate\")\n\n\n\n\n\n\n\n\n\nWe have estimated \\(\\beta = 2\\) consistently, but this is not the partial effect! We have\n\\[\\frac{\\partial \\text{E}\\left[Y\\mid X\\right]}{\\partial X}= \\frac{\\partial}{\\partial X}\\text{E}\\left[X\\beta + \\varepsilon \\mid X\\right] = \\frac{\\partial}{\\partial X}\\text{E}\\left[X\\beta + X^2 -1\\mid X\\right] = \\frac{\\partial}{\\partial X}[X\\beta + X^2 - 1] = \\beta + 2X \\neq \\beta.\\] This follows from the fact that \\(X\\) is only weakly exogenous, so \\(\\text{E}\\left[\\varepsilon\\mid X\\right]\\neq 0\\).\n\nIn general, we can still have \\(\\text{E}\\left[X_i\\varepsilon_i\\right] = 0\\) for all \\(i\\) where \\(\\varepsilon = g(\\mathbf{X})\\) for some nonlinear function \\(g\\), because weak exogeneity only insures that our error and regressors are uncorrelated (i.e they have no linear relationship). What we need is exogeneity so we can conclude \\[\\text{E}\\left[Y \\mid \\mathbf{X}\\right] = \\text{E}\\left[\\mathbf{X}\\boldsymbol{\\beta}\\mid \\mathbf{X}\\right] + \\underbrace{\\text{E}\\left[\\varepsilon \\mid \\mathbf{X}\\right]}_{\\mathbf{0}} = \\mathbf{X}\\boldsymbol{\\beta},\\] so \\[\\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial X_j} = \\beta_j.\\] At the heart of this issue is the relationship between the linear projection model and the (structural) linear model. Early on we emphasized that there was a difference between what we called the linear projection model and the linear model. The prior is concerned with the statistical association of \\(Y\\) and \\(\\mathbf{X}\\) and describes a feature of their joint density. Proposition @ref(prp:ceferr) established that by definition the error in this model, \\(\\varepsilon_c\\), satisfied \\(\\text{E}\\left[\\varepsilon_c\\mid\\mathbf{X}\\right] = \\mathbf{0}\\). In the case of the linear model, \\(\\varepsilon\\) has a structural interpretation and may not satisfy this property, but if it does the linear projection model and the linear model will coincide in the sense that \\(\\boldsymbol{\\beta}\\) is interpreted as a marginal effect. Some treatments of OLS, such as Cameron and Trivedi (2005), actually restrict their attention to the identification of the linear model such that \\(\\boldsymbol{\\beta}\\) is associated with a marginal effect, requiring exogeneity instead of weak exogeneity for identification.\n\nProposition 5.9 (Identification of Marginal Effects) Suppose \\(\\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\) is a linear model. If \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbf{X}\\right] = \\mathbf{0}\\) and \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), then \\[\\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial \\mathbf{X}} = \\boldsymbol{\\beta},\\] where \\(\\boldsymbol{\\beta}\\) is identified.\n\n\nProof. In this case, \\(\\boldsymbol{\\beta}\\) is identified as \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbf{X}\\right] = \\mathbf{0}\\) implies \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), and we are given \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\).\n\nMore generally, if \\(\\mathbf f(\\mathbf{X}) = [f_1(\\mathbf{X}), \\ldots, f_K(\\mathbf{X})]\\) are a series of continuous functions of regressors, and \\(Y = \\mathbf f(\\mathbf{X})\\beta + \\varepsilon\\), then \\[ \\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial X_j} = \\frac{\\partial \\mathbf f}{\\partial X_j}\\boldsymbol{\\beta}= \\sum_{\\ell =1}^K\\frac{\\partial f_\\ell}{\\partial X_j} \\boldsymbol{\\beta}.\\] For example, if \\(Y = \\beta_1 + \\beta_2\\log X_1 + \\beta_3 \\exp[ X_1X_2] + \\varepsilon\\) where \\(\\text{E}\\left[\\varepsilon \\mid X_1\\right]=0\\), then \\[ \\frac{\\partial \\text{E}\\left[Y\\mid X_1\\right]}{\\partial X_1} = \\frac{\\beta_1}{X_1} + \\beta_3X_2\\exp[X_1X_2].\\]\nReiss and Wolak (2007) provide a more nuanced discussion of \\(\\frac{\\partial \\text{E}\\left[Y\\mid X\\right]}{\\partial X}\\) in the context of structural models, and how it relates the the linear projection model we discussed at the opening."
  },
  {
    "objectID": "ols.html#frischwaughlovell-theorem",
    "href": "ols.html#frischwaughlovell-theorem",
    "title": "5  The Classical Linear Model",
    "section": "5.11 Frisch–Waugh–Lovell Theorem",
    "text": "5.11 Frisch–Waugh–Lovell Theorem\nEven if our model contains multiple regressors concatenated in the vector \\(\\mathbf{X}\\), we may be especially interested in a subset of regressors. For instance, in Example Example 5.5 we may be especially interested in the price of cars if we are a manufacturer, as estimating consumers’ sensitivity to price changes could give us valuable insights into maximizing our profit. In situations like this, is it possible to “ignore” the independent variables of secondary importance? The answer, as provided by Frisch and Waugh (1933) Lovell (1963), is “kind of”, and deals with the algebra of OLS.\nSuppose \\(\\mathbb{X}_1\\) and \\(\\mathbb{X}_2\\) are two random matrices of observations where \\(\\mathbb{X}= [\\mathbb{X}_1,\\mathbb{X}_2]\\). If \\(\\boldsymbol{\\beta}=[\\boldsymbol{\\beta}_1,\\boldsymbol{\\beta}_2]'\\), then \\[ \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}= \\mathbb{X}_1\\boldsymbol{\\beta}_1 + \\mathbb{X}_2\\boldsymbol{\\beta}_2 + \\boldsymbol{\\varepsilon}. \\] The first order condition associated with the least squares problem is now \\[ \\begin{bmatrix}\\mathbb{X}_1'\\mathbb{X}_1 & \\mathbb{X}_1'\\mathbb{X}_2\\\\\\mathbb{X}_2'\\mathbb{X}_1 & \\mathbb{X}_2'\\mathbb{X}_2\\end{bmatrix} \\begin{bmatrix} \\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1} \\\\ \\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} \\end{bmatrix} =  \\begin{bmatrix} \\mathbb{X}_1'\\mathbf{Y}\\\\ \\mathbb{X}_2'\\mathbf{Y}\\end{bmatrix}\\] If we expand this, we have \\[\\begin{align*}\n\\mathbb{X}_1'\\mathbb{X}_1\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1} + \\mathbb{X}_1'\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = \\mathbb{X}_1'\\mathbf{Y},\\\\\n\\mathbb{X}_2'\\mathbb{X}_1\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1} + \\mathbb{X}_2'\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = \\mathbb{X}_2'\\mathbf{Y}.\n\\end{align*}\\] If we solve the first equation for \\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1}\\), we have \\[\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1} = (\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'(\\mathbf{Y}-\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2}).\\] If we insert this into the second equation in our system, we have \\[\\begin{align*}\n&\\mathbb{X}_2'\\mathbb{X}_1[(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'(\\mathbf{Y}-\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2})] + \\mathbb{X}_2'\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = \\mathbb{X}_2'\\mathbf{Y}\\\\\n\\implies &  \\mathbb{X}_2'\\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'\\mathbf{Y}-\\mathbb{X}_2'\\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} + \\mathbb{X}_2'\\mathbb{X}_2\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = \\mathbb{X}_2'\\mathbf{Y}\\\\\n\\implies &  \\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2}[\\mathbb{X}_2'\\mathbb{X}_2 - \\mathbb{X}_2'\\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'\\mathbb{X}_2]= \\mathbb{X}_2'\\mathbf{Y}- \\mathbb{X}_2'\\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1'\\mathbf{Y}\\\\\n\\implies & \\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2}[\\mathbb{X}_2'(\\mathbf I - \\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1')\\mathbb{X}_2] = [\\mathbb{X}_2'(\\mathbf I - \\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1')\\mathbf{Y}]\\\\\n\\implies & \\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = [\\mathbb{X}_2'(\\mathbf I - \\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1')\\mathbb{X}_2]^{-1}[\\mathbb{X}_2'(\\mathbf I - \\mathbb{X}_1(\\mathbb{X}_1'\\mathbb{X}_1)^{-1}\\mathbb{X}_1')\\mathbf{Y}]\n\\end{align*}\\] Recalling that we defined a matrix \\(\\mathbb M =\\mathbf I - \\mathbb{X}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\) such that \\(\\hat{\\mathbf{e}} = \\mathbb M\\mathbf{Y}\\) where \\(\\mathbb M = \\mathbb M'\\) and \\(\\mathbb M^2 = \\mathbb M\\), we have \\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} &= [\\mathbb{X}_2'\\mathbb M_1\\mathbb{X}_2]^{-1}[\\mathbb{X}_2'\\mathbb M_1\\mathbf{Y}]\\\\\n& = [\\mathbb{X}_2'\\mathbb M_1'\\mathbb M_1\\mathbb{X}_2]^{-1}[\\mathbb{X}_2'\\mathbb M_1\\mathbf{Y}]\\\\\n& = [\\mathbb X_2^{*\\prime} \\mathbb X_2^{*}]^{-1}[\\mathbb{X}_2'\\mathbf{Y}^*] & (\\mathbb X_2^{*} = \\mathbb M_1\\mathbb{X}_2,\\ \\mathbf{Y}^* =\\mathbb M_1\\mathbf{Y})\n\\end{align*}\\] The estimator \\(\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2}\\) follows from regressing \\(\\mathbb M_1\\mathbb{X}_2\\) on \\(\\mathbb M_1\\mathbf{Y}\\), which correspond to the residuals \\(\\mathbb{X}_2 - \\mathbb{X}_1\\hat{\\boldsymbol\\gamma}_\\text{OLS}\\) and \\(\\mathbf Y - \\mathbb{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},1}\\).\n\nTheorem 5.3 (Frisch–Waugh–Lovell Theorem) For the linear model, \\(\\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}= \\mathbb{X}_1\\boldsymbol{\\beta}_1 + \\mathbb{X}_2\\boldsymbol{\\beta}_2 + \\boldsymbol{\\varepsilon}\\), \\[\\hat{\\boldsymbol{\\beta}}_{\\text{OLS},2} = [\\mathbb X_2^{*\\prime} \\mathbb X_2^{*}]^{-1}[\\mathbb{X}_2'\\mathbf{Y}^*],\\] where \\(\\mathbb X_2^{*\\prime}\\) and \\(\\mathbf{Y}^*\\) are the residual vectors from least squares regression of \\(\\mathbb{X}_2\\) and \\(\\mathbf{Y}\\) on \\(\\mathbb{X}_1\\), respectively.\n\nOne of the useful applications of this theorem deals with visualization.\n\nExample 5.22 Suppose \\(Y = 1 + 4 X_1 + 2 X_2 + 8 X_3 + 3X_4 + \\varepsilon\\). Let’s estimate this model for simulated data.\n\nbeta <- c(1,4,2,8,3)\nn <- 100\nX <- rmvnorm(n, mean = rep(0,4), diag(1,4))\nX <- cbind(1, X)\ne <- runif(n,-1,1)\ny <- X %*% beta + e\n\nOLS(y,X)\n\n   Estimate  Std.Error    t-Stat Lower 95% CI Upper 95% CI p-Value\nβ1 1.001301 0.06590534  15.19302    0.8721294     1.130474       0\nβ2 4.120027 0.06091637  67.63415    4.0006334     4.239421       0\nβ3 1.953073 0.05279017  36.99692    1.8496066     2.056540       0\nβ4 8.018764 0.06786839 118.15167    7.8857443     8.151784       0\nβ5 2.967293 0.06105884  48.59728    2.8476203     3.086967       0\n\n\nIf we’re interested in the \\(\\hat\\beta_{\\text{OLS},2}\\), we may want to visualize it. Unfortunately, our parameter space is a subset of \\(\\mathbb R^5\\), so it isn’t feasible to plot the hyperplane correspond to our estimated model over our sample. Fortunately, we can use the Frisch–Waugh–Lovell Theorem.\n\nX1 <- X[,-2]\nX2 <- X[,2]\ny_res <- y - X1 %*% OLS(y, X1)[,1]\nX2_res <- X2 - X1 %*% OLS(X2, X1)[,1]\nOLS(y_res, X2_res)\n\n   Estimate  Std.Error   t-Stat Lower 95% CI Upper 95% CI p-Value\nβ1 4.120027 0.05967305 69.04335      4.00307     4.236984       0\n\n\nWe end up with the same estimate \\(\\hat\\beta_{\\text{OLS},2}\\), and can visualize it by plotting the dependent and independent variables in this alternate regression.\n\n\nShow code which generates figure\ntibble(\n  x = X2_res, \n  y = y_res\n) %>% \n  ggplot(aes(x,y)) +\n  geom_point() +\n  theme_minimal() +\n  geom_smooth(method = \"lm\", col = \"blue\", size = 0.5, se = FALSE) +\n  labs(x = \"x2 Residuals\", y = \"y Residuals\") \n\n\n\n\n\n\n\n\n\nWhile our estimate is the same, the standard error associated with the estimate is not quite the same. For the first regression, we had \\(K = 5\\) regressors, whereas the second had \\(K = 1\\). This affects how \\(\\widehat{\\text{se}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\) is calculated, as the numerator of \\(S^2\\) which ensures it is unbiased is \\(n - K\\). We have \\(\\widehat{\\text{se}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\propto \\sqrt{n-K}\\), so if we scale the standard error in the second regression by \\(\\sqrt{(n - 1)/(n - 5)}\\).\n\nincorrect_se <- OLS(y_res, X2_res)[2]\nincorrect_se * sqrt((n-1)/(n-5))\n\n[1] 0.06091637\n\n\nDespite being well known, the relationship between the standard errors of each regression seem to have gone unformalized until Ding (2021)."
  },
  {
    "objectID": "ols.html#recap",
    "href": "ols.html#recap",
    "title": "5  The Classical Linear Model",
    "section": "5.12 Recap",
    "text": "5.12 Recap\nThe linear model, along with the array of assumptions and what they yield, can be quite a bit to take in. The following table shows the cumulative properties given by the addition of each assumption.\n\n\n\n\n\n\n\n\n\n\\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\)\n\\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]= \\mathbf{0}\\)\n\\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'\\mid \\mathbb{X}\\right]= \\sigma^2\\mathbf I\\)\n\\(\\boldsymbol{\\varepsilon}\\sim N(\\mathbf{0},\\sigma^2\\mathbf I)\\)\n\n\n\n\n\\((\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})\\) identified\n\\((\\boldsymbol{\\beta},\\boldsymbol{\\Sigma})\\) identified\n\\((\\boldsymbol{\\beta},\\sigma^2)\\) identified\n\\((\\boldsymbol{\\beta},\\sigma^2)\\) identified\n\n\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\)\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\)\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\)\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\)\n\n\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Asymptotically Normal\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Asymptotically Normal\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Asymptotically Normal\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Normal\n\n\n\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Unbiased\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Unbiased\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) Unbiased\n\n\n\n\\(\\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial \\mathbf{X}} = \\boldsymbol{\\beta}\\)\n\\(\\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial \\mathbf{X}} = \\boldsymbol{\\beta}\\)\n\\(\\frac{\\partial \\text{E}\\left[Y\\mid \\mathbf{X}\\right]}{\\partial \\mathbf{X}} = \\boldsymbol{\\beta}\\)\n\n\n\n\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) BLUE\n\\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) BLUE\n\n\n\nWe also have maintained two implicit assumptions throughout: IID regressors, and the model is correctly specified."
  },
  {
    "objectID": "ols.html#rep1",
    "href": "ols.html#rep1",
    "title": "5  The Classical Linear Model",
    "section": "5.13 Example/Replication",
    "text": "5.13 Example/Replication\nChapter 4 of Greene (2018) provides a useful exercise in the form of replication Christensen and Greene (1976) who estimate the economies of scale in US electrical power generation.\nIn response to rising electrical rates in the United States during the mid-20th century, Christensen and Greene (1976) consider whether vertically integrated electrical providers (who generate, transmit, and distribute electricity) should be disintegrated. If these firms could only generate electricity as a result of regulation, then these firms would have to compete to sell generated electricity to separate intermediary firms which would transmit and distribute it to consumers. This competition has the potential to lower prices. At the same time, perhaps the vertical integration of firms cuts costs, which may actually lead to better prices for consumers. To determine if this is the case, we can consider the economies of scale of firms.\nEconomies of scale refers to the phenomenon where the more quantity of a good a firm produces, the lower the costs associated with that production. A firms cost, \\(C\\), will be a function of output \\(Q\\), and prices of inputs \\(P_1,\\ldots,P_k\\) used in production. We will specify our cost function as: \\[\\begin{align*}\n\\log C &= \\alpha + \\beta\\log Q + \\gamma[(\\log Q)^2/2] + \\sum_{i=1}^k\\delta_i\\log P_i & (\\textstyle\\sum_{i=1}^k\\delta_i = 1)\n\\end{align*}\\]\nThe functional form may look a bit arbitrary, but it’s consistent with a handful of appealing properties we expect a cost function to exhibit, the most important of which being a U-shape in quantity corresponding to decreasing costs as quantity increases up to a point (economies of scale), followed by increasing costs. Christensen, Jorgenson, and Lau (1973) provide all the details about this functional form. This point is given as the minimum of \\(C\\), which will coincide with the minimum of \\(\\log C\\) because \\(\\log\\) is a monotonic function. The first order condition for this minimization is \\[\\begin{align*}\n&\\frac{\\partial \\log C}{\\partial \\log Q}  = 1, \\\\\n\\implies & \\beta + \\gamma \\log Q^* = 1,\\\\\n\\implies & Q^* = \\exp\\left(\\frac{1-\\beta}{\\gamma}\\right),\n\\end{align*}\\] where the derivative is set to \\(1\\) instead of \\(0\\) because \\(\\log(0) = 1\\). If we are able to estimate \\((\\beta,\\gamma)\\) we can calculate \\(Q^*\\). If we observe \\(Q_i > Q^*\\), for many firms \\(i,\\) then the same output \\(Q\\) could be produced at a lower cost if the market was comprised of a larger number of firms producing a smaller output.\nThe data from Christensen and Greene (1976) is available here.11\n\nCG_1976 <- read_csv(\"data/christensen_greene_1976.csv\")\n\nThe first few rows of the data give us an idea of what type of information we have.\n\n\n\n\n\ncost\nfuel\noutput\ncapital\nlabor\n\n\n\n\n0.2130\n18.000\n8\n64.945\n6869.47\n\n\n3.0427\n21.067\n869\n68.227\n8372.96\n\n\n9.4059\n41.530\n1412\n40.692\n7960.90\n\n\n0.7606\n28.539\n65\n41.243\n8971.89\n\n\n2.2587\n39.200\n295\n71.940\n8218.40\n\n\n1.3422\n35.510\n183\n74.430\n5063.49\n\n\n\n\n\nWe observe the unit prices of three inputs: capital, labor, and fuel. Taking these into account, the cost function becomes \\[\\log C = \\alpha + \\beta\\log Q + \\frac{1}{2}\\gamma[(\\log Q)^2/2] + \\delta_k \\log P_k + \\delta_l \\log P_l + \\delta_f \\log P_f,\\] where \\(\\delta_k + \\delta_l + \\delta_f = 1\\). We can rewrite out model to implicitly satisfy the constraint if we write \\(\\delta_f = 1 -\\delta_l - \\delta_f\\):\n\\[\\begin{align*}\n&\\log C = \\alpha + \\beta\\log Q + \\gamma[(\\log Q)^2/2] + \\delta_k \\log P_k + \\delta_l \\log P_l + (1 -\\delta_l - \\delta_f) \\log P_f\\\\\n\\implies & \\log C - \\log P_f= \\alpha + \\beta\\log Q + \\gamma[(\\log Q)^2/2] + \\delta_k( \\log P_k - \\log P_f) + \\delta_l (\\log P_l-\\log P_f) \\\\\n\\implies & \\log(C/P_f) = \\alpha + \\beta\\log Q +\\gamma[(\\log Q)^2/2] + \\delta_k \\log (P_k/P_f) + \\delta_l \\log (P_l/P_f)\n\\end{align*}\\]\nThis model cannot account for all the possible factors which influence a firm’s costs, so we will introduce the element \\(\\varepsilon\\) to account for unobserved factors which influence cost, and assume that all our regressors are exogenous.12 This gives us our estimable model. \\[ \\log(C/P_f) = \\alpha + \\beta\\log Q +\\gamma[(\\log Q)^2/2] + \\delta_k \\log (P_k/P_f) + \\delta_l \\log (P_l/P_f) + \\varepsilon\\]\n\nCG_1976 <- CG_1976 %>% \n  mutate(\n    C = log(cost/fuel),\n    Q = log(output),\n    Q2 = Q^2/2,\n    P_kf = log(capital/fuel),\n    P_lf = log(labor/fuel)\n    )\n\nmodel <- lm(C ~ Q + Q2 + P_kf + P_lf, data = CG_1976)\nsummary(model)\n\n\nCall:\nlm(formula = C ~ Q + Q2 + P_kf + P_lf, data = CG_1976)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42576 -0.08891 -0.00223  0.08404  0.37363 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.818163   0.252439 -27.009  < 2e-16 ***\nQ            0.402745   0.031483  12.792  < 2e-16 ***\nQ2           0.060895   0.004325  14.079  < 2e-16 ***\nP_kf         0.162034   0.040406   4.010 9.46e-05 ***\nP_lf         0.152445   0.046597   3.272  0.00132 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1378 on 153 degrees of freedom\nMultiple R-squared:  0.9922,    Adjusted R-squared:  0.992 \nF-statistic:  4880 on 4 and 153 DF,  p-value: < 2.2e-16\n\n\nWe can use our estimates \\(\\hat \\beta\\) and \\(\\hat \\gamma\\) to calculate \\(\\hat Q^*\\).\n\nbeta_hat <- model$coefficients[2]\ngamma_hat <- model$coefficients[3]\nQ_hat <- exp((1-beta_hat)/gamma_hat)\nQ_hat\n\n      Q \n18177.1 \n\n\nThe straightforward way of determining if costs are higher than they need to be as a result of vertical integration is by counting the number of firms for which \\(Q > \\hat Q^*\\). This doesn’t account for the degree to which firms are exceeding the efficient scale. To do this, let’s consider the total output produced by firms which are exceeding the efficient scale, opposed to those which are not.\n\nCG_1976 %>% \n  mutate(inefficient = (output > Q_hat)) %>% \n  group_by(inefficient) %>% \n  summarize(\n    n_firms = n(),\n    total_output = sum(output)\n  ) %>% \n  mutate(prop_output = total_output/sum(total_output)) %>% \n  knitr::kable()\n\n\n\n\ninefficient\nn_firms\ntotal_output\nprop_output\n\n\n\n\nFALSE\n133\n711548.9\n0.4301554\n\n\nTRUE\n25\n942618.0\n0.5698446\n\n\n\n\n\n57% of output is attributed to the 25 firms producing over the efficient scale. This seems to indicate that the market is too vertically integrated."
  },
  {
    "objectID": "ols.html#further-reading",
    "href": "ols.html#further-reading",
    "title": "5  The Classical Linear Model",
    "section": "5.14 Further Reading",
    "text": "5.14 Further Reading\nConditional Expectation and Linear Projection: Chapter 2 of Wooldridge (2010), Chapter 2 of Hansen (2022), Chapter 3 of Angrist and Pischke (2008)\nStructural Modeling: Chapter 1 of Greene (2018) , Reiss and Wolak (2007), Goldberger (1972), portions of Chapter 2 of Cameron and Trivedi (2005)\nOLS: Chapter 4 of Wooldridge (2010), Chapters 3-5 of Greene (2018), portions of Chapter 1-2 of Hayashi (2011)\nModel selection: Phillips (1996) Hansen (2005) Leamer (1978) Hendry (2000) (davidson1981several?) Hendry et al. (1995)"
  },
  {
    "objectID": "ols.html#sec-proj",
    "href": "ols.html#sec-proj",
    "title": "5  The Classical Linear Model",
    "section": "5.15 Math Appendix: Projection",
    "text": "5.15 Math Appendix: Projection\nLinear projection can be generalized far beyond the setting of a random vector \\((Y,\\mathbf{X})\\). I’ll quickly define projection in the general setting of Hilbert spaces. For details, see Rudin (1987), Royden and Fitzpatrick (1988), or Folland (1999).\nA normed vector space \\(V\\) defined over a field \\(F\\) is, as the name implies, a vector space equipped with a norm \\(\\left\\lVert\\cdot\\right\\rVert:V\\mapsto [0,\\infty)\\) satisfying:\n\n\\(\\left\\lVert v\\right\\rVert = 0 \\iff v = 0\\);\n\\(\\left\\lVert av\\right\\rVert = \\left\\lvert a\\right\\rvert\\left\\lVert v\\right\\rVert\\);\n\\(\\left\\lVert w + v\\right\\rVert \\le \\left\\lVert w\\right\\rVert + \\left\\lVert v\\right\\rVert\\);\n\nfor all vectors \\(v,w\\in V\\) and scalars \\(a\\in F\\). The norm tells us how “far” a vector \\(v\\in V\\) is from the zero element (“origin”) \\(0\\in V\\). Any normed vector space is also a metric space if we define a metric as \\(d(v,w)=\\left\\lVert v-w\\right\\rVert\\). With this metric comes the familiar definitions of convergence. If \\(V\\) is a complete metric space (all Cauchy sequences converge in \\(V\\)), then we say \\(V\\) is a Banach space.\nA Hilbert space is a complete vector space \\(H\\) (defined over a field of scalars \\(F\\)) equipped with an inner product \\(\\langle\\cdot,\\cdot\\rangle: H\\times H\\to F\\) which satisfies:\n\n\\(\\langle x,y \\rangle = \\langle y,x \\rangle\\);\n\\(\\langle cx,y \\rangle = c\\langle y,x \\rangle\\)\n\\(\\langle x+z,y \\rangle = \\langle x,z \\rangle + \\langle y,z \\rangle\\);\n\\(\\langle x,x \\rangle > 0 \\iff x\\neq 0\\);\n\nfor all \\(x,y\\in H\\) and \\(c\\in F\\). All Hilbert spaces are normed vector spaces, as \\(\\left\\lVert x\\right\\rVert = \\sqrt{\\langle x,x \\rangle}\\) satisfies all the properties of a norm. This makes a Hilbert space a Banach space, as we’ve assumed \\(H\\) is complete. The development of Hilbert spaces was motivated by Euclidean space, as the vector space \\(\\mathbb R^k\\) (over the field of scalars \\(\\mathbb R\\)) is a Hilbert space: \\[\\begin{align*}\n\\langle \\mathbf{x},\\mathbf{y}\\rangle &= \\mathbf{x}\\cdot\\mathbf{y}= \\sum_{i=1}^k x_iy_i,\\\\\n\\left\\lVert\\mathbf{x}\\right\\rVert &= \\left(\\sum_{i=1}^k x_i^2\\right)^{1/2},\\\\\nd(\\mathbf{x},\\mathbf{y}) &= \\left(\\sum_{i=1}^k(y_i - x_i)^2\\right)^{1/2}.\n\\end{align*}\\]\nAnother useful Hilbert space is a special case of a normed vector space known as an \\(L^p\\) space. For a measure space \\((X,\\mathcal N, \\mu)\\), define \\(L^p\\) to be the set of all measurable (real) functions \\(f:X\\to\\mathbb R\\). The norm for this space is \\[ \\left\\lVert f\\right\\rVert_p = \\left(\\int\\left\\lvert f\\right\\rvert^p\\ d\\mu\\right)^{1/p}.\\] In general \\(L^p\\) spaces are not Hilbert spaces, but if \\(p=2\\), then they are. In this case the inner product is\n\\[\\langle f,g \\rangle = \\int\\left\\lvert f\\right\\rvert\\left\\lvert g\\right\\rvert\\ d\\mu.\\] In the event that the measure space is \\((\\mathbb Z^+, \\mathbb Z^+, \\mu)\\) where \\(\\mu\\) is the counting measure, then a measurable function \\(f:\\mathbb Z^+\\to\\mathbb R\\) is a sequence of numbers real \\(\\{x_1,x_2,\\ldots\\}\\). In this case we denote the \\(L^p\\) space as \\(\\ell^p\\) and have \\[ \\left\\lVert f\\right\\rVert_p = \\left(\\int\\left\\lvert f\\right\\rvert^p\\ d\\mu\\right)^{1/p} = \\left(\\sum_{i=1}^\\infty |x_i|^p\\right)^{1/2}.\\] If we \\(\\{x_1,x_2,\\ldots,x_k, 0,0,0,\\ldots \\}\\in\\ell ^k\\), we could also consider this an element of \\(\\mathbb R^k\\) because the trailing zeros. In this sense, Euclidean space is an \\(L^p\\) space: \\[\\left\\lVert\\mathbf{x}\\right\\rVert_2 = \\left(\\sum_{i=1}^k x_i^2\\right)^{1/2}.\\]\nTwo elements of a Hilbert space are orthogonal if \\(\\langle x,y \\rangle = 0\\). If \\(S\\subset H\\) is a subspace of a Hilbert space \\(H\\), there exists a unique element \\(\\hat y\\in S\\) such that \\[\\begin{align*}\n\\left\\lVert x - \\hat y\\right\\rVert &= \\inf_{y\\in S}\\left\\lVert x - y\\right\\rVert,\\\\\n\\langle x-\\hat y,z \\rangle &= 0 & (\\forall z\\in S).\n\\end{align*}\\] We refer to \\(\\hat y\\) as the projection of \\(x\\) onto \\(S\\).\nFor a probability space \\((\\Omega, \\mathcal F, P)\\). If we define a \\(L^2\\) space as all the random variables (measurable functions) with squared values that are integrable, then \\[ \\langle X,Y \\rangle = \\int\\ xy\\ dP = \\text{E}\\left[XY\\right].\\] This is why we refer to random variables as orthogonal when \\(\\text{E}\\left[XY\\right] = 0\\)! In the event we have a random vector \\(\\mathbf{Z}= (Y,\\mathbf{X})\\), the projection of \\(Y\\) onto \\(\\mathbf{X}\\) happens to be \\(\\text{E}\\left[Y\\mid \\mathbf{X}\\right]\\).\n\n\n\n\n\n\nAngrist, Joshua D, and Jörn-Steffen Pischke. 2008. “Mostly Harmless Econometrics.” In Mostly Harmless Econometrics. Princeton university press.\n\n\nCameron, A Colin, and Pravin K Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge university press.\n\n\nCard, David. 1995. “Using Geographic Variation in College Proximity to Estimate the Return to Schooling.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\n———. 1999. “The Causal Effect of Education on Earnings.” Handbook of Labor Economics 3: 1801–63.\n\n\n———. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60.\n\n\nChristensen, Laurits R, and William H Greene. 1976. “Economies of Scale in US Electric Power Generation.” Journal of Political Economy 84 (4, Part 1): 655–76.\n\n\nChristensen, Laurits R, Dale W Jorgenson, and Lawrence J Lau. 1973. “Transcendental Logarithmic Production Frontiers.” The Review of Economics and Statistics, 28–45.\n\n\nCobb, Charles W, and Paul H Douglas. 1928. “A Theory of Production.” The American Economic Review 18 (1): 139–65.\n\n\nDing, Peng. 2021. “The Frisch–Waugh–Lovell Theorem for Standard Errors.” Statistics & Probability Letters 168: 108945.\n\n\nFolland, Gerald B. 1999. Real Analysis: Modern Techniques and Their Applications. Vol. 40. John Wiley & Sons.\n\n\nFrisch, Ragnar, and Frederick V Waugh. 1933. “Partial Time Regressions as Compared with Individual Trends.” Econometrica: Journal of the Econometric Society, 387–401.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in Hereditary Stature.” The Journal of the Anthropological Institute of Great Britain and Ireland 15: 246–63.\n\n\nGoldberger, Arthur S. 1972. “Structural Equation Methods in the Social Sciences.” Econometrica: Journal of the Econometric Society, 979–1001.\n\n\n———. 1991. A Course in Econometrics. Harvard University Press.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson Education.\n\n\nGriliches, Zvi, and William M Mason. 1972. “Education, Income, and Ability.” Journal of Political Economy 80 (3, Part 2): S74–103.\n\n\nHansen, Bruce. 2005. “Challenges for Econometric Model Selection.” Econometric Theory 21 (1): 60–68.\n\n\n———. 2022. Econometrics. Princeton University Press.\n\n\nHayashi, Fumio. 2011. Econometrics. Princeton University Press.\n\n\nHendry, David F et al. 1995. Dynamic Econometrics. Oxford University Press on Demand.\n\n\nHendry, David F. 2000. Econometrics: Alchemy or Science?: Essays in Econometric Methodology. OUP Oxford.\n\n\nLeamer, Edward E. 1978. Specification Searches: Ad Hoc Inference with Nonexperimental Data. Vol. 53. John Wiley & Sons Incorporated.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of Identification in Econometrics.” Journal of Economic Literature 57 (4): 835–903.\n\n\nLovell, Michael C. 1963. “Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis.” Journal of the American Statistical Association 58 (304): 993–1010.\n\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2 (4): 357–462.\n\n\nPhillips, Peter CB. 1996. “Econometric Model Determination.” Econometrica: Journal of the Econometric Society, 763–812.\n\n\nReiss, Peter C, and Frank A Wolak. 2007. “Structural Econometric Modeling: Rationales and Examples from Industrial Organization.” Handbook of Econometrics 6: 4277–4415.\n\n\nRoyden, Halsey Lawrence, and Patrick Fitzpatrick. 1988. Real Analysis. Vol. 32. Macmillan New York.\n\n\nRudin, Walter. 1987. Real and Complex Analysis. McGraw-hill New York.\n\n\nStock, James H, and Mark W Watson. 2003. Introduction to Econometrics. Vol. 104. Addison Wesley Boston.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. MIT press.\n\n\n———. 2015. Introductory Econometrics: A Modern Approach. Cengage learning."
  },
  {
    "objectID": "endog.html",
    "href": "endog.html",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "",
    "text": "Our first departure from the classical linear model \\(\\mathcal P_\\text{LM}\\) will come in the form of dropping the assumption that \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), i.e our regressors are endogenous. This situation is rather serious, as it prevents the linear model from being identified. Furthermore, the problem is common in applications. There are three main sources of endogeneity:\nFor now we’ll consider the first two, and discuss the third on in Section @ref(endogeniety-ii-simultaneous-equation-models). Fortunately, we can address endogeneity with the instrumental variables estimator (a special case of which is the two-stage least squares estimator)."
  },
  {
    "objectID": "endog.html#omitted-variables-and-measurement-error",
    "href": "endog.html#omitted-variables-and-measurement-error",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.1 Omitted Variables and Measurement Error",
    "text": "6.1 Omitted Variables and Measurement Error\n\nExample 6.1 Recall the Example @ref(exm:endogex) where we considered a model relating income to education and other determinants of salary. Suppose the true model is \\[ \\log(income_i) = \\beta_1 + \\beta_2\\cdot educ_i + \\beta_3 \\cdot experiance_i + \\varepsilon_i,\\] where \\(\\varepsilon_i\\) are the unobserved factors impacting salary, \\(educ_i\\) measure years of post-secondary education, \\(experiance_i\\) measures years of work experience, and \\(\\text{Cov}\\left(educ, experiance\\right) < 0\\) (the longer you go to school, the less work experience you tend to have). Now suppose we incorrectly specify the model \\[ \\log(income_i) = \\gamma_1 + \\gamma_2\\cdot educ_i + u_i,\\] where \\(u_i\\) are all other factors impacting salary. We’ve omitted \\(experiance_i\\) as a regressor, so it’s implicitly included in \\(u_i\\): \\[ u_i =  \\beta_3 \\cdot experiance_i + \\varepsilon_i.\\] We no longer satisfy the assumption \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), as \\(\\text{Cov}\\left(educ_i, u_i\\right) \\neq 0\\) because \\(\\text{Cov}\\left(educ, experiance\\right) < 0\\). What happens if we go ahead and attempt to estimate \\(\\beta_1\\) and \\(\\beta_2\\) anyway? Set \\(\\boldsymbol{\\beta}= [1,3,2]'\\), and \\(\\varepsilon_i \\overset{iid}{\\sim}N(0,1)\\).\n\nbeta <- c(1,3,2)\nn <- 1000\nmu <- c(4,10)\nSigma <- matrix(c(2, -0.5, -0.5, 5), nrow = 2)\nX <- rmvnorm(n, mu, Sigma)\ne <- rnorm(n)\nlog_y = beta[1] + X %*% beta[-1] + e\n\nmodel_df <- tibble(log_income = as.numeric(log_y),\n                       educ = X[,1],\n                       exper = X[,2])\nmodel <- lm(log_income ~ educ, data = model_df)\nsummary(model)$coefficients\n\n             Estimate Std. Error  t value      Pr(>|t|)\n(Intercept) 22.525979  0.4349000 51.79577 4.282241e-285\neduc         2.639973  0.1034311 25.52397 5.178448e-111\n\n\nThe true parameters don’t even fall within the 95% confidence intervals centered at our estimates.\n\nIn general, suppose \\[\\mathbf{Y}= [\\mathbb{X},\\mathbb{Z}][\\boldsymbol{\\beta}, \\boldsymbol \\delta]' + \\boldsymbol{\\varepsilon}= \\mathbb{X}\\boldsymbol{\\beta}+ \\mathbb{Z}\\boldsymbol \\delta + \\boldsymbol{\\varepsilon},\\] where we attempt to estimate \\(\\boldsymbol{\\beta}\\) via \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) despite omitting regressors \\(\\mathbf Z\\) from our model, and all our Gauss-Markov assumptions are met. Note that \\(\\boldsymbol{\\beta}\\) is still identified, as \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\). \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} & = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{Y}\\\\\n& = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'(\\mathbb{X}\\boldsymbol{\\beta}+ \\mathbb{Z}\\boldsymbol \\delta + \\boldsymbol{\\varepsilon})\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{Z}\\boldsymbol\\delta  + (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}.\n\\end{align*}\\] Our estimator is now inconsistent: \\[\\begin{align*}\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} & = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{Z}\\boldsymbol\\delta\\right] + \\mathop{\\mathrm{plim}}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\right]\\underbrace{\\mathop{\\mathrm{plim}}\\left[\\mathbb{X}'\\boldsymbol{\\varepsilon}\\right]}_\\mathbf{0}\\\\\n& =  \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left[(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbb{Z}\\boldsymbol\\delta\\right].\n\\end{align*}\\]\nThis phenomenon is referred to as omitted variable bias (OVB). The use of the word “bias” here is a bit misleading, but follows from an interpretation of inconsistency as a persistent bias despite \\(n\\to\\infty\\)\n\nExample 6.2 (OVB with a Simple Linear Model) Suppose our linear model is \\(Y = \\alpha + \\beta X+ \\gamma Z + \\varepsilon\\), and we attempt to estimate \\((\\beta_0,\\beta_1)\\) with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} \\). In this case, \\[\\begin{align*}\n\\hat \\beta_\\text{OLS,OV} & =\\frac{\\sum_{i=1}^n (X_i -\\bar X)(Y_i - \\bar Y)}{\\sum_{i=1}^n (X_i -\\bar X)^2}\\\\ & = \\frac{\\sum_{i=1}^n (X_i -\\bar X)[(\\alpha + \\beta X+ \\gamma Z + \\varepsilon) - \\bar Y]}{\\sum_{i=1}^n (X_i -\\bar X)^2}\\\\ & = \\beta + \\frac{\\sum_{i=1}^n (X_i -\\bar X)(Z_i - \\bar Z)}{\\sum_{i=1}^n (X_i -\\bar X)^2}\\\\ & = \\boldsymbol{\\beta}+ \\gamma \\cdot \\frac{\\sum_{i=1}^n (X_i -\\bar X)(Z_i-\\bar Z)}{\\sum_{i=1}^n (X_i -\\bar X)^2}.\n\\end{align*}\\] The expectation of this is \\[\\hat\\beta_\\text{OLS,OV} \\overset{p}{\\to}\\beta + \\gamma \\cdot \\mathop{\\mathrm{plim}}\\frac{n^{-1}\\sum_{i=1}^n (X_i -\\bar X)(Z_i-\\bar Z)}{n^{-1}\\sum_{i=1}^n (X_i -\\bar X)^2} =\\boldsymbol{\\beta}+ \\gamma \\frac{\\text{Cov}\\left(X,Z\\right)}{\\text{Var}\\left(X\\right)} .\\]\nIf we let \\(\\alpha = 1\\), \\(\\beta = 2\\), \\(\\gamma = 3\\), and \\(\\text{Var}\\left(X\\right) = 1\\), then \\[\\mathop{\\mathrm{plim}}\\hat\\beta_\\text{OLS,OV} = 2 + 3\\text{Cov}\\left(X,Y\\right).\\] If we simulate this estimator for different values of \\(\\text{Cov}\\left(X,Y\\right)\\), taking \\(n\\) to be very large, then we should see our estimates approximately follow the line \\(2 + 3\\text{Cov}\\left(X,Y\\right)\\) when plotted against \\(\\text{Cov}\\left(X,Y\\right)\\).\n\nn <- 1e6\nmu <- c(4,10)\nestimates <- vector()\nfor (j in 1:100) {\n  Sigma <- matrix(c(1, -1 + j/50, -1 +j/50, 1), nrow = 2)\n  regressors <- rmvnorm(n, mu, Sigma)\n  x <- regressors[,1]\n  z <- regressors[,2]\n  e <- rnorm(n)\n  y <- 1 + 2*x + 3*z + e\n  estimates[j] <- summary(lm(y ~ x))$coefficients[2,1]\n}\n\n\n\n\n\n\ntest\n\n\n\n\n\nWhat happens if instead of omitting a variable from a model, our variables happen to be prone to some degree of measurement error. This is a common scenario in the social sciences where collected data is subject to human error, rounding errors, etc. Suppose a true linear model is specified as \\(\\mathbf{Y}= \\mathbb{X}^*\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\) for regressors \\(\\mathbf{X}^*\\) where \\(\\text{E}\\left[\\mathbf{X}^{*\\prime}\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\). Much like how we do not observe realizations of \\(\\boldsymbol{\\varepsilon}\\), we do not observe realizations of \\(\\mathbf{X}^*\\), instead observing \\(\\mathbf{X}= \\mathbf{X}^* + \\mathbf u\\) where \\(\\mathbf u\\) corresponds to measurement error. We’ll assume that this measurement error is independent of \\(\\boldsymbol{\\varepsilon}\\), independent of \\(\\mathbf{X}^*\\), and is mean zero.1 Our model can be rewritten as \\[ \\mathbf{Y}= \\mathbb{X}^*\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}= (\\mathbf{X}- \\mathbf u)\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}= \\mathbf{X}\\boldsymbol{\\beta}+ \\underbrace{(\\boldsymbol{\\varepsilon}- \\mathbf u\\boldsymbol{\\beta})}_{\\boldsymbol \\nu}.\\] In this case \\[\\text{E}\\left[\\mathbf{X}\\boldsymbol \\nu\\right] = \\text{E}\\left[(\\mathbf{X}^* + \\mathbf u)(\\boldsymbol{\\varepsilon}- \\mathbf u\\boldsymbol{\\beta})\\right] = -\\boldsymbol{\\beta}\\text{E}\\left[\\mathbf u'\\mathbf u\\right] = -\\boldsymbol{\\beta}\\text{Var}\\left(\\mathbf u\\right).\\] Much like in the case of OVB, \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) will be inconsistent.\n\\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} &= \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}\\\\\n     & = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'(\\boldsymbol{\\varepsilon}- \\mathbf u\\boldsymbol{\\beta})\\\\\n     & = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}- (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf u\\boldsymbol{\\beta}\\\\\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} & = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}+ \\mathop{\\mathrm{plim}}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf u\\boldsymbol{\\beta}\\\\\n  & = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf u\\boldsymbol{\\beta}& (\\mathop{\\mathrm{plim}}\\mathbb{X}'\\boldsymbol{\\varepsilon}= \\mathbf{0})\n\\end{align*}\\]\nFor \\(j\\neq 1\\) (there won’t be measurement error when \\(j=1\\), as this is just the regressor of 1 which gives the intercept) this simplifies to\n\\[ \\mathop{\\mathrm{plim}}\\hat \\beta_{\\text{OLS,ME},j} = \\beta_j \\left(\\frac{\\text{Var}\\left(X_j^*\\right)}{\\text{Var}\\left(X_j^*\\right) + \\text{Var}\\left(u_j^*\\right)}\\right).\\] This phenomenon is known as attenuation bias. The term in parentheses will always fall in the interval \\((0,1)\\), so \\(\\left\\lvert\\mathop{\\mathrm{plim}}\\hat \\beta_{\\text{OLS,ME},j}\\right\\rvert < \\left\\lvert\\boldsymbol{\\beta}_j\\right\\rvert\\), hence the name attentuation bias."
  },
  {
    "objectID": "endog.html#an-updated-linear-model-identification-and-the-iv-estimator",
    "href": "endog.html#an-updated-linear-model-identification-and-the-iv-estimator",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.2 An Updated Linear Model, Identification, and the IV estimator",
    "text": "6.2 An Updated Linear Model, Identification, and the IV estimator\nIn general, if \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\), then \\(\\boldsymbol{\\beta}\\) is not identified for the linear model \\(\\mathcal P_\\text{LM}\\). Estimation is a non-starter in this case. Even if we had the “perfect” estimate for \\(\\boldsymbol{\\beta},\\) the parameter may map to multiple elements \\(P_{\\boldsymbol{\\beta},\\sigma^2}\\in \\mathcal P_\\text{LM}\\), so it is impossible to determine which model value our data was drawn from. If we drop the assumption \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\) from the linear model, then we’ll need to replace it with some additional assumptions/structure.\nWhile \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\), perhaps it is the case that there exists some other random vector \\(\\mathbf{Z}\\) which does satisfy \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\). Is this helpful – no. For a given model with some structural error \\(\\boldsymbol{\\varepsilon}\\), there are nearly infinite candidates for \\(\\mathbf{Z}\\) which satisfy this. Consider the model \\[\\log(income_i) = \\beta_0 + \\beta_1\\cdot educ_i + \\varepsilon_i,\\] where \\(\\varepsilon\\) are unobserved factors impacting income. What are some random variables \\(Z\\) which are uncorrelated with \\(\\varepsilon\\). A ton! Weather during \\(i\\)’s tenth birthday, \\(i\\)’s first concert, \\(i\\)’s favorite flavor of ice cream, etc. There is an endless list of random variables that are so completely irrelevant to someone’s income, that they are uncorrelated with \\(\\varepsilon\\). This is why \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\) is sometimes read as “\\(\\mathbf{Z}\\) is orthogonal to \\(\\boldsymbol{\\varepsilon}\\).” Not only does it hold in the mathematical sense of the word, but it also holds in the colloquial sense of the word meaning “has nothing to do with.” What we want is \\(\\mathbf{Z}\\) to also be correlated with \\(\\mathbf{X}\\), such that \\(\\mathbf{Z}\\) is a sort of proxy/surrogate for \\(\\mathbf{X}\\) with no direct impact on \\(\\mathbf{Y}\\). We’ll now generalize the linear model to introduce this set of variables \\(\\mathbf{Z}\\) in lieu of the assumption \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\).\n\nDefinition 6.1 The (linear) instrumental variables (IV) model is defined as \\(\\mathcal P_\\text{IV} = \\{P_{\\boldsymbol{\\beta},\\sigma^2} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\sigma^2\\in\\mathbb R\\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\sigma^2} &= \\{F_{\\mathbb{X},\\mathbb{Z},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}, \\ \\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbf{X}\\right]=\\sigma^2\\mathbf I,\\ f_{(\\mathbb{X},\\mathbb{Z})}=\\textstyle\\prod_{i=1}^n f_{(\\mathbf{X}_i,\\mathbf{Z}_i)},\\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K,\\ \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] =\\boldsymbol \\eta,\\  \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{Z}\\right] = \\mathbf{0}, \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right] \\neq \\mathbf{0}\\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbb{Z}& = [\\mathbf{Z}_1, \\cdots, \\mathbf{Z}_j, \\cdots \\mathbf{Z}_K] = [\\mathbf{Z}_1, \\cdots, \\mathbf{Z}_i, \\cdots \\mathbf{Z}_n]',\\\\\n\\dim(\\mathbf{Z}) & = \\dim(\\mathbf{X}) = K\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n].\n\\end{align*}\\] We refer to the random vector \\(\\mathbf{Z}\\) as instrumental variables (IVs). We have assumed that \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{Z}\\right] = \\mathbf{0}\\), which subsumes the assumption that \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\). The assumption that \\(\\mathbf{Z}\\) is (weakly) exogenous (uncorrelated with \\(\\boldsymbol{\\varepsilon}\\)) is known as instrumental validity, while the assumption that \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right] \\neq \\mathbf{0}\\) (\\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) are correlated) is known as the relevance condition. The assumption that \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\) is also sometimes known as the exclusion restriction (\\(\\mathbf{Z}\\) is excluded from the determinants of \\(Y\\)). Finally, we sometimes refer to \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\) as the rank condition.\n\nInstrumental validity and the relevance condition are usually written as: \\[\\begin{align*}\n\\text{Cov}\\left(\\mathbf{Z},\\boldsymbol{\\varepsilon}\\right)& = \\mathbf{0},\\\\\n\\text{Cov}\\left(\\mathbf{X}, \\mathbf{Z}\\right) &\\neq \\mathbf{0}.\n\\end{align*}\\] Technically, this is not 100% accurate. The relevance condition is actually a combination of \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\) and \\(\\text{Cov}\\left(\\mathbf{X}, \\mathbf{Z}\\right)\\neq \\mathbf{0}\\). As highlighted by Wooldridge (2010), the relevance condition is not “regressors and instruments are uncorrelated”. Instead, it is “instruments and endogenous regressors are partially correlated holding the exogenous regressors fixed.” In other words, the linear projection of the instruments onto all regressors has nontrivial coefficients for endogenous regressors. This is equivalent to \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\) and \\(\\text{Cov}\\left(\\mathbf{X}, \\mathbf{Z}\\right)\\neq \\mathbf{0}\\).\nEven if one regressor is endogenous, \\(\\text{E}\\left[X_j\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\) for some \\(j\\), then \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] \\neq \\mathbf{0}\\). If this is the case, we can simply define a portion of \\(\\mathbf{Z}\\) to be the exogenous regressors. Formally, if \\([X_1,\\ldots, X_J]\\) are weakly exogenous while \\([X_{J+1}, \\ldots, X_K]\\) are endogenous, then define \\(Z_1 = X_1,\\ldots ,Z_J=X_j\\).\n\nExample 6.3 (Linear Model as Special Case of IV Model) A special case of the linear IV model is the classical linear model. Just let \\(\\boldsymbol\\eta = \\mathbf{0}\\) and \\(\\mathbf{X}= \\mathbf{Z}\\). This fact can be written as \\(\\mathcal P_\\text{LM}\\subset \\mathcal P_\\text{IV}\\).\n\nWhenever we define a new model, we need to make sure our parameters are identified.\n\nTheorem 6.1 (Identification of Linear IV Model) The linear IV model is identified as a result of the following assumptions: \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right] \\neq \\mathbf{0}\\), and \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\).\n\n\nProof. First we will show \\(\\boldsymbol{\\beta}\\) is identified. Let \\(P_{\\boldsymbol{\\beta},\\sigma^2} = P_{\\boldsymbol{\\beta}^*,\\sigma^2}\\) for two elements of \\(\\mathcal P_\\text{IV}\\), and suppose for a contradiction that \\(\\boldsymbol{\\beta}\\neq\\boldsymbol{\\beta}^*\\). We can begin by writing \\(\\boldsymbol{\\beta}\\): \\[\\begin{align*}\n&\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\mathbf{Z}'(Y-\\mathbf{X}\\boldsymbol{\\beta})\\right] = \\mathbf{0}& (\\boldsymbol{\\varepsilon}= (Y-\\mathbf{X}\\boldsymbol{\\beta}))\\\\\n\\implies & \\text{E}\\left[\\mathbf{Z}'Y\\right]-\\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]= \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\mathbf{Z}'Y\\right] = \\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\\\\n\\end{align*}\\] By the definition of \\(\\mathcal P_\\text{IV}\\), the moments \\(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\) and \\(\\text{E}\\left[\\mathbf{Z}'Y\\right]\\) are the same for the model values \\(P_{\\boldsymbol{\\beta},\\sigma^2}\\) and \\(P_{\\boldsymbol{\\beta}^*,\\sigma^2}\\), so \\(\\boldsymbol{\\beta}^*\\) must also satisfy \\(\\text{E}\\left[\\mathbf{Z}'Y\\right] = \\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\). This contradicts the assumption that \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\). If \\(\\boldsymbol{\\beta}\\) is identified, then \\(\\sigma^2\\) is as well because we can write it in terms of \\(\\boldsymbol{\\beta}\\): \\[ \\text{E}\\left[(\\mathbf{Y}-\\mathbb{X}\\boldsymbol{\\beta})'(\\mathbf{Y}-\\mathbb{X}\\boldsymbol{\\beta})\\mid \\mathbf{X}\\right]=\\sigma^2\\mathbf I.\\] space\n\nSo how do we estimate \\(\\boldsymbol{\\beta}\\) for the model \\(\\mathcal P_\\text{IV}\\). If it wasn’t clear from the examples dealing with OVB and measurement error, the answer is not with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\).\n\nProposition 6.1 (Inconsisteny of OLS) If \\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{IV}\\), then \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is biased and inconsistent. In particular, \\[\\begin{align*}\n\\text{E}\\left[\\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbf{X}\\right] & = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}\\boldsymbol \\eta \\neq \\boldsymbol{\\beta},\\\\\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS} & = \\boldsymbol{\\beta}+ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\boldsymbol \\gamma \\neq \\boldsymbol{\\beta},\\\\\n\\end{align*}\\] where \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] =\\boldsymbol \\eta\\) and \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\boldsymbol \\gamma\\).\n\n\nProof. \\[\\begin{align*}\n\\text{E}\\left[\\hat{\\boldsymbol\\beta}_\\text{OLS} \\mid \\mathbb{X}\\right] & = \\text{E}\\left[\\boldsymbol{\\beta}+(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right]\\\\\n& = \\boldsymbol{\\beta}+ (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\boldsymbol \\eta\\\\\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS} & = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right)\\right]\\\\\n& = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1}\\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\varepsilon_i\\right) & (\\text{Slutsky's Theorem})\\\\\n& = \\boldsymbol{\\beta}+ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] & (\\text{LLN})\\\\\n& = \\boldsymbol{\\beta}+ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\boldsymbol \\gamma\n\\end{align*}\\] space\n\n\nExample 6.4 Let’s verify that \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is inconsistent. Suppose \\(\\mathbb{X}= (\\mathbf 1, X)\\) where \\((X,\\varepsilon)\\overset{iid}{\\sim}N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\) for \\(\\boldsymbol \\mu = [0,1]'\\) \\[\\boldsymbol{\\Sigma}= \\begin{bmatrix}5&2\\\\2&1  \\end{bmatrix}.\\] We have \\[\\begin{align*}\n\\text{E}\\left[X\\varepsilon\\right] &= \\text{Cov}\\left(X,\\varepsilon\\right) + \\underbrace{\\text{E}\\left[X\\right]}_0\\underbrace{\\text{E}\\left[\\varepsilon\\right]}_1 = 2.\n\\end{align*}\\] Therefore \\[\\begin{align*}\n\\boldsymbol \\gamma &= \\text{E}\\left[\\mathbb{X}'\\boldsymbol{\\varepsilon}\\right] = \\begin{bmatrix}\\text{E}\\left[1\\boldsymbol{\\varepsilon}\\right]\\\\\\text{E}\\left[X\\boldsymbol{\\varepsilon}\\right] \\end{bmatrix} = \\begin{bmatrix}1\\\\2 \\end{bmatrix}\n\\end{align*}\\]\nIf we draw realizations x and e of \\((X,\\varepsilon)\\), we should find that colMeans(X*e) should be approximately \\([1,2]'\\) by the LLN.\n\nn <- 100000\nSigma <- matrix(c(5,2,2,1), nrow = 2)\nmu <- c(0,1)\nrealizations <- rmvnorm(n, mu, Sigma)\nx <- realizations[,1]\nX <- cbind(1,x)\ne <- realizations[,2]\ncolMeans(X*e)\n\n                x \n1.002076 1.984556 \n\n\nLet’s now calculate \\(\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1} &= \\begin{bmatrix}\\text{E}\\left[1\\right] & \\text{E}\\left[X\\right] \\\\ \\text{E}\\left[X\\right] & \\text{E}\\left[X^2\\right] \\end{bmatrix}^{-1} = \\begin{bmatrix}1 & 0 \\\\ 0 & \\text{Var}\\left(X\\right) \\end{bmatrix}^{-1} = \\begin{bmatrix}1 & 0 \\\\ 0 & 5 \\end{bmatrix}^{-1}  =  \\begin{bmatrix}1 & 0 \\\\ 0 & 0.2 \\end{bmatrix}\\\\\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{OLS} & = \\boldsymbol{\\beta}+ \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\boldsymbol \\gamma= \\boldsymbol{\\beta}+ \\begin{bmatrix}1 & 0 \\\\ 0 & 0.2 \\end{bmatrix}\\begin{bmatrix}1\\\\2 \\end{bmatrix}= \\boldsymbol{\\beta}+ \\begin{bmatrix}1\\\\0.4 \\end{bmatrix}\n\\end{align*}\\] If we let \\(\\boldsymbol{\\beta}= [1,2']\\) we should see our estimates converge to \\([2, 2.4]'\\) as \\(n\\to\\infty\\).\n\nN_sim <- 10000\nestimates <- matrix(NA, nrow = N_sim, ncol = 2)\nfor (n in 2:(N_sim + 1)) {\n  realizations <- rmvnorm(n, mu, Sigma)\n  x <- realizations[,1]\n  X <- cbind(1,x)\n  e <- realizations[,2]\n  y <- 1 + 2*x + e\n  summary(lm(y~x))$coefficients[,1]\n  estimates[n-1,] <- summary(lm(y~x))$coefficients[,1]\n}\n\n\n\n\n\n\ntest\n\n\n\n\n\nAn interesting feature of this problem is that our estimator for the intercept term \\(\\beta_1\\) was inconsistent even though the corresponding regressor (the trivial random variable \\(X_1 = 1\\)) is exogenous. In general, the inconsistency of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) in the presence of endogeneity is not limited to the parameters associated with endogenous variables, and will impact each parameter \\(\\beta_j\\).\nInstead of using \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\), we need an estimator which makes use of the exogenous variables \\(\\mathbf{Z}\\). There are several different ways to motivate this estimator, so we’ll go over four particularly approaches which reach the same conclusion.\n\nFor \\(\\mathcal P_\\text{IV}\\) we can write the true population parameter as \\(\\boldsymbol{\\beta}= \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{Z}'Y\\right]\\). We can appeal to the analogy-principle here just like we did when deriving \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). It stands to reason that the estimator defined by the analogous samples will provide consistent estimates of \\(\\boldsymbol{\\beta}\\) by the LLN. This estimator is \\[\\hat {\\boldsymbol{\\beta}}= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_iY_i\\right) = (\\mathbb{Z}'\\mathbb{X})^{-1}\\mathbb{Z}'\\mathbf{Y}.\\]\nAnother way of tackling the problem relates to marginal effects and is presented by Cameron and Trivedi (2005). This will be a little informal and is only to build intuition. In an abuse of notation, we’ll write marginal effects as \\(\\frac{\\partial Y}{\\partial \\mathbf{X}}\\). Ideally, we want \\(\\beta_j = \\frac{\\partial Y}{\\partial X_j}\\) for each \\(j\\). This way, when we estimate \\(\\boldsymbol{\\beta}\\), we estimate the marginal effect of \\(\\mathbf{X}\\) on \\(Y\\). Unfortunately, we cannot estimate this directly with \\(\\mathbf{X}\\) as \\[ \\frac{\\partial Y}{\\partial X_j} = \\beta_j + \\frac{\\partial \\varepsilon}{\\partial X_j} \\implies \\beta_j \\neq \\frac{\\partial Y}{\\partial X_j}.\\] If we appeal to the chain rule, we can vary \\(X_j\\) via our instruments \\(\\mathbf{Z}\\): \\[\\begin{align*}\n&\\frac{\\partial Y}{\\partial \\mathbf{Z}} = \\frac{\\partial Y}{\\partial X_j} \\frac{\\partial X_j}{\\partial \\mathbf{Z}} \\\\\n\\implies & \\frac{\\partial Y}{\\partial X_j} = \\frac{\\partial Y/\\partial \\mathbf{Z}}{\\partial X_j/\\partial \\mathbf{Z}}\n\\end{align*}\\] where \\(\\frac{\\partial Y}{\\partial \\mathbf{Z}}\\) holds \\(\\varepsilon\\) constant as \\(\\text{Cov}\\left(\\mathbf{Z}, \\boldsymbol{\\varepsilon}\\right) = \\mathbf{0}\\). The marginal effects \\(\\frac{\\partial X_j}{\\partial \\mathbf{Z}}\\) and \\(\\frac{\\partial Y}{\\partial \\mathbf{Z}}\\) correspond to the parameters in the linear projection models of \\(\\mathbf{Z}\\) on \\(X_j\\) and \\(Y\\).2 These parameters are given by OLS estimates \\((\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}\\mathbf{X}_j\\) and \\((\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}\\mathbf{Y}\\), respectively, so \\[ \\hat\\beta_j = \\frac{(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}}{(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}\\mathbf{X}_j}.\\] If we do this for all regressors \\(\\mathbf{X}\\), then \\[ \\hat{\\boldsymbol{\\beta}} = \\frac{(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}}{(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}\\mathbb{X}} =  (\\mathbb{Z}'\\mathbb{X})^{-1}\\mathbb{Z}'\\mathbf{Y}\\]\nWe can take a graphical approach given by Pearl (2009) with the simple IV model with one endogenous regressor \\(X\\) and one instrument \\(Z\\). Suppose we have \\(Y = \\beta_1 + \\beta_2 X + \\varepsilon\\), where \\(\\text{Cov}\\left(X,\\varepsilon\\right) \\neq 0\\), along with \\(\\text{Cov}\\left(Z,X\\right) \\neq 0\\) and \\(\\text{Cov}\\left(Z,\\varepsilon\\right) = 0\\) for some instrument \\(Z\\). These relationships can be illustrated in the form of a directed acyclic graph (DAG).\n\n\n\n\n\n\ntest\n\n\n\n\nWe can think of the paths which illustrate causation as being multiplicative in the sense that \\(\\text{Cov}\\left(Z,X\\right)\\cdot \\beta_2 = \\text{Cov}\\left(Z,Y\\right)\\), where \\(\\beta_2\\) is a “conversion rate” between changes in \\(X\\) via \\(Z\\) and changes in \\(Y\\) via \\(Z\\) (just like the chain rule approach). This implies \\(\\beta_2 = \\text{Cov}\\left(Z,Y\\right)/\\text{Cov}\\left(Z,X\\right)\\), so \\[ \\hat\\beta_2 = \\frac{\\widehat{\\text{Cov}}(Z,Y)}{\\widehat{\\text{Cov}}(Z,X)}.\\] This happens to be a special case of \\(\\hat{\\boldsymbol{\\beta}}= (\\mathbb{Z}'\\mathbb{X})^{-1}\\mathbb{Z}'\\mathbf{Y}\\).\n\nAnother approach in the context of the simple linear model takes advantage of a simple substitution. \\[ \\text{Cov}\\left(Y,Z\\right) = \\text{Cov}\\left(\\beta_1 + \\beta_2 X + \\varepsilon, Z\\right) = \\underbrace{\\text{Cov}\\left(\\beta_1, Z\\right)}_0 + \\beta_2\\text{Cov}\\left(X,Z\\right) + \\underbrace{\\text{Cov}\\left(Z,\\varepsilon\\right)}_0 = \\beta_2\\text{Cov}\\left(X,Z\\right).\\] This is the same result we arrived at using the DAG.\n\nWith all roads leading to Rome, we can define this estimator.\n\nDefinition 6.2 The instrumental variables (IV) estimator is defined as \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{IV} (\\mathbb{X},\\mathbb{Z},\\mathbf{Y})= (\\mathbb{Z}'\\mathbb{X})^{-1}(\\mathbb{Z}'\\mathbf{Y})= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_iY_i\\right)\n\\end{align*}\\] An realization of this estimator (an estimate) is \\[\\begin{align*}\n\\hat{\\mathbf b}_\\text{IV} = \\hat{\\boldsymbol{\\beta}}_\\text{IV}(\\mathbf{X},\\mathbf{Z},\\mathbf{y}) &= (\\mathbf{Z}'\\mathbf{X})^{-1}(\\mathbf{Z}'\\mathbf{y})= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{z}_i'\\mathbf{z}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{z}_iy_i\\right)\n\\end{align*}\\] and will exist when the inverse \\((\\mathbf{Z}'\\mathbf{X})^{-1}\\) exists.\n\n\nExample 6.5 Suppose \\((X, Z, \\varepsilon) \\sim N(\\boldsymbol \\mu,\\boldsymbol{\\Sigma})\\) where \\[\\begin{align*}\n\\boldsymbol \\mu & = [10,10,0]',\\\\\n\\boldsymbol{\\Sigma}& = \\begin{bmatrix}20 & 5 & 1\\\\5&20&0\\\\1&0&1 \\end{bmatrix},\n\\end{align*}\\] and \\(Y = 1 + 3X + \\varepsilon\\). Let’s simulate a sample of size \\(n=1000\\) from this model \\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{IV}\\), and then calculate \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\).\n\nn <- 1000\nmu <- c(10, 10, 0)\nSigma <- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)\nsample <- rmvnorm(n, mu, Sigma)\nx <- sample[,1]\nz <- sample[,2]\ne <- sample[,3]\ny <- 1 + 3*x + e\n\nWe should confirm that our drawn sample satisfies the assumptions of the IV model.\n\n# Is x exogenous?\ncov(x,e)\n\n[1] 1.24223\n\n# Is z a valid instrument?\ncov(z,e)\n\n[1] 0.2013187\n\n# Is z relevant?\ncov(z,x)\n\n[1] 5.257776\n\n\nNow let’s calculate \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\), keeping in mind that \\(\\mathbb{Z}\\) is comprised of the instrument \\(Z\\) and the exogenous random variable \\(1\\) which gives the intercept term.\n\nX <- cbind(1, x)\nZ <- cbind(1, z)\nbeta_hat <- solve(t(Z) %*% X) %*% t(Z) %*% y \nbeta_hat\n\n       [,1]\n  0.5934421\nx 3.0382897\n\n\n\n\nExample 6.6 (OLS is a Special Case of IV) In Example @ref(exm:spec1) we saw that \\(\\mathcal P_\\text{LM}\\subset\\mathcal P_\\text{IV}\\). In the event \\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{LM}\\), i.e \\(\\mathbb{Z}= \\mathbb{X}\\) and , then \\[ \\hat{\\boldsymbol\\beta}_\\text{IV} = (\\mathbb{Z}'\\mathbb{X})^{-1}(\\mathbb{Z}'\\mathbf{Y}) = (\\mathbb{X}'\\mathbb{X})^{-1}(\\mathbb{X}'\\mathbf{Y}) = \\hat{\\boldsymbol\\beta}_\\text{OLS} .\\]"
  },
  {
    "objectID": "endog.html#properties-of-the-iv-estimator",
    "href": "endog.html#properties-of-the-iv-estimator",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.3 Properties of the IV Estimator",
    "text": "6.3 Properties of the IV Estimator\nOne of the reasons OLS is so special is because we’re able to characterize its finite sample properties with the Gauss-Markov theorem. This is the exception rather than the rule when assessing estimators. In most cases, we can only arrive at tractable results in the form of an estimator’s asymptotic properties. Is this the case with \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\)? Let’s see if \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) satisfies our “baseline” finite sample property of unbiasedness. \\[\\begin{align*}\n\\text{E}\\left[\\hat{\\boldsymbol\\beta}_\\text{IV} \\right] &= \\text{E}\\left[\\text{E}\\left[\\hat{\\boldsymbol\\beta}_\\text{IV} \\mid \\mathbb{X}, \\mathbb{Z}\\right]\\right]\\\\\n        & =  \\text{E}\\left[\\text{E}\\left[(\\mathbb{Z}'\\mathbb{X})^{-1}(\\mathbb{Z}'\\mathbf{Y}) \\mid \\mathbb{X}, \\mathbb{Z}\\right]\\right]\\\\\n        &=\\text{E}\\left[\\text{E}\\left[(\\mathbb{Z}'\\mathbb{X})^{-1}(\\mathbb{Z}'(\\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon})) \\mid \\mathbb{X}, \\mathbb{Z}\\right]\\right]\\\\\n        &=\\boldsymbol{\\beta}+ \\text{E}\\left[(\\mathbb{Z}'\\mathbb{X})^{-1}\\mathbb{Z}'\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}, \\mathbb{Z}\\right]\\right]\\\\\n        & \\neq \\boldsymbol{\\beta}\n\\end{align*}\\] In general, \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}, \\mathbb{Z}\\right] \\neq \\mathbf{0}\\), so \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) has a bias.\n\nExample 6.7 (IV Estimator is Biased) Return to the model from Example @ref(exm:refex), but let \\(n = 10\\). If we simulate the bias of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) using 10,000 simulations, we can confirm that \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is biased.\n\nN_sim <- 10000\nestimates <- matrix(NA, nrow = N_sim, ncol = 2)\nfor (k in 1:N_sim) {\n  n <- 10\n  mu <- c(10, 10, 0)\n  Sigma <- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)\n  sample <- rmvnorm(n, mu, Sigma)\n  x <- sample[,1]\n  z <- sample[,2]\n  e <- sample[,3]\n  y <- 1 + 3*x + e\n  X <- cbind(1, x)\n  Z <- cbind(1, z)\n  estimates[k,] <- solve(t(Z) %*% X) %*% t(Z) %*% y \n}\ncolMeans(estimates)\n\n[1] 1.613375 2.951230\n\n\nOkay but is this really an issue? In most settings, we would have a sample size much larger than \\(n = 10\\). Let’s repeat this experiment with \\(n = 1000\\) and see what happens to our estimators bias.\n\nN_sim <- 10000\nestimates <- matrix(NA, nrow = N_sim, ncol = 2)\nfor (k in 1:N_sim) {\n  n <- 1000\n  mu <- c(10, 10, 0)\n  Sigma <- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)\n  sample <- rmvnorm(n, mu, Sigma)\n  x <- sample[,1]\n  z <- sample[,2]\n  e <- sample[,3]\n  y <- 1 + 3*x + e\n  X <- cbind(1, x)\n  Z <- cbind(1, z)\n  estimates[k,] <- solve(t(Z) %*% X) %*% t(Z) %*% y \n}\ncolMeans(estimates)\n\n[1] 1.011103 2.998881\n\n\nNow the bias is negligible.\n\nWhile \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is biased, it seems as if it is asymptotically unbiased. Instead of proving this directly, we’ll actually show that \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is root-N CAN, a sufficient condition for asymptotic unbiasedness. Before tackling that, let’s prove that \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is consistent directly. One of the reasons we opted to not estimate \\(\\boldsymbol{\\beta}\\) via \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) was that \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is not consistent for \\(\\mathcal P_\\text{IV}\\), so it shouldn’t be a surprise that \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is consistent.\n\nProposition 6.2 (IV Estimator is Consistent) If \\(P_{\\boldsymbol{\\beta},\\sigma^2}\\in \\mathcal P_\\text{IV}\\), then \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{p}{\\to}\\boldsymbol{\\beta}\\).\n\n\nProof. \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{IV} &= \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_iY_i\\right)\\\\\n    & = \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i[\\mathbf{X}_i\\boldsymbol{\\beta}+ \\varepsilon_i]\\right)\\\\\n    & = \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)}_1\\boldsymbol{\\beta}+ \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i\\right)\\\\\n    & = \\boldsymbol{\\beta}+ \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i\\right)\\\\\n\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol\\beta}_\\text{IV} & = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\cdot \\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i\\right) & (\\text{Slutsky's theorem})\\\\\n& = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\cdot \\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] & (\\text{LLN})\\\\\n& = \\boldsymbol{\\beta}+ \\mathop{\\mathrm{plim}}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\cdot \\mathbf{0}& (\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0})\\\\\n& = \\boldsymbol{\\beta}\n\\end{align*}\\] space\n\n\nTheorem 6.2 (IV Estimator is Root-n CAN) If \\(P_{\\boldsymbol{\\beta}, \\sigma^2} \\in \\mathcal P_\\text{IV}\\), then \\[\\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\sigma^2\\text{E}\\left[\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\text{E}\\left[\\mathbb{Z}'\\mathbb{Z}\\right]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}\\right]^{-1}\\right) =  N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right]^{-1} \\right)\\]\n\n\nProof. The proof is almost identical to that of Theorem @ref(thm:asymols).\n\\[\\begin{align*}\n\\sqrt n(\\hat{\\boldsymbol\\beta}_\\text{IV} - \\boldsymbol{\\beta}) & = \\sqrt{n}\\left[\\boldsymbol{\\beta}+ \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i\\right) - \\boldsymbol{\\beta}\\right]\\\\\n& = \\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i - \\mathbf{0}\\right)\\\\\n& =\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i - \\text{E}\\left[\\mathbf{Z}_i\\varepsilon_i\\right] \\right) & (\\text{E}\\left[\\mathbf{Z}'\\varepsilon\\right] = \\mathbf{0}) \\\\\n& = \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\mathbf{X}_i\\right)^{-1}}_{\\overset{p}{\\to}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}} \\underbrace{\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\mathbf{Z}_i'\\varepsilon_i - \\text{E}\\left[\\mathbf{Z}_i\\varepsilon_i\\right] \\right)}_{\\overset{d}{\\to}N\\left(\\text{E}\\left[\\mathbf{Z}_i\\varepsilon_i\\right], \\text{Var}\\left(\\textstyle \\sum \\mathbf{X}_i\\varepsilon_i\\right)/n\\right)} & (\\text{LLN and CLT})\\\\\n&\\overset{d}{\\to}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\cdot N\\left(\\text{E}\\left[\\mathbf{Z}_i\\varepsilon_i\\right], \\text{Var}\\left(\\textstyle \\sum \\mathbf{Z}_i\\varepsilon_i\\right)/n\\right) & (\\text{Slutsky's Theorem})\\\\\n& = \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\cdot N\\left(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\right) \\\\\n& = \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\cdot N\\left(\\mathbf{0}, \\sigma^2\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\right)\\\\\n& = N\\left(\\mathbf{0}, \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\sigma^2\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\left[\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\right]'\\right)\\\\\n& = N\\left(\\mathbf{0}, \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\sigma^2\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right]^{-1}\\right).\n\\end{align*}\\] This implies that \\[\\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right]^{-1} \\right).\\] If desired, we can write the asymptotic variance in terms of matrices \\(\\mathbb{X}\\) and \\(\\mathbb{Z}\\): \\[\\begin{align*}\n\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) & = \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right]^{-1}\\\\\n& = \\frac{\\sigma^2}{n}\\left[\\frac{\\text{E}\\left[\\mathbb{Z}'\\mathbb{X}\\right]}{n}\\right]^{-1}\\left[\\frac{\\text{E}\\left[\\mathbb{Z}'\\mathbb{Z}\\right]}{n}\\right]\\left[\\frac{\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}\\right]}{n}\\right]^{-1} \\\\\n& = n^2\\cdot\\frac{1}{n}\\cdot\\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\text{E}\\left[\\mathbb{Z}'\\mathbb{Z}\\right]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}\\left[\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\text{E}\\left[\\mathbb{Z}'\\mathbb{Z}\\right]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}\\right]^{-1}\n\\end{align*}\\] space\n\n\nCorollary 6.1 (IV Estimator is Asymptotically Unbiased) If \\(P_{\\boldsymbol{\\beta}, \\sigma^2} \\in \\mathcal P_\\text{IV}\\), then \\[ \\lim_{n\\to\\infty}\\text{Bias}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) = 0.\\]\n\nIn order to appeal to the asymptotic distribution of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) to perform inference, we need a consistent estimator of the asymptotic variance. Fortunately, we can take nearly the same exact approach we took with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\).\n\nProposition 6.3 (Estimation of IV Variance) Define the estimator \\[\\begin{align*}\nS^2 &=  \\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n-K}\\\\\n\\hat{\\mathbf{e}} &= \\mathbf{Y}- \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{IV}\n\\end{align*}\\] in the context of the linear IV model. Then:\n\n\\(S^2\\) is an unbiased for \\(\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbf{X}\\right) = \\sigma^2\\).\n\\(S^2\\) is a consistent estimator \\(\\text{Var}\\left(\\boldsymbol{\\varepsilon}\\mid\\mathbf{X}\\right) = \\sigma^2\\).\nThe estimator \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) = S^2(\\mathbb{Z}'\\mathbb{X})^{-1}(\\mathbb{Z}'\\mathbb{Z})(\\mathbb{X}'\\mathbb{Z})^{-1}\\) is a consistent estimator for \\({\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV} )\\).\n\n\n\nProof. The proof is nearly identical to that of Proposition @ref(prp:olsvar).\n\n\nExample 6.8 (Coding Exercise) R has no base function which implements \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\), so let’s write our own. For reference, we can compare our results with those given by ivreg() from the AER (applied econometrics in R) package.\n\nIV <- function(y, X, Z){\n  #determine dimensions, perform IV\n  n <- length(y)\n  K <- ncol(X)\n  if(ncol(Z) != K) {stop(\"K instruments required\")}\n  if(det(t(X) %*% X) == 0) {stop(\"rank(Z'X) < K\")}\n  \n  hat_beta <- solve(t(Z) %*% X) %*% t(Z) %*% y \n  \n  #use IV estimates to calculate residuals and estimate SEs\n  res <- (y-X %*% hat_beta)\n  S2 <- ((t(res) %*% res)/(n - K)) %>% as.numeric() \n  var_hat <- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z) \n  se_hat <- sqrt(diag(var_hat))\n  \n  #t-stat, confidence intervals, p values\n  t <- hat_beta/se_hat\n  lower_CI <- hat_beta - qnorm(0.975)*se_hat\n  upper_CI <- hat_beta + qnorm(0.975)*se_hat\n  p_val <- 2*(1 - pt(t, n-K))\n  \n  #combine everything into one table to return\n  output <- cbind(hat_beta, se_hat, t, lower_CI, upper_CI, p_val)\n  rownames(output) <- paste(\"β\", 1:K, sep = \"\")\n  colnames(output) <- c(\"Estimate\", \"Std.Error\", \"t-Stat\", \"Lower 95% CI\", \"Upper 95% CI\", \"p-Value\")\n  return(output)\n}\n\nLet’s estimate the model from Example @ref(exm:refex) using ivreg().\n\nn <- 1000\nmu <- c(10, 10, 0)\nSigma <- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)\nsample <- rmvnorm(n, mu, Sigma)\nx <- sample[,1]\nz <- sample[,2]\ne <- sample[,3]\ny <- 1 + 3*x + e\nsummary(ivreg(y ~ x | z))$coefficients\n\n            Estimate Std. Error   t value    Pr(>|t|)\n(Intercept) 1.643405 0.34486642  4.765338 2.16504e-06\nx           2.939228 0.03356573 87.566341 0.00000e+00\nattr(,\"df\")\n[1] 998\nattr(,\"nobs\")\n[1] 1000\n\n\nNow we can use our function and verify that the outputs are the same.\n\nX <- cbind(1,x)\nZ <- cbind(1,z)\nIV(y,X,Z)\n\n   Estimate  Std.Error    t-Stat Lower 95% CI Upper 95% CI     p-Value\nβ1 1.643405 0.34486642  4.765338    0.9674794     2.319331 2.16504e-06\nβ2 2.939228 0.03356573 87.566341    2.8734401     3.005015 0.00000e+00\n\n\n\n\nExample 6.9 (Intuition behind Asymptotic Variance) In Example @ref(exm:csvarols) we provided some intuition as to how \\(\\text{Var}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right) = \\frac{\\sigma^2}{n} \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\) changed in response to changes in \\(\\sigma^2\\), \\(n\\), and components of \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\). The variance of \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is smaller for large samples and when the error term \\(\\boldsymbol{\\varepsilon}\\) has a small variance. The more variance we have in our regressors (which is related to \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\)), the more efficient our estimator. So what is the intuition behind \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) =\\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right]^{-1}\\)? Three things should look familiar. The variance increases with increases in \\(\\sigma^2\\) and decreases with increases in \\(n\\). Consider the model \\(Y = X\\beta + \\varepsilon\\) when we instrument for \\(X\\) with \\(Z\\), such that \\[\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) =\\frac{\\sigma^2}{n}\\frac{\\text{E}\\left[Z^2\\right]}{\\text{E}\\left[ZX\\right]^2} = \\frac{\\text{Var}\\left(Z\\right) + \\text{E}\\left[Z\\right]^2}{[\\text{Cov}\\left(X,Z\\right) + \\text{E}\\left[Z\\right]\\text{E}\\left[X\\right]]^2}.\\] Holding the average of \\(Z\\) and \\(X\\) fixed, we see that the asymptotic variance of our estimator increases with linearly with \\(\\text{Var}\\left(Z\\right)\\), and decreases quadratically as \\(\\text{Cov}\\left(X,Z\\right)\\). The latter of these facts shouldn’t be surprising. The larger \\(\\text{Cov}\\left(X,Z\\right)\\), the more relevant (“stronger”) our instrument \\(Z\\) is, and the better our estimates."
  },
  {
    "objectID": "endog.html#many-instruments-2sls",
    "href": "endog.html#many-instruments-2sls",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.4 Many Instruments, 2SLS",
    "text": "6.4 Many Instruments, 2SLS\nWhen defining \\(\\mathcal P_\\text{IV}\\), we assumed \\(\\dim(\\mathbf{Z}) = \\dim(\\mathbf{X}) = K\\). What happens if we have \\(L = \\dim(\\mathbf{Z}) > \\dim(\\mathbf{X}) = K\\)? Let’s consider a special case first.\n\nExample 6.10 (OLS vs. IV) Consider a model \\(\\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\) which satisfies the Gauss-Markov assumption, but also specifies the existence of \\(K\\) separate instrumental variables \\(\\mathbf{Z}\\). In other words, we have \\(2K\\) instruments in the form of \\(\\mathbf{X}\\) and \\(\\mathbf{Z}\\). Should we estimate \\(\\boldsymbol{\\beta}\\) via OLS (equivalent to IV using \\(\\mathbf{X}\\) as instruments for \\(\\mathbf{X}\\)), or should we estimate \\(\\boldsymbol{\\beta}\\) via IV using \\(\\mathbf{Z}\\)? It may be tempting to pick the latter. What if we are confident that \\(\\mathbf{Z}\\) are valid instruments, but aren’t positive that \\(\\mathbf{X}\\) is exogenous. If we estimated \\(\\boldsymbol{\\beta}\\) with \\(\\hat{\\boldsymbol\\beta}_\\text{IV} = (\\mathbb{Z}'\\mathbb{X})^{-1}\\mathbb{Z}'\\mathbf{Y}\\), then we would be playing it safe and not risk inconsistent estimates via \\(\\hat{\\boldsymbol\\beta}_\\text{IV} '=(\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{Y}= \\hat{\\boldsymbol\\beta}_\\text{OLS} \\). Unfortunately this approach has a cost in the form of variance/standard errors. \\[\\begin{align*}\n\\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) &= \\sigma^2\\text{E}\\left[\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\text{E}\\left[\\mathbb{Z}'\\mathbb{Z}\\right]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}\\right]^{-1}\\\\\n\\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{IV} ') &= \\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) = \\sigma^2\\text{E}\\left[\\mathbb{X}'\\mathbb{X}\\right]^{-1} & (\\mathbb{X}= \\mathbb{Z})\n\\end{align*}\\] The difference \\(\\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) - \\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\) is PSD, so \\[ \\text{se}\\left(\\hat\\beta_{\\text{IV},j}'\\right) = \\text{se}\\left(\\hat\\beta_{\\text{OLS},j}\\right) < \\text{se}\\left(\\hat\\beta_{\\text{IV},j}\\right).\\] As the next simulation shows, this difference can be fiarly large.\n\nN_sim <- 10000\nestimates <- matrix(NA, nrow = 2*N_sim, ncol = 3)\nestimates[,3] <- rep(c(\"OLS\", \"IV\"), N_sim)\n\nfor (k in 1:N_sim) {\n  n <- 1000\n  mu <- c(10, 10, 0)\n  Sigma <- matrix(c(20,5,0, 5,20,0,0,0,1), nrow = 3)\n  sample <- rmvnorm(n, mu, Sigma)\n  x <- sample[,1]\n  z <- sample[,2]\n  e <- sample[,3]\n  y <- 1 + 3*x + e\n  X <- cbind(1,x)\n  Z <- cbind(1,z)\n  estimates[2*k-1 ,1:2] <- solve(t(X) %*% X) %*% t(X) %*% y\n  estimates[2*k,1:2] <- solve(t(Z) %*% X) %*% t(Z) %*% y \n}  \n\n\n\n\n\n\ntest\n\n\n\n\nIf we calculate the simulated standard errors of our estimators, we find that using \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) results in nearly a four fold decrease in standard error.\n\n\n\n\n\nEstimator\nTerm\nStandard Error\n\n\n\n\nIV\nβ1\n0.2929534\n\n\nIV\nβ2\n0.0290895\n\n\nOLS\nβ1\n0.0780588\n\n\nOLS\nβ2\n0.0071179\n\n\n\n\n\n\n\nExample 6.11 (IV vs… IV?) Suppose \\(Y = X\\beta + \\varepsilon\\) for an endogenous \\(X\\). Fortunately, we have two instruments \\(Z_1\\) and \\(Z_2\\). Which instrument do we use to estimate \\(\\beta\\). At first the answer seems simple – just estimate the model using each instrument separately, and then pick the estimates with the lower standard error. This won’t consider all the relevant cases though, as any linear combination of \\(Z_1\\) and \\(Z_2\\) are also instruments. Define \\(Z_3 = aZ_1 + bZ_2\\). \\[\\begin{align*}\n\\text{E}\\left[Z_3\\varepsilon\\right] & = \\text{E}\\left[(aZ_1 + bZ_2)\\varepsilon\\right] = a\\underbrace{\\text{E}\\left[Z_1\\varepsilon\\right]}_0 + b\\underbrace{\\text{E}\\left[Z_2\\varepsilon\\right]}_0 = 0\\\\\n\\text{E}\\left[Z_3X\\right] & = \\text{E}\\left[(aZ_1 + bZ_2)X\\right] = a\\underbrace{\\text{E}\\left[Z_1X\\right]}_{\\neq 0} + b\\underbrace{\\text{E}\\left[Z_2X\\right]}_{\\neq 0} \\neq 0\n\\end{align*}\\] There are an infinite number of candidates for instruments, so what do we do? Let’s simulate some standard errors and see if we can notice patterns between them and the choice of instruments. We’ll restrict our attention to instruments in the set \\(\\{aZ_1 + bZ_2\\mid a,b\\in[0,1]\\}\\). We will calculate the \\(\\text{Var}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\mid \\mathbb{X},\\ \\mathbb{Z}\\right)\\) for a fixed sample drawn from \\((X,Z_1,Z_2,\\varepsilon)\\overset{iid}{\\sim}N(\\boldsymbol \\mu, \\boldsymbol{\\Sigma})\\) where\n\\[\\begin{align*}\n\\boldsymbol \\mu & = [10,10,10,0]',\\\\\n\\boldsymbol{\\Sigma}& = \\begin{bmatrix}20 & 5 & 10 & 1\\\\5&30&7&0\\\\10&7&50&0\\\\1&0&0&1 \\end{bmatrix},\n\\end{align*}\\]\n\nn <- 50\nmu <- c(10, 10, 10, 0)\nSigma <- matrix(c(20,5,10,1,5,30,7,0,10,7,50,0,1,0,0,1), nrow = 4)\nsample <- rmvnorm(n, mu, Sigma)\nx <- sample[,1] \nz1 <- sample[,2]\nz2 <- sample[,3]\ne <- sample[,4]\ny <- 1 + 2*x + e\n\nA <- seq(0.01, 1, length = 500)\nB <- seq(0.01, 1, length = 500) \nstore <- list()\ni <- 0\nfor (a in A) {\n  for (b in B) {\n    i <- i +1\n    X <- as.matrix(x)\n    Z <- as.matrix(a*z1 + b*z2)\n    store[[i]] <- (IV(y, X, Z)[2])^2\n  }\n}\n\nNow let’s plot the calculated variances (conditional on the fixed sample) over the values \\(a\\times b\\in [0,1]^2\\) which determined the instrument used to calculate \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\).\n\n\n\n\n\ntest\n\n\n\n\nAs anticipated, \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is more efficient when using certain linear combinations of elements. Furthermore, it appears that using some non-trivial linear combination of \\(Z_1\\) and \\(Z_2\\) is a better choice than simply using one or the other.\n\nBefore considering this problem in general, let’s extend the IV model to allow for more instruments than regressors.\n\nThe (linear) instrumental variables (IV) model is defined as \\(\\mathcal P_\\text{IV} = \\{P_{\\boldsymbol{\\beta},\\sigma^2} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\sigma^2\\in\\mathbb R\\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\sigma^2} &= \\{F_{\\mathbb{X},\\mathbb{Z},\\boldsymbol{\\varepsilon}} \\mid\\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}\\right]\\right) = L,\\ \\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K, \\ldots  \\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbb{Z}& = [\\mathbf{Z}_1, \\cdots, \\mathbf{Z}_j, \\cdots \\mathbf{Z}_L] = [\\mathbf{Z}_1, \\cdots, \\mathbf{Z}_i, \\cdots \\mathbf{Z}_n]',\\\\\n\\dim(\\mathbf{Z}) & = L,\\\\\n\\dim(\\mathbf{X}) &= K,\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n].\n\\end{align*}\\] A necessary condition for the rank condition \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\) is the order condition, \\(L\\ge K\\). In the event \\(L = K\\) we say the model is exactly identified. If \\(L > K\\), the model is over-identified.\n\nWhen \\(L=K\\) this is just the original IV model, and we know how to estimate this with \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\). In the event \\(L > K\\), the model is still identified, as we haven’t modified the identifying assumptions that \\(\\text{E}\\left[\\mathbf{Z}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\), \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{Z}\\right] \\neq \\mathbf{0}\\), and \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\right) = K\\). In fact, our model would still be identified if we discarded \\(L - K\\) instruments! This is where the term “over-identified” comes from, and as far as “problems” go, it’s not a bad problem to have. We have so many instruments, that we have an infinite number of ways to estimate the model!\nIn general, any linear combination of instruments \\(Z_1,\\ldots, Z_L\\) is also an instrument. We can only use \\(K\\) instruments with the estimator \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\), so we want to find the \\(K\\) linear combinations of our instruments (some of which could reduce to a single \\(Z_j\\)) which give us the most efficient estimator. The \\(K\\) linear combinations of the \\(L\\) instruments can be expressed as a \\(L\\times K\\) matrix \\(\\mathbf{F}\\) which gives instruments \\(\\mathbf{Z}\\mathbf{F}\\). If we elect to use the new instruments \\(\\mathbf{Z}\\mathbf{F}\\), the IV estimator becomes \\[\\hat{\\boldsymbol\\beta}_\\text{IV} = [(\\mathbb{Z}\\mathbf{F})'\\mathbb{X}]^{-1}[(\\mathbb{Z}\\mathbf{F})'\\mathbf{Y}].\\] The problem of selecting \\(\\mathbf{F}\\) such that we maximize the efficiency of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is given as:\n\\[\\mathop{\\mathrm{argmin}}_{\\mathbf{F}} \\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) = \\mathop{\\mathrm{argmin}}_{\\mathbf{F}} \\sigma^2\\text{E}\\left[(\\mathbb{Z}\\mathbf{F})'\\mathbb{X}\\right]^{-1}\\text{E}\\left[(\\mathbb{Z}\\mathbf{F})'(\\mathbb{Z}\\mathbf{F})\\right]\\text{E}\\left[\\mathbb{X}'(\\mathbb{Z}\\mathbf{F})\\right]^{-1}.\\] Solving this looks like a miserable time, so let’s stop appealing directly to math and consider what we’re actually doing here. We’re looking for the “best” instruments. It stands to reason that “good” instruments will be those that have more explanatory power with respect to the endogenous regressors \\(\\mathbf{X}\\) than others. So given draws of \\((\\mathbf{Z},\\mathbf{X})\\), how can we determine the best way to predict/explain \\(\\mathbf{X}\\) given \\(\\mathbf{Y}\\) — by estimating the linear projection associated with \\((\\mathbf{Z},\\mathbf{X})\\) via OLS. For each \\(j = 1,\\ldots,K\\) we have \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{OLS} ^j &= (\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{X}_j & (j = 1,\\ldots, K)\n\\end{align*}\\] where \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} ^j\\) define the instrument formed from linear combinations of \\(Z_1,\\ldots, Z_L\\) which has the most predictive power for \\(X_j\\): \\[\\begin{align*}\n\\hat X_j &= \\mathbf{Z}\\hat{\\boldsymbol\\beta}_\\text{OLS} ^j = \\hat\\beta_1^jZ_1 + \\hat\\beta_2^jZ_2 + \\cdots + \\hat\\beta_L^jZ_L & (j = 1,\\ldots, K)\n\\end{align*}\\] Written compactly using matrices, we have \\[\\begin{align*}\n\\hat{\\mathbf{X}} &=  \\mathbf{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X},\\\\\n\\hat{\\mathbb{X}} &=  \\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X},\n\\end{align*}\\] where \\(\\hat{\\mathbf{X}}\\) is a random vector of instruments, and \\(\\hat{\\mathbb{X}}\\) is \\(n\\) random vectors \\(\\hat{\\mathbf{X}}\\) “stacked” to form a random matrix. If we elect to use these instruments to estimate our model, then \\[ \\hat{\\boldsymbol\\beta}_\\text{IV} = [(\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X})'\\mathbb{X}]^{-1}[(\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X})'\\mathbf{Y}].\\] This estimator is a special case of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) where we have determined the \\(K\\) instruments via linear projection, and is known as two-stage least squares.\n\nDefinition 6.3 The two-stage least squares (2SLS) estimator is defined as \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{2SLS} (\\mathbb{X}, \\mathbb{Z}, \\mathbf{Y}) &= [\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]^{-1}\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}= [\\hat{\\mathbb{X}}'\\mathbb{X}]^{-1}\\hat{\\mathbb{X}}'\\mathbf{Y},\\\\\n                 \\hat{\\mathbb{X}} &=  \\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}.   \n\\end{align*}\\] Calculating \\(\\hat{\\mathbb{X}}\\) via OLS is referred to as the first stage, while calculating \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) using \\(\\hat{\\mathbb{X}}\\) is the second stage. It can be useful to define the projection matrix associated with the first stage, \\[\\begin{align*}\n\\mathbb P_{\\mathbb{Z}} &= \\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\\\\n\\hat{\\mathbb{X}} & = \\mathbb P_{\\mathbb{Z}}\\mathbb{X}.\n\\end{align*}\\]\n\nThe name “two-stage least squares” can be a bit misleading depending on how it is presented. It’s very common to think of each stage of 2SLS as an application of OLS. In the first stage we perform OLS to calculate the instruments \\(\\hat{\\mathbf{X}}\\), and then regress \\(Y\\) on \\(\\hat{\\mathbf{X}}\\) via OLS in the second stage. At first these seems at odds with our definition, because the second stage relies on the IV estimator, but it is equivalent in a sense. We’ll explore this in depth in Example @ref(exm:se).\n\nExample 6.12 (OLS as 2SLS) Consider the situation from Example @ref(exm:ref3) where \\(\\mathbf{X}\\) is comprised of entirely exogenous regressors. In this case our vector of instruments is actually a \\(L+K\\) vector \\([\\mathbf{X},\\mathbf{Z}]\\). Instead of directly calculating \\[\\begin{align*}\n\\hat{\\mathbb{X}} &= [\\mathbb{X}, \\mathbb{Z}]([\\mathbb{X}, \\mathbb{Z}]'[\\mathbb{X}, \\mathbb{Z}])^{-1}[\\mathbb{X}, \\mathbb{Z}]'\\mathbb{X},\\\\\n\\end{align*}\\] we can intuit the result of the first stage. If we regress \\(X_j\\) on \\(X_1,\\ldots, X_j,\\ldots,Z_1,\\ldots,Z_L\\), then all coefficients should be zero except that associated with the independent variable \\(X_j\\) which will be 1. Therefore the OLS estimates from stage one should be a \\(L \\times K\\) matrix where the first \\(K\\) rows are a diagonal matrix of \\(1\\)’s and the bottom \\(L - K\\) rows are 0. If we multiply \\([\\mathbb{X}, \\mathbb{Z}]\\) by this, then we have \\[ \\hat{\\mathbb{X}} = \\mathbf 1\\mathbb{X}+ \\mathbf{0}\\mathbb{Z}= \\mathbb{X}.\\] This gives \\[ \\hat{\\boldsymbol\\beta}_\\text{2SLS} = [\\hat{\\mathbb{X}}'\\mathbb{X}]^{-1}\\hat{\\mathbb{X}}'\\mathbf{Y}= [{\\mathbb{X}}'\\mathbb{X}]^{-1}{\\mathbb{X}}'\\mathbf{Y}= \\hat{\\boldsymbol\\beta}_\\text{OLS} .\\]\n\n\nExample 6.13 (Exactly Identified Case) In the event that \\(L = K\\), there is no\n\nBecause \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) is just a special case of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) using instruments \\(\\hat{\\mathbb{X}}\\), the properties of \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) follow directly from those of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\).\n\nTheorem 6.3 (IV Estimator is Root-n CAN) If \\(P_{\\boldsymbol{\\beta}, \\sigma^2} \\in \\mathcal P_\\text{IV}\\), then \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\) and \\[\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\sigma^2\\text{E}\\left[\\hat{\\mathbb{X}}'\\hat{\\mathbb{X}}\\right]^{-1}\\right) =  N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]^{-1}\\right).\\]\n\n\nProof. We know that \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) is Root-n CAN because \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is. All we need to show is that the asymptotic variance simplifies to the given expression when using instruments \\(\\hat{\\mathbf{X}}\\). \\[\\begin{align*}\n\\text{Avar}(\\hat{\\boldsymbol\\beta}_\\text{2SLS} ) &= \\sigma^2\\text{E}\\left[\\hat{\\mathbb{X}}\\mathbb{X}\\right]^{-1}\\text{E}\\left[\\hat{\\mathbb{X}}\\hat{\\mathbb{X}}\\right]\\text{E}\\left[\\mathbb{X}'\\hat{\\mathbb{X}}\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}\\left[[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]'\\mathbb{X}\\right]^{-1}\\text{E}\\left[[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]\\right]\\text{E}\\left[\\mathbb{X}'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\text{E}[\\mathbb{X}'\\mathbb{Z}\\underbrace{(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{Z}}_{\\mathbf I}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}[\\mathbb{X}'\\underbrace{\\mathbb{Z}\\mathbb{Z}^{-1}}_{\\mathbf I}\\underbrace{(\\mathbb{Z}')^{-1}\\mathbb{Z}'}_{\\mathbf I}\\mathbb{X}]^{-1}\\text{E}[\\mathbb{X}'\\underbrace{\\mathbb{Z}\\mathbb{Z}^{-1}}_{\\mathbf I}\\underbrace{(\\mathbb{Z}')^{-1}\\mathbb{Z}'}_{\\mathbf I}\\mathbb{X}]\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\\\\n& = \\sigma^2\\underbrace{\\text{E}[\\mathbb{X}'\\mathbb{X}]^{-1}\\text{E}[\\mathbb{X}'\\mathbb{X}]}_{\\mathbf I}\\text{E}\\left[\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}\\left[\\mathbb{X}'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}']'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}\\right]^{-1} & (\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\text{ sym. and idem.}) \\\\\n& = \\sigma^2\\text{E}\\left[[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]\\right]^{-1}\\\\\n& = \\sigma^2\\text{E}\\left[\\hat{\\mathbb{X}}'\\hat{\\mathbb{X}}\\right]^{-1} & (\\hat{\\mathbb{X}} = \\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X})\n\\end{align*}\\]\n\nThe motivation for introducing \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) was not all choices of instruments being equal with respect to efficiency, and perhaps the instruments which result from projecting \\(\\mathbf{X}\\) onto \\(\\mathbf{Z}\\) result in an IV estimator which is more efficient than most others. This IV estimator is not just more efficient that most others, it is the most efficient IV estimator. The proof of this is adapted from Wooldridge (2010).\n\nTheorem 6.4 (2SLS is Asymptotically Efficient) If \\(P_{\\boldsymbol{\\beta}, \\sigma^2} \\in \\mathcal P_\\text{IV}\\), then \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) is efficient in the class of \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) estimators calculated using instruments linear in \\(\\mathbf{Z}\\).\n\n\nProof. Suppose \\(\\tilde{\\boldsymbol{\\beta}}\\) is some arbitrary IV estimator which is linear in instruments \\(\\mathbf{Z}\\). The instruments for this estimator can be written as \\(\\tilde{\\mathbf{X}} = \\mathbf{Z}\\mathbf{F}\\) for some \\(L\\times K\\) matrix \\(\\mathbf{F}\\). We want to show that \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\right)\\) is “less than” \\(\\text{Avar}\\left(\\tilde{\\boldsymbol{\\beta}}\\right)\\). Both of these objects are matrices, so “less” than translates to the difference being PSD. This difference is \\[\\begin{align*}\n\\text{Avar}\\left(\\tilde{\\boldsymbol{\\beta}}\\right) - \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\right) & =\\frac{\\sigma^2}{n}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]\\text{E}\\left[\\mathbf{X}'\\tilde{\\mathbf{X}}\\right]^{-1} -  \\frac{\\sigma^2}{n}\\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]^{-1}\\\\& = \\frac{\\sigma^2}{n}\\left[\\text{E}\\left[\\tilde{\\mathbf{X}}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]\\text{E}\\left[\\mathbf{X}'\\tilde{\\mathbf{X}}\\right]^{-1} - \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]^{-1}\\right]\\\\\n& = \\frac{\\sigma^2}{n}\\left[\\left[\\text{E}\\left[\\tilde{\\mathbf{X}}'\\mathbf{X}\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'\\tilde{\\mathbf{X}}\\right]\\right]^{-1} - \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]^{-1}\\right] & (\\text{properties of inversion})\n\\end{align*}\\] Accounting for the bracketed term being the difference of two inverted matrices, this matrix will be PSD if and only if the following is PSD: \\[\\begin{equation}\n\\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - \\text{E}\\left[\\tilde{\\mathbf{X}}'\\mathbf{X}\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'\\tilde{\\mathbf{X}}\\right]  (\\#eq:diff).\n\\end{equation}\\]\nWe can show this by relying on a certain “trick”. Define the difference between the regressors \\(\\mathbf{X}\\) and 2SLS instruments \\(\\hat{\\mathbf{X}}\\) as \\(\\mathbf r = \\hat{\\mathbf{X}} - \\mathbf{X}\\). This remainder term \\(\\mathbf r\\) is uncorrelated with \\(\\mathbf{Z}\\): \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{Z}'\\mathbf r\\right] & = \\text{E}\\left[\\mathbf{Z}'\\hat{\\mathbf{X}}\\right] - \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\\\\n& = \\text{E}\\left[\\mathbf{Z}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}\\right] - \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right] & (\\hat{\\mathbf{X}} = \\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X})\\\\\n& = \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right] - \\text{E}\\left[\\mathbf{Z}'\\mathbf{X}\\right]\\\\\n& = \\mathbf{0}\n\\end{align*}\\] As a result, \\(\\tilde{ \\mathbf{X}}\\) and \\(\\mathbf r\\) are uncorrelated. \\[\\text{E}\\left[\\tilde{ \\mathbf{X}}'\\mathbf r\\right] = \\text{E}\\left[(\\mathbf{Z}\\mathbf{F}')\\mathbf r\\right]= \\mathbf F'\\underbrace{\\text{E}\\left[\\mathbf{Z}'\\mathbf r\\right]}_\\mathbf{0}= \\mathbf{0}\\] Now we use these expectations and the definition of \\(\\mathbf r\\) to arrive at our “trick”. \\[\\begin{align*}\n&\\text{E}\\left[\\tilde{ \\mathbf{X}}'\\mathbf r\\right] = \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\tilde{ \\mathbf{X}}'(\\hat{\\mathbf{X}} -  \\mathbf{X})\\right] = \\mathbf{0}& (\\mathbf r = \\hat{\\mathbf{X}} -  \\mathbf{X})\\\\\n\\implies & \\text{E}\\left[\\tilde{ \\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - \\text{E}\\left[\\tilde{ \\mathbf{X}}'\\mathbf{X}\\right] = \\mathbf{0}\\\\\n\\implies & \\text{E}\\left[\\tilde{ \\mathbf{X}}'\\hat{\\mathbf{X}}\\right] = \\text{E}\\left[\\tilde{ \\mathbf{X}}'\\mathbf{X}\\right]\n\\end{align*}\\] Using this equality, we can rewrite the difference in Equation @ref(eq:diff) in terms of \\(\\tilde{\\mathbf{X}}\\) and \\(\\hat{\\mathbf{X}}\\) only. \\[\\begin{equation}\n\\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - \\text{E}\\left[\\hat{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]  (\\#eq:diff2).\n\\end{equation}\\] This may seem like a random equation, but it’s very special if you consider the linear projection of \\(\\hat{\\mathbf{X}}\\) onto \\(\\tilde{\\mathbf{X}}\\). This projection is given by the coefficient \\(\\boldsymbol\\gamma = \\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right],\\) which happens to show up in @ref(eq:diff2). The errors associated with this linear projection are given as \\(\\boldsymbol \\nu = \\hat{\\mathbf{X}} - \\tilde{\\mathbf{X}}\\boldsymbol\\gamma\\), and have an expected value of \\(\\mathbf{0}\\) by the definition of \\(\\boldsymbol\\gamma\\) and linear projection. But what is the expected value of these errors squared?\n\\[\\begin{align*}\n\\boldsymbol \\nu' \\boldsymbol \\nu& = [\\hat{\\mathbf{X}} - \\tilde{\\mathbf{X}}\\boldsymbol\\gamma]'[\\hat{\\mathbf{X}} - \\tilde{\\mathbf{X}}\\boldsymbol\\gamma]\\\\\n& = \\hat{\\mathbf{X}}'\\hat{\\mathbf{X}} - 2\\boldsymbol\\gamma'\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}} + \\boldsymbol\\gamma'\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\boldsymbol\\gamma\\\\\n\\text{E}\\left[ \\boldsymbol \\nu' \\boldsymbol \\nu\\right] & = \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - 2\\boldsymbol\\gamma'\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] + \\boldsymbol\\gamma'\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]\\boldsymbol\\gamma & (\\boldsymbol\\gamma \\text{ is a constant})\\\\\n& = \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - 2\\left[\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]\\right]'\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] + \\left[\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]\\right]'\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right] \\left[\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]\\right]\\\\\n& = \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - 2\\text{E}\\left[\\tilde{\\mathbf{X}}\\hat{\\mathbf{X}}'\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] + \\text{E}\\left[\\tilde{\\mathbf{X}}\\hat{\\mathbf{X}}'\\right]\\underbrace{\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]}_{\\mathbf I} \\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]\\\\\n& = \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - (2-1)\\text{E}\\left[\\tilde{\\mathbf{X}}\\hat{\\mathbf{X}}'\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] \\\\\n& = \\text{E}\\left[\\hat{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right] - \\text{E}\\left[\\hat{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]\\text{E}\\left[\\tilde{\\mathbf{X}}'\\tilde{\\mathbf{X}}\\right]^{-1}\\text{E}\\left[\\tilde{\\mathbf{X}}'\\hat{\\mathbf{X}}\\right]\n\\end{align*}\\] Equations @ref(eq:diff) and @ref(eq:diff2) are the expectation of the sum of squared errors associated with the linear projection of \\(\\hat{\\mathbf{X}}\\) onto \\(\\tilde{\\mathbf{X}}\\), meaning the matrix in question must be PSD!3 This gives the desired result.\n\nIn Section @ref(generalized-method-of-moments) we’ll see another way to show this result that is a bit more intuitive.\n\nExample 6.14 (What’s in A Name?) Let’s estimate the model from @ref(exm:ex4) via 2SLS. We already have defined a function IV() to calculate \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\), so we can just use pass the instruments \\(\\hat {\\mathbf{X}}\\) to this function to perform 2SLS.\n\nTSLS <- function(y,X,Z){\n  X_hat <- Z %*% solve(t(Z) %*% Z) %*% t(Z) %*% X\n  IV(y, X, X_hat)\n}\n\nFor a simulated sample of size \\(n=50\\) let’s estimate the model using TSLS().\n\nn <- 50\nmu <- c(10, 10, 10, 0)\nSigma <- matrix(c(20,5,10,1,5,30,7,0,10,7,50,0,1,0,0,1), nrow = 4)\nsample <- rmvnorm(n, mu, Sigma)\nx <- sample[,1] \nz1 <- sample[,2]\nz2 <- sample[,3]\ne <- sample[,4]\ny <- 1 + 2*x + e\nX <- cbind(1, x)\nZ <- cbind(1, z1, z2)\nTSLS(y,X,Z)\n\n    Estimate  Std.Error     t-Stat Lower 95% CI Upper 95% CI   p-Value\nβ1 0.1649153 0.72038529  0.2289266    -1.247014     1.576845 0.8198986\nβ2 2.1031513 0.06677504 31.4960690     1.972275     2.234028 0.0000000\n\n\nThis is not the only way we could calculate \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\). Note that \\(\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\) is symmetric and idempotent, so \\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{2SLS} & = \\hat{\\boldsymbol\\beta}_\\text{IV} (\\mathbb{X}, \\hat{\\mathbb{X}}, \\mathbf{Y})\\\\\n& = [\\hat{\\mathbb{X}}'{\\mathbb{X}}]^{-1}\\hat{\\mathbb{X}}'\\mathbf{Y}\\\\\n& = [\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]^{-1}\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}\\\\\n& = [\\mathbb{X}'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}']\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]^{-1}\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}\\\\\n& = [[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]'[\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbb{X}]]^{-1}\\mathbb{X}'\\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}'\\mathbf{Y}\\\\\n& = [\\hat{\\mathbb{X}}'\\hat{\\mathbb{X}}]^{-1}\\hat{\\mathbb{X}}'\\mathbf{Y}\\\\\n& = \\hat{\\boldsymbol\\beta}_\\text{OLS} (\\hat{\\mathbb{X}}, \\mathbf{Y}).\n\\end{align*}\\] So we can interpret \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) in one of two ways:\n\nUse \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\mathbb{Z}, \\mathbb{X})\\) to calculate \\(\\hat{\\mathbb{X}} = \\mathbb{Z}\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\mathbb{Z}, \\mathbb{X})\\) (stage 1), followed by \\(\\hat{\\boldsymbol\\beta}_\\text{IV} (\\mathbb{X}, \\hat{\\mathbb{X}}, \\mathbf{Y})\\) (stage 2). This is how our TSLS() function works.\nUse \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\mathbb{Z}, \\mathbb{X})\\) to calculate \\(\\hat{\\mathbb{X}} = \\mathbb{Z}\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\mathbb{Z}, \\mathbb{X})\\) (stage 1), followed by \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\hat{\\mathbb{X}}, \\mathbf{Y})\\) (stage 2).\n\nThe second interpretation is where 2SLS gets its name – we run OLS twice. But what happens when we implement \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) this way?\n\nX_hat <- lm(x ~ z1 + z2)$fitted.values\nsummary(lm(y ~ X_hat))$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 0.1649153  6.3378518 0.0260207 0.9793487094\nX_hat       2.1031513  0.5874777 3.5799677 0.0007987861\n\n\nWe get the same exact estimates (which we already showed would be the case), but our standard errors don’t coincide with those given by TSLS(). What is happening here?\nWhen we run lm(y ~ X_hat), we are estimating the model associated with linear projection model \\(Y = \\hat{\\mathbf{X}}\\boldsymbol \\delta + \\nu\\). The function lm() has no way of knowing that this is the second step in an estimation process aimed at estimating the structural linear model \\(Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\varepsilon\\). It just happens that \\(\\hat{\\mathbf{X}}\\) is defined such that the population parameters satisfy \\(\\boldsymbol \\delta = \\boldsymbol{\\beta}\\). When lm() goes to calculate the standard errors, it calculates the residuals associated with the model \\(Y = \\hat{\\mathbf{X}}\\boldsymbol \\delta + \\nu\\). These errors have no structural/economic meaning, and we don’t care about them at all (for this purpose). We really want the residuals associated with the error \\(\\varepsilon\\) in the actual IV model \\(Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\varepsilon\\), and these residuals are not the same! \\[ \\hat{\\mathbf u} = \\mathbf{Y}- \\hat{\\mathbb{X}}\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\hat{\\mathbb{X}}, \\mathbf{Y}) \\neq \\mathbf{Y}- \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} (\\hat{\\mathbb{X}}, \\mathbf{Y}) = \\mathbf{Y}- \\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{IV} (\\mathbb{X}, \\hat{\\mathbb{X}}, \\mathbf{Y}) = \\hat{\\mathbf{e}}\\] The moral of the story is that when we are calculating the residuals we need to be very deliberate and remember the actual model we are estimating. Fortunately, any statistical software with an implementation for \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) will calculate the correct standard errors, which is why it’s best to not do each step “by hand” if you favor the interpretation of \\(\\hat{\\boldsymbol\\beta}_\\text{2SLS} \\) as two OLS regressions.\n\nsummary(ivreg(y ~ x | z1 + z2))$coefficients\n\n             Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) 0.1649153 0.72038529  0.2289266 8.198986e-01\nx           2.1031513 0.06677504 31.4960690 1.022975e-33\nattr(,\"df\")\n[1] 48\nattr(,\"nobs\")\n[1] 50\n\n\n\nThe example emphasizes something that has been mentioned in passing before. The models estimated in the first and second stage, \\(\\mathbf{X}= \\mathbf{Z}\\boldsymbol + \\gamma\\) and \\(Y = \\hat{\\mathbf{X}}\\boldsymbol \\delta + \\nu\\), are entirely void of structural meaning. Nevertheless it is useful to us because it expresses the outcome of interest in terms of exogenous variables \\(\\hat{\\mathbf{X}}\\) and \\(\\mathbf{Z}\\). We refer to such an equation as the reduced form of the structural model \\(Y = \\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\). We’ll talk about this term much more in Section @ref(endogeniety-ii-simultaneous-equation-models)."
  },
  {
    "objectID": "endog.html#testing-hypotheses",
    "href": "endog.html#testing-hypotheses",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.5 Testing Hypotheses",
    "text": "6.5 Testing Hypotheses\nAt this point, we have just assumed we know how to select between the classical linear model \\(\\mathcal P_\\text{LM}\\) and the linear IV model \\(\\mathcal P_\\text{IV}\\), but in practice we want to be able to form hypothesis tests that inform our selection between the two models. In particular we want to be able to test:\n\nAre our regressors exogenous, i.e \\(H_0: \\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\)?\nAre our instruments valid, i.e…\n\n\n6.5.1 Durbin–Wu–Hausman Test\nWe’ll start with considering tests for exogenous/endogenous regressors. One sign that our model contains endogenous regressors is if there is a significant difference between \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) and \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\). This is why it’s always a good idea to estimate a model with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) even if you think regressors are endogenous. It provides a good reference to compare IV estimates with. In fact, comparing \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) and \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) serve as the basis for one of the most common tests for endogeneity as formalized by Hausman (1978).4\nIn the face of endogeneity, \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is an abject failure in light of its inconsistency. Fortunately, \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is consistent when \\(\\mathbf{X}\\) is endogenous. Furthermore, \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) is also consistent when \\(\\mathbf{X}\\) is exogenous, as the instruments \\(\\mathbf{Z}\\) are still valid and relevant. In other words, if \\(\\mathbf{X}\\) is exogenous, then \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{p}{\\to}\\boldsymbol{\\beta}\\), so \\[ \\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{p}{\\to}\\mathbf{0}.\\] This suggests the following: \\[ H_0: \\mathbf{X}\\text{ exogenous} \\iff H_0:\\mathop{\\mathrm{plim}}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\right) = \\mathbf{0}.\\] We can construct a Wald test statistic \\[ W = (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} )' \\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} ) \\right]^{-1}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} ),\\] but we will run into a roadblock in the form of \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\right)\\). If we expand this, we have\n\\[ \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\right) = \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) + \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right) - 2\\text{Acov}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} ,\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right),\\] as \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) and \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) are presumably correlated. We don’t have a formula for the covariance of these estimators handy, and deriving one would be a pain. Fortunately, we don’t have to, because Lemma 2.1 of Hausman (1978) asserts that the covariance term is such that \\[  \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\right) =  \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) - \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\] due to \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) being efficient relative to \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\).5 This is a particular useful equality, and interesting result in it’s own right. Hansen (2022) refers to it as the “Hausman equality”, and gives details about it in Section 8.11. This gives the test statistic \\[\\begin{align*}\nW &= (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} )'\\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) - \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\right]^{-1} (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} ).\n\\end{align*}\\]\nUnfortunately, the term corresponding to the asymptotic variance will likely fail to be invertible. Unless \\(\\mathbf{X}\\) and \\(\\mathbf{Z}\\) contain no common variables, some column of \\((\\hat{\\mathbb{X}}'\\hat{\\mathbb{X}})^{-1} - (\\mathbb{X}'\\mathbb{X})^{-1}\\) will be a linear combination of other columns. For example, if our model contains an intercept, than we cannot use this statistic, as \\(\\mathbf{X}\\) and \\(\\mathbf{Z}\\) will both contain a column of 1’s. This rules out just about every single situation we would ever want to consider. To address this shortcoming, we can take the pseudo-inverse of this matrix.\n\\[W = (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} )'\\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV} ) - \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} )\\right]^+ (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} )\\] The hypothesis \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} \\overset{p}{\\to}\\mathbf{0}\\) is comprised of \\(K\\) separate hypotheses, so it’s tempting to conclude \\(W\\sim \\chi_K^2\\). Unfortunately, this will only be the case when \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) - \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\) is invertible, otherwise we will have \\(W\\sim \\chi_q^2\\) where \\(q = \\text{rank} \\left[ \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{IV} \\right) - \\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right) \\right]\\).\n\nTheorem 6.5 (Hausman Specification Test) Given the hypothesis \\(H_0:\\hat{\\boldsymbol{\\theta}}_1\\overset{p}{\\to}\\boldsymbol{\\theta}, \\hat{\\boldsymbol{\\theta}}_2\\overset{p}{\\to}\\boldsymbol{\\theta}\\) for some asymptotically efficient estimator \\(\\hat{\\boldsymbol{\\theta}}_1\\), the Hausman statistic is defined as \\[H = (\\hat{\\boldsymbol{\\theta}}_2 - \\hat{\\boldsymbol{\\theta}}_1)'\\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol{\\theta}}_2) - \\widehat{\\text{Avar}}(\\hat{\\boldsymbol{\\theta}}_1)\\right]^+ (\\hat{\\boldsymbol{\\theta}}_2 - \\hat{\\boldsymbol{\\theta}}_1).\\] Under \\(H_0\\), \\[ H\\sim \\chi_q^2\\] where \\(q = \\text{rank}\\left[\\text{Avar}\\left( \\hat{\\boldsymbol{\\theta}}_2\\right) - \\text{Avar}\\left( \\hat{\\boldsymbol{\\theta}}_1\\right)\\right]\\). The test associated with this statistic is known as the Hausman specification test.\n\nThe Hausman test is far more general that testing for endogeneity, it just happens that testing for endogeneity is a great application of it. In this particular application, there is a much more convenient formulation that allows us to bypass the pseudo-inverse complication. We’ll begin by partitioning our regressors into the exogenous and endogenous regressors, i.e \\(\\mathbf{X}= [\\mathbf{X}_\\text{exo},\\mathbf{X}_\\text{end}]\\). The \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) and \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) estimators can also be partitioned accordingly. \\[\\begin{align*}\n\\hat\\beta_\\text{OLS} & = \\begin{bmatrix}\\hat\\beta_\\text{OLS,exo}  \\\\ \\hat\\beta_\\text{OLS,end}\\end{bmatrix}\\\\\n\\hat\\beta_\\text{IV} & = \\begin{bmatrix}\\hat\\beta_\\text{IV,exo}  \\\\ \\hat\\beta_\\text{IV,end}\\end{bmatrix}\n\\end{align*}\\] With some clever algebra, we can show that \\((\\hat\\beta_\\text{OLS,exo} - \\hat\\beta_\\text{IV,exo})\\) is a linear function of \\((\\hat\\beta_\\text{IV,end} - \\hat\\beta_\\text{OLS,end})\\). The estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) solves the first order condition associated with minimizing the sum of squared residuals: \\[\\mathbb{X}'\\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} = \\mathbb{X}'\\mathbf{Y}.\\] If we partition this first order condition according to \\(\\mathbf{X}= [\\mathbf{X}_\\text{exo},\\mathbf{X}_\\text{end}]\\), we have\n\\[\\begin{align*}\n&\\begin{bmatrix} \\mathbb{X}_\\text{exo} & \\mathbb{X}_\\text{end}\\end{bmatrix}'\\begin{bmatrix} \\mathbb{X}_\\text{exo} & \\mathbb{X}_\\text{end}\\end{bmatrix}\\begin{bmatrix} \\hat\\beta_\\text{OLS,exo} \\\\ \\hat\\beta_\\text{OLS,end}\\end{bmatrix} = \\begin{bmatrix} \\mathbb{X}_\\text{exo} & \\mathbb{X}_\\text{end}\\end{bmatrix}'\\mathbf{Y}\\\\\n\\implies & \\begin{bmatrix} \\mathbb{X}_\\text{exo}' \\\\ \\mathbb{X}_\\text{end}' \\end{bmatrix} \\begin{bmatrix} \\mathbb{X}_\\text{exo} & \\mathbb{X}_\\text{end} \\end{bmatrix} \\begin{bmatrix} \\hat\\beta_\\text{OLS,exo} \\\\ \\hat\\beta_\\text{OLS,end}\\end{bmatrix} = \\begin{bmatrix} \\mathbb{X}_\\text{exo}' \\\\ \\mathbb{X}_\\text{end}'\\end{bmatrix} \\mathbf{Y}\\\\\n\\implies & \\begin{bmatrix} \\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo} \\hat\\beta_\\text{OLS,exo} + \\mathbb{X}_\\text{exo}' \\mathbb{X}_\\text{end}\\hat\\beta_\\text{OLS,end} \\\\\n\\mathbb{X}_\\text{end}'\\mathbb{X}_\\text{exo}\\hat\\beta_\\text{OLS,exo} + \\mathbb{X}_\\text{end}'\\mathbb{X}_\\text{end}\\hat\\beta_\\text{OLS,end} \\end{bmatrix} = \\begin{bmatrix} \\mathbb{X}_\\text{exo}'\\mathbf{Y}\\\\ \\mathbb{X}_\\text{end}'\\mathbf{Y}\\end{bmatrix}\\\\\n\\implies & \\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo} \\hat\\beta_\\text{OLS,exo} + \\mathbb{X}_\\text{exo}' \\mathbb{X}_\\text{end}\\hat\\beta_\\text{OLS,end} = \\mathbb{X}_\\text{exo}'\\mathbf{Y}& (\\text{First Row})\\\\\n\\implies & \\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo} \\hat\\beta_\\text{OLS,exo}  = \\mathbb{X}_\\text{exo}'\\left(\\mathbf{Y}-  \\mathbb{X}_\\text{end}\\hat\\beta_\\text{OLS,end}\\right)\n\\end{align*}\\]\nSolving this for \\(\\hat\\beta_\\text{OLS,exo}\\) gives\n\\[ \\hat\\beta_\\text{OLS,exo} = \\left(\\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo}\\right)^{-1}\\mathbb{X}_\\text{exo}'\\left(\\mathbf{Y}-  \\mathbb{X}_\\text{end}\\hat\\beta_\\text{OLS,end}\\right) . \\tag{6.1}\\]\nWe can repeat these calculations for \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\), and conclude\n\\[ \\hat\\beta_\\text{OLS,exo} = \\left(\\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo}\\right)^{-1}\\mathbb{X}_\\text{exo}'\\left(\\mathbf{Y}-  \\mathbb{X}_\\text{end}\\hat\\beta_\\text{IV,end}\\right) . \\tag{6.2}\\]\nIf we subtract Equation 6.2 from Equation 6.1, we have\n\\[ \\hat\\beta_\\text{OLS,exo} - \\hat\\beta_\\text{IV,exo} = \\left(\\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{exo}\\right)^{-1}\\mathbb{X}_\\text{exo}'\\mathbb{X}_\\text{end}\\left(\\hat\\beta_\\text{IV,end} - \\hat\\beta_\\text{OLS,end}\\right).\\]\nThis equality means that if \\(\\mathop{\\mathrm{plim}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\hat{\\boldsymbol\\beta}_\\text{IV} )\\) (in which case the difference in the exogenous components approaches zero), then the difference in the endogenous portions approach zero as well. In short, we lose nothing from focusing on the difference \\(\\hat\\beta_\\text{OLS,end} - \\hat\\beta_\\text{IV,end}\\), which means the variance term which standardizes the Hausman statistic has full rank.\n\\[ H = (\\hat\\beta_\\text{OLS,end} - \\hat\\beta_\\text{IV,end})'\\left[\\widehat{\\text{Avar}}(\\hat\\beta_\\text{IV,end}) - \\widehat{\\text{Avar}}(\\hat\\beta_\\text{OLS,end})\\right]^{-1} (\\hat\\beta_\\text{OLS,end} - \\hat\\beta_\\text{IV,end})\\]\n\nCorollary 6.2 (Hausman Test for Endogeneity) Given the hypothesis \\(H_0:\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\) where regressors are partitioned as \\(\\mathbf{X}= [\\mathbf{X}_\\text{exo},\\mathbf{X}_\\text{end}]\\), the Hausman statistic simplifies to \\[H = (\\hat{\\boldsymbol\\beta}_\\text{OLS,end} - \\hat{\\boldsymbol\\beta}_\\text{IV,end})'\\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV,end}) - \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS,end})\\right]^{-1} (\\hat{\\boldsymbol\\beta}_\\text{OLS,end} - \\hat{\\boldsymbol\\beta}_\\text{IV,end}).\\] Under \\(H_0\\), \\[ H\\sim \\chi_q^2\\] where \\(q = \\dim(\\mathbf{X}_\\text{end})\\). I will call the test associated with this statistic is known as the Hausman test for endogeneity.\n\n\nExample 6.15 Suppose \\((X, Z, \\varepsilon) \\sim N(\\boldsymbol \\mu,\\boldsymbol{\\Sigma})\\) where \\[\\begin{align*}\n\\boldsymbol \\mu & = [10,10,0]',\\\\\n\\boldsymbol{\\Sigma}& = \\begin{bmatrix}20 & 5 & 0\\\\5&20&0\\\\0&0&1 \\end{bmatrix},\n\\end{align*}\\] and \\(Y = 1 + 3X + \\varepsilon\\). We have \\(\\text{Cov}\\left(X,\\varepsilon\\right) = 0\\), so \\(\\text{E}\\left[X\\varepsilon\\right] = 0\\) and our model meets the assumptions of the classical linear model (\\(P_{\\boldsymbol{\\beta},\\sigma^2} \\in \\mathcal P_\\text{LM}\\subset\\mathcal P_\\text{IV}\\)). If we draw a sample of size \\(n = 1,00\\), we can test \\(H_0:\\text{E}\\left[X\\varepsilon\\right] = 0\\) using the Hausman statistic calculated using only \\(\\beta_2\\) (the slope coefficient). If we repeat this simulation many times, then the distribution of our test statistic should approach \\(\\chi_1^2\\), as \\(H_0\\) is true for our model.\n\nhausman_stat <- function(y, X, Z, end_col){\n  K <- ncol(X)\n  \n  # Calculate OLS and variance\n  OLS <- solve(t(X) %*% X) %*% t(X) %*% y\n  res <- (y-X %*% OLS)\n  S2 <- ((t(res) %*% res)/(n - K)) %>% as.numeric() \n  var_OLS <- (S2) * solve( t(X) %*% X )\n  \n  # calculate IV and variance\n  IV <- solve(t(Z) %*% X) %*% t(Z) %*% y\n  res <- (y-X %*% IV)\n  S2 <- ((t(res) %*% res)/(n - K)) %>% as.numeric() \n  var_IV <- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z)\n  \n  # construct the statistic using only the endogenous columns given by end_col\n  i <- end_col\n  (OLS[i] - IV[i])*solve(var_IV[i,i] - var_OLS[i,i])*(OLS[i] - IV[i])\n}\n\nstore <- vector(\"numeric\", 10000)\nfor (j in 1:10000) {\n  n <- 1000\n  mu <- c(10, 10, 0)\n  Sigma <- matrix(c(20,5,0, 5,20,0,0,0,1), nrow = 3)\n  sample <- rmvnorm(n, mu, Sigma)\n  x <- sample[,1]\n  z <- sample[,2]\n  e <- sample[,3]\n  y <- 1 + 3*x + e\n  X <- cbind(1, x)\n  Z <- cbind(1, z)\n  K <- ncol(X)\n  \n  store[j] <- hausman_stat(y, X, Z, 2)\n}\n\nNow let’s plot the simulated test statistics.\n\n\nShow code which generates figure\ntibble(store) %>% \n  ggplot(aes(store)) +\n  geom_histogram(aes(y=..density..), color = \"black\", fill = \"white\", binwidth = 0.1) +\n  geom_function(fun = dchisq,  args = list(df = 1), color = \"red\") +\n  theme_minimal() +\n  ylim(0,1.5)\n\n\n\n\n\nHistogram of the Hausman statistic calculated for 10,000 simulations with limiting distribution overlaid\n\n\n\n\n\nAnother means of testing for exogeneity is due to Wu (1973) and Durbin (1954). Begin by performing the first step of 2SLS on the possible endogenous regressors. \\[ \\mathbb{X}_\\text{end} =  \\mathbb{Z}\\boldsymbol \\delta + \\mathbb V\\] The residuals now form a matrix, because we have multiple dependent variables \\(\\mathbf{X}_\\text{end}\\). Using the estimated projection coefficient \\(\\hat{\\boldsymbol \\delta}\\), we calculated the residual values \\(\\hat{\\mathbb V}\\). Finally we augment the original linear model by including \\(\\hat{\\mathbb V}\\).\n\\[\\mathbf{y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\hat{\\mathbb V}\\boldsymbol\\gamma + \\boldsymbol{\\varepsilon}^* = \\begin{bmatrix} \\mathbb{X}_\\text{exo} & \\mathbb{X}_\\text{end}\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\beta}_\\text{exo} \\\\ \\boldsymbol{\\beta}_\\text{end}\\end{bmatrix} + \\underbrace{\\hat{\\mathbb V}\\gamma + \\boldsymbol {\\boldsymbol{\\varepsilon}}^*}_{\\boldsymbol{\\varepsilon}}\\] Consider the hypothesis \\(H_0:\\boldsymbol\\gamma = \\mathbf{0}\\), and how that may related to whether \\(\\mathbb{X}_\\text{end}\\) is endogenous or exogenous. When we perform the first step of this process (regressing \\(\\mathbb{X}_\\text{end}\\) on \\(\\mathbb{Z}\\)) we break variation in \\(\\mathbb{X}_\\text{end}\\) into two parts: the exogenous portion explained through \\(\\mathbb{Z}\\),6, and unexplained part \\(\\boldsymbol \\nu\\) which we capture through \\(\\hat{\\mathbb V}\\). If \\(\\boldsymbol \\gamma \\neq \\mathbf{0}\\), then the structural error \\(\\boldsymbol{\\varepsilon}\\) includes the unexplained part of the variation in \\(\\mathbf{X}_\\text{end}\\), which is another way of saying \\(\\text{Cov}\\left(\\mathbf{X}_\\text{end}, \\varepsilon\\right)\\neq 0\\).\nSo do we use the Hausman test, or this new test which uses artificial regressions? Well it turns out, we don’t need to make a choice because they’re equivalent if we are attentive about which residuals we use to calculate asymptotic variance! It also turns out that we could perform the artificial regression test using the fitted values \\(\\hat{\\mathbb{X}}_\\text{end}\\) instead of \\(\\hat{\\mathbb V}\\).\n\nTheorem 6.6 (Durbin-Wu-Hausman Test) Suppose we want to test \\(H_0:P_{\\boldsymbol{\\beta}, \\sigma^2}\\in\\mathcal P_\\text{LM}\\) (i.e \\(H_0: \\mathbf{X}\\) exogenous) for \\(P_{\\boldsymbol{\\beta}, \\sigma^2}\\in \\mathcal P_\\text{IV}\\). The following test statistics are equivalent:\n\nThe Wald statistic associated with the null hypothesis \\(H_0: \\boldsymbol\\gamma = \\mathbf{0}\\) calculated using \\(\\hat{\\boldsymbol\\gamma}_\\text{OLS}\\), where \\(\\boldsymbol\\gamma\\) is from the modified regression \\(\\mathbf{y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\hat{\\mathbf v}\\boldsymbol\\gamma + \\boldsymbol{\\varepsilon}^*\\), and \\(\\hat{\\mathbf v}\\) are the residuals associated with the first step projection \\(\\mathbb{X}_\\text{end} = \\mathbb{Z}\\boldsymbol \\delta + \\boldsymbol \\nu\\). \\[W_{\\boldsymbol\\gamma}= \\hat{\\boldsymbol\\gamma}_\\text{OLS}' \\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\gamma}_\\text{OLS} )\\right]^{-1}\\hat{\\boldsymbol\\gamma}_\\text{OLS}\\]\nThe Wald statistic associated with the null hypothesis \\(H_0: \\boldsymbol\\eta = \\mathbf{0}\\) calculated using \\(\\hat{\\boldsymbol\\eta}_\\text{OLS}\\), where \\(\\boldsymbol\\gamma\\) is from the modified regression \\(\\mathbf{y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\hat{\\mathbb{X}}_\\text{end}\\boldsymbol\\eta + \\boldsymbol{\\varepsilon}^*\\), and \\(\\hat{\\mathbb{X}}_\\text{end}\\) are the fitted values associated with the first step projection \\(\\mathbb{X}_\\text{end} = \\mathbb{Z}\\boldsymbol \\delta + \\boldsymbol \\nu\\). \\[W_{\\boldsymbol\\eta}= \\hat{\\boldsymbol\\eta}_\\text{OLS}' \\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\eta}_\\text{OLS} )\\right]^{-1}\\hat{\\boldsymbol\\eta}_\\text{OLS}\\]\nThe Hausman statistics from Corollary 6.2, \\[H = (\\hat{\\boldsymbol\\beta}_\\text{OLS,end} - \\hat{\\boldsymbol\\beta}_\\text{IV,end})'\\left[\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV,end}) - \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS,end})\\right]^{-1} (\\hat{\\boldsymbol\\beta}_\\text{OLS,end} - \\hat{\\boldsymbol\\beta}_\\text{IV,end}),\\] when \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{IV,end})\\) and \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS,end})\\) are calculated using \\(S^2\\) from the artificial regression \\(\\mathbf{y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\hat{\\mathbf v}\\boldsymbol\\eta + \\boldsymbol{\\varepsilon}^*\\).7\n\n\n\nProof. space\n\\((W_{\\boldsymbol\\gamma} = W_{\\boldsymbol\\eta})\\) Recall that we think of \\(\\hat{\\mathbf v}\\) and \\(\\hat{\\mathbb{X}}_\\text{end}\\) in terms of linear projection. We have\n\\[\\begin{align*}\n\\hat{\\mathbb{X}}_\\text{end} & = \\mathbb P_{\\mathbb{Z}}\\mathbb{X}_\\text{end} & (\\mathbb P_{\\mathbb{Z}} = \\mathbb{Z}(\\mathbb{Z}'\\mathbb{Z})^{-1}\\mathbb{Z}')\\\\\n\\hat{\\mathbb V} & =  \\mathbb M_{\\mathbb{Z}}\\mathbb{X}_\\text{end} & (\\mathbb M_{\\mathbb{Z}} = \\mathbb I - \\mathbb P_{\\mathbb{Z}})\n\\end{align*}\\]\n\\((W_{\\boldsymbol\\gamma} = H)\\)\n\n\nExample 6.16 test\n\nn <- 10000\nmu <- c(10, 10, 0)\nSigma <- matrix(c(20,5,1, 5,20,0,1,0,1), nrow = 3)\nsample <- rmvnorm(n, mu, Sigma)\nx <- sample[,1]\nz <- sample[,2]\ne <- sample[,3]\ny <- 1 + 3*x + e\nX <- cbind(1, x)\nZ <- cbind(1, z)\nK <- ncol(X)\n\n## Method 1\nreg_1.1 <- lm(x ~ z)\nv_hat <- reg_1.1$residuals\nreg_1.2 <- lm(y ~ x + v_hat)\nsummary(reg_1.2)$coefficients\n\n              Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 0.94838236 0.085203007  11.130856 1.307146e-28\nx           3.00233437 0.008444734 355.527411 0.000000e+00\nv_hat       0.04992151 0.008730578   5.718007 1.108732e-08\n\n\n\nsummary(reg_1.2)$coefficients[3,3]^2\n\n[1] 32.6956\n\n\n\n## Method 2\nreg_2.1 <- lm(x ~ z)\nx_hat <- fitted.values(reg_2.1)\nreg_2.2 <- lm(y ~ x + x_hat)\nsummary(reg_2.2)$coefficients\n\n               Estimate  Std. Error     t value     Pr(>|t|)\n(Intercept)  0.94838236 0.085203007   11.130856 1.307146e-28\nx            3.05225587 0.002215732 1377.538278 0.000000e+00\nx_hat       -0.04992151 0.008730578   -5.718007 1.108732e-08\n\n\n\nsummary(reg_2.2)$coefficients[3,3]^2\n\n[1] 32.6956\n\n\n\nc(sum(reg_2.2$residuals^2), \n  sum(reg_1.2$residuals^2))\n\n[1] 9341.802 9341.802\n\n\n\n#Method 3\nS2 <- sum(reg_2.2$residuals^2)/(n-K)\nOLS <- solve(t(X) %*% X) %*% t(X) %*% y\nvar_OLS <- (S2) * solve( t(X) %*% X )\nIV <- solve(t(Z) %*% X) %*% t(Z) %*% y\nvar_IV <- (S2) *  solve(t(Z) %*% X) %*% (t(Z) %*% Z) %*% solve(t(X) %*% Z)\n\n(OLS[2] - IV[2])*solve(var_IV[2,2] - var_OLS[2,2])*(OLS[2] - IV[2])\n\n         [,1]\n[1,] 32.69887\n\n\n\nsummary(ivreg::ivreg(y ~ x | z))\n\n\nCall:\nivreg::ivreg(formula = y ~ x | z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.28835 -0.66660 -0.01466  0.67473  4.06233 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.948382   0.087335   10.86   <2e-16 ***\nx           3.002334   0.008656  346.85   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9909 on 9998 degrees of freedom\nMultiple R-Squared: 0.9948, Adjusted R-squared: 0.9948 \nWald test: 1.203e+05 on 1 and 9998 DF,  p-value: < 2.2e-16 \n\n\n\n\n\n6.5.2 Sargan–Hansen Test"
  },
  {
    "objectID": "endog.html#examplereplication",
    "href": "endog.html#examplereplication",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.6 Example/Replication",
    "text": "6.6 Example/Replication\nLet’s replicate Card (1995), a classic paper which has provided a textbook example of instrumental variables. To make sure our results are correct, we can compare them to the tables in the original paper which is found here. The data is available on David Card’s website.\nCard (1995) is concerned with estimating the returns to schooling with respect to wages. We tend to think that better educated workers earn higher wages, but to what extent does this relationship hold? Denote years of schooling as \\(S_i\\), log wages as \\(Y_i\\), and \\(\\mathbf{X}_i\\) as a collection of attributes associated with a worker. The linear model of interest is \\[ Y_i = \\mathbf{X}_i\\boldsymbol\\alpha + S_i\\beta + u_i,\\] where \\(u_i\\) are unobserved components which affect earnings. We’re assuming that \\(\\text{E}\\left[\\mathbf{X}_iu_i\\right] = 0\\) for all \\(i\\) (\\(\\mathbf{X}_i\\) is exogenous), but we suspect that \\(\\text{E}\\left[S_iu_i\\right] = 0\\) (see Example @ref(exm:ex1)). The parameter \\(\\beta\\) is the returns of education on earnings, and is what we’re primarly interested in. Let’s just ignore this endogeneity for now and attempt to estimate \\(\\beta\\) with OLS. We can do this with data from the National Longitudinal Survey of Young Men (NLSYM) conducted in 1976. This survey contains information about agents’ wages in 1976, along with demographic information about family background, residence in 1976, and past residence (in 1966). The details of this survey, in particular how respondents were sampled, can be found in Card (1995).\n\ncard_95 <- read.table(\"data/nls.dat\")\n\nThe .zip file containing the data also contains a .sas file which processes and cleans the raw data. Each step is readily executed in R:8\n\ncard_95 <- card_95 %>%\n  #assign the appropriate names to each variable\n  rename(\n    id = V1,\n    nearc2 = V2,\n    nearc4 = V3,\n    nearc4a = V4,\n    nearc4b = V5,\n    ed76 = V6,\n    ed66 = V7,\n    age76 = V8,\n    daded = V9,\n    nodaded = V10,\n    momed = V11,\n    nomomed = V12,\n    weight = V13,\n    momdad14  = V14,\n    sinmom14  = V15,\n    step14 = V16,\n    reg661 = V17,\n    reg662 = V18,\n    reg663 = V19,\n    reg664 = V20,\n    reg665 = V21,\n    reg666 = V22,\n    reg667 = V23,\n    reg668 = V24,\n    reg669 = V25,\n    south66 = V26,\n    work76 = V27,\n    work78 = V28,\n    lwage76 = V29,\n    lwage78 = V30,\n    famed = V31,\n    black = V32,\n    smsa76r = V33,\n    smsa78r = V34,\n    south76 = V35,\n    reg78r = V36,\n    reg80r = V37,\n    smsa66r = V38,\n    wage76 = V39,\n    wage78 = V40,\n    wage80 = V41,\n    noint78 = V42,\n    noint80 = V43,\n    enroll76 = V44,\n    enroll78 = V45,\n    enroll80 = V46,\n    kww = V47,\n    iq = V48,\n    marsta76 = V49,\n    marsta78 = V50,\n    marsta80 = V51,\n    libcrd1 = V52\n  ) %>% \n  mutate(\n    # missing values in SAS are \".\", replace those with NAs\n    across(everything(), ~replace(., . ==  \".\" , NA)),\n    # any column that had a \".\" was loaded as a string, convert them\n    across(where(is.character), as.numeric),\n    #create variables corresponding to work experience\n    exp76 = age76 - ed76 - 6,\n    exp76_sq = exp76*exp76,\n    exp76_sq_100 = (exp76*exp76)/100,\n    #create a series of indicators for parents' level of education\n    f1 = (momed > 12 & daded > 12),\n    f2 = (momed >= 12 & daded >= 12 & (momed != 12 | daded != 12)),\n    f3 = (momed == 12 & daded == 12),\n    f4 = (momed >= 12 & nodaded == 1),\n    f5 = (daded >= 12 & momed < 12),\n    f6 = (momed >= 12 & nodaded == 0),\n    f7 = (momed >= 9 & daded >= 9),\n    f8 = (nomomed == 0 & nodaded == 0)\n  )\n\n#Gather mutually exclusive region66 indicators into categorical \ncard_95 <- card_95 %>% \n  select(id, contains(\"reg66\")) %>% \n  gather(region66, count, -id) %>% \n  filter(count == 1) %>% \n  mutate(\n    region66 = str_remove(region66,\"reg66\"),\n    region66 = as.factor(region66)\n  ) %>% \n  right_join(card_95)\n\nThe dependent variable of interest if \\(\\log wage_i\\) (lwage76). Our baseline linear model is \\[ \\log( wage_i) = \\alpha_0 + \\beta\\cdot educ_i + \\alpha_1\\cdot exper_i + \\frac{\\alpha_2}{100}\\cdot exper_i^2 + \\alpha_3 \\cdot black_i + \\alpha_4\\cdot south_i + \\alpha_5\\cdot SMSA_i + \\varepsilon_i,\\] where \\(educ_i\\) (ed76) is level of education, \\(exper_i\\) (exp76) is amount of work experiance, \\(black_i\\) (black) indicated if respondent \\(i\\) is Black, \\(south_i\\) (south76) indicated whether the respondent lives in the South (in 1976), and \\(SMSA_i\\) (smsa76r) indicates whether a respondent lives in the South and in a metropolitan area.9 Along with estimating this model, we will estimate similar models which include various controls for family education, family structure, and where respondents lived in 1966.\n\nlinear_model1 <- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r, data = card_95)\nlinear_model2 <- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)\nlinear_model3 <- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 +\n                      daded + momed + nodaded + nomomed, data = card_95)\nlinear_model4 <- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r +\n                      region66 + daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + \n                      f5 + f6 + f7 + f8, data = card_95)\nlinear_model5 <- lm(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r +\n                     region66+ daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + \n                      f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)\n\nOur results should match Table 2 of Card (1995). The stargazer package gives us a flexible means of tabulating regression results.10\n\n\n\nFor each model \\(\\hat\\beta_\\text{OLS}\\in[0.073,0.075]\\), meaning that an additional year of education corresponds to roughly a 7.4% increase in earnings. We should be suspicious of these results though, as it’s likely the case that \\(educ_i\\) is endogenous, so we need some instrument(s) for \\(educ_i\\). Card (1995) makes the case for using a variable which indicates whether a respondent grew up in a labor market with an accredited 4-year college, \\(near_\\ college_i\\) (nearc4). In other words, the respondent lived near a college in the year 1966. This proximity to a college has no baring on earnings (in theory), but it is likely correlated with \\(educ_i\\), as the cost of higher education is lower for those who have the option to live at home while attending college. Let’s use \\(near_\\ college_i\\) to calculate \\(\\hat\\beta_\\text{IV}\\). We can do this “by hand” by estimating the following reduced form equations via OLS:11\n\\[\\begin{align*}\neduc_i &= \\gamma_0 + \\gamma_1 \\cdot near_\\ college_i + \\gamma_2\\cdot exper_i + \\frac{\\gamma_3}{100}\\cdot exper_i^2 + \\gamma_4 \\cdot black_i + \\gamma_5\\cdot region_i + \\gamma_6\\cdot SMSA_i + \\cdots + u_i\\\\\n\\log( wage_i) &= \\delta_0 + \\delta_1 \\cdot near_\\ college_i + \\delta_2\\cdot exper_i + \\frac{\\delta_3}{100}\\cdot exper_i^2 + \\delta_4 \\cdot black_i + \\delta_5\\cdot region_i + \\delta_6\\cdot SMSA_i + \\cdots + \\nu_i\\\\\n\\end{align*}\\]\nWe could also directly estimate the structural model via \\(\\hat{\\boldsymbol\\beta}_\\text{IV} \\) and not worry about messing up the standard errors. If we have done everything correctly we should see: \\[ \\frac{\\hat \\delta_{\\text{OLS},1}}{\\hat \\gamma_{\\text{OLS},1}} = \\hat\\beta_\\text{IV}.\\]\n\n\n\n\n\ntest\n\n\n\n\nWe will estimate the models with and without family controls, and restrict our attention to respondents where lwage76 is not missing.\n\ncard_95 <- card_95 %>% \n  filter(!is.na(lwage76))\n\n#Without family controls \nreduced_form1_stage1 <- lm(ed76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)\nreduced_form1_stage2 <- lm(lwage76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)\nIV_model1 <- ivreg(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 |\n                     nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66, data = card_95)\n\n#With family controls\nreduced_form2_stage1 <- lm(ed76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + \n                             region66 + daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + \n                             f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)\nreduced_form2_stage2 <- lm(lwage76 ~ nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + \n                             daded + momed + nodaded + nomomed + f1 + f2 + f3 + f4 + f5 + f6 + f7 + f8 + \n                             momdad14 + sinmom14, data = card_95)\nIV_model2 <- ivreg(lwage76 ~ ed76 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + daded + momed +\n                     nodaded + nomomed + f1 + f2 + f3 + f4 +  f5 + f6 + f7 + f8 + momdad14 + sinmom14 |\n                     nearc4 + exp76 + exp76_sq_100 + black + south76 + smsa76r + smsa66r + region66 + daded + momed +\n                     nodaded + nomomed + f1 + f2 + f3 + f4 + f5 + f6 + f7 + f8 + momdad14 + sinmom14, data = card_95)\n\n\n\n\nCompare this to panel A of Table 3 in Card (1995). Note that the ratio of our reduced form estimates do equal our IV estimates. Between our original OLS estimates (of the structural model), and our IV estimates, we have seven estimates for \\(\\beta\\). We can plot these along with the corresponding 95% confidence intervals.\n\n\n\n\n\ntest\n\n\n\n\nWhile our IV estimates are larger, all of our OLS estimates fall within the 95% confidence intervals for the IV estimates. As such, we’re not able to conclude that our IV estimates are significantly larger than our OLS estimates."
  },
  {
    "objectID": "endog.html#further-reading",
    "href": "endog.html#further-reading",
    "title": "6  Endogeniety I: IV and 2SLS",
    "section": "6.7 Further Reading",
    "text": "6.7 Further Reading\nTechnical treatment of IV/2SLS: Chapter 5 of Wooldridge (2010), Chapter 8 of Greene (2018), Chapter 12 of Hansen (2022)\nTesting for Endogeneity: Section 8.7 of Davidson, MacKinnon, et al. (2004), Ruud (1984)\n\n\n\n\n\n\nCameron, A Colin, and Pravin K Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge university press.\n\n\nCard, David. 1995. “Using Geographic Variation in College Proximity to Estimate the Return to Schooling.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\nDavidson, Russell, James G MacKinnon, et al. 2004. Econometric Theory and Methods. Vol. 5. Oxford University Press New York.\n\n\nDurbin, James. 1954. “Errors in Variables.” Revue de l’institut International de Statistique, 23–32.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson Education.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHausman, Jerry A. 1978. “Specification Tests in Econometrics.” Econometrica: Journal of the Econometric Society, 1251–71.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nRuud, Paul A. 1984. “Tests of Specification in Econometrics.” Econometric Reviews 3 (2): 211–42.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. MIT press.\n\n\nWu, De-Min. 1973. “Alternative Tests of Independence Between Stochastic Regressors and Disturbances.” Econometrica: Journal of the Econometric Society, 733–50."
  },
  {
    "objectID": "extremum.html",
    "href": "extremum.html",
    "title": "7  Extremum Estimators",
    "section": "",
    "text": "The estimation of linear models turned out to be fairly straightforward, as all our estimators were somehow related to OLS. When moving beyond linear models, we need to consider more general approaches to estimation, such as the generalized method of moments (GMM) or maximum likelihood estimation (MLE). Both of these estimators fall into a very broad class of estimators formalized by Amemiya (1985) known as extremum estimators. We’ll establish the properties of this broad class of estimators before considering special cases in Section @ref(generalized-method-of-moments) and Section @ref(maximum-likelihood-estimation). A great deal of this will be based off of Newey and McFadden (1994), and requires comfort with real analysis. It may be helpful to review the relationship between continuity and limits, sequences of functions, properties of uniform convergence, compactness (and how it relates to extrema), convexity (in the context of sets and functions), and the mean value theorem as it applies to gradients and Hessians."
  },
  {
    "objectID": "extremum.html#definition-and-identification",
    "href": "extremum.html#definition-and-identification",
    "title": "7  Extremum Estimators",
    "section": "7.1 Definition and Identification",
    "text": "7.1 Definition and Identification\nAs the name implies, an extremum estimator is one that is defined via optimization.\n\nSuppose \\(\\mathbb{W}= [\\mathbf{W}_1,\\ldots, \\mathbf{W}_n]' \\sim P_\\boldsymbol{\\theta}\\) where \\(P_\\boldsymbol{\\theta}\\in\\mathcal P\\) for a known model \\(\\mathcal P\\). An extremum estimator \\(\\hat{\\boldsymbol\\theta}_\\text{EX} :\\mathcal W\\to \\boldsymbol{\\Theta}\\) is an estimator of the form \\[ \\hat{\\boldsymbol\\theta}_\\text{EX} = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}\\mid \\mathbb{W})\\] for an objective function \\(Q_n(\\boldsymbol{\\theta}\\mid \\mathbb{W})\\) which depends on realized observations and sample size. A more general definition defines \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) as the value which satisfies \\[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) \\ge \\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) + o_p(1).\\]\n\nWe’ve written \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) as the argument which maximizes an objective function, but this implicitly includes the case where \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is the minimizing argument of a function, as \\[ \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) = \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} [-Q_n(\\boldsymbol{\\theta})].\\] The idea that an estimate should arise from an optimization problem should feel natural. We generally want an estimator to be “better” than other estimators, and extremum estimators achieve this in terms of some criterion \\(Q_n(\\boldsymbol{\\theta})\\).\n\nThe estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) is an extremum estimator. \\[\\hat{\\boldsymbol\\beta}_\\text{OLS} = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{Y}= \\mathop{\\mathrm{argmin}}_{\\mathbf b \\in \\mathbb R^k} \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\mathbf b)^2 = \\mathop{\\mathrm{argmax}}_{\\mathbf b \\in \\mathbb R^k}\\left[ - \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\mathbf b)^2\\right] = \\mathop{\\mathrm{argmax}}_{\\mathbf b \\in \\mathbb R^k} -\\left\\lVert\\mathbf{Y}- \\mathbb{X}\\mathbf b\\right\\rVert^2\\] We could also maximize any monotonic transformation of this, giving way to a few other common objectives: \\[\\begin{align*}\nQ_n(\\boldsymbol{\\beta}) = -\\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\boldsymbol{\\beta})^2 \\propto -\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\boldsymbol{\\beta})^2 \\propto -\\frac{1}{2} \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\boldsymbol{\\beta})^2 = -\\frac{1}{2}\\left\\lVert\\mathbf{Y}- \\mathbb{X}\\mathbf b\\right\\rVert^2\n\\end{align*}\\]\n\nBefore we consider the properties of extremum estimators and give a handful of example, we need to address a major problem from optimization that could plague us. If asked to calculate \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), it’s tempting to inspect the first order condition \\[\\frac{\\partial Q_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = \\mathbf{0}.\\] Not only does this assume \\(Q_n\\) is differentiable (which it may not be), we may find that the first order condition has several solutions, even if there is a unique maximum. Furthermore that unique maximum could be local and not global. To avoid these complications, we’ll think about \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) as a global maximum, and not a solution to a first order condition.\n\nIn most non-trivial situations, extremum estimators will not have an analytic form, so we need to turn to numerical methods to solve the requisite optimization problem. Numerical optimization is a behemoth subject spanning applied math, computer science, operations research, and engineering. Most standard econometrics references dedicate some time to the subject:\n\nChapter 10 of Cameron and Trivedi (2005)\nSection 7.5 of Hayashi (2011)\nSection 12.7 of Wooldridge (2010)\nAppendix E of Greene (2018)\nChapter 12 of B. E. Hansen (2022)\n\nA more comprehensive treatment is Judd (1998)."
  },
  {
    "objectID": "extremum.html#consistency",
    "href": "extremum.html#consistency",
    "title": "7  Extremum Estimators",
    "section": "7.2 Consistency",
    "text": "7.2 Consistency\nDespite the definition of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) being wildly general, we can establish that \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}\\) under certain conditions. The only defining features of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) are the function being maximized (\\(Q_n\\)), and the space over which it is being maximized (\\(\\boldsymbol{\\Theta}\\)), so we should expect that we’ll need to impose some conditions on one of (if not both) of these objects. Let’s start by writing the definition of convergence in this case. \\[ \\mathop{\\mathrm{plim}}\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}_0\\]\nThe fact that we are dealing with the limiting process of \\(\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta})\\) instead of \\(Q_n\\) complicates things. Limiting processes of function do not play well with many of our favorite operators.1\n\nSuppose \\(\\Theta = [-1,\\infty]\\) and \\(\\theta_0 = -1/2\\). Define functions \\(Q\\), \\(g_n\\), and \\(Q_n\\) as: \\[\\begin{align*}\nQ(\\theta) &= \\begin{cases}1+\\theta & -1\\le\\theta\\le\\theta_0 \\\\ -\\theta & \\theta_0<\\theta\\le 0 \\\\ 0 & \\text{otherwise}  \\end{cases}\\\\\ng_n(\\theta) &= \\begin{cases}\\theta - n & n\\le\\theta\\le n + 1 \\\\ n + 2 - \\theta & n + 1\\le\\theta\\le n + 2 \\\\ 0 & \\text{otherwise} \\end{cases}\\\\\nQ_n(\\theta) & = Q(\\theta) + g_n(\\theta)\n\\end{align*}\\] These functions don’t contain random variables, so the \\(\\mathop{\\mathrm{plim}}\\) operator coincides with \\(\\lim_{n\\to\\infty}\\) The apparently nonsense function \\(Q_n\\) is actually defined such that we can determine it’s limit and maximum by inspecting a plot.\n\n\n\n\n\ntest\n\n\n\n\nIt appears that \\(\\mathop{\\mathrm{plim}}Q_n = Q\\), as the height of the the “triangle” formed on \\(\\theta \\in [n,n+2]\\) shrinks (this is the the contribution of \\(g_n/n\\)), leaving the triangle formed by \\(f\\). Formally, \\[ \\mathop{\\mathrm{plim}}Q_n(\\theta) = \\mathop{\\mathrm{plim}}Q(\\theta) + \\mathop{\\mathrm{plim}}g_n(\\theta) = Q(\\theta)  + 0 = Q(\\theta).\\] If we compare the limit of the \\(\\mathop{\\mathrm{argmax}}\\) and the \\(\\mathop{\\mathrm{argmax}}\\) of the limit, we have \\[\\begin{align*}\n\\mathop{\\mathrm{plim}}\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) & = \\mathop{\\mathrm{plim}}n + 1 = \\infty\\\\\n\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\mathop{\\mathrm{plim}}Q_n(\\boldsymbol{\\theta}) & = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} f = \\theta_0\n\\end{align*}\\]\n\nOne reason this happens, is because the nature in which \\(Q_n\\) converges. While we have \\(\\mathop{\\mathrm{plim}}Q_n(\\boldsymbol{\\theta}) = Q(\\boldsymbol{\\theta})\\) for all \\(\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\), the function does not converge uniformly in the sense that \\(Q_n\\) does not approaches \\(Q\\) on all of \\(\\boldsymbol{\\Theta}\\) simultaneously. We need to strengthen our definition of convergence in probability to insure this happens.\n\nA sequence of functions random variables \\(f_n(X)\\) converges in probability uniformly (on \\(\\mathcal X\\)) to a function \\(f\\) if for all \\(\\varepsilon > 0\\) and \\(\\delta > 0\\) there exists a fixed \\(N\\) independent of \\(X_n\\) such that \\[ \\Pr(\\left\\lvert f_n(X) - f\\right\\rvert > \\varepsilon,\\ \\forall X \\in\\mathcal X) < \\delta,\\] which is equivalent to \\[ \\lim_{n\\to\\infty}\\Pr(\\left\\lvert f_n(X) - f\\right\\rvert < \\varepsilon,\\ \\forall X \\in\\mathcal X) = 0  \\]. Yet another definition is \\[\\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}\\left\\lvert f_n(X) - f\\right\\rvert \\overset{p}{\\to}0.\\]\n\nThe definition using the supremum follows from the fact that if the maximum distance between \\(f_n(X)\\) and \\(f\\) shrinks as \\(n\\to\\infty\\), then is must be the case that the distance between \\(f_n\\) and \\(f\\) at any point in the support is shrinking as well. So let’s assume that \\(Q_n\\) converges uniformly to some limit \\(Q_0\\). Let’s look at another example, but this time define \\(Q_n\\) such that it converges uniformly to some \\(Q_0\\).\n\nSuppose \\(\\Theta = [0,\\theta_0]\\) and \\(Q_n(\\theta) = \\theta/n\\). Once again, there are no random variables in the picture, so \\(\\mathop{\\mathrm{plim}}\\) reduces to \\(\\lim_{n\\to\\infty}\\). We have \\[ \\sup_{\\theta \\in [0,1]}\\left\\lvert\\theta/n - 0\\right\\rvert = 1/n \\to 0,\\] so \\(Q_n(\\theta)\\) converges uniformly to \\(Q_0(\\theta) = 0\\) on \\([0,\\theta_0]\\). The maximum of \\(Q_n\\) is achieved at \\(\\theta_0\\) for all \\(n\\). Despite the uniform convergence, we have \\[\\begin{align*}\n\\mathop{\\mathrm{plim}}\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) & = \\mathop{\\mathrm{plim}}\\theta_0 = \\theta_0\\\\\n\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\mathop{\\mathrm{plim}}Q_n(\\boldsymbol{\\theta}) & = \\mathop{\\mathrm{argmax}}_{\\theta \\in[0,\\theta_0]} Q_0(\\theta) = \\mathop{\\mathrm{argmax}}_{\\theta \\in[0,1]} 0 = [0,\\theta_0]\n\\end{align*}\\]\n\nWe need to rule out situations where the limit of \\(Q_n\\) does not have a unique maximum, or that unique maximum is not achieved at the true value \\(\\boldsymbol{\\theta}_0\\). Finally, consider a third example.\n\nSuppose \\(\\Theta = [-2,1]\\) and \\(\\theta_0 = -1\\). Define \\[Q_n(\\theta) = \\begin{cases}(1-2^{1-n})\\theta + 2 - 2^{2-n} & -2\\le \\theta\\le -1\n\\\\(2^{1-n}-1)\\theta & -1 < \\theta \\le 0 \\\\ \\frac{2^{n+1}-2}{2^{n+1}-1}\\theta & 0  < \\theta\\le 1-2^{1-n}\\\\ \\frac{2^{n+1}-2}{2^{n+1}-1}\\theta + 2 - 2^{2-n} & 1-2^{1-n}\\le \\theta < 1\\\\0 & \\theta = 1\\end{cases}\\].\n\n\n\n\n\ntest\n\n\n\n\nBy inspecting \\(Q_n\\) for \\(n = 1,\\ldots, 6\\), we see that \\[\\begin{align*}\n\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\mathop{\\mathrm{plim}}Q_n(\\boldsymbol{\\theta}) & = \\mathop{\\mathrm{plim}}\\theta_0 = \\theta_0,\\\\\n\\mathop{\\mathrm{plim}}\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}  Q_n(\\boldsymbol{\\theta}) & = 1.\n\\end{align*}\\] The limit \\(Q_0\\) has a discontinuity at the point where \\(\\mathop{\\mathrm{plim}}\\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}) = 1\\). If we wanted to insure that \\(Q_0\\) is continuous on all of \\(\\boldsymbol{\\Theta}\\), we could redefine \\(\\boldsymbol{\\Theta}= [-2,1)\\). Unfortunately, this would mean that the limit of the sequence formed by _{} Q_n() converges to a point outside of \\(\\boldsymbol{\\Theta}\\).\n\nThis final examples shows that things can go wrong when our function \\(Q_0\\) is not continuous, or when \\(\\boldsymbol{\\Theta}\\) does not contain the limit of the maximized objective. In this example the limit in question was not in \\(\\boldsymbol{\\Theta}\\) because \\(\\boldsymbol{\\Theta}\\) did not contain all its limit points, i.e it isn’t a closed set. Something like this could also happen if \\(\\boldsymbol{\\Theta}\\) is unbounded and the limit diverges. To ensure that this limit is always in \\(\\boldsymbol{\\Theta}\\) we need \\(\\boldsymbol{\\Theta}\\) to be compact.2 If \\(\\boldsymbol{\\Theta}\\subset \\mathbb R^k\\), then this is equivalent to being closed and bounded.\nWe now have all the building blocks required to show that \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is consistent: continuity of \\(Q_0\\),3 \\(Q_n \\overset{p}{\\to}Q_0\\) uniformly, \\(\\boldsymbol{\\Theta}\\) compact, and \\(Q_0(\\boldsymbol{\\theta})\\) is uniquely maximized at \\(\\boldsymbol{\\Theta}_0\\). There are a few ways to prove this result, but I’ll follow Amemiya (1985). The first proof seems to be due to Amemiya (1973). Other great proofs are given by econometrician Xiaoxia Shi in these notes, and by Newey and McFadden (1994).\n\nSuppose there exists some (non-stochastic) function \\(Q_0(\\boldsymbol{\\theta})\\) such that:\n\n\\(\\boldsymbol{\\theta}_0 = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}Q_0(\\boldsymbol{\\theta})\\);\n\\(\\boldsymbol{\\Theta}\\) is compact;\n\\(Q_0(\\boldsymbol{\\theta})\\) is continuous on \\(\\boldsymbol{\\Theta}\\);\n\\(Q_n(\\boldsymbol{\\theta})\\) converges uniformly in probability to \\(Q_0(\\boldsymbol{\\theta})\\).\n\nThen \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\).\n\n\nProof. Let \\(N_r(\\boldsymbol{\\theta}_0)\\) be the open neighborhood/ball centered at \\(\\boldsymbol{\\theta}_0\\) with radius of \\(r > 0\\), where \\(r\\) is arbitrary. The neighborhood \\(N_r(\\boldsymbol{\\theta}_0)\\) is not necessarily a subset of \\(\\boldsymbol{\\Theta}\\).4 If \\(N^c_r(\\boldsymbol{\\theta}_0)\\) is the compliment of \\(N\\), then \\(N^c_r(\\boldsymbol{\\theta}_0)\\) is closed, and \\(N^c_r(\\boldsymbol{\\theta}_0) \\cap \\boldsymbol{\\Theta}\\subset \\boldsymbol{\\Theta}\\) is compact.5 Define \\[\\boldsymbol{\\theta}^* = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in N^c_r(\\boldsymbol{\\theta}_0) \\cap \\boldsymbol{\\Theta}} Q_0(\\boldsymbol{\\theta}).\\] The point \\(\\boldsymbol{\\Theta}^*\\) is guaranteed to exists because \\(Q_0\\) is a continuous function and \\(N^c_r(\\boldsymbol{\\theta}_0)\\cap \\boldsymbol{\\Theta}\\) is a compact set. Define \\(\\varepsilon = Q_0(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\theta}^*\\), and let \\(A_n\\) be the event \\(``|Q_n(\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| < \\varepsilon/2\\) for all \\(\\boldsymbol{\\theta}\"\\).6 Event \\(A_n\\) holds for all \\(\\boldsymbol{\\theta}\\), including \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) and \\(\\boldsymbol{\\theta}_0\\), so: \\[\\begin{align}\n& |Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} )| < \\varepsilon/2 \\\\\n\\implies & Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) < \\varepsilon/2\\\\\n\\implies &  Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) > Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - \\varepsilon/2 (\\#eq:a1)\\\\\\\\\n& |Q_n(\\boldsymbol{\\theta}_0) - Q_0(\\boldsymbol{\\theta}_0)| < \\varepsilon/2 \\\\\n\\implies & Q_n(\\boldsymbol{\\theta}_0) - Q_0(\\boldsymbol{\\theta}_0) < -\\varepsilon/2\\\\\n\\implies &  Q_n(\\boldsymbol{\\theta}_0) > Q_0(\\boldsymbol{\\theta}_0) - \\varepsilon/2 (\\#eq:a2)\n\\end{align}\\] By the definition of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), \\(Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) \\ge Q_n(\\boldsymbol{\\theta}_0)\\). If we combine this with @ref(eq:a1), we have \\[\\begin{equation}\nQ_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) > Q_n(\\boldsymbol{\\theta}_0) - \\varepsilon/2 (\\#eq:a3).\n\\end{equation}\\] If we add inequalities @ref(eq:a2) and @ref(eq:a3), event \\(A_n\\) implies \\[\\begin{align*}\n&Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} )  + Q_n(\\boldsymbol{\\theta}_0) > Q_n(\\boldsymbol{\\theta}_0) - \\varepsilon/2 + Q_0(\\boldsymbol{\\theta}_0) - \\varepsilon/2\\\\\n\\implies & Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) >  Q_0(\\boldsymbol{\\theta}_0) - \\varepsilon \\\\\n\\implies &  Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) >  Q_0(\\boldsymbol{\\theta}_0) - Q_0(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\theta}^* & (\\varepsilon = Q_0(\\boldsymbol{\\theta}_0) - \\boldsymbol{\\theta}^*)\\\\\n\\implies &  Q_0(\\hat{\\boldsymbol\\theta}_\\text{EX} ) >  \\boldsymbol{\\theta}^* \\\\\n\\implies & \\hat{\\boldsymbol\\theta}_\\text{EX} \\notin N^c \\cap \\boldsymbol{\\Theta}& \\left(\\boldsymbol{\\theta}^* = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in N^c_r(\\boldsymbol{\\theta}_0) \\cap \\boldsymbol{\\Theta}} Q_0(\\boldsymbol{\\theta})\\right)\\\\\n\\implies & \\hat{\\boldsymbol\\theta}_\\text{EX} \\in N_r(\\boldsymbol{\\theta}_0) &(\\hat{\\boldsymbol\\theta}_\\text{EX} \\in \\boldsymbol{\\Theta})\\\\\n\\implies & \\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r & (\\text{definition of }N_r(\\boldsymbol{\\theta}_0))\n\\end{align*}\\] If the event \\(A_n\\) implies that \\(\\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r\\), then \\(\\Pr(A_n) \\le \\Pr\\left(\\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r\\right)\\). We have assumed \\(Q_n \\overset{p}{\\to}Q_0\\) uniformly, so \\[\\begin{align*}\n&\\lim_{n\\to \\infty}\\Pr(|Q_n(\\boldsymbol{\\theta}) - Q_0(\\boldsymbol{\\theta})| < \\varepsilon/2) = 1\\\\\n\\implies & \\lim_{n\\to \\infty}\\Pr(A_n) = 1\\\\\n\\implies & \\lim_{n\\to \\infty}\\Pr\\left(\\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r\\right) = 1 & \\left(\\Pr(A_n) \\le \\Pr\\left(\\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r\\right)\\right)\n\\end{align*}\\] We’ve taken the radius \\(r\\) to be arbitrary, so \\[\\begin{align*}\n& \\lim_{n\\to \\infty}\\Pr\\left(\\left\\lvert\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0\\right\\rvert < r\\right) = 1 & (\\forall r > 0).\n\\end{align*}\\] This is the definition of convergence in probability, so \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\)!\n\nThis theorem applies to any metric space \\(\\boldsymbol{\\Theta}\\), not just subsets of Euclidean space.7 Newey and McFadden (1994) provide slightly weaker conditions under which this theorem holds, the details of which can be studied in Aliprantis and Border (n.d.).\nActually applying Theorem, which we would like to do @ref(thm:excon) in Section @ref(generalized-method-of-moments) and Section @ref(maximum-likelihood-estimation), can be a bit tricky. On particular problem comes with the assumption that \\(\\boldsymbol{\\Theta}\\) is compact. This will not hold whenever \\(\\boldsymbol{\\Theta}= \\mathbb R^k\\), which is the cases we’ve been most concerned with. Realistically, we could restrict our attention to some closed subset \\(\\boldsymbol{\\Theta}' \\subset \\mathbb R^k\\) in most applications. For example, if we’re estimating a linear model where we want to estimate the returns of schooling to log earnings \\(\\beta\\), we can confidently assume \\(\\beta \\in [0, 10]\\). There are situations where we cannot do this though, so we need a second consistency result holds in the absence of compactness.\n\nSuppose there exists some (non-stochastic) function \\(Q_0(\\boldsymbol{\\theta})\\) such that:\n\n\\(\\boldsymbol{\\theta}_0 = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}Q_0(\\boldsymbol{\\theta})\\);\n\\(\\boldsymbol{\\theta}_0\\) is an interior point of \\(\\boldsymbol{\\Theta}\\);\n\\(\\boldsymbol{\\Theta}\\) is a convex set;\n\\(Q_n(\\boldsymbol{\\theta})\\) is a concave function on \\(\\boldsymbol{\\Theta}\\);\n\\(Q_n(\\boldsymbol{\\theta}) \\overset{p}{\\to}Q_0(\\boldsymbol{\\theta})\\) (pointwise) on \\(\\boldsymbol{\\Theta}\\).\n\nThen \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) exists with probability approaching 1, and \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\).\n\n\nProof. The point \\(\\boldsymbol{\\theta}_0\\) is an interior point of \\(\\boldsymbol{\\Theta}\\), so there exists a compact neighborhood of radius \\(2\\varepsilon\\) centered at \\(\\boldsymbol{\\theta}_0\\) contained in \\(\\boldsymbol{\\Theta}\\), \\(C\\subset \\boldsymbol{\\Theta}\\). The boundary of this neighborhood is \\(\\partial C\\). Appealing to some more obscure real analysis results, we can conclude:\n\n\\(Q_0\\) is concave (the pointwise limit of concave functions is concave).\n\\(Q_0\\) is continuous on \\(C\\) (concave functions are continuous on the interior of their domains).\n\\(Q_n \\overset{p}{\\to}Q_0\\) uniformly on \\(C\\) (pointwise convergence of concave functions of a dense subset of an open set implies uniform convergence on compact subsets of an open set)\n\nDefine \\(\\tilde {\\boldsymbol{\\theta}} = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in C} Q_n(\\boldsymbol{\\theta})\\). In light of points 2 and 3, we have \\(\\tilde {\\boldsymbol{\\theta}} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) by @ref(thm:excon). We will now show that \\(\\tilde {\\boldsymbol{\\theta}}\\) not only maximizes \\(Q_n\\) over \\(C\\), but also over \\(\\boldsymbol{\\Theta}\\), so \\(\\tilde{\\boldsymbol{\\theta}} = \\hat{\\boldsymbol\\theta}_\\text{EX} \\) and \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\).\nIf \\(\\tilde{\\boldsymbol{\\theta}} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\), then \\(\\Pr(\\left\\lvert\\tilde{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0\\right\\rvert < \\varepsilon)\\to 1\\). This is equivalent to \\[\\Pr\\left(Q_n(\\tilde{\\boldsymbol{\\theta}}) \\ge \\max_{\\partial C} Q_n(\\boldsymbol{\\theta})\\right)\\to 1.\\] By the convexity of \\(\\boldsymbol{\\Theta}\\), in the event that \\(Q_n(\\tilde{\\boldsymbol{\\theta}}) \\ge \\max_{\\partial C} Q_n(\\boldsymbol{\\theta})\\), there exists a convex combination \\(\\alpha \\tilde{\\boldsymbol{\\theta}} + (1-\\alpha)\\tilde{\\boldsymbol{\\theta}} \\in \\partial C\\) (\\(\\alpha\\) < 1) for any \\(\\boldsymbol{\\theta}\\notin C\\). If we evaluate \\(Q_n\\) at this convex combination, we have \\[\\begin{equation}\nQ_n(\\tilde{\\boldsymbol{\\theta}}) \\ge Q_n(\\alpha \\tilde{\\boldsymbol{\\theta}} + (1-\\alpha)\\tilde{\\boldsymbol{\\theta}} ). (\\#eq:a4)\n\\end{equation}\\] But \\(Q_n\\) is concave, so \\[\\begin{equation}\nQ_n(\\alpha \\tilde{\\boldsymbol{\\theta}} + (1-\\alpha)\\tilde{\\boldsymbol{\\theta}} ) \\ge \\alpha Q_n( \\tilde{\\boldsymbol{\\theta}}) + (1-\\alpha)Q_n( \\tilde{\\boldsymbol{\\theta}}). (\\#eq:a5)\n\\end{equation}\\] If we combine inequalities @ref(eq:a4) and @ref(eq:a5), we have \\[(1-\\alpha)Q_n(\\tilde{\\boldsymbol{\\theta}}) \\ge (1-\\alpha)Q_n({\\boldsymbol{\\theta}}),\\] so \\(\\tilde{\\boldsymbol{\\theta}} = \\hat{\\boldsymbol\\theta}_\\text{EX} \\)."
  },
  {
    "objectID": "extremum.html#extremum-identification",
    "href": "extremum.html#extremum-identification",
    "title": "7  Extremum Estimators",
    "section": "7.3 Extremum Identification",
    "text": "7.3 Extremum Identification\nPerhaps the most important stipulation in Theorems @ref(thm:excon) and @ref(thm:excon2) is that \\(\\mathop{\\mathrm{plim}}Q_n = Q_0\\) is uniquely maximized at the true value \\(\\boldsymbol{\\theta}_0 \\in \\boldsymbol{\\Theta}\\). We can think of \\(Q_0\\) as a “population” counterpart to \\(Q_n\\). In the event his non-stochastic/true population function \\(Q_0\\) is maximized at multiple values \\(\\{\\boldsymbol{\\theta}_0, \\boldsymbol{\\theta}_0'\\}\\), then we have no way of knowing if our extremum estimator is consistently estimating \\(\\boldsymbol{\\theta}_0\\) or \\(\\boldsymbol{\\theta}_0'\\). This problem should sound very familiar. It seems to be similar, if not the same, exact problem that arises when a model \\(\\mathcal P\\) is unidentified! It happens to be the same exact problem.\nSuppose we have a model \\(\\mathcal P\\) with a parameterization \\(\\mu : \\boldsymbol{\\Theta}\\to \\mathcal P\\), where \\(\\mu\\) is injective such that \\(\\mathcal P\\) is identified. The true parameter value for \\(P\\) is \\(\\mu^{-1}(P)\\) (where this is the left inverse of \\(\\mu\\)). Consider the problem of assigning real numbers to combinations of parameters and model values \\((\\boldsymbol{\\theta}, P) \\in \\boldsymbol{\\Theta}\\times \\mathcal P\\) such that we can identify each \\(P\\) using these numbers. We can use a function \\(Q_0:\\boldsymbol{\\Theta}\\times \\mathcal P \\to \\mathbb R\\) for this, and define \\(\\phi:\\boldsymbol{\\Theta}\\to \\mathcal P\\) such that its left inverse is: \\[ \\phi^{-1}(P) = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}Q_0(P, \\boldsymbol{\\theta}).\\] If we wanted to reparameterize \\(\\mathcal P\\) with \\(\\phi\\) and maintain identification, then what conditions would \\(Q_0\\) need to satisfy? It would need to be the case that \\(Q_0\\) has a unique maximum (otherwise \\(\\phi\\) would assign multiple parameters to \\(P\\) and we would not have identification), and that unique maximization needs to occur at true parameter \\(\\theta = \\mu^{-1}(P)\\) associated with \\(P\\): \\[ \\mu^{-1}(P) = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}}Q_0(P, \\boldsymbol{\\theta}).\\] This is precisely the first condition required for consistency! For more details about this, see Davidson and MacKinnon (1993) and Lewbel (2019), the latter of which coined the term “extremum identification”. A concrete example may illuminate the link between identification and \\(Q_0\\) achieving a unique maximum at \\(\\boldsymbol{\\theta}_0\\)\n\nReturn to the classic linear model \\(\\mathcal P_\\text{LM}\\) and the estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). \\[ \\hat{\\boldsymbol\\beta}_\\text{OLS} = \\mathop{\\mathrm{argmin}}_{\\mathbf b} \\sum_{i=1}^n(Y_i-\\mathbf{X}_i\\mathbf b)^2 = \\mathop{\\mathrm{argmax}}_{\\mathbf b}\\underbrace{ -\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mathbf{X}_i\\mathbf b)^2}_{Q_n}.\\] By the law of large numbers \\[ \\underbrace{ -\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mathbf{X}_i\\mathbf b)^2}_{Q_n}\\overset{p}{\\to}\\underbrace{-\\text{E}\\left[(Y - \\mathbf{X}\\mathbf b)^2\\right]}_{Q_0}\\]\nUnder what conditions is \\(Q_0\\) uniquely maximized at the true parameter value \\(\\boldsymbol{\\beta}\\)? The first order condition for \\(\\mathop{\\mathrm{argmax}}Q_0\\) can be reduced to two different equations: \\[\\begin{align*}\n\\text{E}\\left[\\mathbf{X}'Y\\right]&=\\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\\\\n\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]&=0\n\\end{align*}\\] Therefore \\(Q_0\\) is uniquely maximized at the true parameter value \\(\\boldsymbol{\\beta}\\) when \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]=0\\) and \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) is invertible. These are precisely the same conditions which we determined identified \\(\\mathcal P_\\text{LM}\\).\n\n\nConsider the linear projection model \\(\\mathcal P_\\text{LP}\\) where \\(\\boldsymbol{\\beta}\\) was defined as \\[\\boldsymbol{\\beta}=\\mathop{\\mathrm{argmax}}_{\\mathbf b}-\\text{E}\\left[(Y - \\mathbf{X}\\mathbf b)^2\\right]\\] to begin with (opposed to \\(\\boldsymbol{\\beta}\\) having a structural interpretation like in \\(\\mathcal P_\\text{LM}\\)). The parameterization is already given by a maximization problem here! In this special case, we can think of an extremum estimator as originating from the analog principle, because \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) (when written as the solution to the maximization problem) is the sample analog to \\(\\boldsymbol{\\beta}\\). This model is identified when \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\), which has the roll of insuring that the maximization problem which defines \\(\\boldsymbol{\\beta}\\) has a unique solution. Once again, this is condition 1 in @ref(thm:excon).\n\n\nIn general, consistency and identification are inherently related. Consider the classical linear model \\(\\mathcal P_\\text{LM}\\) where \\(\\boldsymbol{\\beta}= \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'Y\\right]\\). The model is identified, so \\(\\boldsymbol{\\beta}\\) is unique to \\(P_{\\boldsymbol{\\beta}, \\sigma^2}\\in\\mathcal P\\). Heuristically, we can think of identification as the ability to estimate the parameter perfectly at the population level with an infinite amount of data. If given an infinite amount of data, we could calculate the moments \\(\\text{E}\\left[\\mathbf{X}'Y\\right]\\) and \\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\), enabling us to calculate \\(\\boldsymbol{\\beta}\\).8 We will never have infinite data, so the best we can do is define the sample analog of \\(\\boldsymbol{\\beta}\\) and \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\), and appeal to the fact that as our sample size grows and approaches infinity, our estimates become arbitrarily better. This just happens to be consistency. Formally, suppose under an assumed model value \\(P\\in\\mathcal P\\) we have an estimator \\(\\hat{\\boldsymbol{\\theta}}\\) with a unique limit in probability \\(\\mathop{\\mathrm{plim}}\\hat{\\boldsymbol{\\theta}}\\). If we define the parameterization \\(\\boldsymbol{\\theta}\\mapsto P_{\\boldsymbol{\\theta}}\\) as \\(\\boldsymbol{\\theta}= \\mathop{\\mathrm{plim}}\\hat{\\boldsymbol{\\theta}}\\), then we’ve leveraged consistency such that our model is identified by construction."
  },
  {
    "objectID": "extremum.html#asymptotic-normality",
    "href": "extremum.html#asymptotic-normality",
    "title": "7  Extremum Estimators",
    "section": "7.4 Asymptotic Normality",
    "text": "7.4 Asymptotic Normality\nOur next technical result is that \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) happens to be root-n CAN under certain conditions. This will require a bit more work now that we aren’t restricting our attention to linear models. Recall the steps we took to prove Theorem @ref(thm:asymols). From the onset of this proof, we had an closed form solution for \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\). Given \\(Q_n = -\\left\\lVert\\mathbf{Y}- \\mathbb{X}\\mathbf b\\right\\rVert^2\\), we were able to solve the associated first order condition for a unique solution in the form of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} = \\hat{\\boldsymbol\\beta}_\\text{OLS} = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\mathbf{Y}\\), because the first order condition was itself a linear equation with one root: \\[ \\frac{\\partial Q_n}{\\partial \\mathbf b} = 2\\mathbb{X}'(\\mathbf{Y}-\\mathbb{X}\\mathbf b).\\] Once we move beyond linear models, it may be the case that the first order condition \\(\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}\\) is nonlinear and cannot be solved explicitly for \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), even if we assume the first order condition has a unique root. The easiest way to handle this is using the mean value theorem to approximate the first order condition linearly (i.e a first order Taylor expansion). Doing this requires additional assumptions about \\(Q_n\\), in particular assumptions about the existence of derivatives. Using the mean value theorem require continuous differentiability, but in our case we’re going to apply it to the derivative \\(\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}\\), so we will need \\(Q_n\\) to be twice continuous differentiability. This means our theorem will involve the hessian \\(\\mathbf H(\\boldsymbol{\\theta})\\) of \\(Q_n\\).\n\nSuppose that:\n\n\\(\\frac{\\partial Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} )}{\\partial \\boldsymbol{\\theta}} = o_p(n^{-1/2})\\) (the FOC holds as \\(n\\to\\infty\\));\n\\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\)\n\\(\\boldsymbol{\\Theta}_0\\) is in the interior of \\(\\boldsymbol{\\Theta}\\);\n\\(Q_n(\\boldsymbol{\\theta})\\) is twice continuously differentiable in a neighborhood \\(N_r(\\boldsymbol{\\theta}_0)\\);\n\\(\\sqrt n \\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\);\n\\(\\frac{\\partial^2 Q_n(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} \\overset{p}{\\to}\\mathbf H(\\boldsymbol{\\theta})\\) uniformly for some \\(\\mathbf H(\\boldsymbol{\\theta})\\) continuous at \\(\\boldsymbol{\\theta}_0\\);\n\\(\\mathbf H(\\boldsymbol{\\theta}_0)\\) is invertible. Then \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) &\\overset{d}{\\to}N(\\mathbf{0}, \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\boldsymbol \\Omega \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}).\n\\end{align*}\\]\n\n\n\nProof. We will begin by performing a first-order Taylor expansion of \\(\\frac{\\partial Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} )}{\\partial \\boldsymbol{\\theta}}\\) about \\(\\boldsymbol{\\theta}_0\\).9\n\\[\\begin{align}\n&\\frac{\\partial Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} )}{\\partial \\hat{\\boldsymbol\\theta}_\\text{EX} } = o_p(n^{-1/2})\\\\\n\\implies & o_p(n^{-1/2})= \\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} + \\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) & (\\hat\\theta_{\\text{EE},j} < \\tilde\\theta_{j} <  \\theta_{0,j} \\ \\forall j) (\\#eq:a7)\n\\end{align}\\] The vector \\(\\tilde{\\boldsymbol{\\theta}}\\) “lies between” \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) and \\(\\boldsymbol{\\theta}_0\\) (element-wise, where the elements of \\(\\tilde{\\boldsymbol{\\theta}}\\) are given by the \\(K\\) expansions), so \\(\\tilde{\\boldsymbol{\\theta}}\\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) as a consequence of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\). Using assumption 6, \\[\\begin{align}\n&\\frac{\\partial^2 Q_n(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} \\overset{p}{\\to}\\mathbf H(\\boldsymbol{\\theta}) & (\\forall \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta})\\\\\n\\implies & \\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} \\overset{p}{\\to}\\mathbf H(\\tilde{\\boldsymbol{\\theta}})\\\\\n\\implies & \\mathop{\\mathrm{plim}}\\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} = \\mathop{\\mathrm{plim}}\\mathbf H(\\tilde{\\boldsymbol{\\theta}})\\\\\n\\implies & \\mathop{\\mathrm{plim}}\\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} = \\mathbf H(\\mathop{\\mathrm{plim}}\\tilde{\\boldsymbol{\\theta}}) & (\\mathbf H\\text{ continuous at } \\boldsymbol{\\theta}_0 = \\mathop{\\mathrm{plim}}\\tilde{\\boldsymbol{\\theta}})\\\\\n\\implies & \\mathop{\\mathrm{plim}}\\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} = \\mathbf H(\\boldsymbol{\\theta}_0) & (\\boldsymbol{\\theta}_0 = \\mathop{\\mathrm{plim}}\\tilde{\\boldsymbol{\\theta}})\\\\\n\\implies & \\frac{\\partial^2 Q_n(\\tilde{\\boldsymbol{\\theta}})}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'} = \\mathbf H(\\boldsymbol{\\theta}_0) + o_p(1) (\\#eq:a8)\n\\end{align}\\] If we substitute equation @ref(eq:a8) into equation @ref(eq:a7), then \\[\\begin{align*}\n& o_p(n^{-1/2}) = \\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} + (\\mathbf H(\\boldsymbol{\\theta}_0) + o_p(1))(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0)\\\\\n\\implies & \\underbrace{\\sqrt n \\cdot o_p(n^{-1/2})}_{o_p(1)} = \\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} + (\\mathbf H(\\boldsymbol{\\theta}_0) + o_p(1))\\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0)\\\\\n\\implies &\\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) = -(\\mathbf H(\\boldsymbol{\\theta}_0) + o_p(1))^{-1}\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} - \\underbrace{o_p(1)(\\mathbf H(\\boldsymbol{\\theta}_0) + o_p(1))^{-1}}_{o_p(1)}\\\\\n\\implies & \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) \\overset{p}{\\to}-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}}\\\\\n\\implies & \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) \\overset{d}{\\to}-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\underbrace{\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}}}_{\\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)} & (\\overset{p}{\\to}\\implies \\overset{d}{\\to})\\\\\n\\implies & \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) \\overset{d}{\\to}-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} N(\\mathbf{0}, \\boldsymbol \\Omega)\\\\\n\\implies & \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) \\overset{d}{\\to}N(\\mathbf{0}, [-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}]'\\boldsymbol \\Omega[-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}]) & (\\mathbf H(\\boldsymbol{\\theta}_0)\\text{ invertible})\\\\\n\\implies & \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0) \\overset{d}{\\to}N(\\mathbf{0}, \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}) & (\\mathbf H(\\boldsymbol{\\theta}_0)\\text{ symmetric})\n\\end{align*}\\] space\n\n\nThe asymptotic variance of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) exhibits a nice pattern. It’s the variance \\(\\boldsymbol\\Omega\\) “sandwiched” between \\(\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\). This is why a(n) (asymptotic) variance/covariance matrix of this form, \\[\\text{Avar}\\left(\\hat{\\boldsymbol{\\theta}}\\right) = a \\mathbf B^{-1}\\mathbf A \\mathbf B^{-1},\\] is called a sandwich variance/covariance matrix. Many root-n CAN estimators have a sandwich variance/covariance matrix because the delta method along with the properties of variance naturally lend themselves to this sandwich form. \\[\\begin{align*}\n\\text{Var}\\left(\\mathbf B \\mathbf{X}+ \\mathbf c\\right) &= \\mathbf B\\text{Var}\\left(\\mathbf{X}\\right)\\mathbf B'\\\\\n\\sqrt{n}[\\mathbf g(\\mathbf{X}_n) - \\mathbf g(\\mathbf t)] &\\overset{d}{\\to}N\\left(\\mathbf{0}, \\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf{x}}(\\mathbf t)\\right]\\text{Avar}\\left(\\mathbf{X}_n\\right)\\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf{x}}(\\mathbf t)\\right]'\\right) & (\\mathbf{X}_n \\overset{d}{\\to}N(\\mathbf{0}, \\text{Avar}\\left(\\mathbf{X}_n\\right)))\n\\end{align*}\\] In the event that \\(\\text{Avar}\\left(\\mathbf{X}_n\\right)\\) is a symmetric matrix \\(\\mathbf B'\\), we have the sandwich form \\(\\mathbf B^{-1}\\mathbf A \\mathbf B^{-1}\\) (which may be scaled by some \\(a\\)). In certain special cases, \\(a \\mathbf B^{-1}\\mathbf A \\mathbf B^{-1}\\) may reduce to \\(a \\mathbf B^{-1}\\)."
  },
  {
    "objectID": "extremum.html#the-hypothesis-testing-trinity",
    "href": "extremum.html#the-hypothesis-testing-trinity",
    "title": "7  Extremum Estimators",
    "section": "7.5 The Hypothesis Testing “Trinity”",
    "text": "7.5 The Hypothesis Testing “Trinity”\nThere are three main approaches to testing hypotheses using \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), one that we are familiar with, and two new ones. In each case, we’ll need a consistent estimator \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{EX} )\\). Unfortunately, Theorem @ref(thm:exasy) is so general that it doesn’t provide much guidance as to estimating \\[\\text{Avar}\\left(\\hat{\\boldsymbol\\theta}_\\text{EX} \\right) = \\frac{\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}}{n}.\\] We only know that \\(\\mathbf H\\) is the limit of the Hessian of \\(Q_n\\), and that \\(\\boldsymbol \\Omega\\) is the asymptotic variance of the gradient of \\(\\mathbf H\\). Without more information about \\(Q_n\\), there isn’t much we can do right now to estimate the asymptotic variance of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\). Instead, we’ll derive consistent estimators \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{EX} )\\) for special cases of extremum estimators, and assume they exist for now, allowing us to construct test statistics.\nTheorem @ref(thm:exasy) establishes that \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is root-n CAN, so we can test a general nonlinear hypothesis \\(\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\) using a Wald test: \\[ W = \\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{EX} )'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{EX} )\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{EX} )\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{EX} )'\\right]^{-1}\\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{EX} ).\\] We can also test hypotheses about \\(\\hat\\theta_{\\text{EX,}j}\\) using the \\(t-\\)test \\[ t = \\frac{\\hat\\theta_{\\text{EX},j} - \\theta_0}{\\widehat{\\text{se}}(\\hat\\theta_{\\text{EX},j})}.\\] These tests are based on the discrepancy between \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) and \\(\\boldsymbol{\\theta}_0\\) scaled by the precision (asymptotic variance) of our estimator \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\). This is not the only way to measure how “far” estimates are from a null hypothesis.\nConsider the definition of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) in general, and under \\(H_0:\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\): \\[\\begin{align*}\n\\hat{\\boldsymbol\\theta}_\\text{EX} & = \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} Q_n(\\boldsymbol{\\theta}),\\\\\n\\bar{\\boldsymbol{\\theta}}_\\text{EE} & = \\mathop{\\mathrm{argmax}}_{\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}} Q_n(\\boldsymbol{\\theta}).\n\\end{align*}\\] We usually refer to an estimator such as \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) as a restricted estimator, because it is calculated under the restriction that the null hypothesis is true.\nRao (1948) proposed an alternate statistic based solely on \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\). Appealing to the method of Lagrange multipliers, the restricted estimator \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) is given as the solution to the following first order conditions (after being scaled by \\(\\sqrt{n}\\)): \\[\\begin{align*}\n\\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) + \\mathbf h(\\bar{\\boldsymbol{\\theta}}_\\text{EE})'\\sqrt{n}\\boldsymbol\\lambda  & = \\mathbf{0}(\\#eq:lm1)\\\\\n\\sqrt{n}\\boldsymbol\\lambda\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}} & = \\mathbf{0}(\\#eq:lm2)\n\\end{align*}\\]\nIn the event that \\(H_0\\) is true, then the multiplier \\(\\boldsymbol\\lambda = \\mathbf{0}\\). This suggests testing the equivalent hypothesis \\(H_0:\\boldsymbol\\lambda = \\mathbf{0}\\). The Wald statistic associated with this problem is \\[ \\boldsymbol\\lambda'\\left[\\widehat{\\text{Avar}}(\\boldsymbol\\lambda)\\right]^{-1}\\boldsymbol\\lambda,\\] which can be simplified if we find \\(\\text{Avar}\\left(\\boldsymbol\\lambda\\right)\\).\nWe can derive a test statistic from these FOCs assuming \\(\\sqrt n \\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\) and that we are able to perform a mean value expansion on \\(\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\) and \\(\\mathbf h\\). For some values \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}<\\tilde{\\boldsymbol{\\theta}}<\\boldsymbol{\\theta}_0\\) and \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}<\\boldsymbol{\\theta}^\\dagger<\\boldsymbol{\\theta}_0\\),10 \\[\\begin{align*}\n\\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) &= \\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) + \\frac{\\partial^2 Q_n}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'}(\\boldsymbol{\\theta}^\\dagger)\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)\\\\\n\\sqrt{n}\\mathbf h(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) &= \\underbrace{\\sqrt{n}\\mathbf h(\\boldsymbol{\\theta}_0)}_\\mathbf{0}+ \\frac{\\partial\\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\tilde{\\boldsymbol{\\theta}})\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)\n\\end{align*}\\] If we assume that \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\overset{p}{\\to}\\boldsymbol{\\theta}_0\\), then \\(\\tilde{\\boldsymbol{\\theta}} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) and \\(\\boldsymbol{\\theta}^\\dagger\\overset{p}{\\to}\\boldsymbol{\\theta}_0\\), because they’re “squeezed” in between \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) and \\(\\boldsymbol{\\theta}_0\\). If we take the limit (in probability) of these expansions, we have \\[\\begin{align*}\n\\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) &= \\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) +\\mathbf H(\\boldsymbol{\\theta}_0)\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0) + o_p(1) (\\#eq:lm3)\\\\\n\\sqrt{n}\\mathbf h(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) &= \\frac{\\partial\\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0) + o_p(1) (\\#eq:lm4)\n\\end{align*}\\] where the definition of \\(\\mathbf H\\) is given in Theorem @ref(thm:exasy). If we assume that \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) under \\(H_0\\), then \\[\\begin{align*}\n\\mathbf h(\\bar{\\boldsymbol{\\theta}}_\\text{EE})'\\boldsymbol\\lambda = \\mathbf h(\\boldsymbol{\\theta}_0)'\\boldsymbol\\lambda + o_p(1) (\\#eq:lm5)\n\\end{align*}\\] If we substitute equations @ref(eq:lm3), @ref(eq:lm4), and @ref(eq:lm5) into the first order conditions given by equations (#eq:lm1) and (#eq:lm2), we have (after some consolidation into matrices): \\[ \\begin{bmatrix} \\mathbf H(\\boldsymbol{\\theta}_0) & \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\\\\n\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) & \\mathbf{0}\\end{bmatrix}\\begin{bmatrix}\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)\\\\ \\sqrt n \\boldsymbol \\lambda \\end{bmatrix} = \\begin{bmatrix} -\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\\\ \\mathbf{0}\\end{bmatrix} + o_p(1)\\]\nIf we solve this system we have: \\[\\begin{align*}\n&\\begin{bmatrix}\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)\\\\ \\sqrt n \\boldsymbol \\lambda \\end{bmatrix}  = \\begin{bmatrix} \\mathbf H(\\boldsymbol{\\theta}_0) & \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\\\\n\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) & \\mathbf{0}\\end{bmatrix}^{-1}\\begin{bmatrix} -\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\\\ \\mathbf{0}\\end{bmatrix} + o_p(1)\\\\\n\\implies & \\sqrt{n}\\boldsymbol \\lambda = -\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) + o_p(1)\\\\\n& \\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)=-\\left[\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} - \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left(\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\right]\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} + o_p(1) (\\#eq:lm6)\\\\\n\\implies & \\sqrt{n}\\boldsymbol \\lambda \\overset{p}{\\to}-\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\underbrace{\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) }_{N(\\mathbf{0}, \\boldsymbol \\Omega)}\\\\\n\\implies & \\sqrt{n}\\boldsymbol \\lambda \\overset{d}{\\to}N\\left(\\mathbf{0},  \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega  \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1} \\right)\\\\\n\\implies & \\text{Avar}\\left(\\boldsymbol \\lambda\\right) = \\frac{1}{n}\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega  \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\n\\end{align*}\\] Therefore, the Wald statistic associated with \\(H_0:\\boldsymbol\\lambda = \\mathbf{0}\\) (taking the asymptotic variance to be known at the moment) is \\[\\begin{align*}\n&\\boldsymbol\\lambda' \\left[\\text{Avar}\\left(\\boldsymbol\\lambda\\right)\\right]^{-1}\\boldsymbol\\lambda\\\\\n\\implies & \\boldsymbol\\lambda' \\left[\\frac{1}{n}\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega  \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\right]^{-1}\\boldsymbol\\lambda\\\\\n\\implies & n\\boldsymbol\\lambda'\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega  \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\boldsymbol\\lambda.\n\\end{align*}\\] Under \\(H_0\\) we have \\(\\nabla_\\boldsymbol{\\theta}Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) = \\mathbf{0}\\). Combining this with @ref(eq:lm2) gives \\[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\boldsymbol \\lambda =  \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}),\\] which can be used to simplify our test statistic.\n\\[ n\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})'\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol \\Omega  \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})'.\\] In practice, we only know \\(H_0:\\mathbf h(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}_0\\), \\(Q_n\\), and \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\), so we need to estimate \\(\\boldsymbol \\Omega\\) and \\(\\mathbf H(\\boldsymbol{\\theta}_0)\\). Once we substitute in suitable estimators, we have our Wald statistic for \\(H_0:\\boldsymbol \\lambda = \\mathbf{0}\\), which is usually considered it’s own seperate test statistic.\n\nSuppose \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is an extremum estimator for which \\(\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\), and \\(\\frac{\\partial^2 Q_n}{\\partial \\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\boldsymbol{\\theta}_0) \\overset{p}{\\to}\\mathbf H(\\boldsymbol{\\theta}_0)\\). If a null hypothesis \\(H_0\\) is summarized by some (possibly nonlinear) function \\(\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\) the Lagrange multiplier statistic is defined as \\[ LM = n\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\hat{\\boldsymbol \\Omega}\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}),\\] where \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) is the extremum estimator subject to \\(H_0\\). The statistic is also known as the Rao test statistic or score test statistic\n\nIf \\(\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) \\gg 0\\), then \\(LM \\gg 0\\), and it is likely that \\(H_0\\) is false. An immediate consequence of our derivation of \\(LM\\) is that \\(LM \\overset{d}{\\to}\\chi_q^2\\), as it is a special case of the Wald statistic. A formal statement of this will be presented after deriving one last test statistic.\nA third option to test \\(H_0\\) is by looking at the difference \\(Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\) (which is always positive). If \\(H_0\\) is likely to be true then \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\approx \\bar{\\boldsymbol{\\theta}}_\\text{EE}\\), so \\(Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\approx 0\\). A test statistic based on this criterion would have a major advantage over the Wald and Lagrange multiplier statistics because it would not require us to estimate any asymptotic variances. Unfortunately, as we’ll soon see, this advantage comes at the cost of an assumption about \\(\\boldsymbol \\Omega\\) and \\(\\mathbf H(\\boldsymbol{\\theta}_0)\\). First, let’s look at the second-order Taylor expansion of \\(Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\) about \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\). For some \\(\\hat{\\boldsymbol\\theta}_\\text{EX} <\\tilde{\\boldsymbol{\\theta}} < \\bar{\\boldsymbol{\\theta}}_\\text{EE}\\), \\[\\begin{align*}\n& Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) =  Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) + \\frac{\\partial Q_n}{\\partial\\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} ) + \\frac{1}{2}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )'\\frac{\\partial^2 Q_n}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\tilde{\\boldsymbol{\\theta}})(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )\\\\\n\\implies & 2[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) -  Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})] = -\\frac{\\partial Q_n}{\\partial\\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} ) - \\frac{1}{2}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )'\\frac{\\partial^2 Q_n}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\tilde{\\boldsymbol{\\theta}})(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )\n\\end{align*}\\] Under the assumption that \\(\\sqrt{n}\\frac{\\partial Q_n}{\\partial\\boldsymbol{\\theta}} \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\), the first term of this expansion is \\(o_p(1)\\). \\[\\begin{align*}\n& 2\\sqrt{n}[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})] = -\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )'\\frac{\\partial^2 Q_n}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\tilde{\\boldsymbol{\\theta}})(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )+ o_p(1)\\\\\n\\implies & 2n[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})] = -[\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )]'\\frac{\\partial^2 Q_n}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\tilde{\\boldsymbol{\\theta}})[\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )] + o_p(1)\n\\end{align*}\\] If \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\), then we also have \\(\\tilde{\\boldsymbol{\\theta}} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\), so \\[2n[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})] = -[\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )]'\\mathbf H(\\boldsymbol{\\theta}_0)[\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )] + o_p(1).\\] The term \\([\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} )]\\) looks a lot like something that would be asymptotically normal, in which case \\(2n[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})]\\) would be a function of a quadratic form of (asymptotically) normal vectors meaning it could be distributed according to some \\(\\chi^2\\) distribution. From the Taylor expansion used to prove Theorem @ref(thm:exasy) and Equation Theorem @ref(eq:lm6), we have \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0)&-\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\sqrt{n}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) + o_p(1),\\\\\n\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0)&=-\\left[\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} - \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left(\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\right]\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}} + o_p(1).\n\\end{align*}\\] These imply \\[\\begin{align*}\n\\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}-\\hat{\\boldsymbol\\theta}_\\text{EX} ) & = \\sqrt{n}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}- \\boldsymbol{\\theta}_0) - \\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{EX} - \\boldsymbol{\\theta}_0)\\\\\n& = -\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\left[\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}}\\right] + o_p(1)\n\\end{align*}\\] If we plug this into our last expression for \\(2n[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})]\\),11 we have \\[\\begin{align*}\n2n[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})] & = \\underbrace{\\left[\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}}\\right]}_{\\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)}' \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\underbrace{\\left[\\sqrt{n}\\frac{\\partial Q_n(\\boldsymbol{\\theta}_0)}{\\partial \\boldsymbol{\\theta}}\\right]}_{\\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)} + o_p(1)\\\\\n&  \\overset{d}{\\to}\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} N(\\mathbf{0}, \\boldsymbol \\Omega)\\right]' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1}\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} N(\\mathbf{0}, \\boldsymbol \\Omega)\\right]\\\\\n& = N\\left(\\mathbf{0},\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\boldsymbol \\Omega \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\right)' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1} N\\left(\\mathbf{0},\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\boldsymbol \\Omega \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\right)\n\\end{align*}\\] The limiting distribution is a quadratic form of normally distributed variables, but the matrix in the center which “weights” the quadratic form does not correspond properly to the variance of the distributions such that we have a chi-squared distribution. We need this matrix in the center to be equal to the variance of the normal distribution in order to “standardize” it and give us the square of two standard normal distributions (which is the definition of a chi-squared distributed). \\[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\boldsymbol \\Omega \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\neq  \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\] For this relation to hold with equality, we need to make an assumption about the relationship between \\(\\mathbf H(\\boldsymbol{\\theta}_0)\\) and \\(\\boldsymbol \\Omega\\).\n\nSuppose \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is an extremum estimator for which \\(\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\), and \\(\\frac{\\partial^2 Q_n}{\\partial \\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\boldsymbol{\\theta}_0) \\overset{p}{\\to}\\mathbf H(\\boldsymbol{\\theta}_0)\\). In addition assume that there exists some scalar \\(c\\in \\mathbb R\\) such that \\[\\boldsymbol \\Omega = c \\mathbf H(\\boldsymbol{\\theta}_0),\\] giving and \\(\\boldsymbol \\Omega \\propto \\mathbf H(\\boldsymbol{\\theta}_0)\\). The equality associated with this assumption is the generalized information matrix equality.\n\nThis assumption seems completely arbitrary, but once we start working with concrete examples of extremum estimators, we’ll see it hold in many familiar settings that motivate it holding in this general case.12 Also note that as we’ve defined \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), \\(c < 0\\). If \\(Q_0\\) is uniquely maximized at \\(\\boldsymbol{\\theta}_0\\), then its Hessian \\(\\mathbf H(\\boldsymbol{\\theta}_0)\\) is negative semi-definite, while \\(\\boldsymbol \\Omega\\) corresponds to a variance (meaning its positive semi-definite). These facts imply \\(c< 0\\).\nIf we assume the generalized information matrix equality holds, then \\[\\begin{align*}\n\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\boldsymbol \\Omega \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'& = \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} c \\mathbf H(\\boldsymbol{\\theta}_0) \\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'= c\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\n\\end{align*}\\] All we need to do is divide our statistic by \\(c\\) to account for the proportionality. \\[\\frac{2n}{c}[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})]\\overset{d}{\\to}N\\left(\\mathbf{0},\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}  \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\right)' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1} N\\left(\\mathbf{0},\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\mathbf H(\\boldsymbol{\\theta}_0)^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\right) = \\chi_q^2.\\]\n\nSuppose \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) is an extremum estimator for which \\(\\sqrt n \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\overset{d}{\\to}N(\\mathbf{0}, \\boldsymbol \\Omega)\\), and \\(\\frac{\\partial^2 Q_n}{\\partial \\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}'}(\\boldsymbol{\\theta}_0) \\overset{p}{\\to}\\mathbf H(\\boldsymbol{\\theta}_0)\\), and \\(\\boldsymbol \\Omega = c \\mathbf H(\\boldsymbol{\\theta}_0)\\) for some \\(c < 0\\). The distance metric statistic for the null hypothesis \\(H_0\\) is \\[DM = \\frac{2n}{\\hat c}[Q_n(\\hat{\\boldsymbol\\theta}_\\text{EX} ) - Q_n(\\bar{\\boldsymbol{\\theta}}_\\text{EE})],\\] where \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) is the extremum estimator subject to \\(H_0\\).\n\nNow we can formalize things into a theorem.\n\nLet \\(H_0:\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\) be some hypothesis where \\(\\mathbf h:\\boldsymbol{\\Theta}\\to \\mathbb R^q\\) (\\(q \\le \\dim(\\boldsymbol{\\Theta}) = K\\)), \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\) be an extremum estimator for \\(\\boldsymbol{\\theta}\\), and \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\) be the restricted extremum estimator for \\(\\boldsymbol{\\theta}\\) under \\(H_0\\). Suppose:\n\nThe function \\(\\mathbf h(\\boldsymbol{\\theta})\\) is continuously differentiable in a neighborhood of \\(\\boldsymbol{\\theta}_0\\);\nThe conditions of Theorem @ref(thm:exasy) are satisfied.\n\\(\\frac{\\partial \\mathbf h(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\) is invertible;\n\\(\\hat {\\mathbf H}(\\boldsymbol{\\theta}_0)\\) and \\(\\hat {\\boldsymbol\\Omega}\\) are consistent estimators of \\(\\mathbf H(\\boldsymbol{\\theta}_0)\\) and \\(\\boldsymbol \\Omega\\), respectively;\n\\(\\bar{\\boldsymbol{\\theta}}_\\text{EE} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) under \\(H_0\\).\nThere exists some scalar \\(c\\in \\mathbb R\\) such that \\(\\boldsymbol \\Omega = c \\mathbf H(\\boldsymbol{\\theta}_0)\\), and \\(\\hat c\\) is a consistent estimator for \\(c\\).\n\nThen:\n\nUnder assumptions a-d, \\(W \\overset{d}{\\to}\\chi_q^2\\);\nUnder assumptions a-e, \\(LM \\overset{d}{\\to}\\chi_q^2\\). If assumption f holds, then \\[LM = \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat {\\boldsymbol\\Omega}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE}) \\]\nUnder assumptions a-f, \\(DM \\overset{d}{\\to}\\chi_q^2\\).\n\n\n\nProof. space\n\nWe proved that \\(W\\overset{d}{\\to}\\chi_q^2\\) in Theorem @ref(thm:wald). All we need is a consistent estimator for \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\theta}_\\text{EX} \\right) = \\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\boldsymbol\\Omega\\mathbf H(\\boldsymbol{\\theta}_0)^{-1}\\). By Assumption d, we have this in the form of \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{EX} ) = \\hat {\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1} \\hat {\\boldsymbol\\Omega}{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\)\nWe showed that \\(LM\\) is a special case of \\(W\\) with the additional assumption that \\(\\bar{\\boldsymbol{\\theta}}_\\text{EE}\\overset{p}{\\to}\\boldsymbol{\\theta}_0\\) under \\(H_0\\). IIf assumption f holds, we have a consistent estimator for \\(\\boldsymbol \\Omega\\) in the form of \\(\\hat {\\boldsymbol\\Omega} = \\hat c\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)\\): \\[\\begin{align*}\nLM &= n\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\hat c\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\\\\n& = \\frac{n}{\\hat c}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\\\\n& = \\frac{n}{\\hat c}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\underbrace{\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\right]^{-1}}_{\\mathbf I}\\underbrace{\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}\\right]^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}}_{\\mathbf I}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\\\\n& = \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})'\\frac{\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)^{-1}}{\\hat c} \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\\\\\n& = \\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})' \\hat {\\boldsymbol\\Omega}^{-1}\\frac{\\partial Q_n}{\\partial \\boldsymbol{\\theta}}(\\bar{\\boldsymbol{\\theta}}_\\text{EE})\n\\end{align*}\\]\nWe already established that the test statistic converges in distribution to \\(\\chi_q^2\\) when \\(c\\), is known. By Slutsky’s theorem, this will still hold for a consistent estimator \\(\\hat c\\).\n\nspace\n\nIf we assume \\(\\dim(\\boldsymbol{\\Theta}) = \\dim(\\mathbf h) = 1\\), we can plot \\(Q_n\\), \\(H_0:h(\\theta) = 0\\) and the distances corresponding to our these three statistics.\n\n\n\n\n\ntest\n\n\n\n\nDespite our trio of our test statistics being asymptotically equivalent, there are still some advantages and disadvantages associated with each one. The Wald test and Lagrange multiplier test only require us to estimate the unrestricted model, whereas the distance metric test requires estimating the unrestricted model and restricted model. The distance metric test doesn’t require us to estimate any asymptotic variances, only the constant \\(c\\) where \\(\\boldsymbol \\Omega = c \\mathbf H(\\boldsymbol{\\theta}_0)\\). The restricted model may also be much simpler than the unrestricted model (for example \\(H_0\\) could be “the model is linear”), in which case the Lagrange multiplier test is much easier to use."
  },
  {
    "objectID": "extremum.html#m-estimators",
    "href": "extremum.html#m-estimators",
    "title": "7  Extremum Estimators",
    "section": "7.6 M-Estimators",
    "text": "7.6 M-Estimators\nA particular class of extremum estimators that we will work with often define \\(Q_n\\) to be a sample average of some function of the data.\n\nSuppose \\(\\mathbb{W}= [\\mathbf{W}_1,\\ldots, \\mathbf{W}_n]' \\sim P_\\boldsymbol{\\theta}\\) where \\(P_\\boldsymbol{\\theta}\\in\\mathcal P\\) for a known model \\(\\mathcal P\\). An M-estimator \\(\\hat{\\boldsymbol\\theta}_\\text{M} :\\mathcal W\\to \\boldsymbol{\\Theta}\\) is an extremum estimator where \\[\\begin{align*}\nQ_n(\\boldsymbol{\\theta}) &= \\frac{1}{n}\\sum_{i=1}^n m(\\boldsymbol{\\theta},\\mathbf{W}_i),\\\\\n\\hat{\\boldsymbol\\theta}_\\text{M} &= \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\frac{1}{n}\\sum_{i=1}^n m(\\boldsymbol{\\theta}, \\mathbf{W}_i).\n\\end{align*}\\]\n\nHuber (1964) formalize M-estimators in an effort to generalize MLE (hence the letter M). Clearly M-estimators are consistent and asymptotically normal under the conditions of Theorems @ref(thm:excon) and @ref(thm:exasy), but perhaps these conditions simplify in the special case of \\(Q_n(\\boldsymbol{\\theta})=n^{-1}\\sum_{i=1}^n m(\\boldsymbol{\\theta}, \\mathbf{W}_i)\\). For instance, \\(Q_n\\) is the sum of sample averages, so perhaps we can use some LLN to conclude \\(Q_n \\overset{p}{\\to}Q_0\\) uniformly instead of directly verifying that \\(\\sup_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} \\left\\lvert Q_n - Q_0\\right\\rvert \\overset{p}{\\to}0\\).\n\nSuppose that \\(\\mathbf{W}_i \\overset{iid}{\\sim}P_\\boldsymbol{\\theta}\\), \\(\\boldsymbol{\\Theta}\\) is compact, \\(m(\\boldsymbol{\\theta}, \\mathbf{W}_i)\\) is continuous on \\(\\boldsymbol{\\Theta}\\) for all \\(\\mathbf{W}_i \\in \\mathcal W\\), and there exists some \\(d(\\mathbf{W}_i)\\) such that \\(\\left\\lVert m(\\boldsymbol{\\theta},\\mathbf{W}_i)\\right\\rVert \\le d(\\mathbf{W}_i)\\) on \\(\\boldsymbol{\\Theta}\\) where \\(\\text{E}\\left[d(\\mathbf{W}_i)\\right] < \\infty\\). Then \\(\\text{E}\\left[m(\\boldsymbol{\\theta}, \\mathbf{W}_i)\\right]\\) is continuous and \\[n^{-1}\\sum_{i=1}^n m(\\boldsymbol{\\theta}, W_i) \\overset{p}{\\to}\\text{E}\\left[m(\\boldsymbol{\\theta}, \\mathbf{W}_i)\\right]\\] uniformly on \\(\\boldsymbol{\\Theta}\\).\n\nWhile I haven’t been able to locate a formal proof of this result, it seems like a direct application of the Weierstrass M-test (Theorem 7.10 in Rudin (1976)) and the fact that the uniform limit of continuous functions is continuous. The lemma also includes two of the other conditions from @ref(thm:excon): \\(\\boldsymbol{\\Theta}\\) is compact, and \\(Q_0 = \\text{E}\\left[m(\\boldsymbol{\\theta}, \\mathbf{W}_i)\\right]\\).\n\nIf the conditions of Lemma @ref(lem:uwlln) hold and \\(\\text{E}\\left[m(\\boldsymbol{\\theta}_0, \\mathbf W)\\right] > \\text{E}\\left[m(\\boldsymbol{\\theta}, \\mathbf W)\\right]\\) for all \\(\\boldsymbol{\\theta}\\neq\\boldsymbol{\\theta}_0\\), then \\(\\hat{\\boldsymbol\\theta}_\\text{M} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\).\n\n\nProof. Conditions 2-4 of @ref(thm:excon) are satisfied under the assumption that Lemma @ref(lem:uwlln) holds. Condition 1 is satisfied as well because \\[ \\mathop{\\mathrm{plim}}Q_n = \\mathop{\\mathrm{plim}}\\frac{1}{n}\\sum_{i=1}^n m(\\boldsymbol{\\theta}, W_i) = \\text{E}\\left[m(\\boldsymbol{\\theta}_0, \\mathbf W)\\right] = Q_0,\\] and we’ve assumed \\(Q_0 = \\text{E}\\left[m(\\boldsymbol{\\theta}_0, \\mathbf W)\\right]\\) is uniquely maximized at \\(\\boldsymbol{\\theta}_0\\).\n\nWe can also restate Theorem @ref(thm:exasy) in the context of M-estimators. Let’s write the Jacobian and Hessian of \\(Q_n\\) in the event that \\(\\hat{\\boldsymbol\\theta}_\\text{EX} = \\hat{\\boldsymbol\\theta}_\\text{M} \\). \\[\\begin{align*}\n\\frac{\\partial Q_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} &= \\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\left[\\frac{1}{n}\\sum_{i=1}^n m(\\boldsymbol{\\theta},\\mathbf{W}_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}}\\\\\n\\frac{\\partial Q_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'} & = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'}\n\\end{align*}\\] The derivatives of \\(Q_n\\) are sample averages of the derivatives of \\(m\\). Following Wooldridge (2010) define the random matrix \\(\\mathbb{H}(\\boldsymbol{\\theta}, \\mathbf{W}_i) = \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'}\\) and random vector \\(\\mathbf S(\\boldsymbol{\\theta}, \\mathbf{W}_i)= \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}}\\).13 Theorem @ref(thm:exasy) also involved the probability limits of the Jacobian and Hessian of \\(Q_n\\), and in the case of M-estimators, these happen to be expected values because of the LLN. \\[\\begin{align*}\n\\frac{\\partial Q_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} &= \\frac{1}{n}\\sum_{i=1}^n \\mathbf S(\\boldsymbol{\\theta}, \\mathbf{W}_i) \\overset{p}{\\to}\\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}, \\mathbf{W})\\right]\\\\\n\\frac{\\partial Q_n(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'} & = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{H}(\\boldsymbol{\\theta}, \\mathbf{W}_i) \\overset{p}{\\to}\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}, \\mathbf{W})\\right]\n\\end{align*}\\]\n\nSuppose the conditions of @ref(thm:exasy) are met where \\[\\begin{align*}\nQ_n(\\boldsymbol{\\theta}) &= \\frac{1}{n}\\sum_{i=1}^n m(\\boldsymbol{\\theta},\\mathbf{W}_i),\\\\\n\\mathbf S(\\boldsymbol{\\theta}, \\mathbf{W}_i) &= \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}},\\\\\n\\mathbb{H}(\\boldsymbol{\\theta}, \\mathbf{W}_i) &= \\frac{\\partial m(\\boldsymbol{\\theta},\\mathbf{W}_i)}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}'}.\n\\end{align*}\\] Then \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol\\theta}_\\text{M} - \\boldsymbol{\\theta}) &\\overset{d}{\\to}N\\left(\\mathbf{0}, \\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\text{Var}\\left(S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right)\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\right),\\\\\n\\hat{\\boldsymbol\\theta}_\\text{M} & \\overset{a}{\\sim}N\\left(\\boldsymbol{\\theta}, \\frac{1}{n}\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\text{Var}\\left( S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right)\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\right).\n\\end{align*}\\]\n\nWe are also able to use Lemma @ref(lem:uwlln) to verify condition 6 of Theorem @ref(thm:exasy). In the case of Corollary @ref(cor:asyM), Condition 5 of Theorem @ref(thm:exasy) establishes that \\(\\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right] = \\mathbf{0}\\), so the expectation of \\(\\mathbf S\\) “squared” is the variance, giving \\[ \\hat{\\boldsymbol\\theta}_\\text{M} \\overset{a}{\\sim}N\\left(\\boldsymbol{\\theta}, \\frac{1}{n}\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})'\\right]\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\right).\\] In general, we didn’t provide an estimator for \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\theta}_\\text{EX} \\right)\\), but we should be able to use the LLN to find a consistent estimator for \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\theta}_\\text{M} \\right)\\). Define the Hessian and score functions evaluated for a single observation \\(\\mathbf{W}_i\\): \\[\\begin{align*}\n\\hat{\\mathbb{H}}_i &= \\mathbb{H}(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i) & (i =1,\\ldots,n)\\\\\n\\hat{\\mathbf S}_i &= \\mathbf{S}(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i) & (i =1,\\ldots,n)\n\\end{align*}\\] If the LLN holds for these random quantities, then \\[\\begin{align*}\n\\widehat{\\text{E}\\left[\\mathbb{H}\\right]} &= \\frac{1}{n}\\sum_{i=1}^n {\\mathbb{H}}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i) \\overset{p}{\\to}\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]\\\\\n\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]} &= \\frac{1}{n}\\sum_{i=1}^n {\\mathbf S}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i){\\mathbf S}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i)'  \\overset{p}{\\to}\\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})'\\right]\n\\end{align*}\\] because \\(\\hat{\\boldsymbol\\theta}_\\text{M} \\overset{p}{\\to}\\boldsymbol{\\theta}_0\\). The matrices of constants \\(\\boldsymbol \\Omega\\) and \\(\\mathbf H\\) from Theorem @ref(thm:exasy) now correspond to the variance of the mean-zero random vector \\(\\mathbf S\\) and expectation of random matrix \\(\\mathbb{H}\\), respectively.\n\nSuppose \\(\\hat{\\boldsymbol\\theta}_\\text{M} \\) is an M-estimator, and define \\[ \\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{M} ) = \\frac{1}{n}\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]}\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1} = \\left[\\sum_{i=1}^n {\\mathbb{H}}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i)\\right]^{-1}\\left[\\sum_{i=1}^n {\\mathbf S}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i){\\mathbf S}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i)'\\right]\\left[\\sum_{i=1}^n {\\mathbb{H}}_i(\\hat{\\boldsymbol\\theta}_\\text{M} , \\mathbf{W}_i)\\right]^{-1}.\\] Then \\(\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\theta}_\\text{M} ) \\overset{p}{\\to}\\text{Avar}\\left(\\hat{\\boldsymbol\\theta}_\\text{M} \\right)\\).\n\n\nProof. The consistency follows from the above derivation. If you factor out the \\(n\\) terms from \\(\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\) and \\(\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]}\\), they (including the \\(1/n\\)) will all cancel.\n\n\nSuppose we are estimating the classical linear model \\(\\mathcal P_\\text{LM}\\) with \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\). In this case we can partition the random vector \\(\\mathbf{W}_i\\) into \\((\\varepsilon_i,\\mathbf{X}_i)\\), where the model defines \\(Y_i = \\mathbf{X}_i\\boldsymbol{\\beta}+ \\varepsilon_i\\). This is an M-estimator where \\[m(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i) = -(Y_i - \\mathbf{X}_i\\boldsymbol{\\beta})^2.\\] We have \\[\\begin{align*}\n\\mathbf S_i(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i) & = 2\\mathbf{X}_i'(Y_i - \\mathbf{X}_i\\boldsymbol{\\beta}) = 2\\mathbf{X}_i\\varepsilon_i\\\\\n\\mathbb{H}_i(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i) & = 2\\mathbf{X}_i'\\mathbf{X}_i\n\\end{align*}\\]\nIn Example @ref(exm:olsEX), we saw that \\(Q_0\\) is uniquely maximized at \\(\\boldsymbol{\\beta}_0\\). The parameter space \\(\\boldsymbol{\\Theta}= \\mathbb R^k\\) is convex, \\(Q_n\\) is concave, \\(\\boldsymbol{\\beta}_0\\) is an interior point of \\(\\boldsymbol{\\Theta}\\) (otherwise it would be infinity), and by @ref(lem:uwlln) \\(Q_n \\overset{p}{\\to}Q_0\\) uniformly, so by Theorem @ref(thm:excon2) \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\overset{p}{\\to}\\boldsymbol{\\beta}_0\\). Note that by the assumptions of \\(\\mathcal P_\\text{LM}\\), \\(\\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right] = \\mathbf{0}\\) (\\(\\mathbf{X}_i\\) is weakly exogenous) and \\(\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i)\\right]\\) is invertible (\\(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) has full rank), along with all the other assumptions of Corollary @ref(cor:asyM) being met. Therefore we have\n\\[\\begin{align*}\n\\hat{\\boldsymbol\\beta}_\\text{OLS} &\\overset{a}{\\sim}N\\left(\\mathbf{0}, \\frac{1}{n}\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\text{Var}\\left(\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf{W})'\\right)\\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf{W})\\right]^{-1}\\right)\\\\\n& \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\frac{1}{n}\\text{E}\\left[2\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[2\\mathbf{X}'\\boldsymbol{\\varepsilon}[2\\mathbf{X}'\\boldsymbol{\\varepsilon}]'\\right]\\text{E}\\left[2\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\right)\\\\\n& \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\frac{1}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}\\mathbf{X}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\right)\\\\\n& \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\right)\\\\\n& \\overset{a}{\\sim}N\\left(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{n}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]^{-1}\\right).\n\\end{align*}\\]\nThis is the exact same asymptotic distribution from Theorem @ref(thm:asymols)! We can also derive an asymptotic estimator for \\(\\text{Avar}\\left(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\right)\\) using properties of M-estimators. \\[\\begin{align*}\n\\widehat{\\text{E}\\left[\\mathbb{H}\\right]} &= \\frac{2}{n}\\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\\\\n\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]} &= \\frac{4}{n}\\sum_{i=1}^n \\hat{e}_i^2\\mathbf{X}_i'\\mathbf{X}_i =  \\left(\\frac{2}{n}\\sum_{i=1}^n \\hat e_i^2\\right) \\left(\\frac{2}{n}\\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\right)\\\\\n\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]} \\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1} & = \\left(\\frac{2}{n}\\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1} \\left(\\frac{2}{n}\\sum_{i=1}^n \\hat e_i^2\\right) \\left(\\frac{2}{n}\\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\right)\\left(\\frac{2}{n}\\sum_{i=1}^n \\mathbf{X}_i'\\mathbf{X}_i\\right)^{-1} =\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\left(\\mathbb{X}'\\mathbb{X}\\right)^{-1}  \\\\\n\\widehat{\\text{Avar}}(\\hat{\\boldsymbol\\beta}_\\text{OLS} ) & = \\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\left(\\mathbb{X}'\\mathbb{X}\\right)^{-1}\n\\end{align*}\\] But this doesn’t look right, because \\(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n} \\neq S^2\\), as it doesn’t have the bias correction of \\(n-K\\). Technically, this isn’t the end of the world because the estimator for \\(\\sigma^2\\) is asymptotically unbiased.\n\nIn general, we weren’t able to estimate the asymptotic variance of \\(\\hat{\\boldsymbol\\theta}_\\text{EX} \\), but we can for \\(\\hat{\\boldsymbol\\theta}_\\text{M} \\). This means we can use the trio of test statistics developed in the previous section.\n\nIf \\(\\hat{\\boldsymbol\\theta}_\\text{M} \\) is an M-estimator, the test statistics in Theorem @ref(thm:trintest) become: \\[\\begin{align*}\nW & = n\\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]}\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\right]^{-1}   \\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} );\\\\\nLM & =  n\\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]}'\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)' \\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0) \\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}}{[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})']}\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)'\\right]^{-1} \\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_0)\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]};\\\\\nDM & = \\frac{2}{\\hat c} \\left[\\sum_{i=1}^n m(\\hat{\\boldsymbol\\theta}_\\text{M} ,\\mathbf{W}_i) - \\sum_{i=1}^n m(\\bar{\\boldsymbol{\\theta}}_\\text{M},\\mathbf{W}_i)\\right].\n\\end{align*}\\] In the generalized information inequality holds when calculating \\(LM\\), we have \\[ LM = n\\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]}'\\widehat{\\text{E}}{[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})']}^{-1} \\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]}\\]\n\n\nLet’s employ our tests in the context of the classical linear model. In this case, let \\[m_i(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i) = -\\frac{1}{2} \\sum_{i=1}^n (Y_i - \\mathbf{X}_i\\boldsymbol{\\beta})^2,\\] such that the 2s are eliminated when differentiating \\(m_i(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i)\\).\nFirst, let’s confirm the generalized information inequality holds and that we can use the distance metric statistic. \\[\\begin{align*}\n\\mathbf H(\\boldsymbol{\\theta}_0) & = \\text{E}\\left[\\mathbb{H}(\\boldsymbol{\\theta}_0, \\mathbf W)\\right] = \\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] \\\\\n\\boldsymbol \\Omega & = \\text{E}\\left[\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf W)\\mathbf S(\\boldsymbol{\\theta}_0, \\mathbf W)'\\right] = \\text{E}\\left[\\mathbf{X}'\\varepsilon\\varepsilon'\\mathbf{X}\\right] = \\text{E}\\left[\\varepsilon^2\\right]\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right] = \\sigma^2\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\n\\end{align*}\\] The inequality holds, as \\(\\boldsymbol \\Omega = c\\mathbf H(\\boldsymbol{\\theta}_0)\\), where \\(c = \\sigma^2\\). We could let \\(\\hat c = S^2\\), but here we’ll use \\[\\hat c = \\frac{\\hat{\\boldsymbol \\Omega}}{\\hat{\\mathbf H}(\\boldsymbol{\\theta}_0)} = \\frac{1}{n}\\sum_{i=1}^n \\hat e_i^2 = \\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}.\\]\nOur test statistics are \\[\\begin{align*}\nW & = n\\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\widehat{\\text{E}\\left[\\mathbf S\\mathbf S'\\right]}\\widehat{\\text{E}\\left[\\mathbb{H}\\right]}^{-1}\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\right]^{-1}   \\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} ) \\\\\n& = \\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\left[\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )\\left[\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\left(\\mathbb{X}'\\mathbb{X}\\right)^{-1}\\right]\\frac{\\partial \\mathbf h}{\\partial \\boldsymbol{\\theta}}(\\hat{\\boldsymbol\\theta}_\\text{M} )'\\right]^{-1}   \\mathbf h(\\hat{\\boldsymbol\\theta}_\\text{M} ) \\\\\\\\\nLM &= n\\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]}'\\widehat{\\text{E}}{[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})']} \\widehat{\\text{E}\\left[\\mathbf S(\\bar{\\boldsymbol{\\theta}}_\\text{M})\\right]}\\\\\n& = n\\left[\\frac{1}{n}\\sum_{i=1}^n  \\bar{e}_i\\mathbf{X}_i \\right]'\\left[\\frac{1}{n}\\sum_{i=1}^n \\bar{e}_i^2\\mathbf{X}_i'\\mathbf{X}_i \\right]^{-1} \\left[\\frac{1}{n}\\sum_{i=1}^n \\bar e_i\\mathbf{X}_i \\right]\\\\\n& = \\left[\\sum_{i=1}^n  \\bar{e}_i\\mathbf{X}_i \\right]'\\left[\\frac{\\bar{\\mathbf{e}}'\\bar{\\mathbf{e}}}{n}\\sum_{i=1}^n\\mathbf{X}_i'\\mathbf{X}_i \\right]^{-1} \\left[\\sum_{i=1}^n \\bar e_i\\mathbf{X}_i \\right]\\\\\n& = \\mathbb{X}'\\bar{\\mathbf{e}}\\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\mathbb{X}'\\mathbb{X}\\right)^{-1}\\bar{\\mathbf{e}}'\\mathbb{X}\\\\\\\\\n\nDM & = \\frac{2}{\\hat c} \\left[\\sum_{i=1}^n m(\\hat{\\boldsymbol\\theta}_\\text{M} ,\\mathbf{W}_i) - \\sum_{i=1}^n m(\\bar{\\boldsymbol{\\theta}}_\\text{M},\\mathbf{W}_i)\\right]\\\\\n   & = \\frac{2}{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}/n}\\left[\\sum_{i=1}^n -\\frac{1}{2}(Y_i-\\mathbf{X}_i\\hat{\\boldsymbol\\beta}_\\text{OLS} )^2 - \\sum_{i=1}^n -\\frac{1}{2}(Y_i-\\mathbf{X}_i\\bar{\\boldsymbol{\\beta}}_\\text{OLS})^2\\right]\\\\\n   & = \\frac{1}{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}/n}\\left[\\sum_{i=1}^n \\bar e_i^2 - \\sum_{i=1}^n \\bar e_i^2\\right]\\\\\n   & = \\frac{1}{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}/n}\\left[\\bar{\\mathbf{e}}'\\bar{\\mathbf{e}} - \\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\right]\n\\end{align*}\\] The residuals associated with the unrestricted estimator \\(\\hat{\\boldsymbol\\beta}_\\text{OLS} \\) are \\(\\hat e_i\\), while \\(\\bar e_i\\) are the residuals associated with the restricted estimator. If we let \\(SSR = \\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\) and \\(SSR_0 = \\bar{\\mathbf{e}}'\\bar{\\mathbf{e}}\\) denote the sum of squared residuals for the unrestricted and restricted models respectively, then\n\\[ DM = \\frac{SSR_0 - SSR}{SSR/n}.\\] Suppose \\(Y = 1 + 2X + \\varepsilon\\), where \\(\\boldsymbol{\\beta}= [1,2]'\\). Let’s test the null hypothesis that the parameters coincide with their true underlying values, \\(H_0: \\boldsymbol{\\beta}= \\boldsymbol{\\beta}_0\\). In this case \\[\\begin{align*}\n\\mathbf h(\\boldsymbol{\\beta}) &= \\begin{bmatrix}\\beta_1 - \\beta_{0,2}\\\\ \\beta_2 - \\beta_{0,1} \\end{bmatrix}\\\\\n\\frac{\\partial\\mathbf h(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\mathbf I\n\\end{align*}\\] where \\(H_0: \\mathbf h(\\boldsymbol{\\beta}) = \\mathbf{0}\\). The image \\(\\mathbf h(\\boldsymbol{\\Theta})\\) is a singleton, so when we maximize \\(Q_n\\) such that \\(\\mathbf h(\\boldsymbol{\\theta}) = \\mathbf{0}\\) to get our restricted estimate, we trivially have \\(\\bar{\\boldsymbol{\\beta}}_\\text{OLS} = \\boldsymbol{\\beta}_0\\). For this hypothesis we have \\[\\begin{align*}\nW & = (\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0)'\\left[\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\left(\\mathbb{X}'\\mathbb{X}\\right)^{-1}\\right]^{-1}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0)\\\\\n  & = \\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\right)^{-1}(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0)'\\left(\\mathbb{X}'\\mathbb{X}\\right)(\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\boldsymbol{\\beta}_0) \\\\\n  & = \\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\right)^{-1}(\\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\mathbb{X}\\boldsymbol{\\beta}_0)'(\\mathbb{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} - \\mathbb{X}\\boldsymbol{\\beta}_0)\\\\\n  & = \\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\right)^{-1}((\\mathbf{Y}- \\hat{\\mathbf{e}}) - (\\mathbf{Y}- \\bar{\\mathbf{e}}))'((\\mathbf{Y}- \\hat{\\mathbf{e}}) - (\\mathbf{Y}- \\bar{\\mathbf{e}})) & (\\mathbf{Y}= \\mathbf{X}\\hat{\\boldsymbol\\beta}_\\text{OLS} + \\hat{\\mathbf{e}},\\ \\mathbf{Y}= \\mathbf{X}\\boldsymbol{\\beta}_0 + \\bar{\\mathbf{e}})\\\\\n  & = \\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\right)^{-1}(\\bar{\\mathbf{e}} - \\hat{\\mathbf{e}})'(\\bar{\\mathbf{e}} - \\hat{\\mathbf{e}})\\\\\n  & = \\left(\\frac{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{n}\\right)^{-1}(\\bar{\\mathbf{e}}'\\bar{\\mathbf{e}} - \\bar{\\mathbf{e}}'\\hat{\\mathbf{e}}  - \\hat{\\mathbf{e}}'\\bar{\\mathbf{e}} + \\hat{\\mathbf{e}}'\\hat{\\mathbf{e}})\n\\end{align*}\\] It happens to be the case that \\(\\bar{\\mathbf{e}}'\\hat{\\mathbf{e}} = \\hat{\\mathbf{e}}'\\bar{\\mathbf{e}} = \\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}\\),14 so we have \\[\\begin{align*}\nW  =\\frac{\\bar{\\mathbf{e}}'\\bar{\\mathbf{e}} - \\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}}{\\hat{\\mathbf{e}}'\\hat{\\mathbf{e}}/n} = DM.\n\\end{align*}\\]\n\n#generate data\nn <- 10\nK <- 2\nx <- runif(n, 0, 10)\ne <- runif(n, -1, 1)\nX <- cbind(1,x)\nbeta <- c(1,2)\nbeta_0 <- c(1,2)\ny <- X %*% beta + e\n\n#estimate model and calculate OLS residuals and restricted OLS residual\nOLS <- solve(t(X) %*% X) %*% t(X) %*% y\nres <- y - X %*% OLS\nres_0 <- y - X %*% beta_0\n\n#Wald stat\nee_n <- as.numeric((t(res) %*% res)/(n))\navar <- ee_n * solve(t(X) %*% X)\nW <- t(OLS - beta_0) %*% solve(avar) %*% (OLS - beta_0)\n\n#LM stat\nLM <- t(res_0) %*% X %*% solve(ee_n * (t(X) %*% X)) %*% t(X) %*% res_0\n\n#DM stat\nSSR <- as.numeric(t(res) %*% res)\nSSR_0 <-  as.numeric(t(res_0) %*% res_0)\nDM <- (SSR_0 - SSR) / (ee_n)\n\n\nc(W, LM, DM)\n\n[1] 3.199298 3.199298 3.199298\n\n\n\n\n7.6.1 Nonlinear Least Squares\nWe’ve generalized the classical linear model \\(\\mathcal P_\\text{LM}\\) several ways. Section @ref(endogeniety-i-iv-and-2sls) dropped the assumption \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right]=\\mathbf{0}\\) and introduced the model \\(\\mathcal P_\\text{IV}\\). Section @ref(generalized-least-squares) dropped the assumption \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\right] =\\sigma^2 \\mathbf I\\) and introduced the general linear regression model \\(\\mathcal P_\\text{GLRM}\\). Section @ref(endogeniety-ii-simultaneous-equation-models) extended the linear model by allowing for multiple “seemingly unrelated” linear models, \\(\\mathcal P_\\text{SUR}\\). In each of these case, we assumed the structural relationship between the dependent variable and regressors was linear. Let’s now drop this assumptions.\nSuppose we posit that \\(Y\\) is related to regressors \\(\\mathbf{X}\\) and parameters \\(\\boldsymbol{\\beta}\\) through some deterministic relationship given by a function \\(f(\\mathbf{X}, \\boldsymbol{\\beta})\\). We can add a structural error such that \\(Y_i = g(\\mathbf{X}_i,\\boldsymbol{\\beta}) + \\varepsilon_i\\), or \\[ \\mathbf{Y}= \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) + \\boldsymbol{\\varepsilon}\\] where \\(\\mathbf g\\) is a vector-valued function of \\(n\\) copies of \\(f\\). An element of our model \\(\\mathcal P\\) will contain elements \\(P\\) which are collections of joint distributions \\(F_{\\mathbb{X},\\boldsymbol{\\varepsilon}}\\) satisfying some common conditions. We also want our model to reduce to \\(\\mathcal P_\\text{LM}\\) in the event that \\(\\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) = \\mathbb{X}\\boldsymbol{\\beta}\\). Two of the assumptions of the \\(\\mathcal P_\\text{LM}\\) are pretty easy as we can replace \\(\\mathbb{X}\\) with $g(,) $:\n\nStrict exogeneity, \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta})\\right] = \\mathbf{0}\\implies \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\mathbf{0}\\) when \\(\\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) = \\mathbb{X}\\boldsymbol{\\beta}\\). This assumption also implies weak exogeneity \\(\\text{E}\\left[\\mathbf g(\\mathbb{X},\\boldsymbol{\\beta})'\\boldsymbol{\\varepsilon}\\right] = \\mathbf{0}\\).\nSpherical errors, \\(\\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta})\\right] = \\sigma^2\\mathbf I \\implies \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbb{X}\\right] = \\sigma^2\\mathbf I\\) when \\(\\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) = \\mathbb{X}\\boldsymbol{\\beta}\\)\n\nThe one assumption of \\(\\mathcal P_\\text{LM}\\) that takes some thought to generalize is \\(\\text{rank}\\left(\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\right) = K\\). What role did this assumption play in the linear model? It made sure that \\(\\boldsymbol{\\beta}\\neq \\boldsymbol{\\beta}'\\) whenever \\(\\mathbf{X}\\boldsymbol{\\beta}\\neq \\mathbf{X}\\boldsymbol{\\beta}'\\), as \\(\\boldsymbol{\\beta}\\) satisfies \\(\\text{E}\\left[\\mathbf{X}'\\boldsymbol{\\varepsilon}\\right] = \\boldsymbol{\\beta}\\text{E}\\left[\\mathbf{X}'\\mathbf{X}\\right]\\) in this case. In the event \\(g(\\mathbf{X}_i,\\boldsymbol{\\beta}) = \\mathbf{X}_i\\boldsymbol{\\beta}\\) for \\(i = 1,\\ldots,n\\), then this condition ensures that \\(\\boldsymbol{\\beta}\\neq \\boldsymbol{\\beta}'\\) implies \\(g(\\mathbf{X}_i,\\boldsymbol{\\beta}) \\neq g(\\mathbf{X}_i,\\boldsymbol{\\beta}')\\) for all \\(i\\). This will be the analogous condition – \\(\\boldsymbol{\\beta}\\neq \\boldsymbol{\\beta}'\\) implies \\(g(\\mathbf{X}_i,\\boldsymbol{\\beta}) \\neq g(\\mathbf{X}_i,\\boldsymbol{\\beta}')\\).\n\nThe (classical) nonlinear model is defined as \\(\\mathcal P_\\text{NLM} = \\{P_{\\boldsymbol{\\beta},\\sigma^2} \\mid \\boldsymbol{\\beta}\\in \\mathbb R^{K}, \\sigma^2\\in\\mathbb R\\}\\), where \\[\\begin{align*}\nP_{\\boldsymbol{\\beta},\\sigma^2} &= \\{F_{\\mathbb{X},\\boldsymbol{\\varepsilon}} \\mid \\mathbf{Y}= \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) + \\boldsymbol{\\varepsilon}, \\ \\text{E}\\left[\\boldsymbol{\\varepsilon}'\\boldsymbol{\\varepsilon}\\mid \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta})\\right]=\\sigma^2\\mathbf I, \\ f_{\\mathbb{X}}=\\textstyle\\prod_{i=1}^n f_{\\mathbf{X}_i}, \\boldsymbol{\\beta}\\neq\\boldsymbol{\\beta}' \\implies \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}) \\neq \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta}'),\\ \\text{E}\\left[\\boldsymbol{\\varepsilon}\\mid \\mathbf g(\\mathbb{X},\\boldsymbol{\\beta})\\right] = \\mathbf{0}\\},\\\\\n\\mathbb{X}& = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_j, \\cdots \\mathbf{X}_K] = [\\mathbf{X}_1, \\cdots, \\mathbf{X}_i, \\cdots \\mathbf{X}_n]',\\\\\n\\mathbf{Y}& = [Y_1, \\ldots, Y_n],\n\\end{align*}\\] for a known function \\(\\mathbf g(\\mathbb{X}, \\boldsymbol{\\beta})\\).\n\nThis definition of the model assumes that we’ve correctly specified \\(\\mathbf g(\\mathbb{X}, \\boldsymbol{\\beta})\\), just like how the linear model assumed that the relationship between \\(\\mathbf{Y}\\) and \\(\\mathbb{X}\\) was actually \\(\\mathbf{Y}= \\mathbb{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\). For \\(\\mathcal P_\\text{NLM}\\) this assumption is much stronger. We are not only assuming that the model contains the correct independent variables, but we are also assuming the relationship \\(\\mathbf g(\\mathbb{X}, \\boldsymbol{\\beta})\\) is correct. The natural estimator for the nonlinear model is nonlinear least squares.\n\nThe nonlinear least squares estimator is defined as \\[ \\hat{\\boldsymbol{\\beta}}_\\text{NLLS}(\\mathbb{X}, \\mathbf{Y}) = \\mathop{\\mathrm{argmin}}_\\boldsymbol{\\beta}\\frac{1}{n}\\sum_{i=1}^n(Y_i - g(\\mathbf{X}_i,\\boldsymbol{\\beta}))^2 \\]\n\nThis estimator is an M-estimator where \\(m(\\boldsymbol{\\beta}, Y_i, \\mathbf{X}_i) = -\\(Y_i - g(\\mathbf{X}_i,\\boldsymbol{\\beta}))^2\\), and \\[Q_0 = \\mathop{\\mathrm{plim}}-\\frac{1}{n}\\sum_{i=1}^n(Y_i - g(\\mathbf{X}_i,\\boldsymbol{\\beta}))^2= - \\text{E}\\left[Y - g(\\mathbf{X}, \\boldsymbol{\\beta})\\right]\\]\n\n\n7.6.2 Least Absolute Deviations"
  },
  {
    "objectID": "extremum.html#minimum-distance-estimators",
    "href": "extremum.html#minimum-distance-estimators",
    "title": "7  Extremum Estimators",
    "section": "7.7 Minimum Distance Estimators",
    "text": "7.7 Minimum Distance Estimators\n\nSuppose \\(\\mathbb{W}= [\\mathbf{W}_1,\\ldots, \\mathbf{W}_n]' \\sim P_\\boldsymbol{\\theta}\\) where \\(P_\\boldsymbol{\\theta}\\in\\mathcal P\\) for a known model \\(\\mathcal P\\). A minimum distance estimator \\(\\hat{\\boldsymbol\\theta}_\\text{MDE} :\\mathcal W\\to \\boldsymbol{\\Theta}\\) is an extremum estimator where \\[\\begin{align*}\nQ_n(\\boldsymbol{\\theta}) &= -[\\hat {\\boldsymbol \\pi} - \\mathbf g(\\boldsymbol{\\theta})]'\\boldsymbol \\Phi [\\hat {\\boldsymbol \\pi}- \\mathbf g(\\boldsymbol{\\theta})],\\\\\n\\hat{\\boldsymbol\\theta}_\\text{MDE} &= \\mathop{\\mathrm{argmax}}_{\\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}} -[\\hat {\\boldsymbol \\pi} - \\mathbf g(\\boldsymbol{\\theta})]'\\boldsymbol \\Phi [\\hat {\\boldsymbol \\pi}- \\mathbf g(\\boldsymbol{\\theta})],\n\\end{align*}\\] for some sample statistic \\(\\hat {\\boldsymbol \\pi}\\) satisfying \\(\\hat {\\boldsymbol \\pi} \\overset{p}{\\to}\\mathbf g(\\boldsymbol{\\theta})\\) and a weighting matrix \\(\\boldsymbol \\Phi\\)."
  },
  {
    "objectID": "extremum.html#recap",
    "href": "extremum.html#recap",
    "title": "7  Extremum Estimators",
    "section": "7.8 Recap",
    "text": "7.8 Recap"
  },
  {
    "objectID": "extremum.html#examplereplication",
    "href": "extremum.html#examplereplication",
    "title": "7  Extremum Estimators",
    "section": "7.9 Example/Replication",
    "text": "7.9 Example/Replication"
  },
  {
    "objectID": "extremum.html#futher-reading",
    "href": "extremum.html#futher-reading",
    "title": "7  Extremum Estimators",
    "section": "7.10 Futher Reading",
    "text": "7.10 Futher Reading\nExtremum estimators: Newey and McFadden (1994), Chapter 4 of Amemiya (1985), these awesome slides posted by Xiaoxia Shi, Chapter Chapter 7 of Hayashi (2011), Chapter 12 of Greene (2018)\nTrinity of Hypothesis Tests: Engle (1984), Section 12.4 of Romano and Lehmann (2005), Chapter 16 of Van der Vaart (2000)\nM-estimators: Chapter 12 of Wooldridge (2010), Chapter 22 of B. Hansen (2022), Chapter 12 of Greene (2018), Chapter 7 of Hayashi (2011), Chapter 17 of Davidson and MacKinnon (1993)\nMinimum distances estimators:\nNLLS: Chapter 2 of Davidson and MacKinnon (1993), Chapter 7 of Greene (2018), Chapter 12 of Wooldridge (2010), Chapter 23 of B. Hansen (2022), Mizon (1977), Chapter 5 of Cameron and Trivedi (2005)\nLAD:\n\n\n\n\n\n\nAliprantis, Charalambos D, and Kim C Border. n.d. “Infinite Dimensional Analysis a Hitchhiker’s Guide.”\n\n\nAmemiya, Takeshi. 1973. “Regression Analysis When the Dependent Variable Is Truncated Normal.” Econometrica: Journal of the Econometric Society, 997–1016.\n\n\n———. 1985. Advanced Econometrics. Harvard university press.\n\n\nCameron, A Colin, and Pravin K Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge university press.\n\n\nDavidson, Russell, and James G MacKinnon. 1993. Estimation and Inference in Econometrics. Vol. 63. Oxford New York.\n\n\nEngle, Robert F. 1984. “Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics.” Handbook of Econometrics 2: 775–826.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson Education.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHansen, Bruce E. 2022. Probability and Statistics for Economists. Princeton University Press.\n\n\nHayashi, Fumio. 2011. Econometrics. Princeton University Press.\n\n\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35 (1): 73–101. http://www.jstor.org/stable/2238020.\n\n\nJudd, Kenneth L. 1998. Numerical Methods in Economics. MIT press.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of Identification in Econometrics.” Journal of Economic Literature 57 (4): 835–903.\n\n\nMizon, Grayham E. 1977. “Inferential Procedures in Nonlinear Models: An Application in a UK Industrial Cross Section Study of Factor Substitution and Returns to Scale.” Econometrica: Journal of the Econometric Society, 1221–42.\n\n\nNewey, Whitney K, and Daniel McFadden. 1994. “Large Sample Estimation and Hypothesis Testing.” Handbook of Econometrics 4: 2111–2245.\n\n\nRao, C Radhakrishna. 1948. “Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation.” In Mathematical Proceedings of the Cambridge Philosophical Society, 44:50–57. 1. Cambridge University Press.\n\n\nRomano, Joseph P, and EL Lehmann. 2005. Testing Statistical Hypotheses. Vol. 3. Springer.\n\n\nRudin, Walter. 1976. Principles of Mathematical Analysis. Vol. 3. McGraw-hill New York.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. MIT press."
  },
  {
    "objectID": "binary.html",
    "href": "binary.html",
    "title": "8  Binary Choice",
    "section": "",
    "text": "n <- 10000\nx <- runif(n, -5, 5)\np <- plogis(0.1 + 0.2*x)\nX <- cbind(1, x)\ny <- sapply(p, rbinom, size = 1, n = 1)\n\nbinary_choice_MLE <- function(y, X, Fun){\n  K <- ncol(X)\n  \n  # define negative log-likelihood\n  neg_LL <- function(beta) {\n    -sum(y*log(Fun(X %*% beta)) + (1-y)*log(1 - Fun(X %*% beta)))\n  }\n  \n  # minimize neg_LL\n  mle <- optim(\n    par = rep(0,K), \n    fn = neg_LL,\n    hessian = TRUE, \n    control = list(reltol = 1e-8)\n  )\n  \n  # access mle parameter and SEs from optimization\n  beta_hat <- mle$par\n  var_hat <- solve(mle$hessian)\n  se_hat <- sqrt(diag(var_hat))\n  \n  #calculate z score, CI, and p-val\n  z <- beta_hat/se_hat\n  lower_CI <- beta_hat - pnorm(0.975)*se_hat\n  upper_CI <- beta_hat + pnorm(0.975)*se_hat\n  p_val <- 2*(1 - pnorm(z))\n  \n  #combine everything into one table to return\n  output <- cbind(beta_hat, se_hat, z, lower_CI, upper_CI, p_val)\n  rownames(output) <- paste(\"β\", 1:K, sep = \"\")\n  colnames(output) <- c(\"Estimate\", \"Std.Error\", \"z-Stat\", \"Lower 95% CI\", \"Upper 95% CI\", \"p-Value\")\n  return(output)\n}\n\n\nbinary_choice_MLE(y, X, plogis) \n\n    Estimate   Std.Error    z-Stat Lower 95% CI Upper 95% CI      p-Value\nβ1 0.0960422 0.020799919  4.617432    0.0786697    0.1134147 3.885187e-06\nβ2 0.1955894 0.007411982 26.388276    0.1893988    0.2017801 0.000000e+00\n\nglm(y ~ x, family = \"binomial\") %>% summary()\n\n\nCall:\nglm(formula = y ~ x, family = \"binomial\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6540  -1.1060   0.7865   1.0626   1.5675  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 0.095896   0.020800    4.61 4.02e-06 ***\nx           0.195641   0.007412   26.39  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13844  on 9999  degrees of freedom\nResidual deviance: 13098  on 9998  degrees of freedom\nAIC: 13102\n\nNumber of Fisher Scoring iterations: 4\n\nlm(y ~ x)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    0.52225      0.04663"
  },
  {
    "objectID": "nonpar.html",
    "href": "nonpar.html",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "",
    "text": "Perhaps one of the most fundamental questions one can ask given observations of a random variable is “what distribution did these come from?” This section aims to answer this question under only one major assumption – the observed data is an IID sample.\nThings get a bit annoying with notation as a result of models (roughly speaking) being the joint distribution of a sample. We’re interested in a situation where we observe \\(n\\) values of some random vector \\(\\mathbf{X}\\) and want to estimate \\(F_\\mathbf{X}\\) and/or \\(f_\\mathbf{X}\\). This sample is a collection of identical random vectors \\((\\mathbf{X}_1,\\ldots,\\mathbf{X}_n)\\), so the model is technically the collection of these joint distributions / densities. Of course, because \\((\\mathbf{X}_1,\\ldots,\\mathbf{X}_n)\\) are identically distributed, considering the joint density is moot, and the model is identified by parameterizing it by the distribution / density of \\(\\mathbf{X}\\). As such, we want to develop estimators \\(\\hat F_{\\mathbf{X}}(\\mathbf{x})\\) and \\(\\hat{f}_\\mathbf{X}(\\mathbf{x})\\). The novelty of this task comes from the fact that the parameter space, which is the set of all probability distributions or probability densities,1 has infinite dimension. This makes our model nonparametric.\nFor most of the section, we’ll handle the simple case of estimating the distribution and density of a random scalar \\(X\\), whose sample is the joint random vector \\(\\mathbf{X}= (X_1, \\ldots, X_n)\\). For many of the examples, we’ll simulate our data from a specific distribution that is sufficiently “weird” enough to give a sense for how estimators behave in the absence of “nice” distributions like the standard normal. This specific distribution will be mixed normal distribution that is defined in the following example."
  },
  {
    "objectID": "nonpar.html#math-preliminaries",
    "href": "nonpar.html#math-preliminaries",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.1 Math Preliminaries",
    "text": "9.1 Math Preliminaries\nUntil now, we’ve be focused on estimating parametric models, i.e models where parameters had finite dimension. Even more precisely, all the the estimands up until this point have been elements of \\(\\mathbb{R}^n\\), a space we’re familiar with. Now that we’re concerned with estimating a probability density \\(f,\\) we’re estimating entire functions that belong to spaces of functions with infinite dimension. This will require the introduction of some light functional analysis. Some of this discussion will be similar to that in Section 5.15.\nThe space \\(\\mathbb{R}^k\\) is a normed vector space equipped with, as the name implies, a special function called a norm which measures a vector’s distance from the origin. In the case of \\(\\mathbb{R}^k\\), this norm is usually given as \\[ \\left\\lVert\\mathbf{x}\\right\\rVert_2 = \\left(\\sum_{i=1}^n x_i^2\\right)^{1/2},\\] and is called the Euclidean norm. Any norm can be used to define a metric (a function which measures the distance between two vectors), and in the case of \\(\\mathbb{R}^k\\) we have \\[ \\left\\lVert\\mathbf{x}-\\mathbf{y}\\right\\rVert_2 = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{1/2}.\\] All we’ve done is introduce some definitions and notation to the familiar formula used to measure the distance between two points in Euclidean space. In general we can define the \\(p\\)-norm on \\(\\mathbb{R}^k\\) as \\[ \\left\\lVert\\mathbf{x}\\right\\rVert_p = \\left(\\sum_{i=1}^n \\left\\lvert x_i\\right\\rvert^p\\right)^{1/p}.\\] If we let \\(p = 2\\), we end up with the Euclidean norm. Why bother considering other values of \\(p\\)? We can think of \\(p\\) as the way we weight the component-wise “distances” from the origin \\(\\left\\lvert x_1\\right\\rvert,\\left\\lvert x_2\\right\\rvert,\\ldots \\left\\lvert x_k\\right\\rvert\\). For \\(p = 2\\), we square each of these distances (meaning the result is always positive rendering the absolute value moot), assigning more weight to components where \\(x_i\\) is relatively large.\n\nExample 9.2 (p = 1) If we let \\(p = 1\\), the \\(p\\)-norm simplifies to\n\\[\\left\\lVert\\mathbf{x}\\right\\rVert_1 = \\sum_{i=1}^n \\left\\lvert x_i\\right\\rvert.\\]\nThis norm induces the “taxi-cab metric”,\n\\[\\left\\lVert\\mathbf{x}- \\mathbf{y}\\right\\rVert_1 = \\sum_{i=1}^n \\left\\lvert x_i - y_i\\right\\rvert,\\] whose name follows from how you would define distance if you were driving between two points in a city with a grid layout.\n\n\nExample 9.3 (Increasing p) What happens if we let \\(p\\) get arbitrarily large? Let’s plot \\(\\left\\lVert\\mathbf{x}\\right\\rVert_p\\) for various values of \\(\\mathbf{x}\\).\n\n\nShow code which generates figure\np_norm <- function(p, x){\n  (sum(abs(x)^p))^(1/p)\n}\n\ntibble(x1 = c(1, 1, 1, 1),\n       x2 = c(0, 2, 2, 2),\n       x3 = c(0, 0, 3, 3),\n       x4 = c(0, 0, 0, -4)\n       ) %>% \n  mutate(vector = ifelse(x4 == -4, \"x = (1,2,3,-4)\", ifelse(x3 == 3, \"x = (1,2,3)\", ifelse(x2 == 2, \"x = (1,2)\", \"x = 1\"))))  %>% \n  expand_grid(p = seq(1,10, length = 100)) %>% \n  rowwise() %>% \n  mutate(norm = p_norm(p, c(x1, x2, x3, x4))) %>% \n  ggplot(aes(p, norm)) +\n  geom_line() + \n  facet_wrap(~vector, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"p\", y = \"p-norm\")\n\n\n\n\n\nDistribution of estimators for the population variance\n\n\n\n\nAn interesting pattern comes to light – it appears that as \\(p \\to \\infty\\), \\(\\left\\lVert\\mathbf{x}\\right\\rVert_p\\) approaches the largest possible value of \\(\\left\\lvert x_i\\right\\rvert\\).\n\nAs this example hints at, \\[\\left\\lVert\\mathbf{x}\\right\\rVert_{\\infty} = \\max_{x_i}\\left\\lvert x_i\\right\\rvert.\\] As we let \\(p\\to\\infty\\), we put increasingly more weight on the components \\(x_i\\) which have the largest distance from \\(0\\) in their dimension. Eventually, \\(p\\) becomes so large that the contribution of \\(\\left(\\max_{x_i}\\left\\lvert x_i\\right\\rvert\\right)^p\\) to the sum dwarfs those of the other components, and we end up with something which is practically \\(\\max_{x_i}\\left\\lvert x_i\\right\\rvert\\) after taking the \\(p\\)-th root of the sum. The result of this limiting process gives \\(\\left\\lVert\\mathbf{x}\\right\\rVert_\\infty\\). An excellent way to illustrate this is by graphing the set \\(\\{\\mathbf{x}\\in \\mathbb{R}^2 \\mid \\left\\lVert\\mathbf{x}\\right\\rVert_p = 1\\}\\), i.e the boundary of the unit ball in \\(\\mathbb{R}^2\\) equipped with \\(\\left\\lVert\\cdot\\right\\rVert_p\\).\n\n\n\n\n\nFigure 9.3: test\n\n\n\n\nSo what does this all have to do with statistics and econometrics? Quite a bit if you consider the important role loss functions play in estimation. For a scalar estimator \\(\\hat \\theta\\), the quadratic loss function \\(l(\\hat\\theta,\\theta) = (\\hat\\theta - \\theta)^2\\) gave the special risk function known as mean square error. \\[ \\text{MSE}\\left(\\hat \\theta \\right)= \\text{E}\\left[(\\hat\\theta - \\theta)^2\\right].\\]\nIf \\(\\hat{\\boldsymbol{\\theta}}\\) is a vector, then we can write \\(\\text{MSE}\\left(\\hat{\\boldsymbol{\\theta}}\\right)\\) in terms of \\(\\left\\lVert\\cdot\\right\\rVert_2\\).\n\\[\\text{MSE}\\left(\\hat{ \\boldsymbol{\\theta}} \\right)= \\text{E}\\left[\\textstyle\\sum_{i=1}^n(\\hat\\theta_i - \\theta_i)^2\\right] = \\text{E}\\left[\\left\\lVert\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}\\right\\rVert_2^2\\right].\\]\nIn general, you’re welcome to define a risk function using the \\(p\\)-norm for any value of \\(p\\), and there may be situations which call for this depending on which components of \\(\\hat{\\boldsymbol{\\theta}}\\) are deemed important.\n\\[R(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) = \\text{E}\\left[\\left\\lVert\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}\\right\\rVert_p^p\\right]\\]\nNow we’re ready to extend this to vector spaces of functions. Suppose \\(\\mathcal F\\) is a vector space of real valued functions \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\). For the sake of ease, let’s also assume all of these functions are continuous and bounded. In this case, \\(f\\) is defined at an uncountably infinite number of points, so we cannot possibly sum over values of \\(\\left\\lvert f(\\mathbf{x})\\right\\rvert^p\\) for all \\(\\mathbf{x}\\in\\mathbb{R}\\). Instead we’ll have to integrate over \\(\\mathbf{x}\\). This gives\n\\[\\left\\lVert f\\right\\rVert_p =\\left(\\int_{-\\infty}^\\infty \\left\\lvert f(\\mathbf{x})\\right\\rvert^p\\ d\\mathbf{x}\\right)^{1/p}.\\]\nJust like in the case of Euclidean space, the \\(p-\\)norm also endows \\(\\mathcal F\\) with a metric that measures the distance between two functions which belong to \\(\\mathcal F\\). \\[ \\left\\lVert f - g\\right\\rVert_p =\\left(\\int_{-\\infty}^\\infty \\left\\lvert f(\\mathbf{x}) - g(\\mathbf{x})\\right\\rvert^p\\ d\\mathbf{x}\\right)^{1/p}\\] For \\(f\\in \\mathcal F\\) we have \\[ \\left\\lVert f\\right\\rVert_\\infty = \\sup_{\\mathbf{x}\\in \\mathbb{R}^n}\\left\\lvert f(\\mathbf{x})\\right\\rvert,\\] where we take the supremum instead of maximum to account for the possibility that as \\(p\\to\\infty\\) \\(\\max_{\\mathbf{x}\\in \\mathbb{R}^n}\\left\\lvert f(\\mathbf{x})\\right\\rvert\\) may approach a limit whose value it never takes on. This norm is particularly important because the role it plays in the convergence of sequences of functions. If we have a sequence of functions \\(f_n\\), then \\(f_n\\) converges to \\(f\\) uniformly if and only if \\[ \\lim_{n\\to\\infty}\\left\\lVert f_n - f\\right\\rVert_\\infty = 0.\\] For this reason, the supremum norm is also known as the “uniform norm”.\nAll of this can be defined for general vector spaces of functions, and not just the space of real valued functions which are bounded and continuous. It’s also possible to unify the separate discussions of \\(\\left\\lVert\\cdot\\right\\rVert_p\\) on Euclidean space and \\(\\left\\lVert\\cdot\\right\\rVert_p\\) on function spaces using a bit of measure theory.3"
  },
  {
    "objectID": "nonpar.html#estimating-a-distribution",
    "href": "nonpar.html#estimating-a-distribution",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.2 Estimating a Distribution",
    "text": "9.2 Estimating a Distribution\nWe’ll start with the task of developing an estimator \\(\\hat F_X\\) for a random variable’s distribution function \\(X\\). The distribution \\(F_X\\) is defined as \\[F_X(t) = \\Pr(X \\le t),\\] so all we are doing is estimating a probability for all possible values of \\(x \\in \\mathcal X\\). If that sounds easy, it’s because it is. We can just look at the proportion of observations which are less than or equal to \\(t,\\) and take this to be \\(\\Pr(X \\le t)\\).\n\\[\\hat F_X(t) = \\frac{\\text{number of observations} \\le t}{\\text{number of observations}}.\\]\n\nDefinition 9.2 Given an IID sample of size \\(n\\) of a random variable \\(X\\), the empirical (cumulative) distribution function (eCDF) is defined as \\[\\hat F_X(t) = \\frac{1}{n}\\sum_{i=1}^n I\\left[X_i \\le t\\right].\\]\n\n\nExample 9.4 Let’s calculate \\(\\hat F_X\\) for our mixed normal distribution using a sample of size \\(n = 100\\).\n\nemp_dist <- function(sample, t){\n  n <- length(sample)\n  sum(sample <= t)/n\n}\n\nn <- 100\nX <- draw_X(n)\n\n# calculate the estimate by calculating values on approximate support \nF_hat <- sapply(seq(-1, 12, length = 1e5), emp_dist, sample = X)\n\nNow let’s plot the estimated distribution and contrast it with the versus the true distribution.\n\n\nShow code which generates figure\ntibble(x = seq(-1, 12, length = 1e5), `Empirical Distribution` = F_hat) %>% \n  mutate(`True Distribution` = F_X(x)) %>% \n  gather(\"func\", \"prob\", -x) %>% \n  ggplot(aes(x, prob, color = func)) + \n  geom_line() +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  labs(x = \"t\", color = \"\", y = \"Pr(X <= t)\")  +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 9.4: ?(caption)\n\n\n\n\nBy construction, \\(\\hat F_X(x)\\) is a discontinuous step function because it is the sum of discontinuous indicator functions. Nevertheless, \\(\\hat F_X\\) looks pretty close to \\(F_X\\)!"
  },
  {
    "objectID": "nonpar.html#properties-of-the-empirical-distribution",
    "href": "nonpar.html#properties-of-the-empirical-distribution",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.3 Properties of the Empirical Distribution",
    "text": "9.3 Properties of the Empirical Distribution\n\n9.3.1 Pointwise Properties\nThe challenge of determining the properties of \\(\\hat F_X\\) comes from it being an entire function opposed to a single value. Let’s start with an easier task by reframing the problem to consider \\(\\hat F_X\\) evaluated at some fixed point \\(x_0\\). That is, what are the pointwise properties of \\(\\hat F_X(x_0)\\)?\nFirst, let’s consider the bias of \\(\\hat F_X(x_0)\\). The empirical distribution function is a linear combination of indicator functions, so taking its expectation is straightforward. \\[\\begin{align*}\n\\text{E}\\left[\\hat F_X(x_0)\\right] & = \\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^n I\\left[X_i \\le x_0\\right]\\right]\\\\\n& = \\frac{1}{n}\\sum_{i=1}^n \\text{E}\\left[I\\left[X_i \\le x_0\\right]\\right]\\\\\n& = \\frac{1}{n}\\sum_{i=1}^n 1\\cdot \\Pr(X_i \\le x_0) + 0\\cdot \\Pr(X_i > x_0) & (\\text{definition of indicator})\\\\\n& = \\frac{1}{n}\\left(n\\cdot  \\Pr(X_i \\le x_0)\\right)\\\\\n& =  \\Pr(X_i \\le x_0)\\\\\n& = F_X(x_0)\n\\end{align*}\\] The estimator, as perhaps anticipated, is unbiased. Now let’s calculate its variance. \\[\\begin{align*}\n\\text{Var}\\left(\\hat F_X(x_0)\\right) & = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n I\\left[X_i \\le x_0\\right]\\right)\\\\\n& = \\frac{1}{n^2} \\sum_{i=1}^n \\left\\{\\text{E}\\left[I\\left[X_i \\le x_0\\right]^2\\right] - \\left(\\text{E}\\left[I\\left[X_i \\le x_0\\right]\\right]\\right)^2\\right\\}\\\\\n& = \\frac{1}{n^2} \\sum_{i=1}^n \\left\\{\\text{E}\\left[I\\left[X_i \\le x_0\\right]\\right] - \\left(\\text{E}\\left[I\\left[X_i \\le x_0\\right]\\right]\\right)^2\\right\\} & (I\\left[X_i \\le x_0\\right]^2 = I\\left[X_i \\le x_0\\right])\\\\\n& = \\frac{1}{n^2} \\sum_{i=1}^n \\left[\\Pr(X_i \\le x_0) - \\Pr(X_i \\le x_0)^2\\right] & (\\text{E}\\left[I\\left[X_i \\le x_0\\right]\\right] = \\Pr(X_i \\le x_0))\\\\\n& = \\frac{1}{n^2}[n\\Pr(X_i \\le x_0)(1- \\Pr(X_i \\le x_0))]\\\\\n& = \\frac{F_X(x_0)(1-F_X(x_0))}{n}\n\\end{align*}\\] If calculating the expectation and variance of \\(\\hat F_X(x_0)\\) feels “familiar”, that may be because we’ve just calculated the expected value and variance of a binomial random variable. Define \\(Y = n\\hat F_X(x)\\) such that \\[ Y = \\sum_{i=1}^n I\\left[X_i \\le x_0\\right].\\] The random variable \\(Y\\) is the sum of \\(n\\) Bernoulli trials, each with succeeding with probability \\(\\Pr(X_i \\le x_0) = F_X(x_0)\\), meaning \\(Y \\sim \\text{Bernoulli}(n, F_X(x_0))\\). As such, we can just appeal to the expected value and variance of \\(Y\\) to calculate the corresponding moments for \\(\\hat F_X(x)\\). \\[\\begin{align*}\n\\text{E}\\left[\\hat F_X(x)\\right] & = \\text{E}\\left[\\frac{1}{n}Y\\right] = \\frac{1}{n}\\left(nF_X(x_0)\\right) = F_X(x_0)\\\\\n\\text{Var}\\left(\\hat F_X(x)\\right) & = \\text{Var}\\left(\\frac{1}{n}Y\\right) = \\frac{1}{n^2}\\left(nF_X(x_0)(1-F_X(x_0))\\right) = \\frac{F_X(x_0)(1-F_X(x_0))}{n}\n\\end{align*}\\]\nNoting that \\(\\lim_{n\\to\\infty}\\text{Var}\\left(\\hat F_X(x_0)\\right) = 0\\), Corollary 2.2 establishes that \\(\\hat F_X(x_0)\\) is consistent. \\[ \\hat F_X(x_0) \\overset{p}{\\to}F_X(x_0)\\] Finally, because \\(\\hat F_X(x_0)\\) is the average of independent Bernoulli trials, so we can apply the CLT to conclude that \\(\\sqrt n[\\hat F_X(x_0)- F_X(x_0)] \\overset{d}{\\to}N(0, F_X(x_0)(1-F_X(x_0)))\\).\n\nTheorem 9.1 (Pointwise Properties of Empirical Distribution) The empirical distribution function \\(\\hat F_X(x_0)\\), evaluated at a fixed point \\(x_0\\), exhibits the following properties:\n\nUnbiasedness, \\(\\text{E}\\left[\\hat F_X(x_0)\\right] = F_X(x_0)\\);\n\\(\\text{Var}\\left(\\hat F_X(x_0)\\right) = \\frac{F_X(x_0)(1-F_X(x_0))}{n}\\);\nConsistency, \\(\\hat F_X(x_0) \\overset{p}{\\to}F_X(x_0)\\);\nAsymptotic Normality, \\(\\hat F_X(x_0) \\overset{a}{\\sim}N(0, F_X(x_0)(1-F_X(x_0))/n)\\).\n\n\n\nExample 9.5 To illustrate these properties, let’s simulate the estimation of \\(F_X(0)\\) where \\(X\\) is distributed according to the mixed normal distribution we’ve been using. First, let’s see what the true value of \\(F_X(X_0)\\) is, along with \\(F_X(X_0)(1-F_X(X_0))/n\\).\n\nn <- 100\nx_0 <- 0\n\nF_X(x_0)\n\n[1] 0.1253375\n\n(1 - F_X(x_0))*F_X(x_0)/n\n\n[1] 0.00109628\n\n\nFor our first simulations, we’ll simulate one million estimates using samples of size \\(n = 1,000\\).\n\nN_sim <- 1e6\n\nestimates <- vector(\"numeric\", length = N_sim)\nfor (k in 1:N_sim) {\n  X <- draw_X(n)\n  estimates[k] <- emp_dist(X, x_0)\n}\n\nmean(estimates)\n\n[1] 0.1253935\n\nvar(estimates)\n\n[1] 0.001099587\n\n\nThe simulated expectation and variance are in line with our calculations. Let’s now look at the histogram of our values compared with the theoretical limiting distribution.\n\n\nShow code which generates figure\ntibble(estimates) %>% \n  ggplot(aes(estimates)) +\n  geom_histogram(aes(y = ..density..), fill = \"white\", color = \"black\", binwidth = .01) +\n  stat_function(fun = dnorm, args = list(mean = F_X(x_0), sd = sqrt((1 - F_X(x_0))*F_X(x_0)/n)), color = \"red\") +\n  theme_minimal() +\n  labs(x = \"Empirical Distribution Evaluated at 0\", y = \"Density\")\n\n\n\n\n\nFigure 9.5: ?(caption)\n\n\n\n\nA second set of simulations for varying sample sizes can illustrate consistency.\n\nN_sim <- 1e4\nestimates <- vector(\"numeric\", length = N_sim)\nfor (n in 1:N_sim) {\n  X <- draw_X(n)\n  estimates[n] <- emp_dist(X, x_0) \n}\n\n\n\nShow code which generates figure\ntibble(n = 1:N_sim, estimates) %>% \n  ggplot(aes(n, estimates)) +\n  geom_line() +\n  geom_hline(yintercept = F_X(x_0), color = \"red\", linetype = \"dashed\") +\n  labs(y = \"Empirical Distribution Evaluated at 0\", x = \"Sample Size, n\") +\n  theme_minimal()\n\n\n\n\n\nFigure 9.6: ?(caption)\n\n\n\n\n\n\nExample 9.6 (Confidence Intervals) It’s possible to construct pointwise confidence intervals using the fact that \\(\\hat F_X(x_0) \\overset{a}{\\sim}N(0, F_X(x_0)(1-F_X(x_0))/n)\\). A level \\((1-\\alpha)\\) confidence interval for \\(F_X(x_0)\\) would be given by \\[ \\left[\\hat F_X(x_0) - \\Phi^{-1}(1-\\alpha/2)\\sqrt{\\frac{\\hat F_X(x_0)(1-\\hat F_X(x_0))}{n}}, \\hat F_X(x_0) + \\Phi^{-1}(1-\\alpha/2)\\sqrt{ \\frac{\\hat F_X(x_0)(1-\\hat F_X(x_0))}{n}}\\right].\\] If we want to construct these intervals for multiple values of \\(F_X(x_0)\\), we need to be careful because they’re pointwise. For the sake of illustration, let’s construct these confidence intervals (taking \\(\\alpha=0.05\\)) over the support of \\(F_X\\) and graph our results.\n\nn <- 100\nX <- draw_X(n)\n\nCI <- function(a, sample, t){\n  n <- length(sample)\n  F_hat <- emp_dist(sample, t)\n  lower <- F_hat  - qnorm(1 - a/2)*sqrt((F_hat*(1-F_hat))/n)\n  upper <- F_hat  + qnorm(1 - a/2)*sqrt((F_hat*(1-F_hat))/n)\n  return(c(lower, upper))\n}\n\nF_hat <- sapply(seq(-1, 12, length = 1e5), emp_dist, sample = X)\nF_CI <- sapply(seq(-1, 12, length = 1e5), CI, a = 0.05, sample = X)\n\nLet’s plot our confidence intervals along the support of \\(F_X\\).\n\n\nShow code which generates figure\nt(F_CI) %>% \n  as_tibble() %>% \n  rename(\n    lower = V1,\n    upper = V2\n  ) %>% \n  mutate(\n    x = seq(-1, 12, length = 1e5),\n    F_hat = F_hat\n  ) %>% \n  ggplot(aes(x, F_hat)) + \n  geom_line(aes(color = \"Empirical Distribution\")) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = \"95% (Pointwise) Confidence Interval\"), alpha = 0.3) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"black\")) +\n  scale_fill_manual(values = c(\"black\")) +\n  labs(x = \"t\", y = \"Pr(X < t)\", color = \"\", fill = \"\") +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\nFigure 9.7: ?(caption)\n\n\n\n\nHow do we interpret these intervals? For each separate \\(x_0\\) there is a 95% chance our interval contains the true value \\(F_X(x_0)\\). It is not the case that there is a 95% chance that the entire function \\(\\hat F_X\\) is contained in the interval, because we’re not considering the behavior of \\(\\hat F_X\\) as a function yet.\n\n\n\n9.3.2 Empirical Process Theory\nLet’s now change gears and consider the properties of \\(\\hat F_X\\) as a function, and not just the properties of \\(\\hat F_X(x_0)\\) for some fixed \\(x_0\\). The study of \\(\\hat F_X\\) as an entire function is the subject of empirical process theory, which is a fairly technical branch of asymptotic theory.\n\nDefinition 9.3 The function \\(t \\mapsto \\hat F_X(t)\\) is an empirical process.\n\nWhen considering the properties of \\(\\hat F_X\\) as an empirical process, we need to think about it as an element of a space of functions. Instead of looking at the discrepancy of \\(\\hat F_X(t) - F_X(t)\\) for a fixed \\(t\\), we will work with \\[\\left\\lVert\\hat F_X - F_X\\right\\rVert_p = \\left(\\int_{\\mathcal X} \\left\\lvert\\hat F_X(t) - F_X(t)\\right\\rvert^p\\ dt \\right)^{1/p}.\\] In particular, we’ll focus on the case where \\(p=\\infty\\), \\[\\left\\lVert\\hat F_X - F_X\\right\\rVert_\\infty = \\sup_{t\\in\\mathcal X}\\left\\lvert\\hat F_X(t) - F_X(t)\\right\\rvert.\\] Not only is the supremum norm the conservative choice because it assess \\(\\hat F_X\\) at the point where the discrepancy \\(\\left\\lvert\\hat F_X(t) - F_X(t)\\right\\rvert\\) is the “largest”4, but \\(\\left\\lVert\\cdot\\right\\rVert_\\infty\\) is also used to define uniform convergence.\nThe first, and perhaps most important, result about \\(\\hat F_X(t)\\)’s behavior as a function of \\(t\\) involves consistency. We know that \\(\\hat F_X(t)\\overset{p}{\\to}F_X(t)\\) for each separate \\(t\\), but does this convergence occur uniformly for all \\(t\\)?\nAs famously shown by Glivenko (1933) and Cantelli (1933).\n\nTheorem 9.2 (Glivenko-Cantelli) If \\(X_1,\\ldots,X_n\\) are IID random variables with a distribution \\(F_X\\), then \\[ \\left\\lVert\\hat F_X - F_X\\right\\rVert_\\infty \\overset{p}{\\to}0.\\]\n\n\nProof. See\n\n\nExample 9.7 If we take the Glivenko-Cantelli theorem at face value, then illustrating it is a matter of looking at \\(\\left\\lVert\\hat F_X - F_X\\right\\rVert_\\infty\\) for simulated samples of increasing size \\(n\\).\n\nN_sim <- 1e3\nsup_diff <- vector(\"numeric\", N_sim)\nfor (n in 1:N_sim) {\n  X <- draw_X(n)\n  F_hat <- sapply(seq(-1, 12, length = 1e5), emp_dist, sample = X)\n  F_0 <- sapply(seq(-1, 12, length = 1e5), F_X)\n  sup_diff[n] <- max(abs(F_hat - F_0))\n}\n\n\n\nShow code which generates figure\ntibble(\n  n = 1:N_sim,\n  sup_diff\n) %>% \n  ggplot(aes(n, sup_diff)) +\n  geom_line() + \n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(y = \"sup |eCDF - CDF|\", x =\"Sample Size, n\") +\n  theme_minimal() \n\n\n\n\n\nFigure 9.8: ?(caption)\n\n\n\n\nWhile this illustration corroborates the Glivenko-Cantelli theorem, it doesn’t look at it through the lens that matters. A better way to visualize the theorem is by looking at how the shape of \\(\\hat F_X\\) behaves as \\(n\\to\\infty\\).\n\nestimates <- list()\niter <- 0\nfor (n in c(10, 25, 50, 100, 500, 1000)) {\n  iter <- iter + 1\n  X <- draw_X(n)\n  F_hat <- sapply(seq(-1, 12, length = 1e5), emp_dist, sample = X)\n  estimates[[iter]] <- tibble(F_hat, n , x = seq(-1, 12, length = 1e5))\n}\n\n\n\nShow code which generates figure\nestimates %>% \n  bind_rows() %>% \n  ggplot(aes(x, F_hat)) +\n  geom_line(color = \"red\") + \n  facet_wrap(~n) +\n  stat_function(fun = F_X, alpha = 0.3) +\n  theme_minimal()\n\n\n\n\n\nFigure 9.9: ?(caption)\n\n\n\n\nAs \\(n\\to\\infty\\), it does in fact appear that the empirical distribution \\(\\hat F_X\\) converges to the true distribution \\(F_X\\) uniformly, and not just pointwise.\n\nThe key theorem in empirical process theory gives the limiting distribution of the standardized empirical process \\(\\sqrt{n}(\\hat F_X - F)\\). We already know that \\[\\sqrt n[\\hat F_X(x_0)-  F_X(x_0)] \\overset{d}{\\to}N(0, F_X(x_0)(1-F_X(x_0)))\\] for all fixed \\(x_0\\), but showing the analogous result for the empirical process \\(\\sqrt{n}(\\hat F_X - F)\\) requires a bit more care. One needs to make sure that \\(\\hat F_X\\) “behaves well” on its support.5 Hansen (2022) provides a nice discussion of this, while omitting many of the technical details that can be found in Van der Vaart (2000).\n\nTheorem 9.3 (Donsker’s Theorem) If \\(X_1,\\ldots,X_n\\) are IID random variables with a distribution \\(F_X\\), then \\[ \\sqrt{n}.\\]\n\n\nProof. See Theorem 19.3 and Theorem 19.5 of Van der Vaart (2000).\n\nMassart (1990)\n\nProposition 9.1 (DKW Inequality) See\n\n\nProof. See Massart (1990).\n\n\nExample 9.8 (Confidence Bands) t\n\n\nn <- 1000\nX <- draw_X(n)\nF_hat <- sapply(seq(-1, 12, length = 1e5), emp_dist, sample = X)\nF_0 <- sapply(seq(-1, 12, length = 1e5), F_X)\n\ntibble(\n  x = seq(-1, 12, length = 1e5),\n  y = sqrt(n) * (F_hat - F_0)\n) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\nProposition 9.2 (Goodness of Fit Statistics) t\n\n\nProof. See Corollary 19.21 of Van der Vaart (2000). The proof amounts to an application of the continuous mapping theorem to Theorem 9.3.\n\n\nProof. See Corollary 19.21 of Van der Vaart (2000). The proof amounts to an application of the continuous mapping theorem to Theorem 9.3."
  },
  {
    "objectID": "nonpar.html#from-estimating-distributions-to-estimating-densities",
    "href": "nonpar.html#from-estimating-distributions-to-estimating-densities",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.4 From Estimating Distributions to Estimating Densities",
    "text": "9.4 From Estimating Distributions to Estimating Densities\n\nDefinition 9.4 A kernel function \\(K:\\mathbb{R}\\to\\mathbb{R}\\) is a function satisfying \\[ \\int_{-\\infty}^\\infty K(u)\\ du = 1.\\] It is often convenient to assume \\(K\\) is nonnegative, and symmetric: \\[\\begin{align*}\nK(u) & = K(-u) & \\forall u\\in \\mathbb{R}\\\\\nK(u) & \\ge 0 & \\forall u\\in \\mathbb{R}\n\\end{align*}\\]\n\n\nRemark. This definition of a kernel function in this context is very specific. In general, kernels are generalized versions of positive-definite functions and positive-definite matrices. For instance, when working with spaces of functions there are many situations where we want to apply a transformation to some function \\(f\\) via integration, giving us another function using some “kernel function” \\(K\\): \\[ (Tf)(u) = \\int_{a}^{b}f(x)\\cdot K(u,x)\\ dx\\] On famous example of such a transform is the Fourier transform of a function \\(f\\), given as \\[ \\tilde f(t) = \\int_{-\\infty}^\\infty f(x)\\cdot e^{-i2\\pi t x}\\ dx.\\] In this case the kernel function is \\(K(u,x)=e^{-i2\\pi t x}\\). Kernel transformations also appear in probability and statistics,6 The density of the sum of two independent random variables, say \\(Z = Y + X\\), is given as \\[ f_Z(t) = \\int_{-\\infty}^\\infty f_X(x)\\cdot f_Y(u-x)\\ dx.\\] The kernel associated with this transformation is \\(K(u,x)= f_Y(u-x)\\).\n\n\nDefinition 9.5 KDE estimator\n\n\nRemark. convolution\n\n\nkde <- function(x, sample, K, h){\n  # sample size\n  n <- length(sample)\n  #calculate the kernel density estimate at x\n  (1/(n*h)) * sum(K((x - sample)/h))\n}\n\n\nuniform <- function(x){\n  (1/2)*(abs(x) <= 1)\n}\n\ntriangular <- function(x){\n  (1 - abs(x))*(abs(x) <= 1)\n}\n\ntriweight <- function(x){\n  (35/32*(1-x^2)^3)*(abs(x) <= 1)\n}\n\ngauss <- function(x){\n  dnorm(x)\n}\n\n\n#| code-fold: true\n#| fig-align: center\n#| fig-asp: 1\n#| fig-width: 11\n#| fig-cap: \"Distribution of estimators for the population variance\"\n#| code-summary: \"Show code which generates figure\"\n\ntibble(\n  x = seq(-2, 2, length = 10000),\n  Uniform = uniform(x),\n  Triangular = triangular(x),\n  Triweight = triweight(x),\n  Gaussian = gauss(x)\n) %>% \n  gather(\"Kernel\", \"val\", -x) %>% \n  ggplot(aes(x, val, color = Kernel)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"Density\")\n\n\n\n\n\nX <- draw_X(1000)\n\n#set elements to loop over and support to calculate KDE on \nkernels <- c(uniform, triangular, triweight, gauss)\nkernel_names <- c(\"Uniform\", \"Triangular\", \"Triweight\", \"Gaussian\")\nbandwidths <- c(.05, .1, .5, 0.75, 1, 2)\nsupport <-seq(-1, 12, length = 1e3)\n\n\nestimates <- expand_grid(x = support, \n            h = bandwidths, \n            ker_index = 1:4\n            ) %>% \n  rowwise() %>% \n  mutate(\n    f_est = kde(x, sample = X, K = kernels[[ker_index]], h),\n    Kernel = kernel_names[ker_index]\n    )\n\n\n\nShow code which generates figure\nestimates %>% \n  mutate(h = paste(\"h =\", h)) %>% \n  ggplot(aes(x, f_est, color = Kernel)) +\n  geom_line() +\n  geom_function(fun = f_X, color = \"black\") + \n  facet_wrap(~h) +\n  theme_minimal() +\n  labs(color = \"Kernel\", y = \"Kernel Density Estimate\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nDistribution of estimators for the population variance"
  },
  {
    "objectID": "nonpar.html#properties-of-the-kernel-density-estimator",
    "href": "nonpar.html#properties-of-the-kernel-density-estimator",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.5 Properties of the Kernel Density Estimator",
    "text": "9.5 Properties of the Kernel Density Estimator"
  },
  {
    "objectID": "nonpar.html#kernel-selection",
    "href": "nonpar.html#kernel-selection",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.6 Kernel Selection",
    "text": "9.6 Kernel Selection"
  },
  {
    "objectID": "nonpar.html#bandwidth-selection",
    "href": "nonpar.html#bandwidth-selection",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.7 Bandwidth Selection",
    "text": "9.7 Bandwidth Selection\n\n9.7.1 Silverman’s Rule of Thumb\n\n\n9.7.2 Cross-Validation"
  },
  {
    "objectID": "nonpar.html#multivariate-kde",
    "href": "nonpar.html#multivariate-kde",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.8 Multivariate KDE",
    "text": "9.8 Multivariate KDE"
  },
  {
    "objectID": "nonpar.html#extensions",
    "href": "nonpar.html#extensions",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.9 Extensions",
    "text": "9.9 Extensions"
  },
  {
    "objectID": "nonpar.html#further-reading",
    "href": "nonpar.html#further-reading",
    "title": "9  Nonparametrics I: Distribution and Density Estimation",
    "section": "9.10 Further Reading",
    "text": "9.10 Further Reading\nEmpirical Process Theory: Chapter 19 of Van der Vaart (2000), Chapter 18 of Hansen (2022), these notes, or these notes\nKernels: https://www.randomservices.org/random/expect/Kernels.html\nchrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/http://amadeus.csam.iit.edu/~fass/PDKernels.pdf\n\n\n\n\n\n\nCantelli, FP. 1933. “Sulla Determinazione Empirica Delle Leggi Di Probabilita.” Gion. Ist. Ital. Attauri. 4: 421–24.\n\n\nGlivenko, Valery. 1933. “Sulla Determinazione Empirica Delle Leggi Di Probabilita.” Gion. Ist. Ital. Attauri. 4: 92–99.\n\n\nHansen, Bruce E. 2022. Probability and Statistics for Economists. Princeton University Press.\n\n\nMassart, Pascal. 1990. “The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality.” The Annals of Probability, 1269–83.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aliprantis, Charalambos D, and Kim C Border. n.d. “Infinite\nDimensional Analysis a Hitchhiker’s Guide.”\n\n\nAmemiya, Takeshi. 1973. “Regression Analysis When the Dependent\nVariable Is Truncated Normal.” Econometrica: Journal of the\nEconometric Society, 997–1016.\n\n\n———. 1985. Advanced Econometrics. Harvard university press.\n\n\nAngrist, Joshua D, and Jörn-Steffen Pischke. 2008. “Mostly\nHarmless Econometrics.” In Mostly Harmless Econometrics.\nPrinceton university press.\n\n\nBickel, Peter J, and Kjell A Doksum. 2015. Mathematical Statistics:\nBasic Ideas and Selected Topics, Volume i. 2nd ed. CRC Press.\n\n\nBillingsley, Patrick. 2008. Probability and Measure. John Wiley\n& Sons.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern\nRecognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nCameron, A Colin, and Pravin K Trivedi. 2005. Microeconometrics:\nMethods and Applications. Cambridge university press.\n\n\nCameron, Adrian Colin, Pravin K Trivedi, et al. 2010.\nMicroeconometrics Using Stata. Vol. 2. Stata press College\nStation, TX.\n\n\nCantelli, FP. 1933. “Sulla Determinazione Empirica Delle Leggi Di\nProbabilita.” Gion. Ist. Ital. Attauri. 4: 421–24.\n\n\nCard, David. 1995. “Using Geographic Variation in College\nProximity to Estimate the Return to Schooling.” National Bureau\nof Economic Research Cambridge, Mass., USA.\n\n\n———. 1999. “The Causal Effect of Education on Earnings.”\nHandbook of Labor Economics 3: 1801–63.\n\n\n———. 2001. “Estimating the Return to Schooling: Progress on Some\nPersistent Econometric Problems.” Econometrica 69 (5):\n1127–60.\n\n\nCasella, George, and Roger L Berger. 2021. Statistical\nInference. Cengage Learning.\n\n\nChristensen, Laurits R, and William H Greene. 1976. “Economies of\nScale in US Electric Power Generation.” Journal of Political\nEconomy 84 (4, Part 1): 655–76.\n\n\nChristensen, Laurits R, Dale W Jorgenson, and Lawrence J Lau. 1973.\n“Transcendental Logarithmic Production Frontiers.” The\nReview of Economics and Statistics, 28–45.\n\n\nClarke, Francis. 2013. Functional Analysis, Calculus of Variations\nand Optimal Control. Vol. 264. Springer.\n\n\nCobb, Charles W, and Paul H Douglas. 1928. “A Theory of\nProduction.” The American Economic Review 18 (1):\n139–65.\n\n\nDarmois, Georges. 1935. “Sur Les Lois de Probabilitéa\nEstimation Exhaustive.” CR Acad. Sci. Paris 260 (1265):\n85.\n\n\nDasGupta, Anirban. 2011. Probability for Statistics and Machine\nLearning: Fundamentals and Advanced Topics. Springer.\n\n\nDavidson, Russell, and James G MacKinnon. 1993. Estimation and\nInference in Econometrics. Vol. 63. Oxford New York.\n\n\nDavidson, Russell, James G MacKinnon, et al. 2004. Econometric\nTheory and Methods. Vol. 5. Oxford University Press New York.\n\n\nDeGroot, Morris H, and Mark J Schervish. 2012. Probability and\nStatistics. Pearson Education.\n\n\nDing, Peng. 2021. “The Frisch–Waugh–Lovell Theorem for Standard\nErrors.” Statistics & Probability Letters 168:\n108945.\n\n\nDurbin, James. 1954. “Errors in Variables.” Revue de\nl’institut International de Statistique, 23–32.\n\n\nDurrett, Rick. 2019. Probability: Theory and Examples. Vol. 49.\nCambridge university press.\n\n\nEngle, Robert F. 1984. “Wald, Likelihood Ratio, and Lagrange\nMultiplier Tests in Econometrics.” Handbook of\nEconometrics 2: 775–826.\n\n\nFisher, Ronald A. 1922. “On the Mathematical Foundations of\nTheoretical Statistics.” Philosophical Transactions of the\nRoyal Society of London. Series A, Containing Papers of a Mathematical\nor Physical Character 222 (594-604): 309–68.\n\n\nFolland, Gerald B. 1999. Real Analysis: Modern Techniques and Their\nApplications. Vol. 40. John Wiley & Sons.\n\n\nFrisch, Ragnar, and Frederick V Waugh. 1933. “Partial Time\nRegressions as Compared with Individual Trends.”\nEconometrica: Journal of the Econometric Society, 387–401.\n\n\nFu, Anqi, Balasubramanian Narasimhan, and Stephen Boyd. 2017.\n“CVXR: An r Package for Disciplined Convex Optimization.”\narXiv Preprint arXiv:1711.07582.\n\n\nGalton, Francis. 1886. “Regression Towards Mediocrity in\nHereditary Stature.” The Journal of the Anthropological\nInstitute of Great Britain and Ireland 15: 246–63.\n\n\nGlivenko, Valery. 1933. “Sulla Determinazione Empirica Delle Leggi\nDi Probabilita.” Gion. Ist. Ital. Attauri. 4: 92–99.\n\n\nGoldberger, Arthur S. 1972. “Structural Equation Methods in the\nSocial Sciences.” Econometrica: Journal of the Econometric\nSociety, 979–1001.\n\n\n———. 1991. A Course in Econometrics. Harvard University Press.\n\n\nGrant, Michael, Stephen Boyd, and Yinyu Ye. 2006. “Disciplined\nConvex Programming.” In Global Optimization, 155–210.\nSpringer.\n\n\nGreene, William H. 2018. Econometric Analysis. 8th ed. Pearson\nEducation.\n\n\nGriliches, Zvi, and William M Mason. 1972. “Education, Income, and\nAbility.” Journal of Political Economy 80 (3, Part 2):\nS74–103.\n\n\nHansen, Bruce. 2005. “Challenges for Econometric Model\nSelection.” Econometric Theory 21 (1): 60–68.\n\n\n———. 2022. Econometrics. Princeton University Press.\n\n\nHansen, Bruce E. 2022. Probability and Statistics for\nEconomists. Princeton University Press.\n\n\nHausman, Jerry A. 1978. “Specification Tests in\nEconometrics.” Econometrica: Journal of the Econometric\nSociety, 1251–71.\n\n\nHayashi, Fumio. 2011. Econometrics. Princeton University Press.\n\n\nHendry, David F et al. 1995. Dynamic Econometrics. Oxford\nUniversity Press on Demand.\n\n\nHendry, David F. 2000. Econometrics: Alchemy or Science?: Essays in\nEconometric Methodology. OUP Oxford.\n\n\nHuber, Peter J. 1964. “Robust Estimation of a Location\nParameter.” The Annals of Mathematical Statistics 35\n(1): 73–101. http://www.jstor.org/stable/2238020.\n\n\nJaynes, Edwin T. 1957. “Information Theory and Statistical\nMechanics.” Physical Review 106 (4): 620.\n\n\n———. 2003. Probability Theory: The Logic of Science. Cambridge\nuniversity press.\n\n\nJudd, Kenneth L. 1998. Numerical Methods in Economics. MIT\npress.\n\n\nKoopman, Bernard Osgood. 1936. “On Distributions Admitting a\nSufficient Statistic.” Transactions of the American\nMathematical Society 39 (3): 399–409.\n\n\nLeamer, Edward E. 1978. Specification Searches: Ad Hoc Inference\nwith Nonexperimental Data. Vol. 53. John Wiley & Sons\nIncorporated.\n\n\nLehmann. 1993. “The Fisher, Neyman-Pearson Theories of Testing\nHypotheses: One Theory or Two?” Journal of the American\nStatistical Association 88 (424): 1242–49.\n\n\n———. 2011. Fisher, Neyman, and the Creation of Classical\nStatistics. Springer Science & Business Media.\n\n\nLehmann, Erich L, and George Casella. 1998. Theory of Point\nEstimation. 2nd ed. Springer.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of\nIdentification in Econometrics.” Journal of Economic\nLiterature 57 (4): 835–903.\n\n\nLovell, Michael C. 1963. “Seasonal Adjustment of Economic Time\nSeries and Multiple Regression Analysis.” Journal of the\nAmerican Statistical Association 58 (304): 993–1010.\n\n\nMassart, Pascal. 1990. “The Tight Constant in the\nDvoretzky-Kiefer-Wolfowitz Inequality.” The Annals of\nProbability, 1269–83.\n\n\nMcCullagh, Peter. 2002. “What Is a Statistical Model?”\nThe Annals of Statistics 30 (5): 1225–1310.\n\n\nMizon, Grayham E. 1977. “Inferential Procedures in Nonlinear\nModels: An Application in a UK Industrial Cross Section Study of Factor\nSubstitution and Returns to Scale.” Econometrica: Journal of\nthe Econometric Society, 1221–42.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An\nIntroduction. MIT Press. probml.ai.\n\n\nNewey, Whitney K, and Daniel McFadden. 1994. “Large Sample\nEstimation and Hypothesis Testing.” Handbook of\nEconometrics 4: 2111–2245.\n\n\nNeyman, Jerzy, and Egon Sharpe Pearson. 1933. “On the Problem of\nthe Most Efficient Tests of Statistical Hypotheses.”\nPhilosophical Transactions of the Royal Society of London. Series A,\nContaining Papers of a Mathematical or Physical Character 231\n(694-706): 289–337.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of Inheritance in\nMan: I. Inheritance of Physical Characters.” Biometrika\n2 (4): 357–462.\n\n\nPhillips, Peter CB. 1996. “Econometric Model\nDetermination.” Econometrica: Journal of the Econometric\nSociety, 763–812.\n\n\nPitman, Edwin James George. 1936. “Sufficient Statistics and\nIntrinsic Accuracy.” In Mathematical Proceedings of the\nCambridge Philosophical Society, 32:567–79. 4. Cambridge University\nPress.\n\n\nRao, C Radhakrishna. 1948. “Large Sample Tests of Statistical\nHypotheses Concerning Several Parameters with Applications to Problems\nof Estimation.” In Mathematical Proceedings of the Cambridge\nPhilosophical Society, 44:50–57. 1. Cambridge University Press.\n\n\nReiss, Peter C, and Frank A Wolak. 2007. “Structural Econometric\nModeling: Rationales and Examples from Industrial Organization.”\nHandbook of Econometrics 6: 4277–4415.\n\n\nRomano, Joseph P, and EL Lehmann. 2005. Testing Statistical\nHypotheses. Vol. 3. Springer.\n\n\nRoyden, Halsey Lawrence, and Patrick Fitzpatrick. 1988. Real\nAnalysis. Vol. 32. Macmillan New York.\n\n\nRudin, Walter. 1976. Principles of Mathematical Analysis. Vol.\n3. McGraw-hill New York.\n\n\n———. 1987. Real and Complex Analysis. McGraw-hill New York.\n\n\nRuud, Paul A. 1984. “Tests of Specification in\nEconometrics.” Econometric Reviews 3 (2): 211–42.\n\n\nShannon, Claude Elwood. 1948. “A Mathematical Theory of\nCommunication.” The Bell System Technical Journal 27\n(3): 379–423.\n\n\nStock, James H, and Mark W Watson. 2003. Introduction to\nEconometrics. Vol. 104. Addison Wesley Boston.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3.\nCambridge university press.\n\n\nWald, Abraham. 1943. “Tests of Statistical Hypotheses Concerning\nSeveral Parameters When the Number of Observations Is Large.”\nTransactions of the American Mathematical Society 54 (3):\n426–82.\n\n\nWhite, Halbert. 1984. Asymptotic Theory for Econometricians.\nAcademic press.\n\n\nWolak, Frank A. 1989. “Local and Global Testing of Linear and\nNonlinear Inequality Constraints in Nonlinear Econometric\nModels.” Econometric Theory 5 (1): 1–35.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section\nand Panel Data. MIT press.\n\n\n———. 2015. Introductory Econometrics: A Modern Approach.\nCengage learning.\n\n\nWu, De-Min. 1973. “Alternative Tests of Independence Between\nStochastic Regressors and Disturbances.” Econometrica:\nJournal of the Econometric Society, 733–50."
  }
]